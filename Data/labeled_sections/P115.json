{
  "title": "An Examination of Expansive Multimodal Models:\nInsights from an Educational Overview",
  "abstract": "This document provides a summary of a presentation centered on extensive multi-\nmodal models, specifically their development to a level comparable to and poten-\ntially exceeding that of multimodal GPT-4. The exploration is divided into three\nsections. Initially, the context is established by discussing recent large-scale models\nakin to GPT, which are designed for vision and language processing. This sets the\nstage for exploring research in large multimodal models (LMMs) that are fine-tuned\nwith instructions. Subsequently, the foundational aspects of instruction tuning in\nlarge language models are covered, which is a method that is further adapted to\nthe multimodal domain. The final section demonstrates the creation of a basic\nversion of multimodal models similar to GPT-4 using publicly available resources.\nAdditionally, a review of newly developing areas in this field is presented.",
  "introduction": "With the widespread integration of advanced language models into modern society, there\u2019s a burgeon-\ning enthusiasm among scholars and scientists to create open-source large language models (LLMs)\nand to investigate their growth into large multimodal models (LMMs). This manuscript concentrates\non leveraging LLMs for multimodal applications and training LMMs in a comprehensive manner,\nenabling them to process visual data and engage in conversation.\n\n2 Background\n\n2.1\n\nImage-to-Text Generative Models\n\nIn their present configuration, LMMs predominantly function as image-to-text generators, accepting\nimages as input and producing textual content as output. The architectural design of these models\ngenerally includes an image encoder for deriving visual characteristics and a language model for\ngenerating textual sequences. These visual and linguistic components can be interconnected through\nan adaptable module. Both the image encoder and the language model have the flexibility to be\ndeveloped from the ground up or based on previously trained models.\n\nThe training methodology typically involves employing an auto-regressive loss on the generated text\ntokens. Within the Transformer framework, image tokens have the capability to interact with one\nanother, and each text token is influenced by the preceding text tokens and all image tokens.\n\n2.2 Case Studies\n\nWe will analyze several established LMMs to demonstrate how the architecture can be actualized\nacross various models while adhering to the same auto-regressive training principle.\n\n**Case Study I: LMM Trained with Image-Text Pairs**\n\nMany LMMs are developed using extensive collections of image-text pairs. Notable models like Gen-\nerative Image-to-Text Transformer (GIT) and Bootstrapping Language-Image Pre-training (BLIP2)\n\n.\n\n\fhave set high standards across various datasets. GIT utilizes an image encoder from a contrastive\npre-trained model and builds a language model independently. Conversely, BLIP2 maintains the\npre-trained image and language models in a fixed state while incorporating a trainable Querying\nTransformer (Q-former), demonstrating efficiency through a unique bootstrapping technique.\n\n**Case Study II: LMM Trained with Interleaved Image-Text Sequences**\n\nFlamingo serves as an exemplary model in this category, incorporating pre-trained image and language\nmodels with the addition of new integrative components. It includes a Perceiver Sampler to streamline\ncomputational demands and a Gated Transformer to enhance stability during the early training phase.\nFlamingo is trained on a diverse mix of large-scale multimodal data sourced exclusively from the web,\nbypassing the need for conventionally annotated machine learning datasets. Post-training, Flamingo\ncan adapt to vision-based tasks through few-shot learning without additional task-specific tuning.\n\nA standout feature of Flamingo is its capability for multimodal in-context learning. When presented\nwith image-text pairs as a demonstration, Flamingo can generalize to new, unseen tasks, such\nas visual math problems, without further training. It successfully interprets the patterns in task\ninstructions from examples and applies this understanding to new images. Flamingo represents\na significant advancement in multimodal learning, akin to the breakthroughs seen with GPT-3 in\nlanguage processing.\n\n2.3 OpenAI Multimodal GPT-4 and Research Gaps\n\nReleased in March 2023, OpenAI\u2019s GPT-4 showcases advanced capabilities in understanding and\nreasoning with visual data. Although specifics of the model remain undisclosed, its ability to facilitate\nnew applications is evident from highlighted examples in technical reports. For instance, it can\ndiscern unusual elements within images and demonstrate sophisticated reasoning across text and\nimages.\n\nThe inquiry into constructing models akin to Multimodal GPT-4 leads us to examine OpenAI\u2019s\nadvanced models, as depicted in Figure 7. Key observations are: (i) GPT-2 serves as the auto-\nregressive equivalent in the era dominated by BERT\u2019s pre-training then fine-tuning paradigm. (ii)\nGPT-3, a 175-billion parameter model trained on extensive web text, showcases emergent properties\nsuch as in-context learning and chain-of-thoughts (CoT) reasoning without requiring further training.\nThis model represents a shift from fine-tuning model weights to utilizing prompts for broader\ngeneralization and reduced adaptation costs. (iii) ChatGPT and InstructGPT emphasize the importance\nof models following instructions and aligning with human intentions by fine-tuning on high-quality\ninstruction data and using a reinforcement learning framework. (iv) GPT-4 not only enhances previous\nmodels\u2019 language capabilities but also incorporates visual inputs for comprehension and reasoning.\n\n3 Pre-requisite: Instruction Tuning in Large Language Models\n\nInstruction-following is a concept that originated in the field of natural language processing (NLP). To\nunderstand this concept more deeply and trace its development, we revisit the practice of instruction\ntuning in conjunction with LLMs.\n\n3.1\n\nInstruction Tuning\n\n**Traditional Language Data**\n\nIn the realm of natural language processing, the seq2seq format is frequently employed, where\neach data point comprises an input sequence and a corresponding output sequence. Typically, task\ninstructions are implicitly understood rather than explicitly stated. Models trained on this data format\noften struggle to adapt to new tasks in a zero-shot manner because they lack the ability to interpret\nand generalize task instructions during testing.\n\n**Instruct Language Data**\n\nRecent advancements involve the explicit incorporation of task instructions during model training.\nThese instructions, often articulated in natural language, lead to a structured format of instruction-\ninput-output triplets. This enables the training of a single model capable of handling multiple tasks\n\n2\n\n\fwith clear directives. The exposure to varied task instructions and examples during training allows\nthe model to generalize to novel tasks through task composition during inference.\n\n3.2 Self-Instruct and Open-Source LLMs\n\nThe collection of a wide array of high-quality instruction-following data can be achieved through\ntwo primary methods: human-human interaction and human-machine interaction. The former is\nresource-intensive, involving human task providers and annotators, while the latter involves machines\nor models performing the annotation tasks under human guidance.\n\nSelf-Instruct tuning represents a streamlined and potent method for aligning LLMs with human\nintent, utilizing instruction-following data produced by leading teacher LLMs. This technique,\nwhich leverages the in-context learning capability of LLMs, has significantly enhanced the zero- and\nfew-shot generalization abilities of LLMs. The iterative process, as illustrated in Figure 9, involves\nhumans providing initial examples, which the LLM then uses to generate further instructions and\nresponses, refining the dataset iteratively.\n\n4\n\nInstructed Tuned Large Multimodal Models\n\nThis section describes the development of a minimal multimodal GPT-4 model using open-source\ntools, with a focus on the LLaVA model, and a similar approach in the MiniGPT-4 project.\n\n4.1 Open-Source Prototypes: LLaVA / MiniGPT4\n\nInspired by successful concepts in NLP, we apply the self-instruct methodology from language\nprocessing to the vision-and-language domain. A significant challenge is the absence of a robust\nmultimodal teacher model. Thus, we explore how language-only models like GPT-4 can generate\nmultimodal instruction-following data.\n\n4.1.1 Data Creation\n\nInstead of directly inputting images into OpenAI GPT, symbolic sequence representations are used,\nas shown in Figure 12 (a). LLaVA utilizes captions and bounding boxes for several reasons: (1)\nGPT-4 is found to comprehend these representations effectively, unlike ChatGPT, which struggles\nwith bounding box data; (2) these elements are crucial for an informative representation of the image.\n\nAs demonstrated in Figure 12 (b), three forms of instruction-following data are used: multi-turn\nconversations for interactive user engagement, detailed descriptions for comprehensive response\ngeneration, and complex reasoning to address the implications beyond the image content.\n\n4.1.2 Network Architecture and Training\n\nAs shown in Figure 13, LLaVA\u2019s architecture is a specific implementation of the general image-to-text\ngenerative model framework discussed in Section 2 and Figure 3. LLaVA integrates a pre-trained\nCLIP ViT-L/14 visual encoder with the Vicuna large language model via a projection matrix. The\ntraining process involves two stages:\n\n- **Stage 1: Pre-training for Feature Alignment.** Only the projection matrix is updated using\na portion of the CC3M dataset, focusing solely on image captioning. - **Stage 2: End-to-End\nFine-tuning.** Both the projection matrix and the LLM are fine-tuned to cater to various application\nscenarios.\n\n4.1.3 Performance\n\n**Performance on Visual Chat**\n\nWhen fine-tuned on diverse multimodal instruction-following data, LLaVA demonstrates effectiveness\nin user-oriented applications. Empirical evidence suggests that adjusting only the linear projection\nlayer is adequate for conversational scenarios, although it necessitates longer training periods.\n\nIn an evaluation using 30 unseen images, each paired with three types of instructions, LLaVA achieved\nan 85.1\n\n3\n\n\f**Performance on Science QA**\n\nLLaVA, when fine-tuned on a scientific multimodal reasoning dataset, achieved a 90.92\n\n**Performance on OCR in the Wild**\n\nDespite not being explicitly trained on OCR data, LLaVA exhibits a surprising zero-shot OCR\ncapability, as illustrated in Figure 16.\n\nEmerging Topics\n\n4.1.4 More Modalities (Beyond VL)\n\n- **ChatBridge**: This model innovates by employing a Large Language Model as a linguistic\nmediator to connect different modalities [65]. - **PandaGPT**: A comprehensive model designed to\nadhere to instructions across various modalities [41]. - **SpeechGPT**: Enhances large language\n- **X-LLM**:\nmodels by incorporating inherent cross-modal conversational capabilities [61].\nAdvances large language models by conceptualizing multi-modalities as different languages [4].\n\nAlthough there is considerable diversity in the types of models, the fundamental concept of integrating\nmultiple modalities is consistent with the approach used in LMMs, which augment LLMs with visual\ncapabilities.\n\n4.1.5 Multitask Instruct with Established Academic Datasets/Tasks\n\n- **MultiInstruct**: This initiative aims to enhance zero-shot learning across various modalities\nby employing instruction tuning [57]. - **mPlug-OWL**: Utilizes modularization to enrich large\nlanguage models with multimodality, thereby improving their versatility [58]. - **InstructBLIP**:\nDevelops general-purpose vision-language models by incorporating instruction tuning, making them\nadaptable to a wide range of tasks [6]. - **Multimodal-GPT**: A model that integrates vision and\nlanguage to facilitate natural dialogues with users [13]. - **Instruction-ViT**: Introduces multi-\nmodal prompts to enhance instruction learning within the Vision Transformer (ViT) architecture\n[54].\n\nMultimodal In-Context-Learning\n\n- **OpenFlamingo**: An open-source initiative that replicates the Flamingo model by DeepMind,\ntrained on the extensive Multimodal C4 dataset, which includes images interleaved with text [2]. -\n**Otter**: This model stands out for its in-context instruction tuning capabilities, allowing it to adapt\nto new tasks based on the context provided in the instructions [18]. - **M3IT**: A comprehensive\ndataset designed for multi-modal multilingual instruction tuning, facilitating the development of\nmodels that can understand and generate content across different languages and modalities [22].\n- **MetaVL**: Focuses on transferring the in-context learning ability from language models to\nvision-language models, enabling them to perform tasks based on contextual examples without prior\ntraining [30].\n\nParameter-Efficient Training\n\n- **LLaMA-Adapter V2**: A parameter-efficient visual instruction model that demonstrates how\nto effectively adapt large language models for visual tasks with minimal parameter adjustments\n[10]. - **LAVIN**: Another parameter-efficient model that showcases efficient tuning strategies for\nvision-language tasks, emphasizing minimal computational resources [27]. - **QLoRA**: Introduces\na method for efficient fine-tuning of quantized LLMs, significantly reducing the memory footprint\nrequired for training large models [7].\n\n4.1.6 Benchmarks\n\n- **Hidden Mystery of OCR in Large Multimodal Models**: Investigates the unexpected proficiency\nof LMMs in optical character recognition (OCR) without explicit training in this area [25].\n-\n**Evaluating Object Hallucination**: Addresses the challenge of object hallucination in large\n-\nvision-language models, providing a framework for assessing and mitigating this issue [23].\n**Adversarial Robustness of Large Vision-Language Models**: Examines the resilience of LMMs\nagainst adversarial attacks, which is crucial for their deployment in security-sensitive applications\n[64]. - **LAMM**: Introduces a language-assisted multi-modal instruction-tuning dataset, along\n\n4\n\n\fwith a framework and benchmark for evaluating the performance of LMMs [59]. - **LVLM-eHub**:\nPresents a comprehensive evaluation benchmark for assessing the capabilities of large vision-language\nmodels across a variety of tasks [56].\n\n4.1.7 Applications\n\n- **PathAsst**: Reimagines the field of pathology by integrating a generative AI assistant, showcasing\nthe potential of LMMs in specialized domains [42]. - **PMC-VQA**: Focuses on visual instruction\ntuning for medical visual question answering, demonstrating the applicability of LMMs in healthcare\n[63]. - **LLaVA-Med**: A model trained to assist in biomedicine, highlighting the use of LMMs\nfor generating responses to open-ended research questions based on biomedical images [19].\n\n5 How Close Are We to Reaching or Surpassing OpenAI\u2019s Multimodal\n\nGPT-4?\n\nThe open-source community has rapidly produced a range of models and prototypes that introduce\na variety of new functionalities. For instance, LLaVA and Mini-GPT4 are leading the way in the\ncreation of multimodal chatbots, replicating some of the functions described in OpenAI\u2019s GPT-4\ntechnical documentation. Additionally, GILL has broadened the capabilities of LMMs to include\ncomprehensive image generation, a feature not currently present in GPT-4. From the standpoint of\nintroducing basic versions of new multimodal features, the open-source community is seemingly on\npar with OpenAI\u2019s Multimodal GPT-4, taking initial steps toward developing a versatile multimodal\nassistant.\n\nNevertheless, there remains a significant disparity when it comes to enhancing a particular func-\ntionality, such as the visual reasoning seen in LLaVA. The technical documentation from OpenAI\nprovides examples of complex visual tasks that necessitate models capable of processing numerous\nhigh-resolution images and extended sequences, in addition to delivering responses that require spe-\ncialized knowledge. This demands significantly greater computational power and more sophisticated\nlanguage models, which are generally not accessible to most individuals.",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "This paper has outlined the foundational aspects and advanced functionalities of large multimodal\nmodels (LMMs). It has revisited the concept of instruction tuning in large language models (LLMs)\nand demonstrated the steps to construct a basic model akin to LLaVA and MiniGPT4 with open-source\ntools. Furthermore, it has categorized and summarized the most recent advancements in this research\narea, offering a starting point for those keen to embark on LMM exploration.\n\nThe paper also proposes future directions for community-driven efforts. It suggests that entities with\nsubstantial resources should concentrate on scaling existing capabilities and exploring new emergent\nproperties. Meanwhile, others can focus on creating prototypes for new features, developing evalua-\ntion methods, and devising strategies to lower computational demands, thereby making advanced\nmodel computation more widely accessible.\n\nAcknowledgments\n\nWe express our gratitude to all the researchers who have contributed to the papers on LLMs and\nLMMs, which have been instrumental in the creation of this tutorial. While we aimed to cover the\nrelevant literature up to June 19, 2023, the rapid evolution of LMM research may mean that some\ncontributions have been unintentionally omitted. We apologize for any such oversights.\n\n5",
  "is_publishable": 1,
  "venue": NaN
}
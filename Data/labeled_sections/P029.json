{
  "title": "OpenOmni: An Open-Source Multimodal Systems",
  "abstract": "Multimodal conversational systems are increasingly sought after for their ability\nto facilitate natural and human-like interactions. However, comprehensive, col-\nlaborative development and benchmarking solutions remain scarce. Proprietary\nmodels like GPT-4o and Gemini have showcased impressive integration of audio,\nvisual, and textual data, achieving response times between 200-250 milliseconds.\nNonetheless, challenges persist in managing the trade-offs between latency, pre-\ncision, financial cost, and data confidentiality. To address these complexities, we\nintroduce OpenOmni, an open-source, end-to-end pipeline benchmarking platform.\nOpenOmni incorporates advanced technologies such as Speech-to-Text, Emotion\nDetection, Retrieval Augmented Generation, and Large Language Models, while\nalso offering the capability to integrate custom models. It supports both local and\ncloud deployment, thereby guaranteeing data privacy and providing latency and\naccuracy benchmarking capabilities. This adaptable architecture allows researchers\nto tailor the pipeline to pinpoint performance bottlenecks and expedite the de-\nvelopment of proof-of-concept solutions. OpenOmni holds significant potential\nto improve applications, including indoor assistance for individuals with visual\nimpairments, thereby advancing human-computer interaction.",
  "introduction": "Large Language Models (LLMs) have shown remarkable proficiency in interpreting user intent and\nadhering to instructions. However, text-based human-computer interaction (HCI) is often inadequate.\nThe recent introduction of models that process audio, video, and text in real-time highlights the\nprogress towards multimodal interaction. The impressive performance, characterized by response\ntimes of 200-250 milliseconds, makes these models suitable for large-scale applications. This marks\na trend towards multimodal generative models and applications. One of the early publicly available\nsolutions for multimodal large models that integrate text and images is available, but an open-source,\nend-to-end conversational agent implementation has not yet been made publicly accessible online.\n\nThe preferred mode of multimodal HCI should replicate human interaction, incorporating visual\nand auditory inputs alongside audio outputs. Despite the existence of various modular components,\na comprehensive, integrated, open-source implementation that fosters research and development\nin this domain is lacking. The integration of existing models, such as audio speech recognition\n(Speech2Text), multimodal large models (MLMs), and text-to-speech synthesis (TTS), into a mul-\ntimodal conversation framework reveals substantial difficulties in managing latency and ensuring\naccuracy. Traditionally, accuracy has posed a significant challenge. However, progress in large\nlanguage models (LLMs) has significantly enhanced contextual relevance. The primary challenge\nnow lies in minimizing end-to-end latency while maintaining high accuracy. Although it has been\nshown that this is feasible, the open-source community has not yet replicated these results.\n\nData privacy is another concern. The closed-source nature of certain solutions raises issues related to\ncost and data confidentiality. Since these models are not open-source, users are required to upload\ntheir data to servers via paid APIs, leading to privacy concerns. The privacy policy indicates that\nvarious types of personal information are collected when users create accounts to access services,\nsuch as account details, user-generated content, communication data, and social media information.\n\n\fTo facilitate the swift and responsible development of this new form of HCI, it is crucial to establish\nrobust evaluation and benchmarking protocols. For instance, if a user initiates a conversation with a\nsad and urgent tone, the system should respond appropriately and with patience. Evaluating these\ninteractions is both crucial and difficult for widespread adoption. This project aims to bridge these\ngaps by:\n\n\u2022 Creating an open-source framework to facilitate the development of customizable, end-to-\n\nend conversational agents.\n\n\u2022 Offering a fully local or controllable end-to-end multimodal conversation solution to address\n\nprivacy concerns.\n\n\u2022 Establishing tools for annotating and benchmarking latency and accuracy, allowing for rapid\n\nproof-of-concept development and research.\n\nTo accomplish this, we propose the OpenOmni framework, an open-source, end-to-end multimodal\npipeline that integrates advanced technologies such as Speech-to-Text (Speech2Text), Emotion\nDetection, Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and Text-to-\nSpeech (TTS). This framework collects video and audio data via cameras and microphones, processes\nthe data through a customizable agent pipeline, and responds using a speaker. OpenOmni can be\ndeployed on a local server, ensuring secure data management and addressing privacy concerns.\n\nFor research purposes, OpenOmni includes tools for straightforward annotation and benchmarking,\noffering real-time monitoring and performance evaluation of latency. Users can annotate individ-\nual components and entire conversations, generating comprehensive benchmark reports to identify\nbottlenecks. The open-source nature of OpenOmni allows for adaptation across various application\ndomains, such as aged care and personal assistants. Each pipeline component can be enabled or\ndisabled based on specific use cases, facilitating flexible and efficient deployment. Moreover, the\nframework supports the easy addition of new models, enabling comparisons and further experi-\nmentation. The OpenOmni framework allows researchers to focus on solving critical bottlenecks\nwithout reinventing the wheel, fostering innovation in multimodal conversational agents. It enables\nrapid proof-of-concept development, such as indoor conversational robots assisting visually impaired\nindividuals.",
  "related_work": "Traditional end-to-end multimodal conversation systems typically employ a divide-and-conquer\napproach, separating the process into sub-tasks: speech-to-text (automatic speech recognition), image-\nto-text, text generation, and text-to-speech. Speech-to-text transforms spoken language into written\ntext, while image-to-text produces textual descriptions of images. Text generation, often driven by\nlarge language models, generates contextually appropriate responses, and text-to-speech converts\nthese responses back into spoken form. These core components constitute the fundamental structure\nof the conversational pipeline. The inclusion of image-to-text provides essential context, enhancing\nnatural human-computer interaction, and additional functions like emotion detection adjust responses\nbased on the user\u2019s emotional state. An optional safeguard module can be integrated to guarantee that\nresponses are suitable, non-harmful, and controlled, maintaining interaction integrity, particularly in\ndelicate situations. Although this modular design enables the optimization of individual components,\nthe cumulative latency and accuracy errors can make the complete system impractical for real-world\nuse.\n\nWhile certain models are presented as fully end-to-end solutions, capable of handling video, audio, or\ntext inputs and producing audio, image, or text outputs, their technical specifics remain undisclosed.\nIt is postulated that audio and video frames are processed by modules that generate text, audio, and\nimage outputs. Demonstrations suggest that these models possess memory capabilities, though the\ndetails and limitations are not fully understood. Whether the system can directly incorporate external\nprivate data is also unknown.\n\nUnlike the divide-and-conquer method, a fully end-to-end neural network can integrate more contex-\ntual information, such as tone, the presence of multiple speakers, and background noises, leading to\nmore adaptable outputs. Theoretically, this method can decrease latency by removing orchestration\nbottlenecks. Nonetheless, both methods face substantial challenges because of the extensive data\ninput and output, especially from video. The large size of video files puts a strain on servers and\n\n2\n\n\fmodels, raising computational costs and introducing latency from data transfer and model inference.\nReal-time conversation necessitates streaming processing, posing additional latency challenges. It\nwas highlighted that a stable internet connection is needed to ensure smooth operation, underscoring\nthese challenges.\n\nA technology company has introduced a planned open-source, fully end-to-end multimodal conver-\nsational AI, which supports text and audio modalities but excludes images. This model claims to\nachieve an end-to-end latency of 200 milliseconds. Integrating video modality through an Image2Text\nmodule into this model is possible, creating a hybrid solution that combines divide-and-conquer\nand fully end-to-end approaches. Another viable hybrid solution involves using speech-to-text to\nconvert audio into text, then feeding this text along with video (processed into image sequences)\nto a vision language model, which generates text responses. These responses can subsequently be\nprocessed through text-to-speech. Multimodal end-to-end conversational agents show promise, yet\nlarge-scale implementation is challenging due to the need to balance latency, accuracy, and cost.\nGenerating real-time responses within 200-400 milliseconds is difficult. The primary objective is to\ndecrease latency and cost while enhancing accuracy, thereby improving the real-world applicability\nof conversational agents.\n\n2.1 Evaluation Metrics\n\nTo ensure productive and effective collaboration, it is crucial to have consistent and comparable\nevaluation metrics. For speech-to-text, the Word Error Rate (WER) is used to assess transcription\naccuracy, where a lower WER signifies better performance. Evaluating text-to-speech involves\nobjective metrics like the Mean Opinion Score (MOS) for naturalness and intelligibility, and the\nSignal-to-Noise Ratio (SNR) for clarity, along with subjective human ratings. Text generation is the\nmost difficult to evaluate, using metrics such as BLEU, ROUGE, and METEOR, which compare\ngenerated text to reference texts but may not completely capture the quality and relevance of responses.\nAssessing text generation often necessitates large-scale datasets, which are not always accessible.\nThese metrics are widely adopted by the research community. Nevertheless, real-world applications\nrequire evaluation in production environments, taking into account various factors beyond these\nmetrics. For instance, a conversational agent designed for aged care should steer clear of sensitive\ntopics that may be specific to each individual. Subjective opinions differ by region, emphasizing\nthe necessity for adaptable and innovative automatic or semi-automatic evaluation methods for\nconversational agents.\n\n3 System Design\n\n3.1 Requirement Analysis\n\nThe system is designed to accept audio and video inputs and produce audio as output. Initially, two\nmodules are required: one for gathering audio and video data from the microphone and camera, and\nanother for emitting audio through a speaker. These Client modules must be compatible with a variety\nof devices, such as smartphones, laptops, or Raspberry Pi. The data collected will be transmitted to a\nserver.\n\nThe server, known as the API, should handle audio and video data along with associated metadata.\nIt should have access to a storage layer that includes a relational database, file management, and a\ngraph database for potential GraphRAG integration. Although the API can be located on the same\ndevice as the Client module, it is preferable to keep them separate for enhanced adaptability. This\nseparation introduces the difficulty of transferring large volumes of data between modules. If the\nAPI is cloud-based, audio and video data must be uploaded to the cloud, for instance, using AWS\nS3, Azure Blob Storage, or Google Cloud Storage. However, the upload process can introduce a\nbottleneck, making data transfer time-intensive. If the server is local, within the same network as the\nClient, transfer latency will be reduced. Nevertheless, this configuration necessitates running the large\nlanguage model locally, which addresses data ownership and privacy issues but may increase model\ninference latency and reduce accuracy due to limited computational resources. Another approach is\nedge computing, where video data is pre-processed on edge devices and summarized for the API.\nAlthough this could be a research direction, data compression might result in information loss and\ndecrease overall performance.\n\n3\n\n\fThe pipeline components will require adjustments if developers intend to adopt the framework and\nintegrate it with their work. To maintain flexibility, this part should be an independent module capable\nof running locally or in the cloud. Researchers and developers should be able to easily incorporate\nnew components into this Agent module, further complicating the sharing of large datasets between\nmodules.\n\nFinally, benchmarks are needed to comprehend the latency and accuracy performance of the entire\npipeline. For tasks that are challenging to evaluate automatically, such as assessing the appropriateness\nof the LLM response, we propose and develop an annotation module to allow human annotators to\neasily evaluate results and generate benchmark reports.\n\n3.2 System Architecture\n\nBased on these requirements, the system architecture was designed as depicted in Figure 1. The\nsystem is divided into five modules: Client, API, Storage, User Interface, and Agent, all primarily\ndeveloped in Python. The Client module includes two submodules: the Listener for collecting video\nand audio data, and the Responder for playing audio. The Storage module consists of file storage for\nmedia, a relational database (PostgreSQL) for metadata, and a graph database (Neo4j) for potential\nGraphRAG integration. The API module, built with the Django framework, extends Django\u2019s admin\ninterface and permission control system to develop the benchmark and annotation interface. Django\u2019s\nmaturity and large support community make it ideal for production development. The Agent module,\nalso in Python, includes all agent-related submodules, allowing deployment on suitable compute\nnodes without altering the architecture. Communication between the Client, API, and Agent modules\nwill be via RESTful endpoints. For sharing large data between modules, local deployments (e.g.,\nClient on Raspberry Pi, API and Agent on local servers) will use FTP for file synchronization. In\ncloud solutions (e.g., AWS), files will be uploaded to AWS S3, triggering a Lambda function to\ndownload files to an AWS Elastic File Storage (EFS) shared by the API and Agent modules. Docker\nand Docker Compose are used to manage all modules, allowing easy setup with a single docker\ncompose up command.\n\n4 Demonstration\n\n4.1 Datasets\n\nMost multimodal question-answering datasets concentrate on multiple-choice questions rather than\nopen-ended conversations. Some datasets involve multimodal conversations with images as additional\ninput, but the output is often limited to multiple-choice or text. A significant challenge in developing\nmultimodal conversational agents is the scarcity of suitable datasets.\n\nAlthough there is an abundance of data from human-human interactions or data extracted from movies\nand YouTube videos, efficient methods to organize this data into structured datasets are lacking. For\nspecific domain applications, collecting data from human interactions and extracting datasets to train\nsystems would be advantageous, enabling the agents to mimic human behavior. The OpenOmni\nFramework offers both capabilities: extracting conversational datasets from videos and testing them\nthrough the pipeline to assess agents\u2019 responses, or gathering data from real-world scenarios to create\ndatasets for further research.\n\n4.2 Can \"AI\" be your president?\n\nOne intensive conversational scenario is a debate. Segments were extracted from a US Presidential\nDebate, focusing on a candidate addressing the public and handling questions. After downloading\nthe videos, a prepared script in our codebase can be used to split them into segments. This script\nallows for the specification of the start and end times of each conversation, enabling the creation\nof a conversational dataset from the videos. These segments were fed into our pipeline to evaluate\nits performance under different configurations: one using a commercial speech-to-text model, a\nvision model, and text-to-speech (Configuration A); a locally deployed quantization LLM with a\nspeech-to-text model, text-to-speech, and our emotion detection model for video input (Configuration\nB); a version using a different LLM for inference (Configuration C); and a version using only a speech-\nto-text model, a language model, and text-to-speech, ignoring the video modality (Configuration D).\nThe Agent modules were run on a specific GPU with 12GB memory.\n\n4\n\n\fThe latency benchmark statistics are automatically generated. For example, Configuration A has\nan average latency of 45 seconds, with the vision model accounting for 31 seconds. The fastest\nconfiguration is Configuration D, averaging around 15 seconds, with most of the time consumed\nby the text-to-speech part, because the generated content is quite long and comprehensive. The\nslowest configuration is Configuration C, taking around 189 seconds, with the LLM model inference\nstep taking the longest time. Configuration B takes an average of 60 seconds, with the LLM model\ninference averaging 28 seconds and our emotion detection model averaging around 10 seconds.\n\nTable 1: Accuracy: Overall Conversation Quality\n\nTRACK ID USER ID OVERALL COMMENT\n\nOVERALL SCORE\n\nf1\nf2\nf3\nf4\nf5\n\n1\n2\n1\n1\n1\n\nAs the question is quite subjective, the answer is good and in context\nThe answer is quite general, while the candidate is doing much better work with supported evidence.\nFailed to generate proper in-context response; the response is talking about how to respond, not actually responses\nGenerate some general comments without strong support evidence\nGeneral response, however, no good evidence to support.\n\n4\n\n2\n\n2\n\n2\n\n3\n\nAfter annotation with our interface, accuracy statistics are automatically generated. The accuracy\nmetrics here include evaluation metrics like WER, CER for the speech-to-text task, and overall\nscores given by the annotators. As shown in Table 1, the average score for each conversation is 2.4.\nText-to-speech can be improved with more natural emotion or personality. The generated content\nis often too general and sometimes inappropriate. The candidate\u2019s responses are more in-context\nand evidence-supported. The pipeline excelled only in answering a subjective question about the\ncandidate\u2019s age, where Configuration A performed well. Configuration D had the best overall\naccuracy, but its responses were often in-context yet pompous. Thus, the candidate still outperforms\nAI. In conclusion, \"AI cannot be the President of the US just yet, considering both latency and\naccuracy.\"\n\n4.3 Assist the Visually Impaired\n\nWhile latency and the need for external information currently prevent AI from undertaking mission-\ncritical tasks, conversational agents can be production-ready and useful for non-latency-critical areas\nthat do not require extensive external knowledge. Assisting indoor activities for the visually impaired\nis one such application, where high-speed internet can be utilized, or data transfer can be limited to\nlocal exchanges. These types of applications can benefit from maintaining high input/output rates,\nhelping to mitigate latency issues. Questions were prepared for the visually impaired, including\nlocating objects, navigating indoors, and inquiries about the surroundings. Six questions were\nsampled and fed to the Configuration A pipeline. One scenario demonstration is included in our\nprovided video. In this scenario, video and audio data stream from the client side and are saved to\nstorage along with exportable metadata accessible via the admin portal. This setup allows for the\nexportation of annotated datasets, including raw video and audio data, for developing new models.\nThe latency statistics show responses within approximately 30 seconds.\n\nAnnotated results show a 4.7/5 accuracy, but the agent lacks specific skills for assisting the visually\nimpaired. For example, ideally, it should provide step-by-step instructions on grabbing a coffee\ncup rather than just a general description. This indicates that while conversational agents are nearly\nready for assisting the visually impaired with indoor activities, improvements in latency and response\nquality are still needed.",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "Multimodal conversational agents offer a more natural form of human-computer interaction, as\ndemonstrated by models like GPT-4o. However, real-world constraints require a balance between\ncost, latency, and accuracy, which may explain why the full capabilities of such models are not yet\naccessible.\n\nSeveral technical options exist to achieve this balance, including traditional divide-and-conquer\nmethods, fully end-to-end models, and hybrid approaches. The fully end-to-end approach inherently\nallows for lower latency, while the divide-and-conquer method faces latency issues when coordinating\n\n5\n\n\fmultiple components. Both approaches must address the challenge of handling large data I/O. If\nmodels are deployed locally, local network I/O issues can be more manageable. However, some\nmodels are closed-source, making local deployment impractical. While deploying other vision models\nlocally is feasible, achieving high accuracy may be limited by local computational resources. Hybrid\nsolutions provide alternative approaches: pre-processing or compressing large data locally and then\nutilizing cloud-based models, or converting video to text and integrating it into the end-to-end voice\nmodel.\n\nWe developed the OpenOmni framework to enable researchers to integrate their work into an end-to-\nend pipeline. The framework supports various solutions, allows for pipeline customization, generates\nlatency performance reports, and provides an annotation interface for accuracy review. These features\nfacilitate the creation of benchmark reports to identify and address key issues.\n\nTesting with the US Presidential debate scenario highlighted latency as a critical issue, particularly\nwith large video data. Integrating external knowledge remains a challenge, emphasizing the need\nfor efficient Retrieval-Augmented Generation (RAG). For applications like indoor assistance for the\nvisually impaired, latency improvements and model adaptation are both essential.\n\nThe OpenOmni framework can significantly benefit the research community by facilitating the\ncollection and management of new datasets, integrating various conversational agents approaches,\nand generating automatic latency benchmarks. Its annotation interface aids in accuracy performance\nreview, making OpenOmni production-ready for suitable application scenarios and fostering further\ndevelopment in multimodal conversational agents.\n\n6",
  "is_publishable": 1,
  "venue": NaN
}
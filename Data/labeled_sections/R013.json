{
  "title": "Generalization in ReLU Networks via Restricted\nIsometry and Norm Concentration",
  "abstract": "are often constrained by limited training data. Nevertheless, if the hypothesis func-\ntions can be represented effectively by the data, there is potential for identifying a\nmodel that generalizes well. This paper introduces the Neural Restricted Isometry\nProperty (NeuRIPs), which acts as a uniform concentration event that ensures all\nshallow ReLU networks are sketched with comparable quality. To determine the\nsample complexity necessary to achieve NeuRIPs, we bound the covering numbers\nof the networks using the Sub-Gaussian metric and apply chaining techniques. As-\nsuming the NeuRIPs event, we then provide bounds on the expected risk, applicable\nto networks within any sublevel set of the empirical risk. Our results show that all\nnetworks with sufficiently small empirical risk achieve uniform generalization.",
  "introduction": "years, supervised machine learning has seen the development of tools for automated model discovery\nfrom training data. However, these methods often lack a robust theoretical framework to estimate\nmodel limitations. Statistical learning theory quantifies the limitation of a trained model by the\ngeneralization error. This theory uses concepts such as the VC-dimension and Rademacher complexity\nto analyze generalization error bounds for classification problems. While these traditional complexity\nnotions have been successful in classification problems, they do not apply to generic regression\nproblems with unbounded risk functions, which are the focus of this study. Moreover, traditional\ntools in statistical learning theory have not been able to provide a fully satisfying generalization\ntheory for neural networks.\n\nUnderstanding the risk surface during neural network training is crucial for establishing a strong\ntheoretical foundation for neural network-based machine learning, particularly for understanding\ngeneralization. Recent studies on neural networks suggest intriguing properties of the risk surface.\nIn large networks, local minima of the risk form a small bond at the global minimum. Surprisingly,\nglobal minima exist in each connected component of the risk\u2019s sublevel set and are path-connected.\nIn this work, we contribute to a generalization theory for shallow ReLU networks, by giving uniform\ngeneralization error bounds within the empirical risk\u2019s sublevel set. We use methods from the analysis\nof convex linear regression, where generalization bounds for empirical risk minimizers are derived\nfrom recent advancements in stochastic processes\u2019 chaining theory. Empirical risk minimization\nfor non-convex hypothesis functions cannot generally be solved efficiently. However, under certain\nassumptions, it is still possible to derive generalization error bounds, as we demonstrate in this paper\nfor shallow ReLU networks. Existing works have applied methods from compressed sensing to\nbound generalization errors for arbitrary hypothesis functions. However, they do not capture the\nrisk\u2019s stochastic nature through the more advanced chaining theory.\n\nThis paper is organized as follows. We begin in Section II by outlining our assumptions about the\nparameters of shallow ReLU networks and the data distribution to be interpolated. The expected and\nempirical risk are introduced in Section III, where we define the Neural Restricted Isometry Property\n\n.\n\n\f(NeuRIPs) as a uniform norm concentration event. We present a bound on the sample complexity for\nachieving NeuRIPs in Theorem 1, which depends on both the network architecture and parameter\nassumptions. We provide upper bounds on the generalization error that are uniformly applicable\nacross the sublevel sets of the empirical risk in Section IV. We prove this property in a network\nrecovery setting in Theorem 2, and also an agnostic learning setting in Theorem 3. These results\nensure a small generalization error, when any optimization algorithm finds a network with a small\nempirical risk. We develop the key proof techniques for deriving the sample complexity of achieving\nNeuRIPs in Section V, by using the chaining theory of stochastic processes. The derived results are\nsummarized in Section VI, where we also explore potential future research directions.\n\n2 Notation and Assumptions\n\nIn this section, we will define the key notations and assumptions for the neural networks examined\nin this study. A Rectified Linear Unit (ReLU) function \u03d5 : R \u2192 R is given by \u03d5(x) := max(x, 0).\nGiven a weight vector w \u2208 Rd, a bias b \u2208 R, and a sign \u03ba \u2208 {\u00b11}, a ReLU neuron is a function\n\u03d5(w, b, \u03ba) : Rd \u2192 R defined as\n\n\u03d5(w, b, \u03ba)(x) = \u03ba\u03d5(wT x + b).\n\nShallow neural networks are constructed as weighted sums of neurons. Typically they are represented\nby a graph with n neurons in a single hidden layer. When using the ReLU activation function, we can\napply a symmetry procedure to represent these as sums:\n\n\u00af\u03d5 \u00afp(x) =\n\nn\n(cid:88)\n\ni=0\n\n\u03d5pi (x),\n\nwhere \u00afp is the tuple (p1, . . . , pn).\n\nAssumption 1. The parameters \u00afp, which index shallow ReLU networks, are drawn from a set\n\n\u00afP \u2286 (Rd \u00d7 R \u00d7 {\u00b11})n.\nFor \u00afP , we assume there exist constants cw \u2265 0 and cb \u2208 [1, 3], such that for all parameter tuples\n\u00afp = {(w1, b1, \u03ba1), . . . , (wn, bn, \u03ban)} \u2208 \u00afP , we have\n\u2225wi\u2225 \u2264 cw and\n\n|bi| \u2264 cb.\n\nWe denote the set of shallow networks indexed by a parameter set \u00afP by\n\n\u03a6 \u00afP := {\u03d5 \u00afp : \u00afp \u2208 \u00afP }.\n\nWe now equip the input space Rd of the networks with a probability distribution. This distribution\nreflects the sampling process and makes each neural network a random variable. Additionally, a\nrandom label y takes its values in the output space R, for which we assume the following.\nAssumption 2. The random sample x \u2208 Rd and label y \u2208 R follow a joint distribution \u00b5 such that\nthe marginal distribution \u00b5x of sample x is standard Gaussian with density\n\n1\n(2\u03c0)d/2\n\n(cid:18)\n\nexp\n\n\u2212\n\n(cid:19)\n\n.\n\n\u2225x\u22252\n2\n\nAs available data, we assume independent copies {(xj, yj)}m\ndistributed by \u00b5.\n\nj=1 of the random pair (x, y), each\n\n3 Concentration of the Empirical Norm\n\nSupervised learning algorithms interpolate labels y for samples x, both distributed jointly by \u00b5 on\nX \u00d7 Y. This task is often solved under limited data accessibility. The training data, respecting\nAssumption 2, consists of m independent copies of the random pair (x, y). During training, the\ninterpolation quality of a hypothesis function f : X \u2192 Y can only be assessed at the given random\nsamples {xj}m\n\nj=1. Any algorithm therefore accesses each function f through its sketch samples\n\nS[f ] = (f (x1), . . . , f (xm)),\n\n2\n\n\fwhere S is the sample operator. After training, the quality of a resulting model is often measured by\nits generalization to new data not used during training. With Rd \u00d7 R as the input and output space,\nwe quantify a function f \u2019s generalization error with its expected risk:\n\nE\u00b5[f ] := E\u00b5|y \u2212 f (x)|2.\n\nThe functional || \u00b7 ||\u00b5, also gives the norm of the space L2(Rd, \u00b5x), which consists of functions\nf : Rd \u2192 R with\n\n\u2225f \u22252\n\n\u00b5 := E\u00b5x [|f (x)|2].\n\nIf the label y depends deterministically on the associated sample x, we can treat y as an element of\nL2(Rd, \u00b5x), and the expected risk of any function f is the function\u2019s distance to y. By sketching any\nhypothesis function f with the sample operator S, we perform a Monte-Carlo approximation of the\nexpected risk, which is termed the empirical risk:\n\n\u2225f \u22252\n\nm :=\n\n1\nm\n\nm\n(cid:88)\n\nj=1\n\n(f (xj) \u2212 yj)2 =\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\n1\n\u221a\nm\n\n(y1, . . . , ym)T \u2212 S[f ]\n\n(cid:13)\n2\n(cid:13)\n(cid:13)\n(cid:13)\n2\n\n.\n\nThe random functional || \u00b7 ||m also defines a seminorm on L2(Rd, \u00b5x), referred to as the empirical\nnorm. Under mild assumptions, || \u00b7 ||m fails to be a norm.\n\nIn order to obtain a well generalizing model, the goal is to identify a function f with a low expected\nrisk. However, with limited data, we are restricted to optimizing the empirical risk. Our strategy for\nderiving generalization guarantees is based on the stochastic relation between both risks. If {xj}m\nj=1\nare independently distributed by \u00b5x, the law of large numbers implies that for any f \u2208 L2(Rd, \u00b5x)\nthe convergence\n\nlim\nm\u2192\u221e\n\n\u2225f \u2225m = \u2225f \u2225\u00b5.\n\nWhile this establishes the asymptotic convergence of the empirical norm to the function norm for a\nsingle function f , we have to consider two issues to formulate our concept of norm concentration:\nFirst, we need non-asymptotic results, that is bounds on the distance |\u2225f \u2225m \u2212 \u2225f \u2225\u00b5| for a fixed\nnumber of samples m. Second, the bounds on the distance need to be uniformly valid for all functions\nf in a given set.\n\nSample operators which have uniform concentration properties have been studied as restricted\nisometries in the area of compressed sensing. For shallow ReLU networks of the form (1), we define\nthe restricted isometry property of the sampling operator S as follows.\nDefinition 1. Let s \u2208 (0, 1) be a constant and \u00afP be a parameter set. We say that the Neural Restricted\nIsometry Property (NeuRIPs( \u00afP )) is satisfied if, for all \u00afp \u2208 \u00afP it holds that\n\n(1 \u2212 s)\u2225\u03d5 \u00afp\u2225\u00b5 \u2264 \u2225\u03d5 \u00afp\u2225m \u2264 (1 + s)\u2225\u03d5 \u00afp\u2225\u00b5.\n\nIn the following Theorem, we provide a bound on the number m of samples, which is sufficient for\nthe operator S to satisfy NeuRIPs( \u00afP ).\nTheorem 1. There exist universal constants C1, C2 \u2208 R such that the following holds: For\nany sample operator S, constructed from random samples {xj}, respecting Assumption 2, let\n\u00afP \u2282 (Rd \u00d7 R \u00d7 {\u00b11})n be any parameter set satisfying Assumption 1 and ||\u03d5 \u00afp||\u00b5 > 1 for all\n\u00afp \u2208 \u00afP . Then, for any u > 2 and s \u2208 (0, 1), NeuRIPs( \u00afP ) is satisfied with probability at least\n1 \u2212 17 exp(\u2212u/4) provided that\n\nm \u2265\n\nn3c2\nw\n(1 \u2212 s)2 max\n\n(cid:18)\n\nC1\n\n(8cb + d + ln(2))\nu\n\n, C2\n\nn2c2\nw\n(u/s)2\n\n(cid:19)\n\n.\n\nOne should notice that, in Theorem 1, there is a tradeoff between the parameter s, which limits the\ndeviation | \u2225 \u00b7 \u2225m \u2212 \u2225 \u00b7 \u2225\u00b5|, and the confidence parameter u. The lower bound on the corresponding\nsample size m is split into two scaling regimes when understanding the quotient u of |\u2225\u00b7\u2225m \u2212\u2225\u00b7\u2225\u00b5|/s\nas a precision parameter. While in the regime of low deviations and high probabilities the sample size\nm must scale quadratically with u/s, in the regime of less precise statements one observes a linear\nscaling.\n\n3\n\n\f4 Uniform Generalization of Sublevel Sets of the Empirical Risk\n\nWhen the NeuRIPs event occurs, the function norm || \u00b7 ||\u00b5, which is related to the expected risk, is\nclose to || \u00b7 ||m, which corresponds to the empirical risk. Motivated by this property, we aim to find\na shallow ReLU network \u03d5 \u00afp with small expected risk by solving the empirical risk minimization\nproblem:\n\n\u2225\u03d5 \u00afp \u2212 y\u22252\n\nm.\n\nmin\n\u00afp\u2208 \u00afP\n\nSince the set \u03a6 \u00afP of shallow ReLU networks is non-convex, this minimization cannot be solved\nwith efficient convex optimizers. Therefore, instead of analyzing only the solution \u03d5\u2217\n\u00afp of the opti-\nmization problem, we introduce a tolerance \u03f5 > 0 for the empirical risk and provide bounds on the\ngeneralization error, which hold uniformly on the sublevel set\n\u00afQy,\u03f5 := (cid:8)\u00afp \u2208 \u00afP : \u2225\u03d5 \u00afp \u2212 y\u22252\n\nm \u2264 \u03f5(cid:9) .\n\nBefore considering generic regression problems, we will initially assume the label y to be a neural\nnetwork itself, parameterized by a tuple p\u2217 within the hypothesis set P . For all (x, y) in the support of\n\u00b5, we have y = \u03d5p\u2217 (x) and the expected risk\u2019s minimum on P is zero. Using the sufficient condition\nfor NeuRIPs from Theorem 1, we can provide generalization bounds for \u03d5 \u00afp \u2208 \u00afQy,\u03f5 for any \u03f5 > 0.\nTheorem 2. Let \u00afP be a parameter set that satisfies Assumption 1 and let u \u2265 2 and t \u2265 \u03f5 > 0 be\nconstants. Furthermore, let the number m of samples satisfy\n\n(cid:18)\n\nm \u2265 8n3c2\n\nw (8cb + d + ln(2)) max\n\nu\n(t \u2212 \u03f5)2 , C2\nwhere C1 and C2 are universal constants. Let {(xj, yj)}m\nj=1 be a dataset respecting Assumption 2\nand let there exist a \u00afp\u2217 \u2208 \u00afP such that yj = \u03d5 \u00afp\u2217 (xj) holds for all j \u2208 [m]. Then, with probability at\nleast 1 \u2212 17 exp(\u2212u/4), we have for all \u00afq \u2208 \u00afQy,\u03f5 that\n\u2225\u03d5\u00afq \u2212 \u03d5 \u00afp\u2217 \u22252\n\nn2c2\nwu\n(t \u2212 \u03f5)2\n\nC1\n\n,\n\n\u00b5 \u2264 t.\n\n(cid:19)\n\nProof. We notice that \u00afQy,\u03f5 is a set of shallow neural networks with 2n neurons. We normalize such\nnetworks with a function norm greater than t and parameterize them by\n\u00afRt := {\u03d5 \u00afp \u2212 \u03d5 \u00afp\u2217 : \u00afp \u2208 \u00afP , \u2225\u03d5 \u00afp \u2212 \u03d5 \u00afp\u2217 \u2225\u00b5 > t}.\nWe assume that NeuRIPs( \u00afRt) holds for s = (t \u2212 \u03f5)2/t2. In this case, for all \u00afq \u2208 \u00afQy,\u03f5, we have that\n\u2225\u03d5\u00afq \u2212 \u03d5 \u00afp\u2217 \u2225m \u2265 t and thus \u00afq /\u2208 \u00afQ\u03d5 \u00afp\u2217 ,\u03f5, which implies that \u2225\u03d5\u00afq \u2212 \u03d5 \u00afp\u2217 \u2225\u00b5 \u2264 t.\nWe also note that \u00afRt satisfies Assumption 1 with a rescaled constant cw/t and normalization-invariant\ncb, if \u00afP satisfies it for cw and cb. Theorem 1 gives a lower bound on the sample complexity for\nNeuRIPs( \u00afRt), completing the proof.\nAt any network where an optimization method terminates, the concentration of the empirical risk\nat the expected risk can be achieved with less data than needed to achieve an analogous NeuRIPs\nevent. However, in the chosen stochastic setting, we cannot assume that the termination of an\noptimization and the norm concentration at that network are independent events. We overcome this\nby not specifying the outcome of an optimization method and instead stating uniform bounds on\nthe norm concentration. The only assumption on an algorithm is therefore the identification of a\nnetwork that permits an upper bound \u03f5 on its empirical risk. The event NeuRIPs( \u00afRt) then restricts the\nexpected risk to be below the corresponding level t.\n\nWe now discuss the empirical risk surface for generic distributions \u00b5 that satisfy Assumption 2, where\ny does not necessarily have to be a neural network.\nTheorem 3. There exist constants C0, C1, C2, C3, C4, and C5 such that the following holds: Let \u00afP\nsatisfy Assumption 1 for some constants cw, cb, and let \u00afp\u2217 \u2208 \u00afP be such that for some c \u00afp\u2217 \u2265 0 we\nhave\n(cid:18) (y \u2212 \u03d5 \u00afp\u2217 (x))2\nc2\n\u00afp\u2217\nWe assume, for any s \u2208 (0, 1) and confidence parameter u > 0, that the number of samples m is\nlarge enough such that\n\n\u2264 2.\n\nexp\n\nE\u00b5\n\n(cid:19)(cid:21)\n\n(cid:20)\n\nm \u2265\n\n8\n\n(1 \u2212 s)2 max\n\n(cid:18)\n\nC1\n\n(cid:18) n3c2\n\nw(8cb + d + ln(2))\n\n(cid:19)\n\nu\n\n4\n\n, C2n2c2\nw\n\n(cid:17)(cid:19)\n\n.\n\n(cid:16) u\ns\n\n\fWe further select confidence parameters v1, v2 > C0, and define for some \u03c9 \u2265 0 the parameter\n\n\u03b7 := 2(1 \u2212 s)\u2225\u03d5 \u00afp\u2217 \u2212 y\u2225\u00b5 + C3v1v2c \u00afp\u2217\n\n1\n(1 \u2212 s)1/4\n\n\u221a\n\n+ \u03c9\n\n1 \u2212 s.\n\nIf we set \u03f5 = \u2225\u03d5 \u00afp\u2217 \u2212 y\u22252\n\u00afq \u2208 \u00afQy,\u03f5 satisfy\n\nm + \u03c92 as the tolerance for the empirical risk, then the probability that all\n\nis at least\n\n\u2225\u03d5\u00afq \u2212 y\u2225\u00b5 \u2264 \u03b7\n\n1 \u2212 17 exp\n\n(cid:16)\n\n\u2212\n\n(cid:17)\n\nu\n4\n\n(cid:18)\n\n\u2212 C5v2 exp\n\n\u2212\n\nC4mv2\n2\n2\n\n(cid:19)\n\n.\n\nProof sketch. (Complete proof in Appendix E) We first define and decompose the excess risk by\n\nE(\u00afq, \u00afp\u2217) := \u2225\u03d5\u00afq \u2212 y\u22252\n\n\u00b5 \u2212 \u2225\u03d5 \u00afp\u2217 \u2212 y\u22252\n\n\u00b5 = \u2225\u03d5\u00afq \u2212 \u03d5 \u00afp\u2217 \u22252\n\n\u00b5 \u2212\n\n2\nm\n\nm\n(cid:88)\n\nj=1\n\n(\u03d5 \u00afp\u2217 (xj) \u2212 yj)(\u03d5\u00afq(xj) \u2212 \u03d5 \u00afp\u2217 (xj)).\n\nIt suffices to show, that within the stated confidence level we have \u2225\u03d5\u00afq \u2212 y\u2225\u00b5 > \u03b7 . This implies the\nclaim since \u2225\u03d5\u00afq \u2212 y\u2225m \u2264 \u03f5 implies \u2225\u03d5\u00afq \u2212 y\u2225\u00b5 \u2264 \u03b7. We have E[E(\u00afq, \u00afp\u2217)] > 0. It now only remains\nto strengthen the condition on \u03b7 > 3\u2225\u03d5 \u00afp\u2217 \u2212 y\u2225\u00b5 to achieve E(\u00afq, \u00afp\u2217) > \u03c92. We apply Theorem 1\nto derive a bound on the fluctuation of the first term. The concentration rate of the second term is\nderived similar to Theorem 1 by using chaining techniques. Finally in Appendix E, Theorem 12 gives\na general bound to achieve\n\nE(\u00afq, \u00afp\u2217) > \u03c92\n\nuniformly for all \u00afq with \u2225\u03d5\u00afq \u2212 \u03d5 \u00afp\u2217 \u2225\u00b5 > \u03b7. Theorem 3 then follows as a simplification.\n\nIt is important to notice that, in Theorem 3, as the data size m approaches infinity, one can select\nan asymptotically small deviation constant s. In this limit, the bound \u03b7 on the generalization error\nconverges to 3\u2225\u03d5 \u00afp\u2217 \u2212 y\u2225\u00b5 + \u03c9. This reflects a lower limit of the generalization bound, which is the\nsum of the theoretically achievable minimum of the expected risk and the additional tolerance \u03c9.\nThe latter is an upper bound on the empirical risk, which real-world optimization algorithms can be\nexpected to achieve.\n\n5 Size Control of Stochastic Processes on Shallow Networks\n\nIn this section, we introduce the key techniques for deriving concentration statements for the em-\npirical norm, uniformly valid for sets of shallow ReLU networks. We begin by rewriting the event\nNeuRIPs( \u00afP ) by treating \u00b5 as a stochastic process, indexed by the parameter set \u00afP . The event\nNeuRIPs( \u00afP ) holds if and only if we have\n\nsup\n\u00afp\u2208 \u00afP\n\n|\u2225\u03d5 \u00afp\u2225m \u2212 \u2225\u03d5 \u00afp\u2225\u00b5| \u2264 s sup\n\u00afp\u2208 \u00afP\n\n\u2225\u03d5 \u00afp\u2225\u00b5.\n\nThe supremum of stochastic processes has been studied in terms of their size. To determine the size\nof a process, it is essential to determine the correlation between its variables. To this end, we define\nthe Sub-Gaussian metric for any parameter tuples \u00afp, \u00afq \u2208 \u00afP as\n\n(cid:40)\n\n(cid:34)\n\n(cid:32)\n\nd\u03c82(\u03d5 \u00afp, \u03d5\u00afq) := inf\n\nC\u03c82 \u2265 0 : E\n\nexp\n\n|\u03d5 \u00afp(x) \u2212 \u03d5\u00afq(x)|2\nC 2\n\u03c82\n\n(cid:33)(cid:35)\n\n(cid:41)\n\n\u2264 2\n\n.\n\nA small Sub-Gaussian metric between random variables indicates that their values are likely to be\nclose. To capture the Sub-Gaussian structure of a process, we introduce \u03f5-nets in the Sub-Gaussian\nmetric. For a given \u03f5 > 0, these are subsets \u00afQ \u2286 \u00afP such that for every \u00afp \u2208 \u00afP , there is a \u00afq \u2208 \u00afQ\nsatisfying\n\nd\u03c82 (\u03d5 \u00afp, \u03d5\u00afq) \u2264 \u03f5.\nThe smallest cardinality of such an \u03f5-net \u00afQ is known as the Sub-Gaussian covering number\nN (\u03a6 \u00afP , d\u03c82, \u03f5). The next Lemma offers a bound for such covering numbers specific to shallow\nReLU networks.\n\n5\n\n\fLemma 1. Let \u00afP be a parameter set satisfying Assumption 1. Then there exists a set \u02c6P with \u00afP \u2286 \u02c6P\nsuch that\n\nN (\u03a6 \u02c6P , d\u03c82, \u03f5) \u2264 2n \u00b7\n\n(cid:18) 16ncbcw\n\u03f5\n\n(cid:19)n\n\n+ 1\n\n(cid:18) 32ncbcw\n\u03f5\n\n\u00b7\n\n(cid:19)n\n\n+ 1\n\n(cid:18) 1\n\u03f5\n\n\u00b7\n\nsin\n\n(cid:18) 1\n\n16ncw\n\n(cid:19)\n\n(cid:19)d\n\n+ 1\n\n.\n\nThe proof of this Lemma is based on the theory of stochastic processes and can be seen in Theorem 8\nof Appendix C.\n\nTo obtain bounds of the form (6) on the size of a process, we use the generic chaining method. This\nmethod offers bounds in terms of the Talagrand-functional of the process in the Sub-Gaussian metric.\nWe define it as follows. A sequence T = (Tk)k\u2208N0 in a set T is admissible if T0 = 1 and Tk \u2264 2(2k).\nThe Talagrand-functional of the metric space is then defined as\n\n\u03b32(T, d) := inf\n(Tk)\n\nsup\nt\u2208T\n\n\u221e\n(cid:88)\n\nk=0\n\n2kd(t, Tk),\n\nwhere the infimum is taken across all admissible sequences.\n\nWith the bounds on the Sub-Gaussian covering number from Lemma 1, we provide a bound on the\nTalagrand-functional for shallow ReLU networks in the following Lemma. This bound is expected to\nbe of independent interest.\nLemma 2. Let \u00afP satisfy Assumption 1. Then we have\n\n\u03b32(\u03a6 \u00afP , d\u03c82 ) \u2264\n\n(cid:114) 2\n\u03c0\n\n(cid:18) 8n3/2cw(8cb + d + 1)\nln(2)\n\n(cid:19)\n(cid:112)2 ln(2)\n\n.\n\nThe key ideas to show this bound are similar to the ones used to prove Theorem 9 in Appendix C.\n\nTo provide bounds for the empirical process, we use the following Lemma, which we prove in\nAppendix D.\nLemma 3. Let \u03a6 be a set of real functions, indexed by a parameter set \u00afP and define\n\nN (\u03a6) :=\n\n(cid:90) \u221e\n\n(cid:113)\n\n0\n\nln N (\u03a6, d\u03c82, \u03f5)d\u03f5\n\nand \u2206(\u03a6) := sup\n\u03d5\u2208\u03a6\n\n\u2225\u03d5\u2225\u03c82 .\n\nThen, for any u \u2265 2, we have with probability at least 1 \u2212 17 exp(\u2212u/4) that\n\n|\u2225\u03d5\u2225m \u2212 \u2225\u03d5\u2225\u00b5| \u2264\n\nsup\n\u03d5\u2208\u03a6\n\nu\n\u221a\nm\n\n(cid:20)\n\nN (\u03a6) +\n\n(cid:21)\n\n\u2206(\u03a6)\n\n.\n\n10\n3\n\nThe bounds on the sample complexity for achieving the NeuRIPs event, from Theorem 1, are proven\nby applying these Lemmata.\nProof of Theorem 1. Since we assume ||\u03d5 \u00afp||\u00b5 > 1 for all \u00afp \u2208 \u00afP , we have\n\nsup\n\u00afp\u2208 \u00afP\n\n|\u2225\u03d5 \u00afp\u2225m \u2212 \u2225\u03d5 \u00afp\u2225\u00b5| \u2264 sup\n\u00afp\u2208 \u00afP\n\n|\u2225\u03d5 \u00afp\u2225m \u2212 \u2225\u03d5 \u00afp\u2225\u00b5|/\u2225\u03d5 \u00afp\u2225\u00b5.\n\nApplying Lemma 3, and further applying the bounds on the covering numbers and the Talagrand-\nfunctional for shallow ReLU networks, the NeuRIPs( \u00afP ) event holds in case of s > 3. The sample\ncomplexities that are provided in Theorem 1 follow from a refinement of this condition.\n\n6 Uniform Generalization of Sublevel Sets of the Empirical Risk\n\nIn case of the NeuRIPs event, the function norm || \u00b7 ||\u00b5 corresponding to the expected risk is close\nto || \u00b7 ||m, which corresponds to the empirical risk. With the previous results, we can now derive\nuniform generalization error bounds in the sublevel set of the empirical risk.\n\nWe use similar techniques and we define the following sets.\n\n\u2225f \u2225p = sup\n1\u2264q\u2264p\n\n\u2225f \u2225q\n\n\u039bk0,u = inf\n(Tk)\n\nsup\nf \u2208F\n\n\u221e\n(cid:88)\n\nk0\n\n6\n\n2k\u2225f \u2212 Tk(f )\u2225u2k\n\n\fand we need the following lemma:\n\nLemma 9. For any set F of functions and u \u2265 1, we have\n\n\u039b0,u(F ) \u2264 2\n\n\u221a\n\ne(\u03b32(F, d\u03c82) + \u2206(F )).\n\nTheorem 10. Let P be a parameter set satisfying Assumption 1. Then, for any u \u2265 1, we have with\nprobability at least 1 \u2212 17 exp(\u2212u/4) that\n\n\u2225\u03d5 \u00afp\u2225m \u2212 \u2225\u03d5 \u00afp\u2225\u00b5 \u2264\n\nsup\n\u00afp\u2208P\n\n(cid:16)\n\nu\n\u221a\nm\n\n16n3/2cw(8cb + d + 1) + 2ncw\n\n(cid:17)\n\n.\n\nProof. To this end we have to bound the Talagrand functional, where we can use Dudley\u2019s inequality\n(Lemma 6). To finish the proof, we apply the bounds on the covering numbers provided by Theorem\n6.\nTheorem 11. Let \u00afP \u2286 (Rd \u00d7 R \u00d7 \u00b11)n satisfy Assumption 1. Then there exist universal constants\nC1, C2 such that\n\n\u2225\u03d5 \u00afp\u2225m \u2212 \u2225\u03d5 \u00afp\u2225\u00b5 \u2264\n\nsup\n\u00afp\u2208P\n\n(cid:114) 2\n\u03c0\n\n(cid:18) 8n3/2cw(8cb + d + 1)\nln(2)\n\n(cid:19)\n(cid:112)2 ln(2)\n\n.",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "functions other than ReLU. This proposition is based on the concentration of measure phenomenon,\nwhich provides bounds on the Sub-Gaussian norm of functions on normal concentrating input spaces.\nBecause these bounds scale with the Lipschitz constant of the function, they can be used to find \u03f5-nets\nfor neurons that have identical activation patterns.\n\nBroader Impact\n\nSupervised machine learning now affects both personal and public lives significantly. Generalization is\ncritical to the reliability and safety of empirically trained models. Our analysis aims to achieve a deeper\nunderstanding of the relationships between generalization, architectural design, and available data.\nWe have discussed the concepts and demonstrated the effectiveness of using uniform concentration\nevents for generalization guarantees of common supervised machine learning algorithms.\n\n7",
  "conclusion": "concentration events for the empirical norm. We defined the Neural Restricted Isometry Property\n(NeuRIPs) and determined the sample complexity required to achieve NeuRIPs, which depends on\nrealistic parameter bounds and the network architecture. We applied our findings to derive upper\nbounds on the expected risk, which are valid uniformly across sublevel sets of the empirical risk.\nIf a network optimization algorithm can identify a network with a small empirical risk, our results\nguarantee that this network will generalize well. By deriving uniform concentration statements, we\nhave resolved the problem of independence between the termination of an optimization algorithm at\na certain network and the empirical risk concentration at that network. Future studies may focus on\nperforming uniform empirical norm concentration on the critical points of the empirical risk, which\ncould lead to even tighter bounds for the sample complexity.\n\nWe also plan to apply our methods to input distributions more general than the Gaussian distribution.\nIf generic Gaussian distributions can be handled, one could then derive bounds for the Sub-Gaussian\ncovering number for deep ReLU networks by induction across layers. We also expect that our",
  "is_publishable": 1,
  "venue": "NeurIPS"
}
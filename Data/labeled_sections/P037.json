{
  "title": "A Chinese Span-Extraction Dataset for Machine\nReading Comprehension",
  "abstract": "This paper introduces a novel dataset for Chinese machine reading comprehension,\nfocusing on span extraction. The data set is constructed using roughly 20,000 real-\nworld questions that are annotated by experts on passages extracted from Wikipedia.\nA challenge set is also created with questions that demand a deep understanding\nand inference across multiple sentences. We also show several baseline models and\nanonymous submission scores to emphasize the challenges present in this dataset.\nThe release of this dataset facilitated the Second Evaluation Workshop on Chinese\nMachine Reading Comprehension, also called CMRC 2018. We anticipate that this\ndataset will further facilitate research in Chinese machine reading comprehension.",
  "introduction": "The capacity to interpret and comprehend natural language is a crucial component of achieving\nadvanced artificial intelligence. Machine Reading Comprehension (MRC) is designed to understand\nthe context of given texts and respond to related questions. Numerous types of MRC datasets have\nbeen developed, such as cloze-style reading comprehension, span-extraction reading comprehension,\nopen-domain reading comprehension, and multiple-choice reading comprehension. Along with the\nincreasing availability of reading comprehension datasets, several neural network methods have been\nproposed, leading to substantial advancements in this area.\n\nThere have also been various efforts to create Chinese machine reading comprehension datasets.\nIn cloze-style reading comprehension, a Chinese cloze-style reading comprehension dataset was\nproposed, namely People\u2019s Daily Children\u2019s Fairy Tale. To increase the difficulty of the dataset, they\nalso release a human-annotated evaluation set in addition to the automatically generated development\nand test sets. Later, another dataset was introduced using children\u2019s reading materials. To promote\ndiversity and explore transfer learning, they also offer a human-annotated evaluation dataset using\nmore natural queries compared to the cloze type. This dataset was the main component in the first\nevaluation workshop on Chinese machine reading comprehension (CMRC 2017). Furthermore, a\nlarge-scale open-domain Chinese machine reading comprehension dataset (DuReader) was created,\ncontaining 200k queries from search engine user query logs. There is also a reading comprehension\ndataset in Traditional Chinese.\n\nWhile current machine learning techniques have outperformed human-level performance on datasets\nlike SQuAD, it is still unclear whether similar results can be achieved on datasets using different\nlanguages. To accelerate the progress of machine reading comprehension research, we present a\nspan-extraction dataset tailored for Chinese.\n\n2 The Proposed Dataset\n\n2.1 Task Definition\n\nThe reading comprehension task can be described as a triple (P, Q, A), where P is the passage, Q\nrepresents the question, and A is the answer. Specifically, in span-extraction reading comprehension,\n\n\fquestions are created by humans which is a more natural way of creating data than the cloze-style\nMRC datasets. The answer A should consist of a specific span from the given passage P. The task can\nbe simplified by predicting the start and end indices of the answer within the passage.\n\n2.2 Data Pre-Processing\n\nWe downloaded the Chinese portion of Wikipedia from a specified date and used an open-source\ntoolkit to process the raw files into plain text. Additionally, the Traditional Chinese characters were\nconverted to Simplified Chinese to ensure consistency using another open-source tool.\n\n2.3 Human Annotation\n\nThe questions in this dataset were created entirely by human experts, setting it apart from prior works\nthat relied on automated data generation methods. Initially, documents are divided into passages,\neach containing no more than 500 Chinese words. Annotators are required to assess each passage for\nits suitability, discarding those that are too difficult for public understanding. Passages were discarded\nbased on the following rules:\n\n\u2022 If more than 30% of the passage consists of non-Chinese characters.\n\u2022 If the passage includes too many specialized or professional terms.\n\u2022 If the passage has a large number of special characters or symbols.\n\u2022 If the paragraph is written in classical Chinese.\n\nAfter determining that the passage is suitable, annotators generate questions and their corresponding\nprimary answers based on the provided passage. During this question annotation, the following rules\nare used.\n\n\u2022 Each passage should have no more than five questions.\n\u2022 Answers must be a span from the passage.\n\u2022 Question diversity is encouraged such as questions of type who, when, where, why, and\n\nhow.\n\n\u2022 Avoid copying descriptions from the passage directly. Use paraphrasing or syntax transfor-\n\nmations to make answering more difficult.\n\n\u2022 Long answers (over 30 characters) will be discarded.\n\nFor the evaluation sets, which include the development, test, and challenge sets, three answers are\navailable for a more thorough assessment. Besides the primary answer generated by the question\nproposer, two additional annotators write a second and third answer for each question. These\nadditional annotators do not see the primary answer to avoid biased answers.\n\n2.4 Challenge Set\n\nA challenge set was made to evaluate how effectively models can perform reasoning over diverse\nclues in the context, while still maintaining the span-extraction format. This annotation was also\ncompleted by three annotators. The questions in this set need to meet the following criteria:\n\n\u2022 The answer can not be deduced from a single sentence in the passage if the answer is a\nsingle word or a short phrase. The annotation should encourage asking complex questions\nthat need an overall view of the passage to answer correctly.\n\n\u2022 If the answer is a named entity or belongs to a particular genre, it cannot be the only instance\nin the passage. There should be more than one instance to make the correct choice more\ndifficult for the model.\n\n2.5 Statistics\n\nThe overall statistics of the pre-processed data are shown in Table 1. The distribution of question\ntypes in the development set is shown in Figure 2.\n\n2\n\n\fTable 1: Statistics of the CMRC 2018 dataset.\n\nTrain\n\nDev\n\nTest\n\nChallenge\n\nQuestion #\nAnswer # per query\nMax passage tokens\nMax question tokens\nMax answer tokens\nAvg passage tokens\nAvg question tokens\nAvg answer tokens\n\n10,321\n1\n962\n89\n100\n452\n15\n17\n\n3,351\n3\n961\n56\n85\n469\n15\n9\n\n4,895\n3\n980\n50\n92\n472\n15\n9\n\n504\n3\n916\n47\n77\n464\n18\n19\n\n3 Evaluation Metrics\n\nThis paper uses two evaluation metrics. Common punctuations and white spaces are ignored for\nnormalization during evaluation.\n\n3.1 Exact Match\n\nThe Exact Match (EM) score measures the exact overlap between the prediction and the ground truth\nanswer. If the match is exact, then the score is 1; otherwise, the score is 0.\n\n3.2 F1-Score\n\nThe F1-score evaluates the fuzzy overlap at the character level between the prediction and the ground\ntruth answers. Instead of treating the answers as a bag of words, we calculate the longest common\nsequence (LCS) between the prediction and the ground truth and then compute the F1-score. The\nmaximum F1 score among all the ground truth answers is taken for each question.\n\n3.3 Estimated Human Performance\n\nThe estimated human performance is computed to measure the difficulty of the proposed dataset.\nEach question in the development, test, and challenge set has three answers. We use a cross-validation\nmethod to compute the performance. We treat the first answer as a human prediction and consider the\nother two answers as ground truth. Using this process, three human prediction scores are generated.\nFinally, we calculate the average of these three scores as the estimated human performance.\n\n4 Experimental Results\n\n4.1 Baseline System\n\nWe use BERT as the foundation of our baseline system. We modified the original script to accommo-\ndate our dataset. The initial learning rate was set to 3e-5, with a batch size of 32, and the training\nwas conducted for two epochs. The document and query maximum lengths were set to 512 and 64\nrespectively.\n\n4.2 Results\n\nThe results are in Table 2. Besides the baseline results, we include the results of the participants\nin the CMRC 2018 evaluation. The training and development sets were released to the public, and\nsubmissions were accepted to evaluate the models on the hidden test and challenge sets. As we can\nsee that most of the participants achieved an F1 score above 80 in the test set. On the other hand, the\nEM metric shows considerably lower scores in comparison to the SQuAD dataset, highlighting that\ndetermining the precise span boundary is crucial for performance enhancement in Chinese reading\ncomprehension.\n\nAs shown in the last column of Table 2, the top-ranked systems achieve decent results on the\ndevelopment and test sets but struggle to give satisfactory results on the challenge set. The estimated\n\n3\n\n\fTable 2: Baseline results and CMRC 2018 participants\u2019 results.\n\nDevelopment\nF1\nEM\n\nTest\n\nEM\n\nF1\n\nChallenge\nF1\n\nEM\n\nEstimated Human Performance\nZ-Reader (single model)\nMCA-Reader (ensemble)\nRCEN (ensemble)\nMCA-Reader (single model)\nOmegaOne (ensemble)\nRCEN (single model)\nGM-Reader (ensemble)\nOmegaOne (single model)\nGM-Reader (single model)\nR-NET (single model)\nSXU-Reader (ensemble)\nSXU-Reader (single model)\nT-Reader (single model)\nBERT-base (Chinese)\nBERT-base (Multi-lingual)\n\n91.083\n79.776\n66.698\n76.328\n63.902\n66.977\n73.253\n58.931\n64.430\n56.322\n45.418\n40.292\n37.310\n39.422\n63.6\n64.1\n\n97.348\n92.696\n85.538\n91.370\n82.618\n84.955\n89.750\n80.069\n82.699\n77.412\n69.825\n66.451\n66.121\n62.414\n83.9\n84.4\n\n92.400\n74.178\n71.175\n68.662\n68.335\n66.272\n64.576\n64.045\n64.188\n60.470\n50.112\n46.210\n44.270\n44.883\n67.8\n68.6\n\n97.914\n88.145\n88.090\n85.753\n85.707\n82.788\n83.136\n83.046\n81.539\n80.035\n73.353\n70.482\n70.673\n66.859\n86.0\n86.8\n\n90.382\n13.889\n15.476\n15.278\n13.690\n12.103\n10.516\n15.675\n10.119\n13.690\n9.921\nN/A\n6.548\n7.341\n18.4\n18.6\n\n95.248\n37.422\n37.104\n34.479\n33.964\n30.859\n30.994\n37.315\n29.716\n33.990\n29.324\nN/A\n28.116\n22.317\n42.1\n43.8\n\nhuman performance remains similar across the development, test, and challenge sets, indicating\nthat the difficulty is consistent across all three data sets. Even though Z-Reader achieved the best\nperformance on the test set, its EM metric performance was not consistent on the challenge set. This\nhighlights that current models are limited in their ability to process difficult questions that require\ncomplex reasoning over numerous clues throughout the passage.\n\nBERT-based methods demonstrated competitive performance compared to the submissions of par-\nticipants. Traditional models have higher scores in the test set. However, the BERT-based models\nperform better on the challenge set, indicating the importance of rich representations to address\ncomplex questions.",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "This paper introduces a span-extraction dataset for Chinese machine reading comprehension, con-\nsisting of roughly 20,000 questions annotated by human experts, along with a challenge set which\ncontains questions that need reasoning over different clues in the passage. The results from the\nevaluation suggest that models can achieve excellent scores on the development and test sets, close\nto the human performance in F1-score. However, the scores on the challenge set decline drastically,\nwhile human performance remains consistent. This shows there are still potential challenges in\ncreating models that can perform well on difficult reasoning questions. We expect that this dataset\nwill contribute to linguistic diversity in machine reading comprehension and facilitate additional\nresearch on questions that require comprehensive reasoning across multiple clues.\n\n4",
  "is_publishable": 1,
  "venue": NaN
}
{
  "title": "Advanced techniques for through and contextually\nInterpreting Noun-Noun Compounds",
  "abstract": "in the context of a complex semantic classification problem: understanding the\nmeaning of noun-noun compounds. Through a series of detailed experiments and\nan in-depth analysis of errors, we demonstrate that employing transfer learning by\ninitializing parameters and multi-task learning through parameter sharing enables a\nneural classification model to better generalize across a dataset characterized by a\nhighly uneven distribution of semantic relationships. Furthermore, we illustrate\nhow utilizing dual annotations, which involve two distinct sets of relations applied\nto the same compounds, can enhance the overall precision of a neural classifier and\nimprove its F1 scores for less common yet more challenging semantic relations.",
  "introduction": "nouns (or noun phrases in multi-word compounds). For instance, in the compound \"street protest,\"\nthe task is to identify the semantic relationship between \"street\" and \"protest,\" which is a locative\nrelation in this example. Given the prevalence of noun-noun compounds in natural language and its\nsignificance to other natural language processing (NLP) tasks like question answering and information\nretrieval, understanding noun-noun compounds has been extensively studied in theoretical linguistics,\npsycholinguistics, and computational linguistics.\n\nIn computational linguistics, noun-noun compound interpretation is typically treated as an automatic\nclassification task. Various machine learning (ML) algorithms and models, such as Maximum\nEntropy, Support Vector Machines, and Neural Networks, have been employed to decipher the\nsemantics of nominal compounds. These models utilize information from lexical semantics, like\nWordNet-based features, and distributional semantics, such as word embeddings. However, noun-\nnoun compound interpretation remains a challenging NLP problem due to the high productivity\nof noun-noun compounding as a linguistic structure and the difficulty in deriving the semantics of\nnoun-noun compounds from their constituents. Our research contributes to advancing NLP research\non noun-noun compound interpretation through the application of transfer and multi-task learning.\n\nThe application of transfer learning (TL) and multi-task learning (MTL) in NLP has gained significant\nattention in recent years, yielding varying outcomes based on the specific tasks, model architectures,\nand datasets involved. These varying results, combined with the fact that neither TL nor MTL has\nbeen previously applied to noun-noun compound interpretation, motivate our thorough empirical\ninvestigation into the use of TL and MTL for this task. Our aim is not only to add to the existing\nresearch on the effectiveness of TL and MTL for semantic NLP tasks generally but also to ascertain\ntheir specific advantages for compound interpretation.\n\nA key reason for utilizing multi-task learning is to enhance generalization by making use of the\ndomain-specific details present in the training data of related tasks. In this study, we demonstrate that\nTL and MTL can serve as a form of regularization, enabling the prediction of infrequent relations\nwithin a dataset marked by a highly skewed distribution of relations. This dataset is particularly\nwell-suited for TL and MTL experimentation, as elaborated in Section 3.\n\n\fOur contributions are summarized as follows:\n\n1. Through meticulous analysis of results, we discover that TL and MTL, especially when applied\nto the embedding layer, enhance overall accuracy and F1 scores for less frequent relations in a\nhighly skewed dataset, compared to a robust single-task learning baseline. 2. Although our research\nconcentrates on TL and MTL, we present, to our knowledge, the first experimental results on the\nrelatively recent dataset from Fares (2016).",
  "related_work": "relations, as well as the machine learning models and features employed to learn these relations. For\ninstance, some define a broad set of relations, while others employ a more detailed classification.\nSome researchers challenge the idea that noun-noun compounds can be interpreted using a fixed,\npredetermined set of relations, proposing alternative methods based on paraphrasing. We center\nour attention on methods that frame the interpretation problem as a classification task involving a\nfixed, predetermined set of relations. Various machine learning models have been applied to this\ntask, including nearest neighbor classifiers that use semantic similarity based on lexical resources,\nkernel-based methods like SVMs that utilize lexical and relational features, Maximum Entropy\nmodels that incorporate a wide range of lexical and surface form features, and neural networks that\nrely on word embeddings or combine word embeddings with path embeddings. Among these studies,\nsome have utilized the same dataset. To our knowledge, TL and MTL have not been previously\napplied to compound interpretation. Therefore, we review prior research on TL and MTL in other\nNLP tasks.\n\nSeveral recent studies have conducted extensive experiments on the application of TL and MTL to a\nvariety of NLP tasks, such as named entity recognition, semantic labeling, sentence-level sentiment\nclassification, super-tagging, chunking, and semantic dependency parsing. The consensus among\nthese studies is that the advantages of TL and MTL are largely contingent on the characteristics of the\ntasks involved, including the unevenness of the data distribution, the semantic relatedness between\nthe source and target tasks, the learning trajectory of the auxiliary and main tasks (where target tasks\nthat quickly reach a plateau benefit most from non-plateauing auxiliary tasks), and the structural\nsimilarity between the tasks. Besides differing in the NLP tasks they investigate, the aforementioned\nstudies employ slightly varied definitions of TL and MTL. Our research aligns with certain studies in\nthat we apply TL and MTL to learn different semantic annotations of noun-noun compounds using\nthe same dataset. However, our experimental design is more akin to other work in that we experiment\nwith initializing parameters across all layers of the neural network and concurrently train a single\nMTL model on two sets of relations.\n\n3 Task Definition and Dataset\n\nThe objective of this task is to train a model to categorize the semantic relationships between pairs\nof nouns in a labeled dataset, where each pair forms a noun-noun compound. The complexity of\nthis task is influenced by factors such as the label set used and its distribution. For the experiments\ndetailed in this paper, we utilize a noun-noun compounds dataset that features compounds annotated\nwith two distinct taxonomies of relations. This means that each noun-noun compound is associated\nwith two different relations, each based on different linguistic theories. This dataset is derived from\nestablished linguistic resources, including NomBank and the Prague Czech-English Dependency\nTreebank 2.0 (PCEDT). We chose this dataset for two primary reasons: firstly, the dual annotation of\nrelations on the same set of compounds is ideal for exploring TL and MTL approaches; secondly,\naligning two different annotation frameworks on the same data allows for a comparative analysis\nacross these frameworks.\n\nSpecifically, we use a portion of the dataset, focusing on type-based instances of two-word compounds.\nThe original dataset also encompasses multi-word compounds (those made up of more than two\nnouns) and multiple instances per compound type. We further divide the dataset into three parts:\ntraining, development, and test sets. Table 1 details the number of compound types and the vocabulary\nsize for each set, including a breakdown of words appearing in the right-most (right constituents)\nand left-most (left constituents) positions. The two label sets consist of 35 PCEDT functors and 18\n\n2\n\n\fNomBank argument and adjunct relations. As discussed in Section 7.1, these label sets have a highly\nuneven distribution.\n\nTable 1: Characteristics of the noun-noun compound dataset used in our experiments. The numbers\nin this table correspond to a subset of the dataset, see Section 3.\n\nTrain Dev\n\nTest\n\nCompounds\nVocab size\nRight constituents\nLeft constituents\n\n6932\n4102\n2304\n2405\n\n920\n1163\n624\n618\n\n1759\n1772\n969\n985\n\nMany relations in PCEDT and NomBank conceptually describe similar semantic ideas, as they are\nused to annotate the semantics of the same text. For instance, the temporal and locative relations in\nNomBank (ARGM-TMP and ARGM-LOC, respectively) and their PCEDT counterparts (TWHEN\nand LOC) exhibit relatively consistent behavior across frameworks, as they annotate many of the\nsame compounds. However, some relations that are theoretically similar do not align well in practice.\nFor example, the functor AIM in PCEDT and the modifier argument ARGM-PNC in NomBank\nexpress a somewhat related semantic concept (purpose), but there is minimal overlap between the\nsets of compounds they annotate. Nevertheless, it is reasonable to assume that the semantic similarity\nin the label sets, where it exists, can be leveraged through transfer and multi-task learning, especially\nsince the overall distribution of relations differs between the two frameworks.\n\n4 Transfer vs. Multi-Task Learning\n\nIn this section, we employ the terminology and definitions established by Pan and Yang (2010) to\narticulate our framework for transfer and multi-task learning. Our classification task can be described\nin terms of all training pairs (X, Y) and a probability distribution P(X), where X represents the input\nfeature space, Y denotes the set of all labels, and N is the training data size. The domain of a task is\ndefined by X, P(X). Our goal is to learn a function f(X) that predicts Y based on the input features X.\nConsidering two ML tasks, Ta and Tb, we would train two distinct models to learn separate functions\nfa and fb for predicting Ya and Yb in a single-task learning scenario. However, if Ta and Tb are\nrelated, either explicitly or implicitly, TL and MTL can enhance the generalization of either or both\ntasks. Two tasks are deemed related when their domains are similar but their label sets differ, or when\ntheir domains are dissimilar but their label sets are identical. Consequently, noun-noun compound\ninterpretation using the dataset is well-suited for TL and MTL, as the training examples are identical,\nbut the label sets are distinct.\n\nFor clarity, we differentiate between transfer learning and multi-task learning in this paper, despite\nthese terms sometimes being used interchangeably in the literature. We define TL as the utilization of\nparameters from a model trained on Ta to initialize another model for Tb. In contrast, MTL involves\ntraining parts of the same model to learn both Ta and Tb, essentially learning one set of parameters\nfor both tasks. The concept is to train a single model simultaneously on both tasks, where one task\nintroduces an inductive bias that aids the model in generalizing over the main task. It is important to\nnote that this does not necessarily imply that we aim to use a single model to predict both label sets\nin practice.\n\n5 Neural Classification Models\n\nThis section introduces the neural classification models utilized in our experiments. To discern the\nimpact of TL and MTL, we initially present a single-task learning model, which acts as our baseline.\nSubsequently, we employ this same model to implement TL and MTL.\n\n5.1 Single-Task Learning Model\n\nIn our single-task learning (STL) configuration, we train and fine-tune a feed-forward neural network\ninspired by the neural classifier proposed by Dima and Hinrichs (2015). This network comprises four\nlayers: 1) an input layer, 2) an embedding layer, 3) a hidden layer, and 4) an output layer. The input\n\n3\n\n\flayer consists of two integers that indicate the indices of a compound\u2019s constituents in the embedding\nlayer, where the word embedding vectors are stored. These selected vectors are then passed to a fully\nconnected hidden layer, the size of which matches the dimensionality of the word embedding vectors.\nFinally, a softmax function is applied to the output layer to select the most probable relation.\n\nThe compound\u2019s constituents are represented using a 300-dimensional word embedding model trained\non an English Wikipedia dump and the English Gigaword Fifth Edition. The embedding model was\ntrained by Fares et al. (2017). If a word is not found during lookup in the embedding model, we\ncheck if the word is uppercased and attempt to find the lowercase version. For hyphenated words\nnot found in the embedding vocabulary, we split the word at the hyphen and average the vectors of\nits parts, if they are present in the vocabulary. If the word remains unrepresented after these steps, a\ndesignated vector for unknown words is employed.\n\n5.1.1 Architecture and Hyperparameters\n\nOur selection of hyperparameters is informed by multiple rounds of experimentation with the single-\ntask learning model, as well as the choices made by prior work. The weights of the embedding layer\nare updated during the training of all models. We utilize the Adaptive Moment Estimation (Adam)\noptimization function across all models, with a learning rate set to 0.001. The loss function employed\nis the negative-log likelihood. A Sigmoid activation function is used for the units in the hidden layer.\nAll models are trained with mini-batches of size five. The maximum number of epochs is capped\nat 50, but an early stopping criterion based on the model\u2019s accuracy on the validation split is also\nimplemented. This means that training is halted if the validation accuracy does not improve over five\nconsecutive epochs. All models are implemented in Keras, using TensorFlow as the backend. The TL\nand MTL models are trained using the same hyperparameters as the STL model.\n\n5.2 Transfer Learning Models\n\nIn our experiments, transfer learning involves training an STL model on PCEDT relations and then\nusing some of its weights to initialize another model for NomBank relations. Given the neural\nclassifier architecture detailed in Section 5.1, we identify three ways to implement TL: 1) TLE:\nTransferring the embedding layer weights, 2) TLH: Transferring the hidden layer weights, and 3)\nTLEH: Transferring both the embedding and hidden layer weights. Furthermore, we differentiate\nbetween transfer learning from PCEDT to NomBank and vice versa. This results in six setups,\nas shown in Table 2. We do not apply TL (or MTL) to the output layer because it is task- or\ndataset-specific.\n\n5.3 Multi-Task Learning Models\n\nIn MTL, we train a single model to simultaneously learn both PCEDT and NomBank relations,\nmeaning all MTL models have two objective functions and two output layers. We implement two\nMTL setups: MTLE, which features a shared embedding layer but two task-specific hidden layers,\nand MTLF, which has no task-specific layers aside from the output layer (i.e., both the embedding\nand hidden layers are shared). We distinguish between the auxiliary and main tasks based on which\nvalidation accuracy (NomBank\u2019s or PCEDT\u2019s) is monitored by the early stopping criterion. This\nleads to a total of four MTL models, as shown in Table 3.\n\n6 Experimental Results\n\nTables 2 and 3 display the accuracies of the various TL and MTL models on the development and test\nsplits for NomBank and PCEDT. The top row in both tables indicates the accuracy of the STL model.\nAll models were trained solely on the training split. Several insights can be gleaned from these\ntables. Firstly, the accuracy of the STL models decreases when evaluated on the test split for both\nNomBank and PCEDT. Secondly, all TL models achieve improved accuracy on the NomBank test\nsplit, although transfer learning does not significantly enhance accuracy on the development split of\nthe same dataset. The MTL models, especially MTLF, have a detrimental effect on the development\naccuracy of NomBank, yet we observe a similar improvement, as with TL, on the test split. Thirdly,\nboth TL and MTL models demonstrate less consistent effects on PCEDT (on both development and\ntest splits) compared to NomBank. For instance, all TL models yield an absolute improvement of\n\n4\n\n\fabout 1.25 points in accuracy on NomBank, whereas in PCEDT, TLE clearly outperforms the other\ntwo TL models (TLE improves over the STL accuracy by 1.37 points).\n\nTable 2: Accuracy (%) of the transfer learning models.\n\nModel\n\nNomBank\n\nPCEDT\n\nDev\n\nTest\n\nDev\n\nTest\n\n78.15\nSTL\n78.37\nTLE\nTLH\n78.15\nTLEH 78.48\n\n76.75\n78.05\n78.00\n78.00\n\n58.80\n59.57\n59.24\n59.89\n\n56.05\n57.42\n56.51\n56.68\n\nTable 3: Accuracy (%) of the MTL models.\n\nModel\n\nNomBank\n\nPCEDT\n\nDev\n\nTest\n\nDev\n\nTest\n\n78.15\nSTL\nMTLE 77.93\n76.74\nMTLF\n\n76.75\n78.45\n78.51\n\n58.80\n59.89\n58.91\n\n56.05\n56.96\n56.00\n\nOverall, the STL models\u2019 accuracy declines when tested on the NomBank and PCEDT test splits,\ncompared to their performance on the development split. This could suggest overfitting, especially\nsince our stopping criterion selects the model with the best performance on the development split.\nConversely, TL and MTL enhance accuracy on the test splits, despite using the same stopping criterion\nas STL. We interpret this as an improvement in the models\u2019 ability to generalize. However, since\nthese improvements are relatively minor, we further analyze the results to understand if and how TL\nand MTL are beneficial.",
  "methodology": "",
  "experiments": "",
  "results": "This section provides a detailed analysis of the models\u2019 performance, drawing on insights from the\ndataset and the classification errors made by the models. The discussion in the following sections is\nprimarily based on the results from the test split, as it is larger than the development split.\n\n7.1 Relation Distribution\n\nTo illustrate the complexity of the task, we depict the distribution of the most frequent relations in\nNomBank and PCEDT across the three data splits in Figure 1. Notably, approximately 71.18% of the\nrelations in the NomBank training split are of type ARG1 (prototypical patient), while 52.20% of the\nPCEDT relations are of type RSTR (an underspecified adnominal modifier). Such a highly skewed\ndistribution makes learning some of the other relations more challenging, if not impossible in certain\ncases. In fact, out of the 15 NomBank relations observed in the test split, five are never predicted\nby any of the STL, TL, or MTL models. Similarly, of the 26 PCEDT relations in the test split, only\nsix are predicted. However, the unpredicted relations are extremely rare in the training split (e.g., 23\nPCEDT functors appear less than 20 times), making it doubtful whether any ML model could learn\nthem under any circumstances.\n\nGiven this imbalanced distribution, it is evident that accuracy alone is insufficient to determine the\nbest-performing model. Therefore, in the subsequent section, we report and analyze the F1 scores of\nthe predicted NomBank and PCEDT relations across all STL, TL, and MTL models.\n\n7.2 Per-Relation F1 Scores\n\nTables 4 and 5 present the per-relation F1 scores for NomBank and PCEDT, respectively. We only\ninclude results for relations that are actually predicted by at least one of the models.\n\n5\n\n\fTable 4: Per-label F1 score on the NomBank test split.\n\nA0\n\nA1\n\nA2\n\nA3\n\nLOC MNR TMP\n\n132\nCount\n49.82\nSTL\n55.02\nTLE\nTLH\n54.81\nTLEH 53.62\nMTLE 54.07\n53.09\nMTLF\n\n1282\n87.54\n87.98\n87.93\n87.95\n88.34\n88.41\n\n153\n45.78\n41.61\n42.51\n42.70\n42.86\n38.14\n\n75\n60.81\n60.14\n60.00\n61.11\n61.97\n62.69\n\n25\n28.57\n27.91\n25.00\n29.27\n30.00\n00.00\n\n25\n29.41\n33.33\n35.29\n33.33\n28.57\n00.00\n\n27\n66.67\n63.83\n65.31\n65.22\n66.67\n52.17\n\nTable 5: Per-label F1 score on the PCEDT test split.\n\nACT\n\nTWHEN APP\n\nPAT\n\nREG RSTR\n\n89\nCount\n43.90\nSTL\n49.37\nTLE\n53.99\nTLH\nTLEH 49.08\nMTLE 54.09\n47.80\nMTLF\n\n14\n42.11\n70.97\n62.07\n64.52\n66.67\n42.11\n\n118\n22.78\n27.67\n25.00\n28.57\n24.05\n25.64\n\n326\n42.83\n41.60\n43.01\n42.91\n42.03\n40.73\n\n216\n20.51\n30.77\n26.09\n28.57\n27.21\n19.22\n\n900\n68.81\n69.67\n68.99\n69.08\n69.31\n68.89\n\nSeveral noteworthy patterns emerge from Tables 4 and 5. Firstly, the MTLF model appears to be\ndetrimental to both datasets, leading to significantly degraded F1 scores for four NomBank relations,\nincluding the locative modifier ARGM-LOC and the manner modifier ARGM-MNR (abbreviated as\nLOC and MNR in Table 4), which the model fails to predict altogether. This same model exhibits\nthe lowest F1 score compared to all other models for two PCEDT relations: REG (expressing a\ncircumstance) and PAT (patient). Considering that the MTLF model achieves the highest accuracy\non the NomBank test split (as shown in Table 3), it becomes even more apparent that relying solely\non accuracy scores is inadequate for evaluating the effectiveness of TL and MTL for this task and\ndataset.\n\nSecondly, with the exception of the MTLF model, all TL and MTL models consistently improve\nthe F1 score for all PCEDT relations except PAT. Notably, the F1 scores for the relations TWHEN\nand ACT show a substantial increase compared to other PCEDT relations when only the embedding\nlayer\u2019s weights are shared (MTLE) or transferred (TLE). This outcome can be partially understood\nby examining the correspondence matrices between NomBank arguments and PCEDT functors,\npresented in Tables 7 and 6. These tables illustrate how PCEDT functors map to NomBank arguments\nin the training split (Table 6) and vice versa (Table 7). Table 6 reveals that 80% of the compounds\nannotated as TWHEN in PCEDT were annotated as ARGM-TMP in NomBank. Additionally, 47% of\nACT (Actor) relations map to ARG0 (Proto-Agent) in NomBank. While this mapping is not as distinct\nas one might hope, it is still relatively high when compared to how other PCEDT relations map to\nARG0. The correspondence matrices also demonstrate that the presumed theoretical similarities\nbetween NomBank and PCEDT relations do not always hold in practice. Nevertheless, even such\nimperfect correspondences can provide a training signal that assists the TL and MTL models in\nlearning relations like TWHEN and ACT.\n\nSince the TLE model outperforms STL in predicting REG by ten absolute points, we examined\nall REG compounds correctly classified by TLE but misclassified by STL. We found that STL\nmisclassified them as RSTR, indicating that TL from NomBank helps TLE recover from STL\u2019s\novergeneralization in RSTR prediction.\n\nThe two NomBank relations that receive the highest boost in F1 score (about five absolute points)\nare ARG0 and ARGM-MNR, but the improvement in the latter corresponds to only one additional\ncompound, which might be a chance occurrence. Overall, TL and MTL from NomBank to PCEDT\nare more helpful than the reverse. One explanation is that five PCEDT relations (including the four\nmost frequent ones) map to ARG1 in NomBank in more than 60% of cases for each relation, as seen\nin the first rows of Tables 6 and 7. This suggests that the weights learned to predict PCEDT relations\n\n6\n\n\fTable 6: Correspondence matrix between PCEDT functors and NomBank arguments. Slots with \u2019-\u2019\nindicate zero, 0.00 represents a very small number but not zero.\n\nA1\n\n0.70\nRSTR\n0.90\nPAT\n0.78\nREG\n0.62\nAPP\n0.47\nACT\n0.65\nAIM\nTWHEN 0.10\n\nA2\n\n0.11\n0.05\n0.10\n0.21\n0.03\n0.12\n0.03\n\nA0\n\n0.06\n0.01\n0.04\n0.13\n0.47\n0.07\n-\n\nCount\n\n3617\n\n1312\n\n777\n\nA3\n\nLOC TMP MNR\n\n0.06\n0.02\n0.06\n0.02\n0.01\n0.06\n-\n\n499\n\n0.02\n0.01\n0.00\n0.01\n0.01\n0.01\n-\n\n273\n\n0.01\n-\n0.00\n0.00\n-\n-\n0.80\n\n116\n\n0.02\n0.00\n0.00\n-\n0.01\n-\n-\n\n59\n\nTable 7: Correspondence matrix between NomBank arguments and PCEDT functors.\n\nRSTR PAT REG APP ACT AIM TWHEN\n\nA1\nA2\nA0\nA3\nLOC\nTMP\nMNR\n\n0.51\n0.47\n0.63\n0.66\n0.36\n0.78\n0.24\n\nCount\n\n4932\n\n0.54\n0.09\n0.03\n0.08\n0.07\n-\n0.05\n\n715\n\n0.12\n0.11\n0.07\n0.13\n0.02\n0.01\n0.01\n\n495\n\n0.06\n0.14\n0.13\n0.03\n0.05\n0.01\n-\n\n358\n\n0.03\n0.01\n0.26\n0.01\n0.03\n-\n0.03\n\n119\n\n0.02\n0.02\n0.02\n0.02\n0.01\n-\n-\n\n103\n\n0.00\n0.00\n-\n-\n-\n0.01\n-\n\n79\n\noffer little to no inductive bias for NomBank relations. Conversely, the mapping from NomBank to\nPCEDT shows that although many NomBank arguments map to RSTR in PCEDT, the percentages\nare lower, making the mapping more diverse and discriminative, which seems to aid TL and MTL\nmodels in learning less frequent PCEDT relations.\n\nTo understand why the PCEDT functor AIM is never predicted despite being more frequent than\nTWHEN, we found that AIM is almost always misclassified as RSTR by all models. Furthermore,\nAIM and RSTR have the highest lexical overlap in the training set among all PCEDT relation pairs:\n78.35% of left constituents and 73.26% of right constituents of compounds annotated as AIM occur\nin other compounds annotated as RSTR. This explains the models\u2019 inability to learn AIM but raises\nquestions about their ability to learn relational representations, which we explore further in Section\n7.3.\n\nTable 8: Macro-average F1 score on the test split.\n\nModel NomBank\n\nPCEDT\n\nSTL\nTLE\nTLH\nTLEH\nMTLE\nMTLF\n\n52.66\n52.83\n52.98\n53.31\n53.21\n42.07\n\n40.15\n48.34\n46.52\n47.12\n47.23\n40.73\n\nFinally, to demonstrate the benefits of TL and MTL for NomBank and PCEDT, we report the F1\nmacro-average scores in Table 8. This is arguably the appropriate evaluation measure for imbalanced\nclassification problems. Note that relations not predicted by any model are excluded from the macro-\naverage calculation. Table 8 clearly shows that TL and MTL on the embedding layer yield significant\nimprovements for PCEDT, with about a 7-8 point increase in macro-average F1, compared to just\n0.65 in the best case for NomBank.\n\n7\n\n\f7.3 Generalization on Unseen Compounds\n\nWe now analyze the models\u2019 ability to generalize to compounds not seen during training. Recent\nresearch suggests that gains in noun-noun compound interpretation using word embeddings and\nsimilar neural classification models might be due to lexical memorization. In other words, the models\nlearn that specific nouns are strong indicators of specific relations. To assess the role of lexical\nmemorization in our models, we quantify the number of unseen compounds that the STL, TL, and\nMTL models predict correctly.\n\nWe differentiate between \u2019partly\u2019 and \u2019completely\u2019 unseen compounds. A compound is \u2019partly\u2019\nunseen if one of its constituents (left or right) is not present in the training data. A \u2019completely\u2019\nunseen compound is one where neither the left nor the right constituent appears in the training data.\nOverall, nearly 20% of the compounds in the test split have an unseen left constituent, about 16%\nhave an unseen right constituent, and 4% are completely unseen. Table 9 compares the performance\nof the different models on these three groups in terms of the proportion of compounds misclassified\nin each group.\n\nTable 9: Generalization error on the subset of unseen compounds in the test split. L: Left constituent.\nR: Right constituent. L&R: Completely unseen.\n\nNomBank\n\nModel\n\nL\n\nR\n\n351\nCount\n27.92\nSTL\n25.93\nTLE\nTLH\n26.21\nTLEH 26.50\nMTLE 24.50\n22.79\nMTLF\n\n286\n39.51\n36.71\n38.11\n38.81\n33.22\n34.27\n\nL&R\n\n72\n50.00\n48.61\n50.00\n52.78\n38.89\n40.28\n\nPCEDT\n\nL\n\nR\n\n351\n45.01\n43.87\n46.15\n45.87\n44.44\n44.16\n\n286\n47.55\n47.55\n49.30\n47.55\n47.20\n47.90\n\nL&R\n\n72\n41.67\n41.67\n47.22\n43.06\n43.06\n38.89\n\nTable 9 shows that Transfer Learning (TL) and Multi-Task Learning (MTL) approaches reduce\ngeneralization error in NomBank across all scenarios, with the exception of TLH and TLEH for\ncompletely unseen compounds, where error increases. The greatest error reductions are achieved\nby MTL models across all three types of unseen compounds. Specifically, MTLE reduces the error\nby approximately six points for compounds with unseen right constituents and by eleven points for\nfully unseen compounds. Moreover, MTLF reduces the error by five points when the left constituent\nis unseen. It\u2019s important to interpret these results in conjunction with the Count row in Table 9 for\na comprehensive view. For example, the eleven-point error decrease in fully unseen compounds\nrepresents eight compounds. In PCEDT, the largest error reduction is on unseen left constituents,\nwhich is about 1.14 points, corresponding to four compounds; it\u2019s 0.35 on unseen right constituents\n(one compound) and 2.7 on fully unseen compounds, or two compounds.\n\nUpon manual inspection of compounds that led to substantial reductions in the generalization error,\nspecifically within NomBank, we examined the distribution of relations within correctly predicted\nunseen compound sets. Compared to the STL model, MTLE reduces generalization error for\ncompletely unseen compounds by a total of eight compounds, of which seven are annotated with the\nrelation ARG1, which is the most common in NomBank. Regarding the unseen right constituents,\nMTLE\u2019s 24 improved compounds consist of 18 ARG1, 5 ARG0, and 1 ARG2 compounds. A\nsimilar pattern arises when examining TLE model improvements, where most gains come from better\npredictions of ARG1 and ARG0 relations.\n\nA large portion of unseen compounds, whether partly or entirely unseen, that were misclassified by\nevery model, were not of type ARG1 in NomBank, or RSTR in PCEDT. This pattern, along with\ncorrectly predicted unseen compounds primarily annotated with the most common relations, suggests\nthat classification models rely on lexical memorization to learn the compound relation interpretation.\n\nTo better comprehend lexical memorization\u2019s impact, we present the ratio of relation-specific con-\nstituents in both NomBank and PCEDT, as depicted in Figure 2. We define a relation-specific\nconstituent as a left or right constituent that appears with only one specific relation within the training\ndata. Its ratio is calculated as its proportion in the full set of left or right constituents for each\n\n8\n\n\frelation. Analyzing Figure 2 reveals that NomBank relations possess higher ratios of relation-specific\nconstituents compared to PCEDT. This potentially makes learning the former easier if the model\nsolely relies on lexical memorization. Additionally, ARGM-TMP in NomBank and TWHEN in\nPCEDT have distinctly high ratios compared to other relations in Figure 2. These relations also\nhave the second-highest F1 score in their datasets\u2014except for STL on PCEDT (see Tables 4 and\n5). Lexical memorization is therefore a likely cause of these high F1 scores. We also observed that\nlower ratios of relation-specific constituents correlate with lower F1 scores, such as APP and REG in\nPCEDT. Based on these insights, we can\u2019t dismiss the possibility that our models show some degree\nof lexical memorization, despite manual analysis also presenting cases where models demonstrate\ngeneralization and correct predictions in situations where lexical memorization is impossible.",
  "conclusion": "nificant traction, yet considerable ambiguity persists regarding the effectiveness of particular task\ncharacteristics and experimental setups. This research endeavors to clarify the benefits of TL and\nMTL in the context of semantic interpretation of noun-noun compounds. By executing a sequence of\nminimally contrasting experiments and conducting thorough analysis of results and prediction errors,\nwe demonstrate how both TL and MTL can mitigate the effects of class imbalance and drastically\nenhance predictions for low-frequency relations. Overall, our TL, and particularly our MTL models,\nare better at making predictions both quantitatively and qualitatively. Notably, the improvements are\nobserved on the \u2019most challenging\u2019 inputs that include at least one constituent that was not present in\nthe training data. However, clear indications of \u2019lexical memorization\u2019 effects are evident in our error\nanalysis of unseen compounds.\n\nTypically, the transfer of representations or sharing between tasks is more effective at the embedding\nlayers, which represent the model\u2019s internal representation of the compound constituents. Furthermore,\nin multi-task learning, the complete sharing of model architecture across tasks degrades its capacity\nto generalize when it comes to less frequent relations.\n\nThe dataset provided by Fares (2016) is an appealing resource for new neural approaches to compound\ninterpretation because it links this sub-problem with broad-coverage semantic role labeling or\nsemantic dependency parsing in PCEDT and NomBank. Future research will focus on incorporating\nadditional natural language processing tasks defined using these frameworks to understand noun-noun\ncompound interpretation using TL and MTL.\n\n9",
  "is_reference": true,
  "ref_category": "publishable",
  "is_publishable": 1,
  "venue": "EMNLP"
}
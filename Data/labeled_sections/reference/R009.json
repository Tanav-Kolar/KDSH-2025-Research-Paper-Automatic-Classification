{
  "title": "The Importance of Written Explanations in\nAggregating Crowdsourced Predictions",
  "abstract": "individuals when making predictions enhances the accuracy of aggregated crowd-\nsourced forecasts. The research shows that while majority and weighted vote\nmethods are effective, the inclusion of written justifications improves forecast\naccuracy throughout most of a question\u2019s duration, with the exception of its final\nphase. Furthermore, the study analyzes the attributes that differentiate reliable and\nunreliable justifications.",
  "introduction": "expert individuals can produce answers that are as accurate as, or even more accurate than, those\nprovided by a single expert. A classic example of this concept is the observation that the median\nestimate of an ox\u2019s weight from a large group of fair attendees was remarkably close to the actual\nweight. While generally supported, the idea is not without its limitations. Historical examples\ndemonstrate instances where crowds behaved irrationally, and even a world chess champion was able\nto defeat the combined moves of a crowd.\n\nIn the current era, the advantages of collective intelligence are widely utilized. For example, Wikipedia\nrelies on the contributions of volunteers, and community-driven question-answering platforms have\ngarnered significant attention from the research community. When compiling information from\nlarge groups, it is important to determine whether the individual inputs were made independently. If\nnot, factors like group psychology and the influence of persuasive arguments can skew individual\njudgments, thus negating the positive effects of crowd wisdom.\n\nThis paper focuses on forecasts concerning questions spanning political, economic, and social\ndomains. Each forecast includes a prediction, estimating the probability of a particular event, and\na written justification that explains the reasoning behind the prediction. Forecasts with identical\npredictions can have justifications of varying strength, which, in turn, affects the perceived reliability\nof the predictions. For instance, a justification that simply refers to an external source without\nexplanation may appear to rely heavily on the prevailing opinion of the crowd and might be considered\nweaker than a justification that presents specific, verifiable facts from external resources.\n\nTo clarify the terminology used: a \"question\" is defined as a statement that seeks information (e.g.,\n\"Will new legislation be implemented before a certain date?\"). Questions have a defined start and\nend date, and the period between these dates constitutes the \"life\" of the question. \"Forecasters\"\nare individuals who provide a \"forecast,\" which consists of a \"prediction\" and a \"justification.\" The\nprediction is a numerical representation of the likelihood of an event occurring. The justification\nis the text provided by the forecaster to support their prediction. The central problem addressed in\nthis work is termed \"calling a question,\" which refers to the process of determining a final prediction\nby aggregating individual forecasts. Two strategies are employed for calling questions each day\nthroughout their life: considering forecasts submitted on the given day (\"daily\") and considering the\nlast forecast submitted by each forecaster (\"active\").\n\n\fInspired by prior research on recognizing and fostering skilled forecasters, and analyzing written\njustifications to assess the quality of individual or collective forecasts, this paper investigates the\nautomated calling of questions throughout their duration based on the forecasts available each day.\nThe primary contributions are empirical findings that address the following research questions:\n\n* When making a prediction on a specific day, is it advantageous to include forecasts from previous\ndays? (Yes) * Does the accuracy of the prediction improve when considering the question itself\nand the written justifications provided with the forecasts? (Yes) * Is it easier to make an accurate\nprediction toward the end of a question\u2019s duration? (Yes) * Are written justifications more valuable\nwhen the crowd\u2019s predictions are less accurate? (Yes)\n\nIn addition, this research presents an examination of the justifications associated with both accurate\nand inaccurate forecasts. This analysis aims to identify the features that contribute to a justification\nbeing more or less credible.",
  "related_work": "both predictive models (using language samples to predict attributes about the author) and models\nthat provide valuable insights (using language samples and author attributes to identify differentiating\nlinguistic features). Previous studies have examined factors such as gender and age, political ideology,\nhealth outcomes, and personality traits. In this paper, models are constructed to predict outcomes\nbased on crowd-sourced forecasts without knowledge of individual forecasters\u2019 identities.\n\nPrevious research has also explored how language use varies depending on the relationships between\nindividuals. For instance, studies have analyzed language patterns in social networks, online commu-\nnities, and corporate emails to understand how individuals in positions of authority communicate.\nSimilarly, researchers have examined how language provides insights into interpersonal interactions\nand relationships. In terms of language form and function, prior research has investigated politeness,\nempathy, advice, condolences, usefulness, and deception. Related to the current study\u2019s focus,\nresearchers have examined the influence of Wikipedia editors and studied influence levels within\nonline communities. Persuasion has also been analyzed from a computational perspective, including\nwithin the context of dialogue systems. The work presented here complements these previous studies.\nThe goal is to identify credible justifications to improve the aggregation of crowdsourced forecasts,\nwithout explicitly targeting any of the aforementioned characteristics.\n\nWithin the field of computational linguistics, the task most closely related to this research is argumen-\ntation. A strong justification for a forecast can be considered a well-reasoned supporting argument.\nPrevious work in this area includes identifying argument components such as claims, premises,\nbacking, rebuttals, and refutations, as well as mining arguments that support or oppose a particular\nclaim. Despite these efforts, it was found that crowdsourced justifications rarely adhere to these\nestablished argumentation frameworks, even though such justifications are valuable for aggregating\nforecasts.\n\nFinally, several studies have focused on forecasting using datasets similar or identical to the one used\nin this research. From a psychological perspective, researchers have explored strategies for enhancing\nforecasting accuracy, such as utilizing top-performing forecasters (often called \"superforecasters\"),\nand have analyzed the traits that contribute to their success. These studies aim to identify and cultivate\nsuperforecasters but do not incorporate the written justifications accompanying forecasts. In contrast,\nthe present research develops models to call questions without using any information about the\nforecasters themselves. Within the field of computational linguistics, researchers have evaluated the\nlanguage used in high-quality justifications, focusing on aspects like rating, benefit, and influence.\nOther researchers have developed models to predict forecaster skill using the textual justifications\nfrom specific datasets, such as the Good Judgment Open data, and have also applied these models\nto predict the accuracy of individual forecasts in other contexts, such as company earnings reports.\nHowever, none of these prior works have specifically aimed to call questions throughout their entire\nduration.\n\n2\n\n\f3 Dataset\n\nThe research utilizes data from the Good Judgment Open, a platform where questions are posted, and\nindividuals submit their forecasts. The questions primarily revolve around geopolitics, encompassing\nareas such as domestic and international politics, the economy, and social matters. For this study, all\nbinary questions were collected, along with their associated forecasts, each comprising a prediction\nand a justification. In total, the dataset contains 441 questions and 96,664 forecasts submitted\nover 32,708 days. This dataset significantly expands upon previous research, nearly doubling the\nnumber of forecasts analyzed. Since the objective is to accurately call questions throughout their\nentire duration, all forecasts with written justifications are included, regardless of factors such as\njustification length or the number of forecasts submitted by a single forecaster. Additionally, this\napproach prioritizes privacy, as no information about the individual forecasters is utilized.\n\nTable 1: Analysis of the questions from our dataset. Most questions are relatively long, contain two\nor more named entities, and are open for over one month.\n\nMetric\n\nMin Q1 Q2 (Median) Q3 Max Mean\n\n# tokens\n# entities\n# verbs\n# days open\n\n8\n0\n0\n2\n\n16\n2\n2\n24\n\n20\n3\n2\n59\n\n28\n5\n3\n98\n\n48\n11\n6\n475\n\n21.94\n3.47\n2.26\n74.16\n\nTable 1 provides a basic analysis of the questions in the dataset. The majority of questions are\nrelatively lengthy, containing more than 16 tokens and multiple named entities, with geopolitical,\nperson, and date entities being the most frequent. In terms of duration, half of the questions remain\nopen for nearly two months, and 75% are open for more than three weeks.\n\nAn examination of the topics covered by the questions using Latent Dirichlet Allocation (LDA)\nreveals three primary themes: elections (including terms like \"voting,\" \"winners,\" and \"candidate\"),\ngovernment actions (including terms like \"negotiations,\" \"announcements,\" \"meetings,\" and \"passing\n(a law)\"), and wars and violent crimes (including terms like \"groups,\" \"killing,\" \"civilian (casualties),\"\nand \"arms\"). Although not explicitly represented in the LDA topics, the questions address both\ndomestic and international events within these broad themes.\n\nTable 2: Analysis of the 96,664 written justifications submitted by forecasters in our dataset. The\nreadability scores indicate that most justifications are easily understood by high school students (11th\nor 12th grade), although a substantial amount (>25%) require a college education (Flesch under 50 or\nDale-Chall over 9.0).\n\nMin\n\n1\n1\n0\n0\n0\n0\n0\n-2.54\n\nQ1\n\nQ2\n\n1\n10\n0\n1\n0\n0\n0\n0\n\n1\n23\n2\n3\n1\n2\n1\n0\n\nQ3\n\n3\n47\n4\n6\n3\n4\n3\n0.20\n\nMax\n\n56\n1295\n154\n174\n63\n91\n69\n6.50\n\n#sentences\n#tokens\n#entities\n#verbs\n#adverbs\n#adjectives\n#negation\nSentiment\n\nReadability\n\nFlesch\nDale-Chall\n\n-49.68\n0.05\n\n50.33\n6.72\n\n65.76\n7.95\n\n80.62\n9.20\n\n121.22\n19.77\n\nTable 2 presents a fundamental analysis of the 96,664 forecast justifications in the dataset. The median\nlength is relatively short, consisting of one sentence and 23 tokens. Justifications mention named\nentities less frequently than the questions themselves. Interestingly, half of the justifications contain\nat least one negation, and 25% include three or more. This suggests that forecasters sometimes base\ntheir predictions on events that might not occur or have not yet occurred. The sentiment polarity of\n\n3\n\n\fthe justifications is generally neutral. In terms of readability, both the Flesch and Dale-Chall scores\nsuggest that approximately a quarter of the justifications require a college-level education for full\ncomprehension.\n\nRegarding verbs and nouns, an analysis using WordNet lexical files reveals that the most common\nverb classes are \"change\" (e.g., \"happen,\" \"remain,\" \"increase\"), \"social\" (e.g., \"vote,\" \"support,\"\n\"help\"), \"cognition\" (e.g., \"think,\" \"believe,\" \"know\"), and \"motion\" (e.g., \"go,\" \"come,\" \"leave\").\nThe most frequent noun classes are \"act\" (e.g., \"election,\" \"support,\" \"deal\"), \"communication\" (e.g.,\n\"questions,\" \"forecast,\" \"news\"), \"cognition\" (e.g., \"point,\" \"issue,\" \"possibility\"), and \"group\" (e.g.,\n\"government,\" \"people,\" \"party\").",
  "methodology": "",
  "experiments": "with variations where certain components are disabled. Specifically, the representation of a forecast\nis manipulated by incorporating different combinations of information:\n\n4\n\n\f* Only the prediction. * The prediction and the representation of the question. * The prediction and\nthe representation of the justification. * The prediction, the representation of the question, and the\nrepresentation of the justification.\n\n4.4 Quantitative Results\n\nThe evaluation metric used is accuracy, which represents the average percentage of days a model\ncorrectly calls a question throughout its duration. Results are reported for all days combined, as well\nas for each of the four quartiles of the question\u2019s duration.\n\nTable 3: Results with the test questions (Accuracy: average percentage of days a model predicts a\nquestion correctly). Results are provided for all days a question was open and for four quartiles (Q1:\nfirst 25% of days, Q2: 25-50%, Q3: 50-75%, and Q4: last 25% of days).\n\nModel\n\nUsing Daily Forecasts Only\n\nBaselines\nMajority Vote (predictions)\nWeighted Vote (predictions)\n\nNeural Network Variants\nPredictions Only\nPredictions + Question\nPredictions + Justifications\nPredictions + Question + Justifications\n\nUsing Active Forecasts\n\nBaselines\nMajority Vote (predictions)\nWeighted Vote (predictions)\n\nNeural Network Variants\nPredictions Only\nPredictions + Question\nPredictions + Justifications\nPredictions + Question + Justifications\n\nDays When the Question Was Open\n\nAll Days\n\nQ1\n\nQ2\n\nQ3\n\nQ4\n\n71.89\n73.79\n\n77.96\n77.61\n80.23\n79.96\n\n77.27\n77.97\n\n78.81\n79.35\n80.84\n81.27\n\n64.59\n67.79\n\n66.59\n68.71\n\n73.26\n74.16\n\n82.22\n83.61\n\n77.62\n75.44\n77.87\n78.65\n\n77.93\n76.77\n78.65\n78.11\n\n78.23\n78.05\n79.26\n80.29\n\n78.61\n81.56\n84.67\n83.28\n\n68.83\n72.04\n\n73.92\n72.17\n\n77.98\n78.53\n\n87.44\n88.22\n\n77.31\n76.05\n77.86\n78.71\n\n78.04\n78.53\n79.07\n79.81\n\n78.53\n79.56\n79.74\n81.56\n\n81.11\n82.94\n86.17\n84.67\n\nDespite their relative simplicity, the baseline methods achieve commendable results, demonstrating\nthat aggregating forecaster predictions without considering the question or justifications is a viable\nstrategy. However, the full neural network achieves significantly improved results.\n\n**Using Daily or Active Forecasts** Incorporating active forecasts, rather than solely relying on\nforecasts submitted on the day the question is called, proves advantageous for both baselines and all\nneural network configurations, except for the one using only predictions and justifications.\n\n**Encoding Questions and Justifications** The neural network that only utilizes the prediction\nto represent a forecast surpasses both baseline methods. Notably, integrating the question, the\njustification, or both into the forecast representation yields further improvements. These results\nindicate that incorporating the question and forecaster-provided justifications into the model enhances\nthe accuracy of question calling.\n\n**Calling Questions Throughout Their Life** When examining the results across the four quartiles of\na question\u2019s duration, it\u2019s observed that while using active forecasts is beneficial across all quartiles\nfor both baselines and all network configurations, the neural networks surprisingly outperform the\nbaselines only in the first three quartiles. In the last quartile, the neural networks perform significantly\nworse than the baselines. This suggests that while modeling questions and justifications is generally\nhelpful, it becomes detrimental toward the end of a question\u2019s life. This phenomenon can be attributed\nto the increasing wisdom of the crowd as more evidence becomes available and more forecasters\ncontribute, making their aggregated predictions more accurate.\n\n5\n\n\fTable 4: Results with the test questions, categorized by question difficulty as determined by the best\nbaseline model. The table presents the accuracy (average percentage of days a question is predicted\ncorrectly) for all questions and for each quartile of difficulty: Q1 (easiest 25%), Q2 (25-50%), Q3\n(50-75%), and Q4 (hardest 25%).\n\nQuestion Difficulty (Based on Best Baseline)\n\nAll\n\nQ1\n\nQ2\n\nQ3\n\nQ4\n\nUsing Active Forecasts\n\nWeighted Vote Baseline (Predictions)\n\n77.97\n\n99.40\n\n99.55\n\n86.01\n\n29.30\n\nNeural Network with Components...\n\nPredictions + Question\nPredictions + Justifications\nPredictions + Question + Justifications\n\n79.35\n80.84\n81.27\n\n94.58\n95.71\n94.17\n\n88.01\n93.18\n90.11\n\n78.04\n79.99\n78.67\n\n58.73\n57.05\n64.41\n\n**Calling Questions Based on Their Difficulty** The analysis is further refined by examining",
  "results": "incorrectly calls the question. This helps to understand which questions benefit most from the neural\nnetworks that incorporate questions and justifications. However, it\u2019s important to note that calculating\nquestion difficulty during the question\u2019s active period is not feasible, making these experiments\nunrealistic before the question closes and the correct answer is revealed.\n\nTable 4 presents the results for selected models based on question difficulty. The weighted vote\nbaseline demonstrates superior performance for 75\n\n5 Qualitative Analysis\n\nThis section provides insights into the factors that make questions more difficult to forecast and\nexamines the characteristics of justifications associated with incorrect and correct predictions.\n\n**Questions** An analysis of the 88 questions in the test set revealed that questions called incorrectly\non at least one day by the best model tend to have a shorter duration (69.4 days vs. 81.7 days) and a\nhigher number of active forecasts per day (31.0 vs. 26.7). This suggests that the model\u2019s errors align\nwith the questions that forecasters also find challenging.\n\n**Justifications** A manual review of 400 justifications (200 associated with incorrect predictions\nand 200 with correct predictions) was conducted, focusing on those submitted on days when the best\nmodel made an incorrect prediction. The following observations were made:\n\n* A higher percentage of incorrect predictions (78%) were accompanied by short justifications\n(fewer than 20 tokens), compared to 65% for correct predictions. This supports the idea that longer\nuser-generated text often indicates higher quality. * References to previous forecasts (either by the\nsame or other forecasters, or the current crowd\u2019s forecast) were more common in justifications for\nincorrect predictions (31.5%) than for correct predictions (16%). * A lack of a logical argument\nwas prevalent in the justifications, regardless of the prediction\u2019s accuracy. However, it was more\nfrequent in justifications for incorrect predictions (62.5%) than for correct predictions (47.5%). *\nSurprisingly, justifications with generic arguments did not clearly differentiate between incorrect and\ncorrect predictions (16.0% vs. 14.5%). * Poor grammar and spelling or the use of non-English were\ninfrequent but more common in justifications for incorrect predictions (24.5%) compared to correct\npredictions (14.5%).",
  "conclusion": "Forecasting involves predicting future events, a capability highly valued by both governments and\nindustries as it enables them to anticipate and address potential challenges. This study focuses on\nquestions spanning the political, economic, and social domains, utilizing forecasts submitted by a\ncrowd of individuals without specialized training. Each forecast comprises a prediction and a natural\nlanguage justification.\n\n6\n\n\fThe research demonstrates that aggregating the weighted predictions of forecasters is a solid baseline\nfor calling a question throughout its duration. However, models that incorporate both the question\nand the justifications achieve significantly better results, particularly during the first three quartiles of\na question\u2019s life. Importantly, the models developed in this study do not profile individual forecasters\nor utilize any information about their identities. This work lays the groundwork for evaluating the\ncredibility of anonymous forecasts, enabling the development of robust aggregation strategies that do\nnot require tracking individual forecasters.\n\n7",
  "is_reference": true,
  "ref_category": "publishable",
  "is_publishable": 1,
  "venue": "EMNLP"
}
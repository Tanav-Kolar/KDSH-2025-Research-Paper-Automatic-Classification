{
  "title": "Detailed Action Identification in Baseball Game\nRecordings",
  "abstract": "nuanced activity recognition in baseball videos. This dataset is structured to\nsupport two types of analysis: one for classifying activities in segmented videos\nand another for detecting activities in unsegmented, continuous video streams. This\nstudy evaluates several methods for recognizing activities, focusing on how they\ncapture the temporal organization of activities in videos. This evaluation starts\nwith categorizing segmented videos and progresses to applying these methods\nto continuous video feeds. Additionally, this paper assesses the effectiveness of\ndifferent models in the challenging task of forecasting pitch velocity and type\nusing baseball broadcast videos. The findings indicate that incorporating temporal\ndynamics into models is beneficial for detailed activity recognition.",
  "introduction": "sional sporting events are extensively recorded for entertainment, and these recordings are invaluable\nfor subsequent analysis by coaches, scouts, and media analysts. While numerous game statistics\nare currently gathered manually, the potential exists for these to be replaced by computer vision\nsystems. Systems like PITCHf/x and Statcast have been employed by Major League Baseball (MLB)\nto automatically record pitch speed and movement, utilizing a network of high-speed cameras and\nradar to collect detailed data on each player. Access to much of this data is restricted from the public\ndomain.\n\nThis paper introduces MLB-YouTube, a novel dataset that includes densely annotated frames of activi-\nties extracted from broadcast baseball videos. Unlike many current datasets for activity recognition or\ndetection, our dataset emphasizes fine-grained activity recognition. The differences between activities\nare often minimal, primarily involving the movement of a single individual, with a consistent scene\nstructure across activities. The determination of activity is based on a single camera perspective. This\nstudy compares various methods for temporal feature aggregation, both for classifying activities in\nsegmented videos and for detecting them in continuous video streams.",
  "related_work": "successes were achieved with hand-engineered features such as dense trajectories. The focus of more\nrecent studies has shifted towards the application of Convolutional Neural Networks (CNNs) for\nactivity recognition. Two-stream CNN architectures utilize both spatial RGB frames and optical\nflow frames. To capture spatio-temporal characteristics, 3D XYT convolutional models have been\ndeveloped. The development of these advanced CNN models has been supported by large datasets\nsuch as Kinetics, THUMOS, and ActivityNet.\n\nSeveral studies have investigated the aggregation of temporal features for the purpose of activity\nrecognition. Research has compared several pooling techniques and determined that both Long Short-\n\n.\n\n\fTerm Memory networks (LSTMs) and max-pooling across entire videos yielded the best outcomes. It\nhas been discovered that pooling intervals from varying locations and durations is advantageous for\nactivity recognition. It was demonstrated that identifying and classifying key sub-event intervals can\nlead to better performance.\n\nRecently, segment-based 3D CNNs have been employed to capture spatio-temporal data concurrently\nfor activity detection. These methods depend on the 3D CNN to capture temporal dynamics, which\ntypically span only 16 frames. Although longer-term temporal structures have been explored, this was\nusually accomplished with temporal pooling of localized features or (spatio-)temporal convolutions\nwith extended fixed intervals. Recurrent Neural Networks (RNNs) have also been applied to represent\ntransitions in activity between frames.\n\n3 MLB-YouTube Dataset\n\nWe have compiled an extensive dataset from 20 baseball games of the 2017 MLB postseason, available\non YouTube, totaling over 42 hours of video. Our dataset includes two main parts: segmented videos\nintended for activity recognition and continuous videos designed for activity classification. The\ndataset\u2019s complexity is amplified by the fact that it originates from televised baseball games, where a\nsingle camera perspective is shared among various activities. Additionally, there is minimal variance\nin motion and appearance among different activities, such as swinging a bat versus bunting. In\ncontrast to datasets like THUMOS and ActivityNet, which encompass a broad spectrum of activities\nwith diverse settings, scales, and camera angles, our dataset features activities where a single frame\nmight not be adequate to determine the activity.\n\nThe minor differences between a ball and a strike are illustrated in Figure 3. Differentiating between\nthese actions requires identifying whether the batter swings or not, detecting the umpire\u2019s signal\n(Figure 4) for a strike, or noting the absence of a signal for a ball. This is further complicated because\nthe batter or catcher can obstruct the umpire, and each umpire has their unique style of signaling a\nstrike.\n\nOur dataset for segmented video analysis comprises 4,290 clips. Each clip is annotated for multiple\nbaseball actions, including swing, hit, ball, strike, and foul. Given that a single clip may contain\nseveral activities, this is considered a multi-label classification task. Table 1 presents the complete\nlist of activities and their respective counts within the dataset. Additionally, clips featuring a pitch\nwere annotated with the type of pitch (e.g., fastball, curveball, slider) and its speed. Furthermore, a\ncollection of 2,983 hard negative examples, where no action is present, was gathered. These instances\ninclude views of the crowd, the field, or players standing idly before or after a pitch. Examples of\nactivities and hard negatives are depicted in Figure 2.\n\nOur continuous video dataset includes 2,128 clips, each lasting between 1 and 2 minutes. Every\nframe in these videos is annotated with the baseball activities that occur. On average, each continuous\nclip contains 7.2 activities, amounting to over 15,000 activity instances in total.\n\nTable 1: Activity classes and their instance counts in the segmented MLB-YouTube dataset.\n\nActivity\n\nCount\n\nNo Activity\nBall\nStrike\nSwing\nHit\nFoul\nIn Play\nBunt\nHit by Pitch\n\n2983\n1434\n1799\n2506\n1391\n718\n679\n24\n14\n\n2\n\n\f4 Segmented Video Recognition Approach\n\nWe investigate different techniques for aggregating temporal features in segmented video activity\nrecognition. In segmented videos, the classification task is simpler because each frame corresponds to\nan activity, eliminating the need for the model to identify the start and end of activities. Our methods\nare based on a CNN that generates a per-frame or per-segment representation, derived from standard\ntwo-stream CNNs using deep CNNs like I3D or InceptionV3.\n\nGiven video features v of dimensions T \u00d7 D, where T represents the video\u2019s temporal length and D\nis the feature\u2019s dimensionality, the usual approach for feature pooling involves max- or mean-pooling\nacross the temporal dimension, followed by a fully-connected layer for video clip classification, as\ndepicted in Fig. 5(a). This approach, however, yields a single representation for the entire video,\nlosing temporal information. An alternative is to employ a fixed temporal pyramid with various\nlengths, as shown in Fig 5(b), dividing the video into intervals of lengths 1/2, 1/4, and 1/8, and\nmax-pooling each. The pooled features are concatenated, creating a K \u00d7 D representation, where K\nis the number of intervals in the temporal pyramid, and a fully-connected layer classifies the clip.\n\nWe also explore learning temporal convolution filters to aggregate local temporal structures. A kernel\nof size L\u00d71 is applied to each frame, enabling each timestep representation to incorporate information\nfrom adjacent frames. After applying max-pooling to the output of the temporal convolution, a fully-\nconnected layer is used for classification, as illustrated in Fig. 5(c).\n\nWhile temporal pyramid pooling retains some structure, the intervals are fixed and predetermined.\nPrevious studies have shown that learning the sub-interval to pool is beneficial for activity recognition.\nThese learned intervals are defined by three parameters: a center g, a width \u03c3, and a stride \u03b4,\nparameterizing N Gaussians. Given the video length T , the positions of the strided Gaussians are\nfirst calculated as:\n\ngn = 0.5 \u2212\n\nT \u2212 (gn + 1)\nN \u2212 1\n\npt,n = gn + (t \u2212 0.5T + 0.5)\n\nf orn = 0, 1, . . . , N \u2212 1\n\n1\n\u03b4\n\nf ort = 0, 1, . . . , T \u2212 1\n\nThe filters are then generated as:\n\nFm[i, t] =\n\n1\nZm\n\n(cid:18)\n\nexp\n\n\u2212\n\n(cid:19)\n\n(t \u2212 \u00b5i,m)2\n2\u03c32\nm\n\nwhere Zm is a normalization constant.\n\ni \u2208 {0, 1, . . . , N \u2212 1}, t \u2208 {0, 1, . . . , T \u2212 1}\n\nWe apply these filters F to the T \u00d7 D video representation through matrix multiplication, yielding an\nN \u00d7 D representation that serves as input to a fully-connected layer for classification. This method\nis shown in Fig 5(d).\n\nAdditionally, we compare a bi-directional LSTM with 512 hidden units, using the final hidden state\nas input to a fully-connected layer for classification. We frame our tasks as multi-label classification\nand train these models to minimize binary cross-entropy:\n(cid:88)\n\nL(v) =\n\nzc log(p(c|G(v))) + (1 \u2212 zc) log(1 \u2212 p(c|G(v)))\n\nc\n\nwhere G(v) is the function that pools the temporal information, and zc is the ground truth label for\nclass c.\n\n5 Activity Detection in Continuous Videos\n\nDetecting activities in continuous videos poses a greater challenge. The goal here is to classify each\nframe according to the activities occurring. Unlike segmented videos, continuous videos feature\nmultiple sequential activities, often interspersed with frames of inactivity. This necessitates that\nthe model learn to identify the start and end points of activities. As a baseline, we train a single\nfully-connected layer to serve as a per-frame classifier, which does not utilize temporal information\nbeyond that contained in the features.\n\n3\n\n\fWe adapt the methods developed for segmented video classification to continuous videos by imple-\nmenting a temporal sliding window approach. We select a fixed window duration of L features, apply\nmax-pooling to each window (similar to Fig. 5(a)), and classify each pooled segment. This approach\nis extended to temporal pyramid pooling by dividing the window of length L into segments of lengths\nL/2, L/4, and L/8, resulting in 14 segments per window. Max-pooling is applied to each segment,\nand the pooled features are concatenated, yielding a 14 \u00d7 D-dimensional representation for each\nwindow, which is then used as input to the classifier.\n\nFor temporal convolutional models in continuous videos, we modify the segmented video approach by\nlearning a temporal convolutional kernel of length L and convolving it with the input video features.\nThis operation transforms input of size T \u00d7 D into output of size T \u00d7 D, followed by a per-frame\nclassifier. This enables the model to aggregate local temporal information.\n\nTo extend the sub-event model to continuous videos, we follow a similar approach but set T = L in\nEq. 1, resulting in filters of length L. The T \u00d7 D video representation is convolved with the sub-event\nfilters F , producing an N \u00d7 D \u00d7 T -dimensional representation used as input to a fully-connected\nlayer for frame classification.\n\nThe model is trained to minimize per-frame binary classification:\n\nL(v) =\n\n(cid:88)\n\nt,c\n\nzt,c log(p(c|H(vt))) + (1 \u2212 zt,c) log(1 \u2212 p(c|H(vt)))\n\nwhere vt is the per-frame or per-segment feature at time t, H(vt) is the sliding window application of\none of the feature pooling methods, and zt,c is the ground truth class at time t.\n\nA method to learn \u2019super-events\u2019 (i.e., global video context) has been introduced and shown to be\neffective for activity detection in continuous videos. This approach involves learning a set of temporal\nstructure filters modeled as N Cauchy distributions. Each distribution is defined by a center xn and a\nwidth \u03b3n. Given the video length T , the filters are constructed by:\n\nxn =\n\n(T \u2212 1)(tanh(x\u2032\n\nn) + 1)\n\n2\n\nfn(t) =\n\n1\nZn\n\n\u03b3n\n\u03c0((t \u2212 xn)2 + \u03b32\nn)\n\nexp(1 \u2212 2| tanh(\u03b3\u2032\n\nn)|)\n\nwhere Zn is a normalization constant, t \u2208 {1, 2, . . . , T }, and n \u2208 {1, 2, . . . , N }.\n\nThe filters are combined with learned per-class soft-attention weights A, and the super-event repre-\nsentation is computed as:\n\n(cid:88)\n\n(cid:88)\n\nSc =\n\nAc,n\n\nfn(t) \u00b7 vt\n\nn\n\nt\n\nwhere v is the T \u00d7 D video representation. These filters enable the model to focus on relevant\nintervals for temporal context. The super-event representation is concatenated to each timestep and\nused for classification. We also experiment with combining the super- and sub-event representations\nto form a three-level hierarchy for event representation.",
  "methodology": "",
  "experiments": "Implementation Details\n\nFor our base per-segment CNN, we utilize the I3D network, pre-trained on the ImageNet and Kinetics\ndatasets. I3D has achieved state-of-the-art performance on segmented video tasks, providing a reliable\nfeature representation. We also employ a two-stream version of InceptionV3, pre-trained on Imagenet\nand Kinetics, as our base per-frame CNN for comparison. InceptionV3 was chosen for its depth\ncompared to previous two-stream CNNs. Frames were extracted at 25 fps, and TVL1 optical flow\nwas computed and clipped to [\u221220, 20]. For InceptionV3, features were computed every 3 frames\n(8 fps), while for I3D, every frame was used, with I3D having a temporal stride of 8, resulting in\n3 features per second (3 fps). Models were implemented in PyTorch and trained using the Adam\noptimizer with a learning rate of 0.01, decayed by a factor of 0.1 every 10 epochs, for a total of 50\nepochs.\n\n4\n\n\f6.2 Segmented Video Activity Recognition\n\nWe initially conducted binary pitch/non-pitch classification for each video segment. This task is\nrelatively straightforward due to the distinct differences between pitch and non-pitch frames. The",
  "results": "Table 2: Performance on segmented videos for binary pitch/non-pitch classification.\n\nModel\n\nRGB\n\nFlow Two-stream\n\nInceptionV3\nInceptionV3 + sub-events\nI3D\nI3D + sub-events\n\n97.46\n98.67\n98.64\n98.42\n\n98.44\n98.73\n98.88\n98.35\n\n98.67\n99.36\n98.70\n98.65\n\n6.2.1 Multi-label Classification\n\nWe assessed various temporal feature aggregation methods by calculating the mean average precision\n(mAP) for each video clip, a standard metric for multi-label classification. Table 4 compares the\nperformance of these methods. All methods surpass mean/max-pooling, highlighting the importance\nof preserving temporal structure for activity recognition. Fixed temporal pyramid pooling and LSTMs\nshow some improvement. Temporal convolution offers a more significant performance boost but\nrequires substantially more parameters (see Table 3). Learning sub-events, as per previous research,\nyields the best results. While LSTMs and temporal convolutions have been used before, they need\nmore parameters and perform less effectively, likely due to overfitting. Moreover, LSTMs necessitate\nsequential processing of video features, whereas other methods can be fully parallelized.\n\nTable 3: Additional parameters required for models when added to the base model (e.g., I3D or\nInception V3).\n\nModel\n\n# Parameters\n\nMax/Mean Pooling\nPyramid Pooling\nLSTM\nTemporal Conv\nSub-events\n\n16K\n115K\n10.5M\n31.5M\n36K\n\nTable 4: Mean Average Precision (mAP) results on segmented videos for multi-label classification.\nLearning sub-intervals for pooling is found to be crucial for activity recognition.\n\nMethod\n\nRGB Flow Two-stream\n\nRandom\nInceptionV3 + mean-pool\nInceptionV3 + max-pool\nInceptionV3 + pyramid\nInceptionV3 + LSTM\nInceptionV3 + temporal conv\nInceptionV3 + sub-events\nI3D + mean-pool\nI3D + max-pool\nI3D + pyramid\nI3D + LSTM\nI3D + temporal conv\nI3D + sub-events\n\n16.3\n35.6\n47.9\n49.7\n47.6\n47.2\n56.2\n42.4\n48.3\n53.2\n48.2\n52.8\n55.5\n\n16.3\n47.2\n48.6\n53.2\n55.6\n55.2\n62.5\n47.6\n53.4\n56.7\n53.1\n57.1\n61.2\n\n16.3\n45.3\n54.4\n55.3\n57.7\n56.1\n62.6\n52.7\n57.2\n58.7\n53.1\n58.4\n61.3\n\nTable 5 shows the average precision for each activity class. Learning temporal structure is particularly\nbeneficial for frame-based features (e.g., InceptionV3), which capture less temporal information\n\n5\n\n\fcompared to segment-based features (e.g., I3D). Sub-event learning significantly aids in detecting\nstrikes, hits, foul balls, and hit-by-pitch events, which exhibit changes in video features post-event.\nFor instance, after a hit, the camera often tracks the ball\u2019s trajectory, while after a hit-by-pitch, it\nfollows the player to first base, as illustrated in Fig. 6 and Fig. 7.\n\nTable 5: Per-class average precision for segmented videos using two-stream features in multi-\nlabel activity classification. Utilizing sub-events to discern temporal intervals of interest proves\nadvantageous for activity recognition.\n\nMethod\n\nRandom\nInceptionV3 + max-pool\nInceptionV3 + sub-events\nI3D + max-pool\nI3D + sub-events\n\nBall\n\n21.8\n60.2\n66.9\n59.4\n62.5\n\nStrike\n\nSwing Hit\n\nFoul\n\nIn Play Bunt Hit by Pitch\n\n28.6\n84.7\n93.9\n90.3\n91.3\n\n37.4\n85.9\n90.3\n87.7\n88.5\n\n20.9\n80.8\n90.9\n85.9\n86.5\n\n11.4\n40.3\n60.7\n48.1\n47.3\n\n10.3\n74.2\n89.7\n76.1\n75.9\n\n1.1\n10.2\n12.4\n14.3\n16.2\n\n4.5\n15.7\n29.2\n18.2\n21.0\n\n6.2.2 Pitch Speed Regression\n\nEstimating pitch speed from video frames is an exceptionally difficult problem, as it requires the\nnetwork to pinpoint the pitch\u2019s start and end, and derive the speed from a minimal signal. The baseball,\noften obscured by the pitcher, travels at speeds over 100mph and covers 60.5 feet in approximately 0.5\nseconds. Initially, with frame rates of 8fps and 3fps, only 1-2 features captured the pitch in mid-air,\nproving insufficient for speed determination. Utilizing the 60fps rate available in YouTube videos, we\nrecalculated optical flow and extracted RGB frames at this higher rate. Employing a fully-connected\nlayer with a single output for pitch speed prediction and minimizing the L1 loss between predicted\nand actual speeds, we achieved an average error of 3.6mph. Table 6 compares different models, and\nFig. 8 illustrates the sub-events learned for various speeds.\n\nTable 6: Results for pitch speed regression on segmented videos, reporting root-mean-squared errors.\n\nMethod\n\nTwo-stream\n\nI3D\nI3D + LSTM\nI3D + sub-events\nInceptionV3\nInceptionV3 + LSTM\nInceptionV3 + sub-events\n\n4.3 mph\n4.1 mph\n3.9 mph\n5.3 mph\n4.5 mph\n3.6 mph\n\n6.2.3 Pitch Type Classification\n\nWe conducted experiments to determine the feasibility of predicting pitch types from video, a task\nmade challenging by pitchers\u2019 efforts to disguise their pitches from batters and the subtle differences\nbetween pitches, such as grip and rotation. We incorporated pose data extracted using OpenPose,\nutilizing heatmaps of joint and body part locations as input to a newly trained InceptionV3 CNN.\nPose features were considered due to variations in body mechanics between different pitches. Our\ndataset includes six pitch types, with results presented in Table 7. LSTMs performed worse than the\nbaseline, likely due to overfitting, whereas learning sub-events proved beneficial. Fastballs were the\neasiest to detect (68% accuracy), followed by sliders (45%), while sinkers were the most difficult\n(12%).\n\n6.3 Continuous Video Activity Detection\n\nWe evaluate models extended for continuous videos using per-frame mean average precision (mAP),\nwith results shown in Table 8. This setting is more challenging than segmented videos, requiring\nthe model to identify activity start and end times and handle ambiguous negative examples. All\nmodels improve upon the baseline per-frame classification, confirming the importance of temporal\ninformation. Fixed temporal pyramid pooling outperforms max-pooling, while LSTM and temporal\n\n6\n\n\fTable 7: Accuracy of pitch type classification using I3D for video inputs and InceptionV3 for pose\nheatmaps.\n\nMethod\n\nAccuracy\n\nRandom\nI3D\nI3D + LSTM\nI3D + sub-events\nPose\nPose + LSTM\nPose + sub-events\n\n17.0%\n25.8%\n18.5%\n34.5%\n28.4%\n27.6%\n36.4%\n\nconvolution appear to overfit. Convolutional sub-events, especially when combined with super-event\nrepresentation, significantly enhance performance, particularly for frame-based features.\n\nTable 8: Performance on continuous videos for multi-label activity classification (per-frame mAP).\n\nMethod\n\nRGB Flow Two-stream\n\nRandom\nI3D\nI3D + max-pooling\nI3D + pyramid\nI3D + LSTM\nI3D + temporal conv\nI3D + sub-events\nI3D + super-events\nI3D + sub+super-events\nInceptionV3\nInceptionV3 + max-pooling\nInceptionV3 + pyramid\nInceptionV3 + LSTM\nInceptionV3 + temporal conv\nInceptionV3 + sub-events\nInceptionV3 + super-events\nInceptionV3 + sub+super-events\n\n13.4\n33.8\n34.9\n36.8\n36.2\n35.2\n35.5\n38.7\n38.2\n31.2\n31.8\n32.2\n32.1\n28.4\n32.1\n31.5\n34.2\n\n13.4\n35.1\n36.4\n37.5\n37.3\n38.1\n37.5\n38.6\n39.4\n31.8\n34.1\n35.1\n33.5\n34.4\n35.8\n36.2\n40.2\n\n13.4\n34.2\n36.8\n39.7\n39.4\n39.2\n38.5\n39.1\n40.4\n31.9\n35.2\n36.8\n34.1\n33.4\n37.3\n39.6\n40.9",
  "conclusion": "recognition in videos. We conduct a comparative analysis of various recognition techniques that\nemploy temporal feature pooling for both segmented and continuous videos. Our findings reveal that\nlearning sub-events to pinpoint temporal regions of interest significantly enhances performance in\nsegmented video classification. In the context of activity detection in continuous videos, we establish\nthat incorporating convolutional sub-events with a super-event representation, creating a three-level\nactivity hierarchy, yields the most favorable outcomes.\n\n7",
  "is_reference": true,
  "ref_category": "publishable",
  "is_publishable": 1,
  "venue": "CVPR"
}
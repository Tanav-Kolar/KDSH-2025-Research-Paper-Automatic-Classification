{
  "title": "Leveraging Deep Learning for Enhanced Bayesian Optimization in\nScientific Domains with Complex Structures",
  "abstract": "Bayesian optimization (BO) is a widely used technique for the global optimization of costly black-box functions.\nHowever, many real-world scenarios involve functions that are not entirely black-box. These functions may possess\nknown structures, such as symmetries, or the data generation process might be a composite one that provides\nvaluable intermediate information beyond the optimization objective\u2019s value. Traditional surrogate models used\nin BO, like Gaussian Processes (GPs), do not scale well with large datasets and struggle to incorporate known\nstructures. This paper introduces the use of Bayesian neural networks (BNNs), which are scalable and adaptable\nsurrogate models with inductive biases, to enhance BO for intricate, structured problems in high-dimensional\nspaces. We showcase the application of BO on various practical challenges in physics and chemistry. This includes\noptimizing the topology of photonic crystal materials using convolutional neural networks and refining chemical\nproperties of molecules with graph neural networks. Our findings indicate that neural networks frequently surpass\nGPs as surrogate models for BO in these complex tasks, achieving greater sampling efficiency and reduced\ncomputational expenses.",
  "introduction": "Bayesian optimization (BO) is a powerful technique for global optimization, particularly suited for expensive, derivative-free\nfunctions. It has found applications across various scientific and engineering domains, including hyperparameter tuning in machine\nlearning. BO operates by iteratively selecting the next data point to evaluate, aiming to maximize sampling efficiency and minimize\nthe number of evaluations needed to find the optimum. This is crucial when experiments or simulations are time-consuming or\nresource-intensive.\n\nIn numerous fields, the system under investigation is not a complete black box. For instance, high-dimensional input spaces like\nimages or molecules often exhibit known structures, symmetries, and invariances. Moreover, the function might be decomposable\ninto other functions, where the data collection process yields intermediate or auxiliary information that can be used to compute\nthe objective function more efficiently. Examples include scientific experiments or simulations that produce high-dimensional\nobservations or multiple measurements simultaneously, such as the optical scattering spectrum of a nanoparticle across various\nwavelengths or multiple quantum chemistry properties of a molecule from a single density functional theory (DFT) calculation.\nThese physically-informed insights into the system are valuable for designing surrogate models with appropriate inductive biases,\nbut they are often underutilized in current methods.\n\nBO relies on a surrogate model to represent a distribution over potential functions, incorporating uncertainty in its predictions.\nGaussian Processes (GPs) are commonly used as surrogate models due to their analytical tractability. However, GPs face challenges:\n(1) their inference time scales cubically with the number of observations and output dimensionality, making them less suitable for\nlarge datasets or problems with high output dimensionality without kernel approximations, and (2) they are most naturally applied to\ncontinuous, low-dimensional input spaces, requiring careful manual formulation of kernels for high-dimensional data with complex\nstructures. Consequently, encoding inductive biases can be difficult.\n\nNeural networks (NNs) and Bayesian neural networks (BNNs) have emerged as alternatives to GPs due to their scalability and\nflexibility. Another approach involves using neural networks to generate continuous latent spaces, making it easier to apply BO\nwith standard GPs. The ability of BNN architectures to incorporate various constraints, symmetries, and inductive biases opens up\npossibilities for applying BO to more complex tasks involving structured data.\n\nThis work demonstrates the application of deep learning to facilitate BO for complex, real-world scientific datasets, without relying\non pre-trained models. Specifically:\n\n\u2022 We utilize auxiliary or intermediate information to enhance BO for tasks with high-dimensional observations.\n\u2022 We apply BO to complex input spaces, including images and molecules, using convolutional and graph neural networks,\n\nrespectively.\n\n\f\u2022 We implement BO on several realistic scientific datasets, such as the optical scattering of a nanoparticle, topology\n\noptimization of a photonic crystal material, and chemical property optimization of molecules from the QM9 dataset.\n\nOur results demonstrate that neural networks can significantly outperform GPs as surrogate models on these problems. We believe\nthese strong results will generalize to other contexts, enabling the application of BO to a wider range of problems. While our\nmethods build upon existing techniques, we employ a novel combination of these methods to adapt existing BO frameworks to\nreal-world, complex applications.",
  "related_work": "Several methods have been developed to improve the scalability of GPs for larger problems. For example, one framework for\nmulti-output GPs scales linearly with the dimensionality of a low-dimensional subspace of the data. Multi-task GPs have also been\nused for BO over problems with large output dimensionalities. Furthermore, GPs have been demonstrated on very large datasets\nusing GPUs and intelligent preconditioners, or through various approximations.\n\nAnother strategy for scaling BO to larger problems involves combining it with other methods, reducing the need for the surrogate\nmodel to train on the entire dataset. For instance, one method uses a collection of independent probabilistic models in different trust\nregions, iteratively deciding where to perform BO, effectively reducing the problem to a set of local optimizations. Other methods\nbuild upon this approach and dynamically learn the partition function separating different regions.\n\nGPs have been adapted to complex problem settings to broaden the applicability of BO. For example, some approaches decompose\nsynthetic problems as a composition of other functions, leveraging the additional structure to improve BO. However, the multi-output\nGP used in these approaches scales poorly with output dimensionality, limiting their use to simpler problems. GP kernels have also\nbeen developed for complex input spaces, including convolutional and graph kernels. Graph kernels have been used to apply BO to\nneural architecture search (NAS), where the architecture and connectivity of a neural network itself can be optimized.\n\nDeep learning has been employed as a scalable and flexible surrogate model for BO. For instance, neural networks have been used\nas adaptive basis functions for Bayesian linear regression, enabling BO to scale to large datasets. This approach also allows for\ntransfer learning of the adaptive basis across multiple tasks and modeling of auxiliary signals to improve performance. Additionally,\nBayesian neural networks (BNNs) that use Hamiltonian Monte Carlo to sample the posterior have been used for single-task and\nmulti-task BO for hyperparameter optimization.\n\nA popular approach for BO in high-dimensional spaces is latent-space optimization. Here, an autoencoder, such as a VAE, is trained\non a dataset to create a continuous latent space representing the data. Then, conventional optimization algorithms, like BO with GPs,\ncan be used to optimize over this continuous latent space. This approach has been applied to tasks such as arithmetic expression\noptimization and chemical design. Note that these approaches focus on both data generation and optimization, whereas our work\nfocuses solely on the optimization process.\n\nRandom forests have also been used for iterative optimization, such as sequential model-based algorithm configuration (SMAC), as\nthey do not face scaling challenges. Tree-structured Parzen Estimators (TPE) are another popular choice for hyperparameter tuning.\nHowever, these approaches still encounter difficulties in encoding complex, structured inputs like images and graphs.\n\nDeep learning has also been applied to improve tasks other than BO. For example, active learning, similar to BO, aims to optimize a\nmodel\u2019s predictive ability with as few data points as possible. The inductive biases of neural networks have enabled active learning\non various high-dimensional data, including images, language, and partial differential equations. BNNs have also been applied to the\ncontextual bandits problem, where the model chooses between discrete actions to maximize expected reward.",
  "methodology": "3.1 Bayesian Optimization Prerequisites\n\nWe will now briefly introduce the BO methodology. We formulate our optimization task as a maximization problem, where we\naim to find the input x\u02d82217 \u02d82208 X that maximizes a function f, such that x\u02d82217 = arg maxx f(x). The input x can be a real-valued\ncontinuous vector, but it can also be generalized to categorical variables, images, or discrete objects like molecules. The function f\nreturns the objective value y = f(x), which we also refer to as the \"label\" of x, and can represent a performance metric we want to\nmaximize. In general, f can be a noisy function.\n\nA crucial component of BO is the surrogate model, which provides a distribution of predictions instead of a single point estimate.\nIdeally, these surrogate models are Bayesian, but in practice, various approximate Bayesian models or even frequentist distributions\nhave been used. In iteration N, a Bayesian surrogate model M is trained on a labeled dataset Dtrain = (xn, yn)N n=1. An acquisition\nfunction \u02d803b1 then uses M to suggest the next data point xN+1 \u02d82208 X to label, where:\n\nxN +1 = arg max\nx\u2208X\n\n\u03b1(x; M, Dtrain)\n\n(1)\n\n2\n\n\fThe new data is evaluated to obtain yN+1 = f(xN+1), and (xN+1, yN+1) is added to Dtrain.\n\n3.2 Acquisition Function\n\nA key consideration in BO is selecting the next data point xN+1 \u02d82208 X given the model M and labeled dataset Dtrain. This is\nparameterized through the acquisition function \u02d803b1, which is maximized to determine the next data point to label, as shown in\nEquation 1.\nWe utilize the expected improvement (EI) acquisition function \u02d803b1EI. When the posterior predictive distribution of the surrogate\nmodel M is a normal distribution N(\u02d800b5(x), \u02d803c32(x)), EI can be expressed analytically as:\n\n\u03b1EI (x) = \u03c3(x)[\u03b3(x)\u03a6(\u03b3(x)) + \u03d5(\u03b3(x))]\n\n(2)\n\nwhere \u02d803b3(x) = (\u02d800b5(x) \u02d82212 ybest)/\u02d803c3(x), ybest = max(ynN n=1) is the best observed objective function value so far, and \u02d803c6\nand \u02d803a6 are the PDF and CDF of the standard normal distribution N(0, 1), respectively. For surrogate models without an analytical\nform for the posterior predictive distribution, we sample from the posterior NMC times and use a Monte Carlo (MC) approximation\nof EI:\n\n\u03b1M C\n\nEI (x) \u2248\n\n1\nNM C\n\nNM C(cid:88)\n\ni=1\n\nmax(\u00b5(i)(x) \u2212 ybest, 0)\n\n(3)\n\nwhere \u02d800b5(i) is a prediction sampled from the posterior of M. While some works fit the surrogate model\u2019s output to a Gaussian to\nuse Equation 2 for acquisition, this is not valid when the model prediction for y is not Gaussian, which is generally the case for\ncomposite functions (see Section 2.4).\n\nEI has advantages over other acquisition functions because the MC approximation (1) remains differentiable, facilitating optimization\nof the acquisition function in the inner loop (unlike the MC approximation of upper confidence bound (UCB), which is not\ndifferentiable and can result in ties), and (2) is inexpensive (unlike naive Thompson sampling for ensembles, which would require\nretraining a model from scratch in each iteration).\n\n3.3 Continued Training with Learning Rate Annealing\n\nA challenge in BO is the computational cost of training a surrogate model on Dtrain from scratch in every optimization loop,\nespecially since neural networks ideally require extensive training until convergence. To reduce the training time of BNNs in each\noptimization loop, we use the model trained in the Nth optimization loop iteration as the initialization (a \"warm start\") for the\n(N+1)th iteration, rather than starting from a random initialization. Specifically, we employ the cosine annealing learning rate, which\nstarts with a high learning rate and gradually reduces it to 0. For more details, refer to Section A.3 in the Appendix.\n\n3.4 Auxiliary Information\n\nTypically, we assume f is a black-box function, so we train M : X \u02d82192 Y to model f. Here, we consider scenarios where the\nexperiment or observation may provide intermediate or auxiliary information z \u02d82208 Z, such that f can be decomposed as:\n\nwhere g : X \u02d82192 Z is the expensive labeling process, and h : Z \u02d82192 Y is a known objective function that can be computed cheaply.\nThis is also known as \"composite functions\". In this case, we train M : X \u02d82192 Z to model g, and the approximate EI acquisition\nfunction becomes:\n\nf (x) = h(g(x))\n\n(4)\n\n\u03b1M C\u2212aux\n\nEI\n\n(x) \u2248\n\n1\nNM C\n\nNM C(cid:88)\n\ni=1\n\nmax(h(\u00b5(i)(x)) \u2212 ybest, 0)\n\n(5)\n\nwhich can be seen as a Monte Carlo version of the acquisition function presented in prior work. We denote models trained using\nauxiliary information with the suffix \"-aux.\" Because h is not necessarily linear, h(\u02d800b5(i)(x)) is not generally Gaussian even if\n\u02d800b5(i) itself may be, making the MC approximation convenient or even necessary.\n\n3\n\n\f4 Surrogate Models\n\nBayesian models capture uncertainty associated with both data and model parameters in the form of probability distributions. This\nis achieved by placing a prior probability distribution P(\u02d803b8) on the model parameters and calculating the posterior belief of the\nparameters using Bayes\u2019 theorem after observing new data. Fully Bayesian neural networks have been studied in small architectures\nbut are impractical for realistically sized neural networks, as nonlinearities between layers make the posterior intractable, requiring\nMCMC methods to sample the posterior. However, in the last decade, numerous proposals for approximate Bayesian neural networks\nhave emerged, capable of capturing some Bayesian properties and producing a predictive probability distribution. In this work, we\ncompare several different options for the BNN surrogate model, along with other non-BNN baselines. We list some notable models\nhere, with model details and results in Section A.4.1 of the Appendix.\n\nEnsembles combine multiple models to improve predictive performance by averaging their results. Ensembles of neural networks\nhave been reported to be more robust than other BNNs, and we use \"Ensemble\" to denote an ensemble of neural networks with\nidentical architectures but different random initializations, providing enough variation for individual models to give different\npredictions. Using individual models can be interpreted as sampling from a posterior distribution, so we use Equation 5 for\nacquisition. Our ensemble size is NMC = 10.\n\nOther BNNs: We also compare to variational BNNs, including Bayes by Backprop (BBB) and Multiplicative Normalizing Flows\n(MNF); BOHAMIANN; and NeuralLinear. For BBB, we also experiment with KL annealing, denoted by \"-Anneal.\"\n\nGP Baselines: GPs are largely defined by their kernel (also called \"covariance functions\"), which determines the prior and posterior\ndistributions, how different data points relate to each other, and the type of data the GP can operate on. In this work, \"GP\" refers\nto a standard specification using a Mat\u02d800e9rn 5/2 kernel, a popular kernel for real-valued continuous spaces. For images, we use\na convolutional kernel, labeled as \"ConvGP\", implemented using the infinite-width limit of a convolutional neural network. For\ngraphs, we use the Weisfeiler-Lehman (WL) kernel, labeled as \"GraphGP\", which can operate on undirected graphs with node and\nedge features, making it suitable for chemical molecule graphs. We also compare against \"GP-aux,\" which uses multi-output GPs\nfor problems with auxiliary information (composite functions). In the Appendix, we also examine GPs using infinite-width and\ninfinite-ensemble neural network limits as kernels, as well as TuRBO, which combines GP-based BO with trust regions.\n\nVAE-GP uses a VAE trained beforehand on an unlabeled dataset representative of X. This allows us to encode complex input spaces,\nsuch as chemical molecules, into a continuous latent space where conventional GP-based BO methods can be applied, even enabling\nthe generation and discovery of novel molecules not in the original dataset. Here, we modified an existing implementation that uses\na junction tree VAE (JTVAE) to encode chemical molecules. More details can be found in the Appendix.\n\nOther Baselines: We compare against two variations of Bayesian optimization, TuRBO and TPE. We also compare against several\nglobal optimization algorithms that do not use surrogate models and are computationally inexpensive, including LIPO, DIRECT-L,\nand CMA-ES.\n\nWe emphasize that ensembles and variational methods can easily scale to high-dimensional outputs with minimal increase in\ncomputational cost by simply changing the output layer size. Neural Linear and GPs scale cubically with output dimensionality\n(without covariance approximations), making them difficult to train on high-dimensional auxiliary or intermediate information.",
  "experiments": "",
  "results": "We now examine three real-world scientific optimization tasks, all of which provide intermediate or auxiliary information that can be\nleveraged. In the latter two tasks, the structure of the data also becomes important, and hence BNNs with various inductive biases\nsignificantly outperform GPs and other baselines. For simplicity, we highlight results from select architectures (see Appendix for\nfull results, dataset, and hyperparameter details). All BO results are averaged over multiple trials, and the shaded area in the plots\nrepresents \u02d800b1 one standard error over the trials.\n\n5.1 Multilayer Nanoparticle\n\nWe first consider the problem of light scattering from a multilayer nanoparticle, which has various applications requiring a tailored\noptical response, including biological imaging, improved solar cell efficiency, and catalytic materials. The nanoparticle we consider\nconsists of a lossless silica core and 5 spherical shells of alternating TiO2 and silica. The nanoparticle is parameterized by the core\nradius and layer thicknesses, which we restrict to the range of 30 nm to 70 nm. Due to the nanoparticle\u2019s size being on the order of\nthe wavelength of light, its optical properties can be tuned by adjusting the number and thicknesses of the layers. The scattering\nspectrum can be calculated semi-analytically, as detailed in Section A.1.1 of the Appendix.\n\nOur goal is to optimize the scattering cross-section spectrum over a range of visible wavelengths. We compare two different objective\nfunctions: the narrowband objective, which aims to maximize scattering in the small wavelength range of 600 nm to 640 nm and\nminimize it elsewhere, and the highpass objective, which aims to maximize scattering above 600 nm and minimize it elsewhere.\nWhile conventional GPs are trained using the objective function as the label directly, BNNs with auxiliary information can be trained\nto predict the full scattering spectrum (the auxiliary information z \u02d82208 R201), which is then used to calculate the objective function.\n\n4\n\n\fThe BO results are presented in Figure 2. The addition of auxiliary information significantly improves BO performance for BNNs.\nThey are also competitive with GPs, making BNNs a viable approach for scaling BO to large datasets. In Appendix A.5, we observe\nsimilar trends for other types of BNNs. Due to the poor scaling of multi-output GPs with respect to output dimensionality, we can\nonly run GP-aux for a limited number of iterations within a reasonable time frame. Within these few iterations, GP-aux performs\npoorly, only slightly better than random sampling. We also find in the Appendix that BO with either GPs or BNNs is comparable\nwith or outperforms other global optimization algorithms, including DIRECT-L and CMA-ES.\n\n5.2 Photonic Crystal Topology\n\nNext, we examine a more complex, high-dimensional domain with symmetries that are not easily exploited by GPs. Photonic\ncrystals (PCs) are nanostructured materials engineered to exhibit unique optical properties not found in bulk materials, such as\nphotonic band gaps, negative refractive index, and angular selective transparency. With advancements in fabrication techniques\nenabling smaller feature sizes, there is growing interest in inverse design and topology optimization to design more sophisticated\nPCs for applications in photonic integrated circuits, flat lenses, and sensors.\nHere, we consider 2D PCs consisting of periodic unit cells represented by a 32 \u02d800d7 32 pixel image, with white and black regions\nrepresenting vacuum (or air) and silicon, respectively. Optimizing over raw pixel values may lead to pixel-sized features or\nintermediate pixel values that are not physically realizable. Therefore, we parameterize the PCs with a level-set function \u02d803c6 : X\n\u02d82192 V that converts a 51-dimensional feature vector x = [c1, c2, ..., c50, \u02d82206] \u02d82208 R51, representing the level-set parameters, into\nan image v \u02d82208 R32\u02d800d732 representing the PC. More details can be found in Section A.1.2 of the Appendix.\nWe test BO on two different data distributions, PC-A and PC-B. In the PC-A distribution, x spans ci \u02d82208 [\u02d822121, 1], \u02d82206 \u02d82208\n[\u02d822123, 3]. In the PC-B distribution, we arbitrarily restrict the domain to ci \u02d82208 [0, 1]. The PC-A data distribution is translation\ninvariant, meaning that any PC with a translational shift will also be in the data distribution. However, the PC-B data distribution is\nnot translation invariant.\n\nThe optical properties of PCs can be characterized by their photonic density of states (DOS). We choose an objective function that\naims to minimize the DOS in a certain frequency range while maximizing it elsewhere, corresponding to opening up a photonic band\ngap in that frequency range. We train GPs directly on the level-set parameters X, whereas we train the Bayesian convolutional NNs\n(BCNNs) on the more natural unit cell image space V. BCNNs can also be trained to predict the full DOS as auxiliary information z\n\u02d82208 R500.\n\nThe BO results, shown in Figure 4(a), demonstrate that BCNNs outperform GPs by a significant margin on both datasets. This\nis due to both the auxiliary information and the inductive bias of the convolutional layers, as shown in Figure 4(b). Because the\nbehavior of PCs is determined by their topology rather than individual pixel values or level-set parameters, BCNNs are much better\nsuited to analyze this dataset compared to GPs. Additionally, BCNNs can be made much more data-efficient since they directly\nencode translation invariance and thus learn the behavior of a whole class of translated images from a single image. Because\nGP-aux is extremely expensive compared to GP (500\u02d800d7 longer on this dataset), we are only able to run GP-aux for a small\nnumber of iterations, where it performs comparably to random sampling. We also compare to GPs using a convolutional kernel\n(\u02d8201cConvGP-NNGP\u02d8201d) in Figure 4(a). ConvGP-NNGP only performs slightly better than random sampling, likely due to a lack\nof auxiliary information and inflexibility to learn the most suitable representation for this dataset.\n\nFor our main experiments with BCNNs, we use an architecture that respects translation invariance. To demonstrate the effect\nof another commonly used deep learning training technique, we also experiment with incorporating translation invariance into a\ntranslation-dependent architecture using a data augmentation scheme in which each image is randomly translated, flipped, and\nrotated during training. We expect data augmentation to improve performance when the data distribution exhibits the corresponding\nsymmetries. As shown in Figure 4(c), we indeed find that data augmentation improves the BO performance of the translation-\ndependent architecture when trained on the translation-invariant PC-A dataset, even matching the performance of a translation-\ninvariant architecture on PC-A. However, on the translation-dependent PC-B dataset, data augmentation initially hurts the BO\nperformance of the translation-dependent architecture because the model is unable to quickly specialize to the more compact\ndistribution of PC-B, putting its BO performance more on par with models trained on PC-A. These results show that techniques used\nto improve generalization performance (such as data augmentation or invariant architectures) for training deep learning architectures\ncan also be applied to BO surrogate models and, when used appropriately, directly translate into improved BO performance. Note\nthat data augmentation would not be feasible for GPs without a hand-crafted kernel, as the increased size of the dataset would cause\ninference to become computationally intractable.\n\n5.3 Organic Molecule Quantum Chemistry\n\nFinally, we optimize the chemical properties of molecules. Chemical optimization is of significant interest in both academia and\nindustry, with applications in drug design and materials optimization. This is a difficult problem where computational approaches\nsuch as density functional theory (DFT) can take days for simple molecules and are intractable for larger molecules; synthesis is\nexpensive and time-consuming, and the space of synthesizable molecules is large and complex. There have been many approaches\nto molecular optimization that largely revolve around finding a continuous latent space of molecules or hand-crafting kernels to\noperate on molecules.\n\n5\n\n\fHere, we focus on the QM9 dataset, which consists of 133,885 small organic molecules along with their geometric, electronic,\nand thermodynamic quantities calculated with DFT. Instead of optimizing over a continuous space, we draw from the fixed pool\nof available molecules and iteratively select the next molecule to add to Dtrain. This is a problem setting especially common to\nmaterials design, where databases are incomplete and the space of experimentally feasible materials is small.\n\nWe use a Bayesian graph neural network (BGNN) for our surrogate model, as GNNs have become popular for chemistry applications\ndue to the natural encoding of a molecule as a graph with atoms and bonds as nodes and edges, respectively. For baselines that\noperate over continuous spaces (i.e., GPs and simple neural networks), we use the Smooth Overlap of Atomic Positions (SOAP)\ndescriptor to produce a fixed-length feature vector for each molecule.\nWe compare two different optimization objectives derived from the QM9 dataset: the isotropic polarizability \u02d803b1 and (\u02d803b5LUMO\n\u02d82212 \u02d820acg\u201e) where \u02d820acg,;, is the HOMO-LUMO energy gap. Other objectives are included in the Appendix. Because many of the\nchemical properties in the QM9 dataset can be collectively computed by a single DFT or molecular dynamics calculation, we can\ntreat a group of labels from QM9 as auxiliary information z and train our BGNN to predict this entire group simultaneously. The\nobjective function h then simply picks out the property of interest.\n\nAs shown in Figure 5(c), GraphGP and the BGNN variants significantly outperform GPs, showing that the inductive bias in the graph\nstructure leads to a much more natural representation of the molecule and its properties. In the case of maximizing the polarizability\n\u02d803b1, including the auxiliary information improves BO performance, showing signs of positive transfer. However, it does not have a\nsignificant impact on the other objectives, which may be due to the small size of the available auxiliary information (only a handful\nof chemical properties from the QM dataset) compared with the nanoparticle and photonic crystal tasks. In a more realistic online\nsetting, we would have significantly more physically informative information available from a DFT calculation, e.g., we could easily\ncompute the electronic density of states (the electronic analogue of the auxiliary information used in the photonics task).\nAs seen in Figure 5(d), we also note that the GraphGP is relatively computationally expensive (15\u02d800d7 longer than GPs for small N\nand 800\u02d800d7 longer than BGNNs for N = 100) and so we are only able to run it for a limited N in a reasonable time frame. We see\nthat BGNNs perform comparably or better than GraphGPs despite incurring a fraction of the computational cost.\n\nVAE-GP uses a modified version of the latent-space optimization method implementation provided by Tripp et al. (2020). Rather\nthan optimizing over a continuous latent space of the VAE, we feed the data pool through the VAE encoder to find their latent space\nrepresentation and then apply the acquisition function to the latent points to pick out the best unlabeled point to sample. We keep as\nmany hyperparameters the same as the original implementation as possible, except for the weighted retraining, which we forgo\nsince we have a fixed data pool that was used to train the VAE. This setup is similar to GraphNeuralLinear in that a deep learning\narchitecture is used to encode the molecule as a continuous vector, although GraphNeuralLinear is only trained on the labeled data.\nThe results for this experiment show that VAE-GP performs worse than BNNs on two of the three objective functions we tested and\nslightly better on one objective. We also note that the performance of VAE-GP depends very heavily on the pre-training of the VAE,\nas choosing different hyperparameters or even a different random seed can significantly deteriorate performance (see Figure 15 in\nthe Appendix).\n\n6 Discussion\n\nIntroducing physics-informed priors (in the form of inductive biases) into the model is critical for performance. Well-known\ninductive biases in deep learning include convolutional and graph neural networks for images and graph structures, respectively,\nwhich significantly improve BO performance. Another inductive bias we introduce is the addition of auxiliary information present\nin composite functions, which significantly improves the performance of BO for the nanoparticle and photonic crystal tasks. We\nconjecture that the additional information forces the BNN to learn a more consistent physical model of the system since it must\nlearn features shared across the multi-dimensional auxiliary information, thus enabling the BNN to generalize better. For example,\nthe scattering spectrum of the multilayer particle consists of multiple resonances (sharp peaks), the width and location of which\nare determined by the material properties and layer thicknesses. The BNN could potentially learn these more abstract features,\nand thus the deeper physics, to help it interpolate more efficiently, akin to data augmentation. Auxiliary information can also be\ninterpreted as a form of data augmentation. Indeed, tracking the prediction error on a validation set shows that models with auxiliary\ninformation tend to have a lower loss than those without (see Appendix A.5). It is also possible that the loss landscape for the\nauxiliary information is smoother than that of the objective function and that the auxiliary information acts as implicit regularization\nthat improves generalization performance.\n\nInterestingly, GP-aux performs extremely poorly on the nanoparticle and photonic crystal tasks. One possible reason is that we are\nonly able to run GP-aux for a few iterations, and it is not uncommon for GP-based BO to require some critical number of iterations\nto reach convergence, especially in high-dimensional systems where the size of the covariance matrix scales with the square of the\ndimensionality. It may also be possible that GP-aux only works on certain types of function decompositions and cannot be broadly\napplied to all composite functions, as the inductive biases in GPs are often hard-coded.\n\nThere is an interesting connection between how well BNNs are able to capture and explore a multi-modal posterior distribution and\ntheir performance in BO. For example, we have noticed that larger batch sizes tend to significantly hurt BO performance. On the one\nhand, larger batch sizes may result in poorer generalization as the model finds sharper local minima in the loss landscape. Another\nexplanation is that the stochasticity inherent in smaller batch sizes allows the BNN to more easily explore the posterior distribution,\n\n6\n\n\fwhich is known to be highly multi-modal. Indeed, BO often underperforms for very small dataset sizes N but quickly catches up as\nN increases, indicating that batch size is an important hyperparameter that must be balanced with computational cost.\n\nAll our results use continued training (or warm restart) to minimize training costs. We note that re-initializing M and training from\nscratch in every iteration performs better than continued training on some tasks (results in the Appendix), which points to how BNNs\nmay not sufficiently represent a multi-modal posterior distribution or that continued training may skew the training distribution that\nthe BNN sees. Future work will consider using stochastic training approaches such as SG-MCMC methods for exploring posterior\ndistributions, as well as other continual learning techniques to further minimize training costs, especially for larger datasets.\n\nWhen comparing BNN architectures, we find that ensembles tend to consistently perform among the best, which is supported by\nprevious literature showing that ensembles capture uncertainty much better than variational methods, especially in multi-modal loss\nlandscapes. Ensembles are also attractive because they require no additional hyperparameters and are simple to implement. Although\ntraining costs increase linearly with the size of the ensemble, this can be easily parallelized on modern computing infrastructures.\nFurthermore, recent work that aims to model efficient ensembles that minimize computational cost could be an interesting future\ndirection. NeuralLinear variants are also quite powerful and cheap, making them very promising for tasks without high-dimensional\nauxiliary information. Integrating Neural Linear with multi-output GPs is an interesting direction for future work. The other BNNs\neither require extensive hyperparameter tuning or perform poorly, making them difficult to use in practice. Additional discussion can\nbe found in Appendix A.5.5.\n\nAs seen in Appendix A.5.4, VAE-GP performs worse than our method on two of the chemistry objectives and better on one objective.\nWhile latent-space optimization methods are often applied to domains where one wants to simultaneously generate data and optimize\nover the data distribution, these methods can also be applied to the cases in this work, where a data pool (e.g., QM9 dataset for\nthe chemistry task) or separate data generation process (e.g., level-set process for the photonic crystal task) is already available. In\nthese cases, the VAE is not used as a generative model but rather as a way to learn appropriate representations. While latent-space\napproaches can take advantage of well-developed and widely available optimization algorithms, they also require unsupervised\npre-training on a sizable dataset and a suitable autoencoder model with the necessary inductive biases. Such models are available in\nchemistry, where there has been significant development, but are more limited in other domains such as photonics. On the other\nhand, our method can incorporate the data structure or domain knowledge in an end-to-end manner during training, although future\nwork is needed to evaluate more carefully how much of an advantage this is and whether it depends on specific dataset or domain\ncharacteristics. For settings where we do not need a generative model, it would also be interesting to replace the autoencoder with a\nself-supervised model or semi-supervised model to create a suitable latent space.",
  "conclusion": "We have demonstrated global optimization on multiple tasks using a combination of deep learning and BO. In particular, we have\nshown how BNNs can be used as surrogate models in BO, enabling the scaling of BO to large datasets and providing the flexibility to\nincorporate a wide variety of constraints, data augmentation techniques, and inductive biases. We have demonstrated that integrating\ndomain knowledge on the structure and symmetries of the data into the surrogate model, as well as exploiting intermediate or\nauxiliary information, significantly improves BO performance, all of which can be interpreted as physics-informed priors. Intuitively,\nproviding the BNN surrogate model with all available information allows the BNN to learn a more faithful physical model of the\nsystem of interest, thus enhancing the performance of BO. Finally, we have applied BO to real-world, high-dimensional scientific\ndatasets, and our results show that BNNs can outperform our best-effort GPs, even with strong domain-dependent structure encoded\nin the covariance functions. We note that our method is not necessarily tied to any particular application domain and can lower the\nbarrier of entry for design and optimization.\n\nFuture work will investigate more complex BNN architectures with stronger inductive biases. For example, output constraints can be\nplaced through unsupervised learning or by variationally fitting a BNN prior. Custom architectures have also been proposed for\npartial differential equations, many-body systems, and generalized symmetries, which will enable effective BO on a wider range of\ntasks. The methods and experiments presented here enable BO to be effectively applied in a wider variety of settings. There are also\nvariants of BO, including TuRBO, which perform extremely well on our tasks, and so future work will also include incorporating\nBNNs into these variants.\n\n8 Appendix\n\n8.1 Datasets\n\nThe dimensionalities of the datasets are summarized in Table 1. The continuous input dimension for chemical molecules refers\nto the SOAP descriptor. While the space of chemical molecule graphs in general does not have a well-defined dimensionality as\nchemical molecules can be arbitrarily large and complex, we limit the size of molecules by only sampling from the QM9 dataset,\nand can define the dimensionality as the sum of the adjacency, node, and edge matrix dimensionalities.\n\nThe high dimensionalities of all of these problems make Bayesian neural networks well-suited as surrogate models to enable scaling.\nNote that the nanoparticle scattering problem can be adjusted to be less or more difficult by either changing the input dimensionality\n(i.e. the number of nanoparticle layers) or the auxiliary dimension (i.e. the resolution or range of wavelengths that are sampled).\n\n7\n\n\fTable 1: Summary of dataset dimensionalities. Note that alternate inputs for photonic crystal and organic molecule datasets are\nbinary images and molecule graphs, respectively.\n\nCONTINUOUS INPUT\nDIMENSION\n\nALTERNATE INPUT\nDIMENSION\n\nAUXILIARY\nDIMENSION\n\nNANOPARTICLE SCATTERING\nPHOTONIC CRYSTAL DOS\nMOLECULE QUANTUM CHEMISTRY\n\n6\n51\n480\n\nN/A\n32 x 32 = 1024\n9 + 9 \u02d800d7 9 + 9 \u02d800d7 9 = 171\n\n201\n500\n9\n\n8.2 Nanoparticle Scattering\n\nThe multilayer nanoparticle consists of a lossless silica core surrounded by alternating spherical layers of lossless TiO2 and lossless\nsilica. The relative permittivity of silica is \u02d803b5silica = 2.04. The relative permittivity of TiO2 is dispersive and depends on the\nwavelength of light:\n\n\u03b5T iO2 = 5.913 +\n\n0.2441\n\u03bb2 \u2212 0.0803\n\n(6)\n\nwhere \u02d803bb is the wavelength given in units of nm. The entire particle is surrounded by water, which has a relative permittivity of\n\u02d803b5water = 1.77.\nFor a given set of thicknesses, we analytically solve for the scattering spectrum, i.e. the scattering cross-section \u02d803c3(\u02d803bb) as a\nfunction of wavelength \u02d803bb, using Mie scattering. The code for computing \u02d803c3 was adapted from existing work.\n\nThe objective functions for the narrowband and highpass objectives are:\n\nhnb(z) =\n\n(cid:82)\n(cid:82)\n\n\u03bb\u2208nb \u03c3(\u03bb)d\u03bb\n\u03bb /\u2208nb \u03c3(\u03bb)d\u03bb\n\n\u2248\n\n(cid:80)145\ni=126 zi\ni=1 zi + (cid:80)201\n\n(cid:80)125\n\ni=146 zi\n\nhhp(z) =\n\n(cid:82)\n\u03bb\u2208hp \u03c3(\u03bb)d\u03bb\n(cid:82)\n\u03bb /\u2208hp \u03c3(\u03bb)d\u03bb\n\n\u2248\n\n(cid:80)201\n\ni=126 zi\ni=1 zi\n\n(cid:80)125\n\n(7)\n\n(8)\n\nwhere z \u02d82208 R201 is the discretized scattering cross-section \u02d803c3(\u02d803bb) from \u02d803bb = 350 nm to 750 nm.\n\n8.3 Photonic Crystal\n\nThe photonic crystal (PC) consists of periodic unit cells with periodicity a = 1 au, where each unit cell is depicted as a \u02d8201ctwo-\ntone\u02d8201d image, with the white regions representing silicon with permittivity \u02d803b51 = 11.4 and black regions representing vacuum\n(or air) with permittivity \u02d803b50 = 1.\nThe photonic crystal (PC) structure is defined by a spatially varying permittivity \u02d803b5(x, y) \u02d82208 \u02d803b50, \u02d803b51 over a 2D periodic\nunit cell with spatial coordinates x, y \u02d82208 [0, a]. To parameterize \u02d803b5, we choose a level set of a Fourier sum function \u02d803c6,\ndefined as a linear combination of plane waves with frequencies evenly spaced in the reciprocal lattice space up to a maximum cutoff.\nIntuitively, the upper limit on the frequencies roughly corresponds to a lower limit on the feature size such that the photonic crystal\nremains within reasonable fabrication constraints. Here we set the cutoff such that there are 25 complex frequencies corresponding\nto 50 real coefficients c = (c1, c2, ..., c50).\n\nExplicitly, we have\n\n\u03d5[c](x, y) = \u211c\n\n(cid:32) 25\n(cid:88)\n\n(ck + ick+25)e2\u03c0i(nxx+nyy)/a\n\n(cid:33)\n\nk=1\n\n(9)\n\nwhere each exponential term is composed from the 25 different pairs nx, ny with nx, ny \u02d82208 \u02d822122, \u02d822121, 0, 1, 2. We then choose\na level-set offset \u02d82206 to determine the PC structure, where regions with \u02d803c6 > \u02d82206 are assigned to be silicon and regions where\n\u02d803c6 \u02d82264 \u02d82206 are vacuum. Thus, the photonic crystal unit cell topology is parameterized by a 51-dimensional vector, [c1, c2, ...,\nc50, \u02d82206] \u02d82208 R51. More specifically,\n\n\u03b5(x, y) = \u03b5[c, \u2206](x, y) = { \u03b5 1 \u03d5[c](x, y) > \u2206\u03b50\u03d5[c](x, y) \u2264 \u2206\n\n(10)\n\n8\n\n\fwhich is discretized to result in a 32 \u02d800d7 32 pixel image v \u02d82208 \u02d803b50, \u02d803b5132\u02d800d732. This formulation also has the advantage of\nenforcing periodic boundary conditions.\nFor each unit cell, we use the MIT Photonics Bands (MPB) software to compute the band structure of the photonic crystal, \u02d803c9(k),\nup to the lowest 10 bands, using a 32 \u02d800d7 32 spatial resolution (or equivalently, 32 \u02d800d7 32 k-points over the Brillouin zone\n\u02d82212 \u02d803c0 a < k < \u02d803c0 a ). We also extract the group velocities at each k-point and compute the density-of-states (DOS) via an\nextrapolative technique. The DOS is computed at a resolution of 20,000 points, and a Gaussian filter of kernel size 100 is used to\nsmooth the DOS spectrum. To normalize the frequency scale across the different unit cells, the frequency is rescaled via \u02d803c9 \u02d82192\n\u02d803c9norm, where \u02d803b5avg is the average permittivity over all pixels. Finally, the DOS spectrum is truncated at \u02d803c9norm = 1.2 and\ninterpolated using 500 points to give z \u02d82208 R500.\n\nThe objective function aims to minimize the DOS in a small frequency range and maximize it elsewhere. We use the following:\n\nhDOS(z) =\n\n300\n(cid:88)\n\ni=1\n\nzi +\n\n1\ni=351 zi + 1\n\n(cid:80)500\n\n(11)\n\nwhere the 1 is added in the denominator to avoid singular values.\n\n8.4 Organic Molecule Quantum Chemistry\n\nThe Smooth Overlap of Atomic Positions (SOAP) descriptor uses smoothed atomic densities to describe local environments for each\natom in the molecule through a fixed-length feature vector, which can then be averaged over all the atoms in the molecule to produce\na fixed-length feature vector for the molecule. This descriptor is invariant to translations, rotations, and permutations. We use the\nSOAP descriptor implemented by DScribe using the parameters: local cutoff rcut = 5, number of radial basis functions nmax =\n3, and maximum degree of spherical harmonics lmax = 3. We use outer averaging, which averages over the power spectrum of\ndifferent sites.\n\nThe graph representation of each molecule is processed by the Spektral package. Each graph is represented by a node feature matrix\nX \u02d82208 Rs\u02d800d7dn, an adjacency matrix A \u02d82208 Rs\u02d800d7s, and an edge matrix E \u02d82208 Re\u02d800d7de, where s is the number of atoms in\nthe molecule, e is the number of bonds, and dn, de are the number of features for nodes and edges, respectively.\n\nThe properties that we use from the QM9 dataset are listed in Table 2. We separate these properties into two categories: (1) the\nground state quantities which are calculated from a single DFT calculation of the molecule and include geometric, energetic, and\nelectronic quantities, and (2) the thermodynamic quantities which are typically calculated from a molecular dynamics simulation.\n\nTable 2: List of properties from the QM9 dataset used as labels\n\nProperty Unit\n\nDescription\n\nGround State Quantities\n\nA\nB\nC\n\u00b5\n\u03b1\n\u03f5HOM O\n\u03f5LUM O\n\u2206\u03f5\n\u27e8R2\u27e9\n\nGHz\nGHz\nGHz\nD\na3\n0\nHa\nHa\nHa\na2\n0\n\nRotational constant\nRotational constant\nRotational constant\nDipole moment\nIsotropic polarizability\nEnergy of HOMO\nEnergy of LUMO\nGap (\u03f5LUM O \u2212 \u03f5HOM O)\nElectronic spatial extent\n\nThermodynamic Quantities at 298.15 K\n\nU0\nU\nH\nG\ncv\n\nInternal energy at 0 K\nInternal energy at 298.15 K\nEnthalpy at 298.15 K\nFree energy at 298.15 K\n\nHa\nHa\nHa\nHa\ncal\nmolK Heat capacity at 298.15 K\n\nThe auxiliary information for this task consists of the properties listed in Table 2 that are in the same category as the objective\nproperty, as these properties would be calculated together. The objective function then simply picks out the corresponding feature\nfrom the auxiliary information. More precisely, for the ground state objectives, the auxiliary information is:\n\nz = [A, B, C, \u00b5, \u03b1, \u03f5HOM O, \u03f5LUM O, \u03f5gap, < R2 >] \u2208 R9\n\n(12)\n\n9\n\n\fand the objective functions are:\n\nh\u03b1(z) =\n\nh\u03f5gap (z) =\n\nz5\n25\n\nz8\n0.6\n\n\u2212 6\n\n\u2212 0.02\n\n(13)\n\n(14)\n\nwhere the quantities for the latter objective are normalized so that they have the same magnitude.\n\n8.5 Bayesian Optimization and Acquisition Function\n\nOur algorithm for Bayesian optimization using auxiliary information z is shown in Algorithm 1. This algorithm reduces to the basic\nBO algorithm in the case where h is the identity function and Z = Y such that we can ignore mention of z in Algorithm 1.\n\nAlgorithm 1 Bayesian optimization with auxiliary information\n\n1:\n2:\n3:\n4:\n5:\n6:\n7:\n\nInput: Labelled dataset Dtrain = {(xn, zn, yn)}Nstart=5\nfor N = 5 to 1000 do\n\nn=1\n\nTrain M : X \u2192 Z on Dtrain\nForm an unlabelled dataset, Xpool\nFind xN +1 = arg maxx\u2208Xpool \u03b1(x; M, Dtrain)\nLabel the data zN +1 = g(xN +1), yN +1 = h(zN +1)\nDtrain = Dtrain \u222a (xN +1, zN +1, yN +1)\n\nend for\n\nAs mentioned in the main text, the inner optimization loop in line 5 of Algorithm 1 is performed by finding the maximum value\nof \u02d803b1 over a pool of |Xpool| randomly sampled points. We can see in Figure 6 that increasing |Xpool| in the acquisition step\ntends to improve BO performance. Thus, there is likely further room for improvement of the inner optimization loop using more\nsophisticated algorithms, possibly using the gradient information provided by BNNs. Unless otherwise stated, we optimize the inner\nloop of Bayesian optimization to choose the next data point to label by maximizing EI on a pool of |Xpool| = 105 randomly sampled\npoints.\n\nFigure 1: Effect of m = |Xpool| used in the inner optimization loop to maximize the acquisition function on overall BO performance.\nybest is taken from the narrowband objective function using the ensemble architecture. The \u02d8201caux\u02d8201d in the legend denotes\nusing auxiliary information and the numbers represent the architecture (i.e. 8 layers of 256 units or 16 layers of 512 units).\n\n[width=0.5]figures/figure6.png\n\n8.6 Continued Training\n\nAs mentioned in Section 2.3 of the main text, the BNN is ideally trained from scratch until convergence in each iteration loop,\nalthough this comes at a great computational cost. An alternative is the warm restart method of continuing the training from the\nprevious iteration which enables the model\u02d82019s training loss to converge in only a few epochs. However, as shown in Figure 7, we\nhave found that naive continued training can result in poor BO performance. This is likely because (a) training does not converge for\nthe new data point Dnew = (xN +1, yN +1) relative to the rest of the data under a limited computational budget, resulting in the\nacquisition function possibly labeling similar points in consecutive iterations, and (b) the BNN gets trapped in a local minima in the\nloss landscape that is not ideal for learning future data points. To mitigate this, we use the cosine annealing learning rate. The large\nlearning rate at the start of training allows the model to more easily escape local minima and explore a multimodal posterior, while\nthe small learning rate towards the end of the annealing cycle allows the model to converge more easily. Note that the idea of warm\nrestart is similar to \u02d8201ccontinual learning,\u02d8201d which is an open and active sub-problem in machine learning research. In particular,\nwe re-train the BNN using 10 epochs.\n\n[width=0.5]figures/figure7.png\n\nFigure 2: Effect of restarting the BNN training from scratch in each BO iteration.\n\n8.7 Models and Hyperparameters\n\n8.7.1 Additional Surrogate Models\n\nVariational BNNs model a prior and posterior distribution over the neural network weights but use some approximation on the\ndistributions to make the BNN tractable. In particular, we use Bayes by Backprop (BBB) (also referred to as the \u02d8201cmean field\u02d8201d\n\n10\n\n\fapproximation), which approximates the posterior over the neural network weights with independent normal distributions. We also\ncompare Multiplicative Normalizing Flows (MNF), which uses normalizing flows on top of each layer output for more expressive\nposterior distributions.\n\nBOHAMIANN proposed to use BNNs in BO by using stochastic gradient Hamiltonian Monte Carlo (SGHMC) to approximately\nsample the BNN posterior, combined with scale adaptation to adapt it for an iterative setting.\n\nNeuralLinear trains a conventional neural network on the data but then replaces the last layer with Bayesian linear regression such\nthat the neural network serves as an adaptive basis for the linear regression.\n\nTuRBO (trust region Bayesian Optimization) is a method that maintains M trust regions and performs Bayesian optimization within\neach trust region, maintaining M local surrogate models, to scale BO to high-dimensional problems that require thousands of\nobservations. We use M = 1 and M = 5, labeled as \u02d8201cTuRBO-1\u02d8201d and \u02d8201cTuRBO-5\u02d8201d, respectively.\n\nTPE (Tree Parzen Estimator) is a method that instead of modeling p(y|x), models p(x|y) and p(y) for the surrogate model and fits\ninto the BO framework. The tree-structure of the surrogate model allows it to define leaf variables only when node variables take\nparticular values, which makes it well-suited for hyper-parameter search (e.g. the learning rate momentum is only defined for\nmomentum-based gradient descent methods).\n\nLIPO is a parameter-free algorithm that assumes the underlying function is a Lipschitz function and estimates the bounds of the\nfunction. We use the implementation provided by the dlib library.\n\nDIRECT-L (DIviding RECTangles-Local) systematically divides the search domain into smaller and smaller hyperrectangles to\nefficiently search the space. We use the implementation provided by the NLopt library.\n\nCMA-ES (covariance matrix adaptation evolution strategy) is an evolutionary algorithm that samples new data based on a multivariate\nnormal distribution and refines the parameters of this distribution until reaching convergence. We use the implementation provided\nby the pycma library.\n\n8.7.2 Implementation Details\n\nUnless otherwise stated, we set NMC = 30. All BNNs other than the infinitely-wide networks are implemented in TensorFlow v1.\nModels are trained using the Adam optimizer using the cosine annealing learning rate with a base learning rate of 10\u02d822123. All\nhidden layers use ReLU as the activation function, and no activation function is applied to the output layer.\n\nInfinite-width neural networks are implemented using the Neural Tangents library. We use two different types of infinite networks:\n(1) \u02d8201cGP-\u02d8201d refers to a closed-form expression for Gaussian process inference using the infinite-width neural network as\na kernel, and (2) \u02d8201cInf-\u02d8201d refers to an infinite ensemble of infinite-width networks that have been \u02d8201ctrained\u02d8201d with\ncontinuous gradient descent for an infinite time. We compare NNGP and NTK kernels as well as the parameterization of the layers.\nBy default, we use the NTK parameterization, but we also use the standard parameterization, denoted by \u02d8201c-std\u02d8201d.\nWe implement BO using GPs with a Mat\u02d800e9rn kernel using the GPyOpt library. The library optimizes over the acquisition function\nin the inner loop using the L-BFGS algorithm.\n\n8.8 Additional Results\n\n8.8.1 Test Functions\n\nWe test BO on several common synthetic functions used for optimization, namely the Branin and 6-dimensional Hartmann functions.\nWe use BNNs with 4 hidden layers and 256 units in each hidden layer, where each hidden layer is followed by a ReLU activation\nfunction. Plots of the best value ybest at each BO iteration are shown in Figure 8. As expected, GPs perform the best. Ensembles and\nBBB also perform competitively and much better than random sampling, showing that deep BO is viable even for low-dimensional\nblack-box functions.\n\n[width=0.45]figures/branin.png [width=0.45]figures/hartmann.png\n\nFigure 3: BO results for the Branin and Hartmann-6 functions.\n\n8.8.2 Nanoparticle Scattering\n\nDetailed BO results for the nanoparticle scattering problem are shown in Table 3.\n\nAll the BNNs used for the nanoparticle scattering problem use an architecture consisting of 8 hidden layers with 256 units each,\nwith the exception of BOHAMIANN where we used the original architecture consisting of 2 hidden layers with 50 units each. The\ninfinite-width neural networks for the nanoparticle task consist of 8 hidden layers of infinite width, each of which are followed by\nReLU activation functions.\n\n11\n\n\f[width=0.45]figures/narrowbandbnn.png[width = 0.45]f igures/highpassbnn.png[width =\n0.45]f igures/narrowbandother.png[width = 0.45]f igures/highpassother.png\n\nFigure 4: Additional optimization result curves for the nanoparticle scattering task. (Top) Various BNNs. Note that results using\nauxiliary information are denoted by a solid line, while those that do not are denoted by a dashed line. Also note that the y-axis is\nzoomed in to differentiate the curves. (Bottom) Various non-BO algorithms. Ensemble-aux is replicated here for ease of comparison.\n\nWe also experiment with KL annealing in BBB, a proposed method to improve the performance of variational methods for BNNs in\nwhich the weight of the KL term in the loss function is slowly increased throughout training. For these experiments, we exponentially\nanneal the KL term with weight \u02d803c3KL(i) = 10i/500\u02d822125 as a function of epoch i when training from scratch; during the continued\ntraining, the weight is held constant at \u02d803c3KL = 10\u02d822123.\n\nKL annealing in the BBB architecture significantly improves performance for the narrowband objective, although results are mixed\nfor the highpass objective. Additionally, KL annealing has the downside of introducing more parameters that must be carefully tuned\nfor optimal performance. MNF performs poorly, especially on the highpass objective where it is comparable to random sampling,\nand we have found that MNF is quite sensitive to the choice of hyperparameters for uncertainty estimates even on simple regression\nproblems.\n\nThe different variants infinite-width neural networks do not perform as well as the BNNs on both objective functions, despite the\nhyperparameter search.\n\nLIPO seems to perform as well as GPs on both objective functions, which is impressive given the computational speed of the LIPO\nalgorithm. Interestingly DIRECT-L does not perform as well as LIPO or GPs on the narrowband objective, and actually performs\ncomparably to random sampling on the highpass objective. Additionally, CMA performs poorly on both objectives, likely due to the\nhighly multimodal nature of the objective function landscape.\n\nWe also look at the effect of model size in terms of number of layers and units in Figure 10 for ensembles. While including auxiliary\ninformation clearly improves performance across all architectures, there is not a clear trend of performance with respect to the model\nsize. Thus, the performance of BO seems to be somewhat robust to the exact architecture as long as the model is large enough to\naccurately and efficiently train on the data.\n\n[width=0.5]figures/modelsize.png\n\nFigure 5: Comparison of ybest at N = 1000 for the nanoparticle narrowband objective function for a variety of neural network sizes.\nAll results are ensembles, and \u02d8201caux\u02d8201d denotes using auxiliary information.\n\nExamples of the optimized structures by the \u02d8201cEnsemble-aux\u02d8201d architecture are shown in Figure 11. We can see that the\nscattering spectra peak in the shaded region of interest, as desired by the respective objective functions.\n\n[width=0.45]figures/narrowbandoptimized.png[width = 0.45]f igures/highpassoptimized.png\n\nFigure 6: Examples of optimized nanoparticles and their scattering spectrum using the \u02d8201cEnsemble-aux\u02d8201d architecture for the\n(a) narrowband and (c) highpass objectives. Orange shaded regions mark the range over which we wish to maximize the scattering.\n\n8.8.3 Photonic Crystal\n\nThe BNN and BCNN architectures that we use for the PC task are listed in Table 4. The size of the \u02d8201cFC\u02d8201d architectures are\nchosen to have a similar number of parameters as their convolutional counterparts. Unless otherwise stated, all results in the main\ntext and here use the \u02d8201cConv-TI\u02d8201d and \u02d8201cFC\u02d8201d architectures for BCNNs and BNNs, respectively.\n\nThe infinite-width convolutional neural networks (which act as convolutional kernels for GPs) in the PC task consist of 5 convolutional\nlayers followed by 4 fully-connected layers of infinite width. Because the pooling layers in the Neural Tangents library are currently\ntoo slow for use in application, we increased the size of the filters to 5 \u02d800d7 5 to increase the receptive field of each filter.\n\nDetailed BO results for the PC problem are shown in Table 5. For algorithms that optimize over the level set parameterization R51,\nwe see that GPs perform consistently well, although BNNs using auxiliary information (e.g. Ensemble-Aux) can outperform GPs.\nDIRECT-L and CMA perform extremely well on the PC-A distribution but performs worse than GP on the PC-B distribution.\n\nAdding convolutional layers and auxiliary information improves performance such that BCNNs significantly outperform GPs.\nInterestingly, the infinite-width networks perform extremely poorly, although this may be due to a lack of pooling layers in their\narchitecture which limits the receptive field of the convolutions.\nExamples of the optimized structures by the \u02d8201cEnsemble-aux\u02d8201d architecture are shown in Figure 12. The photonic crystal unit\ncells generally converged to the same shape: a square lattice of silicon posts with periodicity.\n\nValidation Metrics\n\n12\n\n\fTable 3: Various architectures for BNNs and BCNNs used in the PC problem. Numbers represent the number of channels and units\nfor the convolutional and fully-connected layers, respectively. All convolutional layers use 3 \u02d800d7 3-sized filters with stride (1, 1)\nand periodic boundaries. \u02d8201cMP\u02d8201d denotes max-pooling layers of size 2 \u02d800d7 2 with stride (2, 2), and \u02d8201cAP\u02d8201d denotes\naverage-pooling layers of size 2 \u02d800d7 2 with stride (1, 1). \u02d8201cConv\u02d8201d denotes BCNNs whereas \u02d8201cFC\u02d8201d denotes BNNs\n(containing only fully-connected layers) that act on the level-set parameterization x rather than on the image v. \u02d8201cTI\u02d8201d denotes\ntranslation invariant architectures, whereas \u02d8201cTD\u02d8201d denotes translation dependent architectures (i.e. not translation invariant).\n\nArchitecture Convolutional Layers\n\nFully-Connected Layers\n\nConv-TI\nConv-TD\nFC\n\n16-MP-32-MP-64-MP-128-MP-256\n8-AP-8-MP-16-AP-32-MP-32-AP\nn/a\n\n256-256-256-256\n256-256-256-256\n256-256-256-256-256\n\n[width=0.45]figures/pcaoptimized.png[width = 0.45]f igures/pcboptimized.png\n\nFigure 7: Examples of optimized photonic crystal unit cells over multiple trials for (a) PC-A distribution and (c) PC-B distribution.\n(b,d) Examples of the optimized DOS. Note that the DOS has been minimized to nearly zero in a thin frequency range. Orange shaded\nregions mark the frequency range in which we wish to minimize the DOS. All results were optimized by the \u02d8201cEnsemble-aux\u02d8201d\narchitecture.\n\nTo explore more deeply why certain surrogate models perform well while others do not, we track various metrics of the model\nduring BO on a validation dataset with 1000 randomly sampled data points. In particular, we look at the mean squared error (MSE),\nthe mean absolute error (MAE), the negative log-likelihood (NLL), and the calibration error on the PC-A data distribution. Results\nare shown in Figure 13(a).\n\nThe calibration error is a quantitative measure of the uncertainty of the model, which is important for the performance of BO as\nthe acquisition function uses the uncertainty to balance exploration and exploitation. Intuitively, we expect that a 50% confidence\ninterval contains the correct answer 50\n\ncal(F1, y1, ..., FT , yT ) =\n\n1\nm\n\nm\n(cid:88)\n\nj=1\n\n(pj \u2212 \u02c6pj)2\n\n(15)\n\nwhere Fj is the CDF of the predictive distribution, pj is the confidence level, and \u02d802c6pj is the empirical frequency. We choose to\nmeasure the error along the confidence levels pj = (j \u02d82212 1)/10 for j = 1, 2, ..., 11. The CDF Fj(yj) an be analytically calculated for\nmodels that have an analytical predictive distribution. For models that do not have an analytical predictive distribution, we use the\nempirical CDF:\n\nF (y) =\n\n1\nn\n\nn\n(cid:88)\n\n\u22ae\n\ni=1\n\n\u00b5(i)\u2264y\n\n(16)\n\nwhere 1 is the indicator function. We also plot the calibration, (pj, \u02d802c6pj)M j=1, in Figure 13(b). Perfectly calibrated predictions\ncorrespond to a straight line.\n\n[width=]figures/figure13.png\n\nFigure 8: (a) Various metrics tracked during BO of the PC-A dataset distribution on a validation dataset of 1000 datapoints. (b)\nUncertainty calibration curves measured at various points during BO. Note that the calibration curve for GP-aux is only shown for N\n= 50, as it becomes computationally intractable for larger N.\n\nFigure 13 shows that the infinite neural network kernel (NTK) has the highest prediction error, which is likely a contributing factor\nto its poor BO performance. Interestingly, vanilla GPs have the lowest MSE, so the prediction error is not the only indicator for\nBO performance. Looking at the calibration, the infinite neural network kernel has the highest calibration error, and we see from\nFigure 13(b) that it tends to be overconfident in its predictions. GPs have a higher calibration error than the ensemble neural network\nmethods and tend to be significantly underconfident in their predictions. GP-aux has higher validation loss, calibration error, and\nNLL than most, if not all, of the other methods, which explain its poor performance.\n\nThe ensemble NN methods tend to be reasonably well-calibrated. Within the ensemble NNs, the \"-aux\" methods have lower MSE\nand calibration error than their respective counterparts, and ConvEnsemble-aux has the lowest NLL calibration error out of all the\nmethods, although interestingly Ensemble-aux seems to have the lowest MSE and MAE out of the ensemble NNs.\n\nThese results together show that calibration of Bayesian models is extremely important for use as surrogate models in BO.\n\n13\n\n\f8.8.4 Organic Molecule Quantum Chemistry\n\nThe Bayesian graph neural networks (BGNNs) used for the chemical property optimization task consist of 4 edge-conditioned graph\nconvolutional layers with 32 channels each, followed by a global average pooling operation, followed by 4 fully-connected hidden\nlayers of 64 units each. The edge-conditioned graph convolutional layers are implemented by Spektral.\n\nMore detailed results for the quantum chemistry dataset are shown in Table 6 and Figure 14. The architecture with the Bayes by\nBackprop variational approximation applied to every layer, including the graph convolutional layers (\u02d8201cBBB\u02d8201d), performs\nextremely poorly, even worse than random sampling in some cases. However, only making the fully-connected layers Bayesian\n(\u02d8201cBBB-FC\u02d8201d) performs surprisingly well.\n\nTable 4: BO results for the four different quantum chemistry objective functions. \u02d82217 denotes that ybest is measured at N = 100 due\nto computational constraints.\n\nModel\n\n\u03b1\n\n\u03f5gap\n\nMean\n\nSD\n\nMean\n\nGP\nGraphGP\nEnsemble\nEnsemble-aux\nGraphEnsemble\nGraphEnsemble-aux\nGraphBBB\nGraphBBB-FC\nGraphNeuralLinear\nVAE-GP\n-\nVAE-GP-latent128\nVAE-GP-LATENT128-BETA0.001\nVAE-GP-LATENT32\nRandom\n\n0.41\n*0.62\n0.62\n0.62\n0.62\n0.62\n0.38\n0.62\n0.62\n0.62\n110.84\n-\n-\n-\n0.38\n\n0.04\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.00\n0.00\n0.06\n16.68\n-\n-\n-\n0.02\n\n-0.10\n*\u02d822120.10\n-0.08\n-0.10\n-0.10\n-0.10\n-0.11\n-0.10\n-0.10\n-0.10\n0.56\n-\n-\n-\n-0.10\n\n\u00b5\n\nMean\n\n101.08\n*131.99\n86.56\n83.86\n143.53\n143.53\n94.46\n135.64\n143.53\n123.3 VAE-GP-2\n\n154.66\n133.66\n114.83\n105.19\n\nSD\n\n0.02\n0.02\n0.00\n0.02\n0.00\n0.00\n0.01\n0.00\n0.00\n0.02\n0.35\n-\n-\n-\n0.02\n\n(\u03f5LUM O \u2212 \u03f5HOM O)/2\n\nSD Mean\n\n1.05\n14.59\n0.31\n4.45\n0.00\n0.00\n1.16\n13.67\n0.00\n-\n\n35.96\n13.25\n14.64\n7.87\n\n0.29\n*0.24\n0.28\n0.13\n0.49\n0.49\n0.25\n0.39\n0.46\n-\n\n0.40\n0.42\n0.53\n0.29\n\nSD\n\n0.07\n0.03\n0.00\n0.05\n0.00\n0.00\n0.01\n0.14\n0.09\n-\n\n0.10\n0.13\n0.38\n0.07\n\n[width=0.45]figures/alpha.png [width=0.45]figures/gap.png\n\nFigure 9: Additional BO results for several different objective functions on the chemistry dataset. GP and GraphEnsemble-aux\ncurves are replicated from the main text for convenience.\n\nEnsembles trained with auxiliary information (\u02d8201cEnsemble-aux\u02d8201d) and neural linear (\u02d8201cNeuralLinear\u02d8201d) perform the best\non all objective functions. Adding auxiliary information to ensembles helps for the \u02d803b1 objective function, and neither helps nor\nhurts for the other objective functions. Additionally, BNNs perform at least as well or significantly better than GPs in all cases. GPs\nperform comparably or worse than random sampling in several cases.\n\nAs noted in the main text, the performance of VAE-GP depends on the quality of the pre-trained VAE, as shown in Figure 15. The\nVAE-GP benchmark uses the same pre-trained VAE, and \u02d8201cVAE-GP-2\u02d8201d refers to the same method using a different random\nseed for the VAE. Even with the exact same method, VAE-GP-2 performs significantly worse on both objective functions. We also\nincrease the latent space dimensionality from 52 to 128 in the \u02d8201cVAE-GP-LATENT128\u02d8201d benchmark, which performs even\nworse on the \u02d803b1 \u02d82212 \u02d820acgap benchmark although it performs significantly better on the \u02d803c9 benchmark. We also adjust the\nlearning rate momentum to \u02d803b7 = 0.001 in \u02d8201cVAE-GP-LATENT128-BETA0.001\u02d8201d, and the latent space dimensionality to 32\nin \u02d8201cVAE-GP-LATENT32\u02d8201d. There is no clear trend with the different hyperparameters, which may point to the random seed\nof the VAE pre-training being a greater factor in BO performance than the hyperparameters.\n\n[width=0.45]figures/vaealpha.png[width = 0.45]f igures/vaegap.png\n\nFigure 10: Additional BO results for VAE-GP using different pre-trained VAEs.\n\nValidation Metrics\n\nAs in Appendix A.5.3, we track the MSE, NLL, and calibration error during optimization on the chemistry task. Results are shown\nin Figure 16. The various metrics correlate with the respective methods\u02d82019 performances during BO. For example, VAE-GP has\nan extremely high MSE and calibration error on the \u02d803b1 objective, where it performs poorly, but has an MSE and calibration\nerror more comparable with that of other methods as well as an extremely low NLL on the \u02d803c9 \u02d82212 \u02d820acgap objective, where it\nperforms extremely well. Likewise, the metrics for GRAPHGP are very high on the \u02d803b1 \u02d82212 \u02d820acgap objective, where it performs\npoorly. GraphEnsemble tends to be among the better methods in terms of these metrics, which translates into good BO performance.\n\n14\n\n\fFigure 11: (a) Various metrics tracked during BO of the chemistry dataset on a validation dataset of 1000 datapoints. (b) Uncertainty\ncalibration curves measured at various points during BO.\n\n[width=]figures/figure16.png\n\n8.9 Additional Discussion\n\nBBB performs reasonably well and is competitive with or even better than ensembles on some tasks, but it requires significant\nhyperparameter tuning. The tendency of variational methods such as BBB to underestimate uncertainty is likely detrimental to their\nperformance in BO. Additionally, prior work shows that BBB has trouble scaling to larger network sizes, which may make them\nunsuitable for more complex tasks such as those in our work. BOHAMIANN performs very well on the nanoparticle narrowband\nobjective and comparable to other BNNs without auxiliary information on the nanoparticle highpass objective. This is likely due to\nits effectiveness in exploring a multi-modal posterior. However, the need for SGHMC to sample the posterior makes this method\ncomputationally expensive, and so we were only able to run it for a limited number of iterations using a small neural network\narchitecture.\n\nInfinitely wide neural networks are another interesting research direction, as the ability to derive infinitely wide versions of various\nneural network architectures such as convolutions, and more recently graph convolutional layers, could potentially bring the power\nof GPs and BO to complex problems in low-data regimes. However, we find they perform relatively poorly in BO, are quite sensitive\nto hyperparameters (e.g. kernel and parameterization), and current implementations of certain operations such as pooling are too\nslow for practical use in an iterative setting. In particular, BO using an infinite ensemble of infinite-width networks performs poorly\ncompared to normal ensembles, suggesting that the infinite-width formulations do not fully capture the dynamics of their finite-width\ncounterparts.\n\nNon-Bayesian global optimization methods such as LIPO and DIRECT-L are quite powerful in spite of their small computational\noverhead and can even outperform BO on some simpler tasks. However, they are not as consistent as BO, performing more\ncomparably to random sampling on other tasks. CMA-ES performs poorly on all the tasks here. Also, like GPs, these non-Bayesian\nalgorithms assume a continuous input space and cannot be effectively applied to structured, high-dimensional problems.\n\n8.10 Compute\n\nAll experiments were carried out on systems with NVIDIA Volta V100 GPUs and Intel Xeon Gold 6248 CPUs. All training and\ninference using neural network-based models, graph kernels, and infinite-width neural network approximations are carried out on\nthe GPUs. All other models are carried out on the CPUs.\n\n15",
  "is_publishable": 1,
  "venue": NaN
}
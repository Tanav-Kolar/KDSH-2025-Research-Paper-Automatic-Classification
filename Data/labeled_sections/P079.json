{
  "title": "OmniPrint: A Configurable Generator for Printed\nCharacters",
  "abstract": "We introduce OmniPrint, a synthetic data generator for isolated printed characters\ndesigned to support machine learning research. While being inspired by popular\ndatasets, such as MNIST, SVHN, and Omniglot, OmniPrint provides the unique\nability to produce a wide range of printed characters from various languages, fonts,\nand styles, with custom distortions. OmniPrint includes 935 fonts from 27 scripts,\nand supports many types of distortions. As a demonstration of its functionality, we\npresent several use cases, including an example of a meta-learning dataset designed\nfor a machine learning competition. OmniPrint is publicly available at a specified\ngithub link.\n\n1\n\nIntroduction and Motivation\n\nBenchmarks and shared datasets have helped propel progress in machine learning. One popular\nbenchmark is MNIST, used worldwide in tutorials, textbooks, and classes. Many variants of MNIST\nexist, including Omniglot, which includes characters from several different scripts. Since Deep\nLearning techniques rely heavily on data, as there is an increasing number of datasets, more, larger\ndatasets are required. Since collecting and labeling data can be time-consuming and expensive,\nartificial data generation can be used to drive ML research. This motivates the creation of OmniPrint,\nan extension of Omniglot, specifically designed for the generation of printed characters.\n\nOur focus is on classification and regression problems, where a vector y, which is composed of either\ndiscrete or continuous labels, is to be predicted using an input vector x of observations, which in\nthe case of OmniPrint, is an image of a printed character. Additionally, data are often affected by\nnuisance variables z, which are discrete or continuous labels that represent metadata or covariates.\nFor our work, z may include character distortions such as shear, rotation, line width variations, or\nbackground changes. Thus, a data generation process with OmniPrint contains the following steps:\n\nZ \u223c P (Z),\n\nY \u223c P (Y |Z), X \u223c P (X|Z, Y ).\n\nIn many domains such as image, video, sound, and text applications, where objects or concepts\nare target values to be predicted from percepts, Z and Y are independent and hence P (Y |Z) =\nP (Y ). This type of data generation is also encountered in medical diagnoses of genetic disease, for\nwhich x would be a phenotype and y a genotype, and also analytical chemistry where x might be\nchromatograms and y would be compounds to be identified. We expect that progress made using\nOmniPrint to benchmark machine learning systems should foster progress in these domains.\n\nCharacter images represent excellent benchmarks for machine learning, given their simplicity, and\nvisual nature, and for enabling the development of real-world applications. However, our exploration\nof available resources revealed that there is no synthesizer that fulfills all of our needs. No available\nsynthesizer allows for the generation of realistic small-sized images, supports a wide variety of\ncharacter sets, and offers control over the variation of realistic conditions through parameters.\nThe synthesizer must support pre-rasterization manipulation of anchor points, post-rasterization\ndistortions, seamless background blending, foreground filling, anti-aliasing rendering, and be easily\nextensible with new fonts and styles.\n\n.\n\n\f2 The OmniPrint Data Synthesizer\n\n2.1 Overview\n\nOmniPrint builds on the open-source software TextRecognitionDataGenerator, adapting it to our\nspecifications. The software is designed to allow researchers to generate data in a form that makes\nit easier to train machine learning models. To obtain a large number of classes (Y labels), we\nmanually selected and filtered characters from the Unicode standard, forming alphabets for over 20\nlanguages. These alphabets are divided into partitions (e.g., Oriya consonants). Nuisance parameters\n(Z) are divided into Font, Style, Background, and Noise. The fonts are selected by an automatic\nfont collection module. We added a feature using the FreeType rasterization engine which enables\nvector-based pre-rasterization transformations. Additionally, we enriched background generation\nwith seamless blending, and enabled custom post-rasterization transformations. We also implemented\nutility code including dataset formatters, and a data loader which generates episodes for meta-learning\napplications. To our knowledge, OmniPrint is the first text image synthesizer geared toward ML\nresearch to support pre-rasterization transforms.\n\n2.2 Technical Aspects of the Design\n\nThe OmniPrint\u2019s design has extensibility as a key feature. Users can add new alphabets, fonts, and\ntransformations to the generation pipeline.\n\nThe design can be summarized as follows:\n\n\u2022 Parameter configuration file: Support for both TrueType and OpenType font files is\nincluded. Style parameters include rotation angle, shear, stroke width, foreground, text\noutline, and other transformations.\n\n\u2022 FreeType vector representation: Text, font, and style parameters are used by the FreeType\n\nrasterization engine.\n\n\u2022 Pre-rasterization transformed character: FreeType performs all the pre-rasterization\n(vector-based) transformations. Pre-rasterization manipulations include linear transforms,\nstroke width variation, random elastic transformation, and variation of character proportion.\nThe RGB bitmaps output by FreeType are called the foreground layer.\n\n\u2022 Pixel character on white background: Post-rasterization transformations are applied to\nthe foreground layer. The layer is kept at a high resolution, using ReLU activations, to avoid\nartifacts. The RGB image is then resized using a three step process; applying a Gaussian\nfilter to smooth the image, reducing the image by an integer factor, and resizing using\nLanczos resampling.\n\n\u2022 Pixel character on textured background: The foreground is then pasted onto the back-\n\nground.\n\n\u2022 Logging and Visualization: The library utilizes a Weights Biases tool to log the training\nprocess and the visualizations. It visualizes the condition\u2019s traversals, latent factor traversals,\nand output reconstructions as static images and animated GIFs.\n\n2",
  "introduction": "",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "",
  "is_publishable": 1,
  "venue": NaN
}
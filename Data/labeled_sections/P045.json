{
  "title": "AM-RADIO: Agglomerative Vision Foundation Model\nReduce All Domains Into One",
  "abstract": "A handful of visual foundation models (VFMs) have recently emerged as the\nbackbones for numerous downstream tasks. VFMs like are trained with distinct\nobjectives, exhibiting unique characteristics for various downstream tasks. We\nfind that despite their conceptual differences, these models can be effectively\nmerged into a unified model through multi-teacher distillation. We name this\napproach AM-RADIO (Agglomerative Model \u2013 Reduce All Domains Into One).\nThis integrative approach not only surpasses the performance of individual teacher\nmodels but also amalgamates their distinctive features, such as zero-shot vision-\nlanguage comprehension, detailed pixel- level understanding, and open vocabulary\nsegmentation capabilities. Additionally, in pursuit of the most hardware-efficient\nbackbone, we evaluated numerous architectures in our multi-teacher distillation\npipeline using the same training recipe. This led to the development of a novel\narchitecture (E-RADIO) that exceeds the performance of its predecessors and is at\nleast 6x faster than the teacher models at matched resolution. Our comprehensive\nbenchmarking process covers downstream tasks including ImageNet classification,\nsemantic segmentation linear probing, COCO object detection and integration into\nLLaVa-1.5.",
  "introduction": "Knowledge Distillation has been a very successful and popular technique for transferring the knowl-\nedge of a \u201cteacher\u201d model (or ensemble of models) into a typically smaller \u201cstudent\u201d model. In the\noriginal formulation, both the student and the teacher operate on the same in-domain dataset, and\nthe student simultaneously matches the logits of the teacher, and the ground truth labels. Instead of\nusing labeled images, an alternative approach is to train the student model to match the features of\nthe teacher model.\n\nInstead of using a smaller student model, employ an iterative learning procedure with a high-capacity\nmodel where a student of equal or greater capacity than the teacher is trained with heavy augmentation\napplied to the student. Once trained, they expand the dataset by pseudo-labeling new data using the\ntrained student. They then make the student become the teacher, and repeat the process. An important\nfinding in this work is that the student is capable of surpassing the performance of the teacher.\n\nThe authors of explore the concept of ensemble distillation, where there are multiple teachers, each\nof which having restricted domain knowledge. provides an overview of multi-teacher distillation, and\nproposes that instead of matching the summary of an ensemble of teachers, the student can match the\nfeatures of each individual teacher via some learned non-shared mapping from the representation\nspace of the student to each teacher. Of interest in their approach is that the student and teacher\ndon\u2019t need to share the same architecture, and also that treating teachers individually yields improved\nperformance.\n\nRecently, the concept of Foundation Models (FMs) has emerged, with the general understanding\nthat these models are large, general, and expensive to train. Through training on very large datasets\nthey are broadly applicable to numerous downstream tasks. A seminal example of such models is\n\n.\n\n\f, which trains on web-scale weakly supervised (image, caption) pairs, and results in exceptional\nzero-shot performances on a wide array of computer vision benchmarks. While is firmly a FM,\nanother model, has emerged with broad capabilities, often surpassing on dense tasks that require\nstrong spatial features, such as ADE20k and Pascal VOC. Separately, is gaining popularity for its\nexcellent open-vocabulary instance segmentation abilities, whose vision encoder we hypothesize has\nstrong dense feature representations.\n\nWe introduce AM-RADIO with the goal of learning from multiple foundational models simultane-\nously. We observe that, when given a student model of sufficient capacity, it is often able to exceed\nany of its teachers on important axes. In addition to performing well on representative foundational\nbenchmarks, by virtue of the training framework, our student models are able to mimic their teacher\nmodels, and thus are able to perform downstream tasks that are otherwise performed by the teachers.\nExamples of this include CLIP-ZeroShot applications, since the language model trained by is com-\npatible with our student, and also Segment-Anything tasks, as the student is able to replace the vision\nencoder and interface with the already-trained mask decoders.\n\nWe also study the effect of using a more hardware-efficient model architecture. Most works on\nefficiency are not directly comparable as they use different training recipes, even when evaluated on\nthe same dataset such as ImageNet-1k, and may be over-tuned. To this end, we evaluate more than\n10 promising architectures under the same training recipe for a direct comparison. We reveal that\nCNN-like architectures are faster but struggle to distill ViT VFMs. This led us to the development of\na novel hybrid architecture, E-RADIO, that exceeds the performance of its predecessors and is at\nleast 6x faster than teacher models at matched resolution.\n\nOur main contributions are as follows:\n\n\u2022 We describe a general methodology for distilling multiple distinct foundation models into\n\none, including models with incompatible input resolutions.\n\n\u2022 We show that these student models are able to outperform their teachers on representative\n\nbenchmarks.\n\n\u2022 We demonstrate that these student models can either drop-in replace their teachers, or their\nfeatures can be used directly in downstream applications such as providing visual encoding\nfor LLaVA.\n\n\u2022 We benchmark a number of efficient architectures and propose a new architecture (E-RADIO)\n\nthat allows for similar model quality at significant speedups.",
  "related_work": "Knowledge Distillation The underpinning of our work is based on the method of Knowledge Dis-\ntillation which aims to train a \u201cstudent\u201d model using soft targets produced by an already-trained\n\u201cteacher\u201d model, using the the teacher\u2019s output logits as \u201csoft\u201d labels. Alternatively, distillation can\nbe performed using intermediate network activations. In general, due to the heterogeneous nature of\nthe different teacher foundation models that we employ, we ignore any potential labels coming from\nthe data, and we ignore the logits of teachers, and simply opt to match the feature representations of\nthe teachers before any task-specific processing stages.\n\nMulti-Teacher Distillation There is also a body of work that studies distilling a student model jointly\nfrom multiple teacher models simultaneously. Because of the heterogeneous domains that our teacher\nmodels cover, we don\u2019t apply approaches that marginalize teachers into a unified label, and instead\nmap students to each teacher independently using teacher-specific projection heads from the unified\nstudent representation. Although the reason behind this method in is different, we find the same\noverall strategy to be effective. While doesn\u2019t study matching the features of multiple teachers\nsimultaneously, we are able to extend their paradigm via the different projection heads. To preserve\ndrop-in compatibility with teacher frameworks, we eliminate the feature normalization in the loss\nfunction.\n\nDistilling Foundation Models Foundation Models are meant to be generalist models that are trained\non massive amounts of data, and are typically resource intensive to train from scratch. In the vein\nof single-teacher distillation, employ self-distillation to train their smaller variants from the larger\nteacher. distills their model from a teacher. Instead of focusing our energy on one teacher in particular,\n\n2\n\n\fwe instead grab high-quality versions of (using OpenCLIP), , and . Concurrently with our work,\ndescribe a methodology for merging a model into a pretrained model via distillation, which is, in\nspirit, quite similar to our approach. In contrast to theirs, we include and also simplify the objective\nto straightforward feature matching. Since we don\u2019t rely on the student model to be pre-trained, it\nalso gives us the flexibility to have the student be an architecture distinct from any teacher.\n\n3 Knowledge Agglomeration\n\nWe propose a framework to train a vision foundation model from scratch via multi-teacher distillation.\nWe demonstrate that each teacher brings unique properties to the foundational vision model, and the\nresulting trained model will agglomerate these attributes.\n\n3.1 Overview\n\nAs an initial assumption, we expect that the teacher models are capable of representing a broad swath\nof images found on the internet, coming from datasets such as ImageNet (1k or 21k), LAION-400M\nor DataComp-1B. With this in mind, we choose to study 3 seminal teacher model families: , ,\nand as they have demonstrated outstanding performance over a broad range of tasks (as in ), or\nspecifically strong performance on downstream dense tasks, such as semantic segmentation under\nlinear probe (as in ), or open-vocabulary segmentation (as in ). Because these teacher models come\nfrom such diverse domains, we omit any form of supplemental ground truth guidance and treat the\naforementioned datasets simply as sources of images. To assess the quality of our models, we adopt a\nset of representative metrics across a few broad domains.\n\n\u2022 Image level reasoning: (i) k-NN Top-1 accuracy on ImageNet-1K, and (ii) Zero-Shot\naccuracy using the teacher\u2019s language model. k-NN embeds the model\u2019s summary feature\nvector for every image in the training set, and then for each validation image, it uses a\nweighted sum of the k nearest training vectors to elect a label.\n\n\u2022 Pixel-level visual tasks: segmentation mIOU on (i) ADE20K and (ii) Pascal VOC - under\n\nthe linear probe setting, details in Section 5.3.\n\n\u2022 Large Vision-Language Models: we plug our frozen vision encoder model into LLaVA-1.5\nand evaluate it on a wide set of tasks including GQA, TextVQA, ScienceQA and VQAv2.\nDetails in Section 5.4.\n\n\u2022 SAM-COCO instance segmentation: From , we adopt their COCO instance segmentation\n\nmethodology to evaluate our ability to replicate SAM visual features.\n\nResults on these tasks, both for teacher models and our AM-RADIO variants, are summarized in\nTable 1.\n\n3.2 Adaptor Heads\n\nWe opt for simplicity in design of the adaptor heads, and leave alternative architectures as future\nwork. To this end, we employ a simple 2-layer MLP, with a LayerNorm and GELU in between. The\ninput dimension is the student embedding dimension, the intermediate dimension is the maximum\nembedding dimension of all teachers, and the output dimension matches the specific teacher. For\neach teacher, we employ two heads, one for the summary vector, and one for the spatial features.\n\n3.3 Distillation Dataset Choice\n\nIn table 2 we study the effect of different datasets on downstream metrics. While the highest image\nclassification metrics are achieved using ImageNet-1K as the training dataset, we argue that it doesn\u2019t\nfairly measure \u201czero shot\u201d performance as the student directly learns the teacher features in the\nevaluation domain. For this reason, we opt for the DataComp-1B dataset.\n\n3.4 Loss Formulation\n\nBecause we don\u2019t have ground truth data for each teacher for each image, we instead opt to match\nthe features coming from each teacher\u2019s vision encoder. In particular, we distinguish between the\n\n3\n\n\fTable 1: Comparison of vision foundation and RADIO models. \u201cZero-Shot\u201d and k-NN are computed\non ImageNet-1K. ADE20K and VOC (PascalVOC2012) refer to linear probe semantic segmentation\nmIOU. GQA, POPE (popular), TextVQA, and VQAv2 are obtained via LLaVa 1.5 by replacing the\nvision encoder. COCO is the instance segmentation metric introduced by to evaluate distillation.\nRADIO attains the best metrics on most benchmarks, and is competitive with the rest, while E-RADIO\nenables high quality results in resource constrained settings. Note that Zero-Shot and COCO use\nteacher\u2019s decoder head that is not finetuned. Throughput computed using NVIDIA A100 GPU, stated\nresolution, and TensorRT v8601. *Denotes teachers used to train our final RADIO. :We failed to\nexport DINOv2-g-reg to TensorRT, so we report DINOv2-g here, which should be fairly close. ::We\nwere unable to get zero shot working using their model code.\n\nModel\nTextVQA\n\nOpenCLIP-H/14\n50.48\nMetaCLIP-H/14\n53.65\nSigLIP-L/14\n56.65\nIntern-ViT-6B\n52.45\n\n60.36\nDFN CLIP-H/14\n56.78\nOpenAI CLIP-L/14\n57.92\nDINOv2-g/14-reg\n47.18\nSAM-H/16\n43.91\nE-RADIO-L (Ours)\n51.47\nRADIO-ViT-H/16 (Ours)\n56.32\n\nParams (M)\nVQAv2\n\nResolution\nSAM COCO\n\nThroughput Zero-shot\n\nk-NN ADE20k VOC GQA POPE\n\n632\n72.24\n632\n75.71\n428\n71.94\n5,902\n76.75\n5,537\n78.83\n633\n78.78\n305\n78.49\n1,137\n76.23\n637\n57.65\n391\n76.73\n653\n79.28\n\n224\n-\n224\n-\n384\n-\n224\n-\n448\n-\n378\n-\n336\n-\n224\n-\n1024\n77.18\n512\n76.31\n432\n76.23\n\n503\n\n486\n\n241\n\n63\n\n14\n\n170\n\n414\n\n294\n\n12\n\n468\n\n158\n\n77.19\n\n81.10\n\n40.04\n\n68.03\n\n57.94\n\n83.61\n\n80.51\n\n82.12\n\n35.39\n\n62.62\n\n60.57\n\n84.76\n\n82.61\n\n85.16\n\n40.53\n\n70.31\n\n57.70\n\n84.85\n\n83.20\n\n78.43\n\n47.20\n\n76.85\n\n60.18\n\n84.02\n\n-\n\n68.64\n\n42.78\n\n74.43\n\n61.19\n\n87.23\n\n83.90\n\n85.27\n\n39.00\n\n70.29\n\n61.73\n\n85.91\n\n75.54\n\n79.80\n\n36.51\n\n67.04\n\n62.20\n\n86.09\n\n-\n\n-\n\n83.41\n\n48.68\n\n82.78\n\n61.88\n\n85.62\n\n22.12\n\n28.08\n\n34.34\n\n49.92\n\n81.76\n\n80.73\n\n83.89\n\n48.22\n\n81.64\n\n61.70\n\n85.07\n\n82.93\n\n86.06\n\n51.34\n\n84.71\n\n63.01\n\n86.20\n\nTable 2: Ablation study on the choice of training dataset. We use MetaCLIP ViT-H/14 and DINOv2\nViT-g/14 teachers, and a ViT-L/14 student model with CPE. Both \u201ck-NN\u201d and \u201cZero Shot\u201d are for\nImageNet-1k. ADE20k refers to mIOU linear probe on ADE20k.\n\nDataset\n\nk-NN Zero Shot ADE20K\n\nImageNet 1K\n84.79\nImageNet 21K 84.61\nLAION-400M 83.77\nDataComp-1B 83.91\n\n80.44\n80.10\n77.46\n78.51\n\n48.11\n48.65\n48.6\n49.01\n\nsummary feature vector and the spatial feature vectors for each teacher. The summary feature is\ncomputed differently based on the model. For and , we use the \u201cclass token\u201d as the summary feature\nvector, and we don\u2019t match a summary for .\n\nLet f (x|\u03980) be the student vision encoder with parameters \u03980, and yi = hi(x1|\u0398i) be the learned\nstudent head matching teacher summary features zi = ti(x|\u03a6i) with student adaptor parameters \u0398i\nand teacher parameters \u03a6i.\n\nx1 = f (x|\u03980); zi = ti(x|\u03a6i), yi = hi(x1|\u0398i); Lsummary(x) =\n\n(cid:88)\n\ni\n\n\u03bbiLcos(yi, zi)\n\n(1)\n\n4\n\n\fWe found empirically that cosine distance loss produced better models compared to L1, MSE,\nSmooth-L1. Additionally, supervising the spatial features of the model by matching the teacher was\nnot only important for downstream dense tasks, but also improved the holistic quality of our model.\n\nFor matching the spatial features, we employ a combination of cosine similarity and smooth L1.\nSimilar to equation (2) where we found that cosine similarity produced the best results, we found the\nsame to be true for the spatial features. However, we want to allow our student model to be a drop-in\nreplacement in the teacher frameworks, thus it\u2019s important that we match the magnitude of the teacher\nvectors, and so we include smooth L1. In (3) we show the formulation of this loss. Let hi(x1|\u0398i)\nbe the learned student head for matching teacher feature vectors, and corresponding ti(x|\u03a6i) be the\nteacher feature vectors, with x1 = f (x|\u03980), then the spatial feature loss is:\n\nLmatch(x, y) = \u03b1Lcos(x, y) + \u03b2Lsmooth\u2212l1(x, y)\n\nLf eatures(x) =\n\n(cid:88)\n\ni\n\n\u03b3iLmatch(hi(x1|\u0398i), ti(x|\u03a6i))\n\n(2)\n\n(3)\n\nWe choose \u03b1 = 0.9 and \u03b2 = 0.1 to mostly rely on the empirically better cosine distance, but to also\nmatch vector magnitudes.\n\n3.4.1 Loss Balancing\n\nDue to the number of possible combinations of loss weights between the different teachers, and\neven which teachers, and possible formulations of loss functions, we mostly opted toward naive loss\nbalancing with all teachers equally weighted for spatial features (\u03b3i = 1). For summary features, we\nhave \u03bbCLIP = \u03bbDIN O = 1 and \u03bbSAM = 0.\n\nWe did experiment with automatic loss balancing using predicted uncertainty, AdaLoss (momentum\n0.99) and separately with AMTML-KD, as ways to learn the balance of \u03bbi and \u03b3i. In the case of\nAMTML-KD, the model would always collapse its entire weight around the teacher and would\nyield worse results than naive manual balancing. Based on the results in table 4, there is very little\nadvantage to the more exotic balancing schemes, so we opt for the \u201cNaive\u201d method throughout the\nrest of the paper.\n\nTable 3: Ablation over which teachers we supervise the spatial features. We use a ViT-L/14 student\nmodel and train on the LAION-400M dataset. Adding this loss term is always beneficial. DINOv2\nappears to provide better spatial features than CLIP, but training the student to match both teachers\nproduces the best results. We don\u2019t ablate SAM as we solely want it for its spatial features.\n\nTeachers Zero Shot\n\nk-NN ADE20K\n\nNone\nCLIP\nDINOv2\nBoth\n\n75.77\n75.64\n74.68\n74.85\n\n82.59\n82.60\n83.02\n82.96\n\n41.18\n44.42\n47.05\n48.13\n\nTable 4: Loss term balancing methods comparison. We use a ViT-B/14 student, and CLIP+DINOv2\nteachers. We found that AdaLoss produces the best results on the ImageNet tasks, but the worst on\nADE20K.\n\nMethod\n\nZero Shot\n\nk-NN ADE20K\n\nNaive\nUncertainty\nAdaLoss\n\n70.63\n70.92\n71.31\n\n79.50\n79.37\n79.77\n\n44.71\n44.57\n44.36\n\n4\n\nImplementation Details\n\nPerforming heterogeneous multi-teacher distillation is not trivial due to a mismatch in feature\ndimensions, input resolutions, concepts for loss computation, and downsampling ratios, as well as\nchallenges in fitting multiple teachers into a single GPU.\n\n5\n\n\fGeneral. We train all student models using the AdamW optimizer, batch size 1024, cosine annealing\nlearning rate schedule and base learning rate of 0.001. We train for 600k steps, resulting in 614M\ntotal examples seen. For our best student model, we train using DFN CLIP ViT-H/14 378px, OpenAI\nCLIP ViT-L/14 336px, DINOv2 ViT-g/14 224px, and SAM ViTDet-H 1024px. We apply random\nscale + cropping to both student and teacher inputs. We chose the DataComp-1B dataset due to it\nhaving the highest quality results of the web-scale datasets we had access to. We train in two stages,\nfirst with CLIP+DINOv2 for 300k steps at 256px, and second with CLIP+DINOv2 at 432px plus\nSAM at 1024px for 300k steps.\n\nStudent architecture. We study two settings for student model architecture:\n\n\u2022 Standard ViT architecture to match the architecture of teachers. Our best model is a ViT-\n\nH/16.\n\n\u2022 Efficient architecture variants prioritizing high throughput on GPUs. See Section 5.1.\n\nMulti-scale Teachers. We choose ViT-H/16 architecture for our student model. To match resolution of\nfeatures, we feed the expected resolution of 10242. Given that our and teachers are patch-14 models,\nwe opt to feed the student 4322 inputs, as that is the same effective resolution as 3782 for patch-14.\nWe found that interpolating features doesn\u2019t degrade results, so the teacher operates at 224px and we\nupsample the outputs to match the student.\n\nRank/Teacher Partitioning. We group teacher models by (batch size, student resolution), and then\ndistribute the groups to different GPUs, such that each GPU processes a consistent batch size and\ninput resolution. We also sample groups at different rates. For our training setups that include , we\ntrain with 64 GPUs, half of which get the CLIP+DINOv2 group with batch size 32 per GPU and\ninput resolution 432, and the other half get with batch size 2 per GPU and input resolution 1024. This\nresults in an effective batch size of 1,152. For CLIP+DINOv2 training, we use 32 GPUs, resulting in\nbatch size 1024.\n\nMulti-Resolution ViTs. Many of our student models use ViT as the base vision architecture. Tradition-\nally, ViTs use a learned position embedding for each input patch in an image, which in turn enforces\nthat the model always operates at a constant resolution. We employ the Cropped Position Embedding\n(CPE) augmentation with the number of positions being equal to 1282. The position embeddings are\nthen randomly cropped and interpolated to match the number of input patches for the student model.\nEven when training with CLIP+DINOv2 at 224 resolution, we found that this technique results in a\nnegligible drop (Table 5) in summary metrics, but improved semantic segmentation linear probing\nmIOU. For heterogeneous-resolution students, this is a seamless technique that allows ViT to operate\nat arbitrary resolutions within some envelope. In addition to enabling arbitrary resolutions, as shown\nin figure 3, CPE reduces the noise artifacts in the position embeddings as compared to other ViT\nmodels.\n\nHigh-Resolution ViT Student. In , they employ the ViTDet architecture as a way to reduce the\ncomputational and memory burden of ViT models at high-resolution. We reformulate this arch\ninstead into a training augmentation, where we sample a window size from a set of possible window\nsizes. This allows us to reduce the computational burden of training the student model with the\nteacher, and, as we make the window size flexible, it provides an additional throughput scaling\nmechanism during inference. Table 8 demonstrates our ability to replace SAM\u2019s encoder. Separately,\nwe found that high resolution training was unstable, so we apply spectral reparametrization and a\nweight decay of 0.02 to prevent attention entropy collapse.\n\nStudent/Teacher Resolution Mismatch. When the student and teacher downsample images through\ntheir processing stack at different rates, it results in the output feature vectors having different\nresolutions. For example, if the teachers use a ViT-H/14 architecture and student a ViT-H/16, it\nmeans that the student outputs a 142 feature map, and the teachers a 162 feature map. For Lf eatures\nwe bilinearly interpolate the outputs to match the larger resolution between the student and teacher\nfeatures.\n\nFeature Summarization. In 3.4 we explained how teacher summary features are extracted using the\n\u201cclass token\u201d of their respective ViT models. We now turn our attention to the summarization of\nstudent features. ViTs have 2 options: (i) a separate summarization \u201cCLS\u201d token or (ii) average\npooling patch tokens. We evaluate both options in Table 6. We observe that average pooling improves\n\n6\n\n\fsummary loss, but has a more significant detrimental effect on the feature loss. Given the importance\nof the latter we choose to use separate CLS tokens.\n\nTable 5: Comparing identical ViT models, with CLS token and average pooling summarization.\n\nZero Shot\n\nk-NN ADE20K VOC VQAv2\n\nCLS token\nAvgpool\n\n78.55\n80.12\n\n83.91\n83.83\n\n49.01\n38.36\n\n83.51\n77.04\n\n77.66\n78.28",
  "methodology": "",
  "experiments": "",
  "results": "In this section, we analyze models obtained with the proposed AM-RADIO framework. First, we\ntouch upon backbone efficiency, then compare with the original teachers (CLIP, DINOv2, SAM), and\nbenchmark models under vision question answering in the LLaVa framework. We will see that the\nproposed models outperform the original teachers in multiple metrics, including throughput. Results\nare shown in Figure 1 and Table 1.\n\n5.1 Efficient Students\n\nWe aim to find an efficient model architecture to speed up the inference of VFM. There are a number\nof architectural designs aimed at high throughput on GPU devices. We use our distillation framework\nto evaluate several backbones with no change in training hyperparameters.\n\nUpon reviewing the literature on efficient vision backbones focused for high GPU throughput, we\npick the following list of architectures: EfficientNetV2, ResNetv2, RegNetY, FasterViT, EfficientViT,\nConvNext, NFNet, SwinV2, MaxViT, PoolformerV2 and MViTV2. We train all the backbones\nvia distillation on the ImageNet-21k dataset, using OpenCLIP ViT-H/14 (laion2B-s32B-b79K) and\nDINOv2 g/14 as teachers. Results are compiled in Table 7.\n\nTable 6: Comparison of backbones. Throughput is measured using TensorRT 9.0.1 on A100 in\nmixed FP16/FP32 precision at batch size 128 on 2242px resolution. Sorted by descending throughput\norder. FD loss is the Feature Distillation training loss against the DINOv2 teacher, it exhibits high\ncorrelation with the ADE20k mIoU. Bolded models form the speed/quality Pareto front.\n\nBackbone\n\nTeachers\nDINOv2 G/14\nOpenCLIP H/14\n\nExisting Efficient Models\nEfficientNetV2-S\nResNetv2-101\nRegNetY-064\nEfficientViT-L1\nConvNext-B\nNFNet-F3\nSwinV2-S\nMaxViT-B\nPoolformerV2-M36\nMViTV2-B\n\nProposed architecture\nE-RADIO-B\nE-RADIO-B w/o upsample\nE-RADIO-L\n\nParam. Count Throughput Zero Shot\n\nk-NN ADE20k\n\nFD loss\n\n1.14B\n632M\n\n21M\n44M\n30M\n38M\n88M\n254M\n49M\n119M\n56M\n51M\n\n118M\n113M\n265M\n\n313\n556\n\n9017\n7283\n6573\n6048\n1805\n1777\n1497\n1486\n1194\n975\n\n6422\n7040\n3472\n\nN/A\n77.19\n\n65.37\n69.58\n69.84\n71.73\n75.43\n76.93\n74.70\n77.49\n74.46\n75.92\n\n75.19\n75.45\n77.87\n\n83.41\n81.10\n\n70.72\n75.32\n74.59\n79.90\n81.73\n80.50\n81.12\n79.34\n80.49\n81.39\n\n82.21\n82.05\n83.73\n\n47.53\n40.04\n\n27.75\n29.61\n28.9\n33.12\n38.95\n38.31\n35.57\n38.46\n35.05\n41.39\n\n44.03\n41.26\n45.5\n\n0.415\n0.405\n0.394\n0.376\n0.358\n0.340\n0.364\n0.340\n0.377\n0.345\n\n0.319\n0.353\n0.265\n\nWe observe that many models lag behind teachers. Additionally, CNN-like models are significantly\nfaster than ViTs, while the latter are more accurate. The relatively low performance of existing\n\n7\n\n\fefficient backbones on the dense ADE20k segmentation task is not unexpected since all of them apply\na spatial dimension reduction factor of 32 for final feature maps of size 72 for input resolution of\n2242px, thus hardly capable of capturing fine-grain spatial information.\n\nE-RADIO: To overcome this issue, we propose a novel hybrid architecture, named E-RADIO\n(Efficient RADIO). This design borrows ideas from existing literature and includes an input stem\nwith strided convolutions to downsample the input image by 4x. It then proceeds with 2 stages of\nYOLOv8 C2f convolution blocks and 2 stages of transformer. For the transformer variant we pick\nwindowed attention (like in SWIN), and interleave local windowed attention with \u201cglobal\u201d windowed\nattention as done in and ViTDet. To perform \u201cglobal\u201d attention we first downsample the feature map\nby 2x, apply windowed attention, and then upsample the feature maps back to the original resolution.\n\n8",
  "conclusion": "",
  "is_publishable": 1,
  "venue": NaN
}
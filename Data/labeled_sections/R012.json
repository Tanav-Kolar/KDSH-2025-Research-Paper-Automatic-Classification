{
  "title": "Safe Predictors for Input-Output Specification\nEnforcement",
  "abstract": "machine learning models, which adhere to a collection of input-output specifica-\ntions. Our method involves the construction of a constrained predictor for each set\nof compatible constraints, and combining these predictors in a safe manner using a\nconvex combination of their predictions. We demonstrate the applicability of this\nmethod with synthetic datasets and on an aircraft collision avoidance problem.",
  "introduction": "applications, such as autonomous vehicles and aircraft collision avoidance, highlights an urgent\nneed for the development of guarantees on safety and robustness. These models may be required\nto satisfy specific input-output specifications to ensure the algorithms comply with physical laws,\ncan be executed safely, and are consistent with prior domain knowledge. Furthermore, these models\nshould demonstrate adversarial robustness, meaning their outputs should not change abruptly within\nsmall input regions \u2013 a property that neural networks often fail to satisfy.\n\nRecent studies have shown the capacity to verify formally input-output specifications and adversarial\nrobustness properties of neural networks. For instance, the Satisability Modulo Theory (SMT) solver\nReluplex was employed to verify properties of networks being used in the Next-Generation Aircraft\nCollision Avoidance System for Unmanned aircraft (ACAS Xu). Reluplex has also been used to\nverify adversarial robustness. While Reluplex and other similar techniques can effectively determine\nif a network satisfies a given specification, they do not offer a way to guarantee that the network will\nmeet those specifications. Therefore, additional methods are needed to adjust networks if it is found\nthat they are not meeting the desired properties.\n\nThere has been an increase in techniques for designing networks with certified adversarial robustness,\nbut enforcing more general safety properties in neural networks is still largely unexplored. One ap-\nproach to achieving provably correct neural networks is through abstraction-refinement optimization.\nThis approach has been applied to the ACAS-Xu dataset, but the network was not guaranteed to meet\nthe specifications until after training. Our work seeks to design networks with enforced input-output\nconstraints even before training has been completed. This will allow for online learning scenarios\nwhere a system has to guarantee safety throughout its operation.\n\nThis paper presents an approach for designing a safe predictor (a neural network or any other\nmachine learning model) that will always meet a set of constraints on the input-output relationship.\nThis assumes that the constrained output regions can be formulated to be convex. Our correct-\nby-construction safe predictor is guaranteed to satisfy the constraints, even before training, and at\nevery training step. We describe our approach in Section 2, and show its use in an aircraft collision\navoidance problem in Section 3. Results on synthetic datasets can be found in Appendix B.\n\n.\n\n\f2 Method\n\nConsidering two normed vector spaces, an input space X and an output space Y , and a collection\nof c different pairs of input-output constraints, (Ai, Bi), where Ai \u2286 X and Bi is a convex subset\nof Y for each constraint i, the goal is to design a safe predictor, F : X \u2192 Y , that guarantees\nx \u2208 Ai \u21d2 F (x) \u2208 Bi.\n\nLet b be a bit-string of length c. Define Ob as the set of points z such that, for all i, bi = 1 implies\nz \u2208 Ai, and bi = 0 implies z /\u2208 Ai. Ob thus represents the overlap regions for each combination of\ninput constraints. For example, O101 is the set of points in A1 and A3, but not in A2, and O0...0 is\nthe set where no input constraints apply. We also define O as the set of bit strings, b, such that Ob\nis non-empty, and define k = |O|. The sets {Ob : b \u2208 O} create a partition of X according to the\ncombination of input constraints that apply.\n\nGiven:\n\n\u2022 c different input constraint proximity functions, \u03c3i : X \u2192 [0, 1], where \u03c3i is continuous and\n\n\u2200x \u2208 Ai, \u03c3i(x) = 0,\n\n\u2022 k different constrained predictors, Gb : X \u2192 Bb, one for each b \u2208 O, such that the domain\n\nof each Gb is non-empty,\n\nWe define:\n\n\u2022 a set of weighting functions, wb(x) =\n\n(cid:80)\n\n(cid:80)\n\nb\u2208O wb(x) = 1, and\n\u2022 a safe predictor, F (x) = (cid:80)\n\nb\u2208O wb(x)Gb(x).\n\n(cid:81)\n\ni:bi=1(1\u2212\u03c3i(x)) (cid:81)\n(cid:81)\n\ni:bi=1(1\u2212\u03c3i(x)) (cid:81)\n\nb\u2208O\n\ni:bi=0 \u03c3i(x)\n\ni:bi=0 \u03c3i(x) , where\n\nTheorem 2.1. For all i, if x \u2208 Ai, then F (x) \u2208 Bi.\n\nA formal proof of Theorem 2.1 is presented in Appendix A and can be summarized as: if an input is\nin Ai, then by construction of the proximity and weighting functions, all of the constrained predictors,\nGb, that do not map to Bi will be given zero weight. Only the constrained predictors that map to\nBi will be given non-zero weight, and because of the convexity of Bi, the weighted average of the\npredictions will remain in Bi.\n\nIf all Gb are continuous and if there are no two input sets, Ai and Aj, for which (Ai \u2229 Aj) \u2282\n(\u2202Ai \u222a\u2202Aj), then F will be continuous. In the worst case, as the number of constraints grows linearly,\nthe number of constrained predictors needed to describe our safe predictor grows exponentially. In\npractice, however, we expect many of the constraint overlap sets, Ob, to be empty. Consequently, any\npredictors corresponding to an empty set can be ignored. This significantly reduces the number of\nconstrained predictors needed for many applications.\n\nSee Figure 1 for an illustrative example of how to construct F (x) for a notional problem with two\noverlapping input-output constraints.\n\n2.1 Proximity Functions\n\nThe proximity functions, \u03c3i, describe how close an input, x, is to a particular input constraint region,\nAi. These functions are used to compute the weights of the constrained predictors. A desirable\nproperty for \u03c3i is for \u03c3i(x) \u2192 1 as d(x, Ai) \u2192 \u221e, for some distance function. This ensures that\nwhen an input is far from a constraint region, that constraint has little influence on the prediction for\nthat input. A natural choice for such a function is:\n\n\u03c3i(x; \u03a3i) = 1 \u2212 exp\n\n\u2212\n\n(cid:18)\n\nd(x, Ai)\n\u03c31\n\n(cid:19)\u03c32\n\n.\n\nHere, \u03a3i is a set of parameters \u03c31 \u2208 (0, \u221e) and \u03c32 \u2208 (1, \u221e), which can be specified based on\nengineering judgment, or learned using optimization over training data. In our experiments in\nthis paper, we use proximity functions of this form and learn independent parameters for each\ninput-constrained region. We plan to explore other choices for proximity functions in future work.\n\n2\n\n\f2.2 Learning\n\nIf we have families of differentiable functions Gb(x; \u03b8b), continuously parameterized by \u03b8b, and\nfamilies of \u03c3i(x; \u03c7i), differentiable and continuously parameterized by \u03c7i, then F (x; \u0398, X), where\n\u0398 = {\u03b8b : b \u2208 O} and X = {\u03c7i : i = 1, ..., c}, is also continuously parameterized and differentiable.\nWe can thus apply standard optimization techniques (e.g., gradient descent) to find parameters of F\nthat minimize a loss function on some dataset, while also preserving the desired safety properties.\nNote that the safety guarantee holds regardless of the parameters. To create each Gb(x; \u03b8b) we\nconsider choosing:\n\n\u2022 a latent space Rm,\n\u2022 a map hb : Rm \u2192 Bb,\n\u2022 a standard neural network architecture gb : X \u2192 Rm,\n\nand then defining Gb(x; \u03b8b) = hb(gb(x; \u03b8b)).\n\nThe framework proposed here does not require an entirely separate network for each b. In many\napplications, it may be advantageous for the constrained predictors to share earlier layers, thus\ncreating a shared representation of the input space. In addition, our definition of the safe predictor is\ngeneral and is not limited to neural networks.\n\nIn Appendix B, we show examples of applying our approach to synthetic datasets in 2-D and 3-D\nwith simple neural networks. These examples show that our safe predictor can enforce arbitrary\ninput-output specifications using convex output constraints on neural networks, and that the learned\nfunction is smooth.\n\n3 Application to Aircraft Collision Avoidance\n\nAircraft collision avoidance requires robust safety guarantees. The Next-Generation Collision\nAvoidance System (ACAS X), which issues advisories to prevent near mid-air collisions, has both\nmanned (ACAS Xa) and unmanned (ACAS Xu) variants. The system was originally designed to\nchoose optimal advisories while minimizing disruptive alerts by solving a partially observable Markov\ndecision process. The solution took the form of a large look-up table, mapping each possible input\ncombination to scores for all possible advisories. The advisory with the highest score would then be\nissued. By using a deep neural network (DNN) to compress the policy tables, it has been necessary to\nverify that the DNNs meet certain safety specifications.\nA desirable \u02d8201csafeability\u02d8201d property for ACAS X was defined in a previous work. This property\nspeci01ed that for any given input state within the \u02d8201csafeable region,\u02d8201d an advisory would never\nbe issued that could put the aircraft into a state where a safe advisory would no longer exist. This\nconcept is similar to control invariance. A simplified model of the ACAS Xa system was created,\nnamed VerticalCAS. DNNs were then generated to approximate the learned policy, and Reluplex was\nused to verify whether the DNNs satisfied the safeability property. This work found thousands of\ncounterexamples where the DNNs did not meet the criteria.\n\nOur approach for designing a safe predictor ensures any collision avoidance system will meet the\nsafeability property by construction. Appendix C describes in detail how we apply our approach to\na subset of the VerticalCAS datasets using a conservative, convex approximation of the safeability\nconstraints. These constraints are defined such that if an aircraft state is in the \"unsafeable region\",\nAunsafeable,i, for the ith advisory, the score for that advisory must not be the highest, i.e., x \u2208\nAunsafeable,i \u21d2 Fi(x) < maxj Fj(x), where Fj(x) is the output score for the jth advisory.\nTable 1 shows the performance of a standard, unconstrained network and our safe predictor. For both\nnetworks, we present the percentage accuracy (ACC) and violations (percentage of inputs for which\nthe network outputs an unsafe advisory). We train and test using PyTorch with two separate datasets,\nbased on the previous advisory being Clear of Conflict (COC) and Climb at 1500 ft/min (CL1500).\nAs shown in the table, our safe predictor adheres to the required safeability property. Furthermore,\nthe accuracy of our predictor remains the same as the unconstrained network, demonstrating we are\nnot losing accuracy to achieve safety guarantees.\n\n3\n\n\fTable 1: Results of the best configurations of \u03b2-TCVAE on DCI, FactorVAE, SAP, MIG, and IRS\nmetrics.\n\nNETWORK\n\nACC (COC) VIOLATIONS (COC) ACC (CL1500) VIOLATIONS (CL1500)\n\nSTANDARD\nSAFE\n\n96.87\n96.69\n\n0.22\n0.00\n\n93.89\n94.78\n\n0.20\n0.00\n\n4 Discussion and Future Work\n\nWe propose an approach for designing a safe predictor that adheres to input-output specifications for\nuse in safety-critical machine learning systems, demonstrating it on an aircraft collision avoidance\nproblem. The novelty of our approach is its simplicity and guaranteed enforcement of specifications\nthrough combinations of convex output constraints during all stages of training. Future work includes\nadapting and using techniques from optimization and control barrier functions, as well as incorporating\nnotions of adversarial robustness into our design, such as extending the work to bound the Lipschitz\nconstant of our networks.\n\nAppendix A: Proof of Theorem 2.1\n\nProof. Fix i and assume that x \u2208 Ai. It follows that \u03c3i(x) = 0, so for all b \u2208 O where bi = 0,\nwb(x) = 0. Thus,\n\nF (x) =\n\n(cid:88)\n\nwb(x)Gb(x).\n\nb\u2208O,bi=1\n\nIf bi = 1, Gb(x) \u2208 Bi, and thus F (x) is also in Bi by the convexity of Bi.\n\nAppendix B: Example on Synthetic Datasets\n\nFigure 2 depicts an example of applying our safe predictor to a notional regression problem. This\nexample uses inputs and outputs in 1-D with one input-output constraint. The unconstrained network\nconsists of a single hidden layer with a dimension of 10, ReLU activations, and a fully connected layer.\nThe safe predictor shares this structure with the unconstrained network but has its own fully connected\nlayer for the constrained predictors, G0 and G1. Training uses a sampled subset of points from\nthe input space. Figure 3 shows an example of applying our safe predictor to a notional regression\nproblem with a 2-D input and 1-D output, using two overlapping constraints. The unconstrained\nnetwork has two hidden layers of dimension 20 and ReLU activations, followed by a fully connected\nlayer. The constrained predictors, G00, G10, G01, and G11, share the hidden layers but also have an\nadditional hidden layer of size 20 with ReLU, followed by a fully connected layer. Training uses a\nsampled subset of points from the input space.\n\nAppendix C: Details of VerticalCAS Experiment\n\nC.1 Safeability Constraints\n\nThe \"safeability\" property, originally introduced and used to verify the safety of the VerticalCAS\nneural networks can be encoded into a set of input-output constraints. The \"safeable region\" for\na given advisory represents input locations where that advisory can be selected such that future\nadvisories exist that will prevent an NMAC. If no future advisories exist, the advisory is \"unsafeable\"\nand the corresponding input region is the \"unsafeable region\". Examples of these regions, and their\nproximity functions are shown in Figure 5 for the CL1500 advisory.\n\nThe constraints we enforce are that x \u2208 Aunsafeable,i \u21d2 Fi(x) < maxj Fj(x), \u2200i, where Aunsafeable,i is\nthe unsafeable region for the ith advisory, and Fj(x) is the output score for the jth advisory. Because\nthe output regions of the safeable constraints are not convex, we make a conservative approximation,\nenforcing Fi(x) = minj Fj(x), for all x \u2208 Aunsafeable,i.\n\n4\n\n\fC.2 Proximity Functions\n\nWe start by generating the unsafeable region bounds from the open source code. We then compute a\n\"distance function\" between input space points (vO - vI, h, \u03c4 ), and the unsafeable region for each\nadvisory. These are not true distances but are 0 if and only if the data point is within the unsafeable\nset. These are then used to produce proximity functions as given in Equation 1.\n\nC.3 Structure of Predictors\n\nThe compressed policy tables for ACAS Xu and VerticalCAS use neural networks with six hidden\nlayers with a dimension of 45, and ReLU activation functions. We used the same architecture for the\nunconstrained network. For our constrained predictors, we use the same structure but have shared\nfirst four layers for all predictors. This provides a common learned representation of the input space,\nwhile allowing each predictor to adapt to its own constraints. After the shared layers, each constrained\npredictor has an additional two hidden layers and their final outputs are projected onto our convex\napproximation of the safe region of the output space, using Gb(x) = minj Gj(x). In our experiments,\nwe set \u03f5 = 0.0001.\n\nWith this construction, we needed 30 separate predictors to enforce the VerticalCAS safeability\nconstraints. The number of nodes for the unconstrained and safe implementations were 270 and 2880,\nrespectively. Our safe predictor is orders of magnitude smaller than the original look-up tables.\n\nC.4 Parameter Optimization\n\nWe use PyTorch for defining our networks and performing parameter optimization. We optimize both\nthe unconstrained and safe predictors using the asymmetric loss function to select advisories while\nalso accurately predicting scores. The data is split using an 80/20 train/test split with a random seed\nof 0. The optimizer is ADAM with a learning rate of 0.0003 and batch size of 216, with training for\n500 epochs.\n\nAppendix A: Proof of Theorem 2.1\n\nProof. Let x \u2208 Ai. Then, \u03c3i(x) = 0, and for all b \u2208 O where bi = 0, wb(x) = 0. Thus,\n\nF (x) =\n\n(cid:88)\n\nwb(x)Gb(x)\n\nb\u2208O,bi=1\n\nIf bi = 1, then Gb(x) \u2208 Bi, and therefore F (x) is in Bi due to the convexity of Bi.\n\nAppendix B: Example on Synthetic Datasets\n\nFigure 2 depicts an example of applying our safe predictor to a notional regression problem with 1-D\ninput and outputs, and one input-output constraint. The unconstrained network has a single hidden\nlayer of dimension 10 with ReLU activations, followed by a fully connected layer. The safe predictor\nshares this structure with constrained predictors, G0 and G1, but each predictor has its own fully\nconnected layer. The training uses a sampled subset of points from the input space and the learned\npredictors are shown for the continuous input space.\n\nFigure 3 shows an example of applying the safe predictor to a notional regression problem with a 2-D\ninput and 1-D output and two overlapping constraints. The unconstrained network has two hidden\nlayers of dimension 20 with ReLU activations, followed by a fully connected layer. The constrained\npredictors G00, G10, G01 and G11 share the hidden layers and have an additional hidden layer of size\n20 with ReLU followed by a fully connected layer. Again, training uses a sampled subset of points\nfrom the input space and the learned predictors are shown for the continuous input space.\n\n5\n\n\fAppendix C: Details of VerticalCAS Experiment\n\nC.1 Safeability Constraints\n\nThe \u201csafeability\u201d property from prior work can be encoded into a set of input-output constraints. The\n\u201csafeable region\u201d for a given advisory is the set of input space locations where that advisory can be\nchosen, for which future advisories exist that will prevent an NMAC. If no future advisories exist for\npreventing an NMAC, the advisory is deemed \u201cunsafeable,\u201d and the corresponding input region is the\n\u201cunsafeable region.\u201d Figure 5 shows an example of these regions for the CL1500 advisory.\n\nThe constraints we enforce in our safe predictor are: x \u2208 Aunsafeable,i \u21d2 Fi(x) < maxj Fj(x),\n\u2200i. To make the output regions convex, we approximate by enforcing Fi(x) = minj Fj(x), for all\nx \u2208 Aunsafeable,i.\n\nC.2 Proximity Functions\n\nWe start by generating the bounds on the unsafeable regions. Then, a distance function is computed\nbetween points in the input space (vO \u2212 vI , h, \u03c4 ), and the unsafeable region for each advisory. While\nthese are not true distances, their values are 0 if and only if the data point is inside the unsafeable set.\nWhen used to produce proximity functions as given in Equation 1, these values help ensure safety.\nFigure 5 shows examples of the unsafeable region, distance function, and proximity function for the\nCL1500 advisory.\n\nC.3 Structure of Predictors\n\nThe compressed versions of the policy tables from prior work are neural networks with six hidden\nlayers, 45 dimensions in each layer, and ReLU activation functions. We use the same architecture\nfor our standard, unconstrained network. For constrained predictors, we use a similar architecture.\nHowever, the first four hidden layers are shared between all of the predictors. This learns a single,\nshared input space representation, and also allows each predictor to adapt to its constraints. Each\nconstrained predictor has two additional hidden layers and their outputs are projected onto our convex\napproximation of the safe output region. We accomplish this by setting the score for any unsafeable\nadvisory i to Gi(x) = minj Gj(x) \u2212 \u03f5. In our experiments, we used \u03f5 = 0.0001.\n\nTo enforce the VerticalCAS safeability constraints, we need 30 separate predictors. This increases\nthe size of the network from 270 to 2880 nodes for the unconstrained and safe implementations\nrespectively. However, our safe predictor remains smaller than the original look-up tables by several\norders of magnitude.\n\nC.4 Parameter Optimization\n\nWe define our networks and perform parameter optimization using PyTorch. We optimize the\nparameters of both the unconstrained network and our safe predictor using the asymmetric loss\nfunction, guiding the network to select optimal advisories while accurately predicting scores from\nthe look-up tables. Each dataset is split using an 80/20 train/test split, with a random seed of 0. The\noptimizer is ADAM, with a learning rate of 0.0003, a batch size of 216, and the number of training\nepochs is 500.\n\n6",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "",
  "is_publishable": 1,
  "venue": "NeurIPS"
}
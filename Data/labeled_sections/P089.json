{
  "title": "Precise Requirements for the Validity of the Neural Tangent Kernel\nApproximation",
  "abstract": "This research investigates the conditions under which the neural tangent kernel (NTK) approximation remains\nvalid when employing the square loss function for model training. Within the framework of lazy training, as\nintroduced by Chizat et al., we demonstrate that a model, rescaled by a factor of \u03b1 = O(T ), maintains the validity\nof the NTK approximation up to a training time of T . This finding refines the earlier result from Chizat et al.,\nwhich necessitated a larger rescaling factor of \u03b1 = O(T 2), and establishes the preciseness of our established\nbound.",
  "introduction": "In contemporary machine learning practice, the weights w of expansive neural network models fw : Rdin \u2192 Rdout are trained\nusing gradient-based optimizers. However, a comprehensive theoretical understanding remains elusive due to the non-linear nature\nof the training dynamics, which complicates analysis. To bridge this gap, an approximation to these dynamics, termed the NTK\napproximation, was introduced, and its validity for infinitely wide networks trained via gradient descent was demonstrated. The NTK\napproximation has proven highly influential, offering theoretical insights into various phenomena, including deep learning\u2019s capacity\nto memorize training data, the manifestation of spectral bias in neural networks, and the differential generalization capabilities of\ndiverse architectures. Nevertheless, empirical evidence suggests that the training dynamics of neural networks frequently deviate\nfrom the NTK approximation\u2019s predictions. Consequently, it becomes crucial to delineate the precise conditions under which the\nNTK approximation remains applicable. This paper seeks to address the following inquiry:\n\nIs it possible to establish precise conditions that guarantee the validity of the NTK approximation?\n\n1.1 The Lazy Training Framework\n\nThe work demonstrated that the NTK approximation is applicable to the training of any differentiable model, provided the model\u2019s\noutputs are rescaled appropriately. This rescaling ensures that significant changes in the model\u2019s outputs can occur even with minor\nadjustments to the weights. The validity of the NTK approximation for models of infinite width stems from this observation, as the\nmodel is inherently rescaled as its width approaches infinity.\nConsider a smoothly parameterized model h : Rp \u2192 F, where F is a separable Hilbert space. Let \u03b1 > 0 be a parameter governing\nthe model\u2019s rescaling, which should be considered large. We train the rescaled model \u03b1h using gradient flow to minimize a smooth\nloss function R : F \u2192 R+. The weights w(t) \u2208 Rp are initialized at w(0) = w0 and evolve according to the gradient flow:\n1\n\u03b12 \u2207wR(\u03b1h(w(t))).\nDefine the linear approximation of the model around the initial weights w0 as:\n\ndw\ndt\n\n= \u2212\n\n(1)\n\n\u00afh(w) = h(w0) + Dh(w0)(w \u2212 w0),\nwhere Dh is the first derivative of h with respect to w. Let \u00afw(t) be weights initialized at \u00afw(0) = w0 that evolve according to the\ngradient flow from training the rescaled linearized model \u03b1\u00afh:\n1\n\u03b12 \u2207 \u00afwR(\u03b1\u00afh( \u00afw(t))).\n\nd \u00afw\ndt\n\n= \u2212\n\n(2)\n\n(3)\n\nThe NTK approximation asserts that:\n\n\u03b1h(w(t)) \u2248 \u03b1\u00afh( \u00afw(t)).\n(4)\nIn essence, this implies that the linearization of the model h remains valid throughout the training process. This greatly simplifies\nthe analysis of training dynamics, as the model \u00afh is linear in its parameters, allowing the evolution of \u00afh( \u00afw) to be understood through\na kernel gradient flow in function space.\n\n\fThe validity of the NTK approximation is contingent on the magnitude of the rescaling parameter \u03b1. Intuitively, a larger \u03b1\nimplies that the weights need not deviate significantly from their initialization to induce substantial changes in the model\u2019s output,\nthereby prolonging the validity of the linearization. This regime of training, where weights remain close to their initialization,\nis referred to as \"lazy training.\" The following bound was established, where R0 = R(\u03b1h(w0))) is the loss at initialization, and\n\u03ba = T \u03b1\u22121Lip(Dh)\n**Proposition 1.1.** Let R(y) = 1\nLipschitz and that Dh is Lip(Dh)-Lipschitz in a ball of radius \u03c1 around w0. Then, for any time 0 \u2264 T \u2264 \u03b1\u03c1/(Lip(h)\n\n2 be the square loss, where y\u2217 \u2208 F are the target labels. Assume that h is Lip(h)-\n\nR0 is a quantity that will also feature in our main results:\n\n2 \u2225y \u2212 y\u2217\u22252\n\nR0),\n\n\u221a\n\n\u221a\n\n\u2225\u03b1h(w(T )) \u2212 \u03b1\u00afh( \u00afw(T ))\u2225 \u2264 T Lip(h)2\u03baR0.\nAs \u03b1 approaches infinity, \u03ba tends to 0, rendering the right-hand side of the inequality small and validating the NTK approximation.\n\n(5)\n\n1.2 Our Contributions\n\nOur primary contribution is the refinement of the bound for extended time scales. We establish the following theorem:\n**Theorem 1.2 (NTK Approximation Error Bound).** Let R(y) = 1\n2 \u2225y \u2212 y\u2217\u22252\nLipschitz in a ball of radius \u03c1 around w0. Then, at any time 0 \u2264 T \u2264 \u03b12\u03c12/R0,\n(cid:112)\n\n2 be the square loss. Assume that Dh is Lip(Dh)-\n\n\u2225\u03b1h(w(T )) \u2212 \u03b1\u00afh( \u00afw(T ))\u2225 \u2264 min(6\u03ba\n\nR0, 8R0).\n\n(6)\n\nFurthermore, we demonstrate that this bound is tight up to a constant factor.\n**Theorem 1.3 (Converse to Theorem 1.2).** For any \u03b1, T , Lip(Dh), and R0, there exists a model h : R \u2192 R, a target y\u2217 \u2208 R, and\nan initialization w0 \u2208 R such that, for the risk R(y) = 1\n2 (y \u2212 y\u2217)2, the initial risk is R(\u03b1h(w0)) = R0, the derivative map Dh is\nLip(Dh)-Lipschitz, and\n\n\u2225\u03b1h(w(T )) \u2212 \u03b1\u00afh( \u00afw(T ))\u2225 \u2265 min\n\nR0\n\n.\n\n(7)\n\n(cid:18) 1\n5\n\n(cid:112)\n\n\u03ba\n\nR0,\n\n1\n5\n\n(cid:19)\n\nIn contrast to prior work, our bound does not depend on the Lipschitz constant of h, and it exhibits a more favorable dependence on\nT . Specifically, if Lip(Dh), Lip(h), and R0 are bounded by constants, our result indicates that the NTK approximation, up to an\n\u03b1\u03f5). Given the practical\nerror of O(\u03f5), holds for times T = O(\u03b1\u03f5), whereas the previously known bound was valid for T = O(\ninterest in long training times T \u226b 1, our result demonstrates that the NTK approximation is valid for significantly longer time\nhorizons than previously recognized.\n\n\u221a\n\n2 Application to Neural Networks\n\nThe bound established in Theorem 1.2 is applicable to the lazy training of any differentiable model. As a specific example, we detail\nits application to neural networks. We parameterize the networks in the mean-field regime, where the NTK approximation does not\nhold even as the width approaches infinity. Consequently, the NTK approximation is valid only when training is conducted in the\nlazy regime.\nLet fw : Rd \u2192 R be a 2-layer network of width m in the mean-field parametrization, with activation function \u03c3 : R \u2192 R,\n\nfw(x) =\n\n1\n\u221a\nm\n\nm\n(cid:88)\n\ni=1\n\n\u221a\n\nai\u03c3(\n\nm\u27e8x, ui\u27e9).\n\n(8)\n\n\u221a\n\nThe weights are w = (a, U ) for a = [a1, . . . , am] and U = [u1, . . . , um]. These are initialized at w0 with i.i.d.\nUnif[\u22121/\nm] entries. Given training data (x1, y1), . . . , (xn, yn), we train the weights of the network with the mean-\nsquared loss\n\nm, 1/\n\n\u221a\n\nL(w) =\n\n1\nn\n\nn\n(cid:88)\n\ni=1\n\n\u2113(fw(xi), yi),\n\n\u2113(a, b) =\n\n1\n2\n\n(a \u2212 b)2.\n\n(9)\n\nIn the Hilbert space notation, we let H = Rn, so that the gradient flow training dynamics with loss (6) correspond to the gradient\nflow dynamics (1) with the following model and loss function\n\nh(w) =\n\n1\n\u221a\nn\n\n[fw(x1), . . . , fw(xn)] \u2208 Rn, R(v) =\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\nv \u2212\n\n1\n2\n\ny\n\u221a\nn\n\n(cid:13)\n2\n(cid:13)\n(cid:13)\n(cid:13)\n2\n\n.\n\n(10)\n\nUnder certain regularity assumptions on the activation function (satisfied, for instance, by the sigmoid function) and a bound on the\nweights, it can be shown that Lip(Dh) is bounded.\n\n**Lemma 2.1 (Bound on Lip(Dh) for mean-field 2-layer network).** Suppose there exists a constant K such that (i) the activation\nfunction \u03c3 is bounded and has bounded derivatives \u2225\u03c3\u2225\u221e, \u2225\u03c3\u2032\u2225\u221e, \u2225\u03c3\u2032\u2032\u2225\u221e, \u2225\u03c3\u2032\u2032\u2032\u2225\u221e \u2264 K, (ii) the weights have bounded norm\n\u2225U \u2225a \u2264 K, and (iii) the data points have bounded norm \u2225x\u2225 \u2264 K. Then there exists a constant K \u2032 depending only on K such that\nLip(Dh) \u2264 K \u2032.\n(11)\n\n2\n\n\fSince the assumptions of Theorem 1.2 are met, we obtain the following corollary for the lazy training dynamics of the 2-layer\nmean-field network.\n\n**Corollary 2.2 (Lazy training of 2-layer mean-field network).** Suppose the conditions of Lemma 2.1 hold, and also that the labels\nare bounded in norm \u2225y\u2225 \u2264 c. Then there exist constants C, c > 0 depending only on K such that for any time 0 \u2264 T \u2264 c\u03b12,\n\n\u2225\u03b1h(w(T )) \u2212 \u03b1\u00afh( \u00afw(T ))\u2225 \u2264 C min(T /\u03b1, 1).\n\n(12)\n\nmfw, where fw is the network in the mean-field\nTraining in the NTK parametrization corresponds to training the model\n\u221a\nparametrization. This is equivalent to setting the lazy training parameter \u03b1 =\nm in the mean-field setting. Therefore, under the\nNTK parametrization with width m, the bound in Corollary 2.2 indicates that the NTK approximation is valid until training time\nO(m) and the error bound is O(T /\n\nm).\n\n\u221a\n\n\u221a\n\n3 Proof Ideas\n\n3.1 Proof Ideas for Theorem 1.2\n\nTo provide intuition for our proof, we first outline the approach used in the original proof. Define residuals r(t), \u00afr(t) \u2208 F\nunder training the original rescaled model \u03b1h(w(t)) and the linearized rescaled model \u03b1\u00afh( \u00afw(t)) as r(t) = y\u2217 \u2212 \u03b1h(w(t)) and\n\u00afr(t) = y\u2217 \u2212 \u03b1\u00afh( \u00afw(t)). These evolve according to\n\ndr\ndt\n\n= \u2212Ktr\n\nand\n\nd\u00afr\ndt\n\n= \u2212K0\u00afr,\n\n(13)\n\nwhere Kt := Dh(w(t))Dh(w(t))\u2217 is the time-dependent kernel. To compare these trajectories, it was observed that, since K0 is\npositive semidefinite,\n\nwhich, dividing both sides by \u2225r \u2212 \u00afr\u2225 and using \u2225r\u2225 \u2264\n\n\u221a\n\nR0, implies\n\nd\ndt\n\n\u2225r \u2212 \u00afr\u22252\n\n2 = \u2212\u27e8r \u2212 \u00afr, Ktr \u2212 K0\u00afr\u27e9 \u2264 \u2212\u27e8r \u2212 \u00afr, (Kt \u2212 K0)r\u27e9\n\n\u2225r \u2212 \u00afr\u2225 \u2264 \u2225Kt \u2212 K0\u2225\u2225r\u2225 \u2264 2Lip(h)Lip(Dh)\u2225w \u2212 w0\u2225\n\n(cid:112)\n\nR0.\n\nd\ndt\n\nUsing the Lipschitzness of the model, it was further shown that the weight change is bounded by \u2225w(t) \u2212 w0\u2225 \u2264 t\nPlugging this into (7) yields the bound in Proposition 1.1,\n\n\u2225\u03b1h(w(T )) \u2212 \u03b1\u00afh( \u00afw(T ))\u2225 = \u2225r(T ) \u2212 \u00afr(T )\u2225 \u2264 2Lip(h)2Lip(Dh)R0\u03b1\u22121\n\n(cid:90) T\n\n0\n\ntdt = T 2Lip(h)2Lip(Dh)R0/\u03b1.\n\n(16)\n\n**First attempt: strengthening of the bound for long time horizons** We demonstrate how to strengthen this bound to hold for longer\ntime horizons by employing an improved bound on the movement of the weights. Consider the following bound on the weight\nchange.\n\n**Proposition 3.1 (Bound on weight change, implicit in proof of Theorem 2.2).**\n\n\u2225w(T ) \u2212 w0\u2225 \u2264 (cid:112)T R0/\u03b1 and\n\n\u2225 \u00afw(T ) \u2212 w0\u2225 \u2264 (cid:112)T R0/\u03b1.\n\n**Proof of Proposition 3.1.** By (a) Cauchy-Schwarz, and (b) the nonnegativity of the loss R,\n\n\u2225w(T ) \u2212 w(0)\u2225 \u2264\n\n(cid:90) T\n\n0\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\ndw\ndt\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\n(a)\n\u2264\n\ndt\n\n(cid:115)\n\nT\n\n(cid:90) T\n\n0\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\ndw\ndt\n\n(cid:13)\n2\n(cid:13)\n(cid:13)\n(cid:13)\n\n(cid:115)\n\ndt =\n\n\u2212\n\nT\n\u03b12\n\n(cid:90) T\n\n0\n\nd\ndt\n\nR(\u03b1h(w(t)))dt\n\n(b)\n\n\u2264 (cid:112)T R0/\u03b1.\n\n(17)\n\n(18)\n\nThe bound for \u00afw is analogous.\n\nThis bound (8) has the advantage of\ninto (7), we obtain\n\n\u221a\n\nt dependence (instead of linear t dependence) and does not depend on Lip(h). Plugging it\n\n\u2225\u03b1h(w(T )) \u2212 \u03b1\u00afh( \u00afw(T ))\u2225 \u2264 2Lip(h)Lip(Dh)R0\u03b1\u22121\n\n(cid:90) T\n\n\u221a\n\n0\n\ntdt =\n\n4\n3\n\nT 3/2Lip(h)Lip(Dh)R0/\u03b1.\n\n(19)\n\nThis improves over Proposition 1.1 for long time horizons, as the time dependence scales as T 3/2 instead of T 2. However, it still\ndepends on the Lipschitz constant Lip(h) and falls short of the linear in T dependence of Theorem 1.2.\n\n**Second attempt: new approach to prove Theorem 1.2** To avoid dependence on Lip(h) and achieve a linear dependence in T ,\nwe develop a new approach. We cannot use (7), which was central to the original proof, as it depends on Lip(h). Furthermore, to\nachieve linear T dependence using (7), we would need \u2225w \u2212 w0\u2225 = O(1) for a constant independent of the time horizon, which is\nnot true unless the problem is well-conditioned.\n\n3\n\n(14)\n\n(15)\n\n\u221a\n\nR0Lip(h)/\u03b1.\n\n\fIn the full proof in Appendix A, we bound \u2225r(T ) \u2212 \u00afr(T )\u2225, which requires working with a product integral formulation of the\ndynamics of r to handle the time-varying kernels Kt. The main technical innovation in the proof is Theorem A.8, which is a new,\ngeneral bound on the difference between product integrals.\n\nTo avoid the technical complications of the appendix, we provide some intuitions here by proving a simplified theorem that does not\nimply the main result. We show:\n**Theorem 3.2 (Simplified variant of Theorem 1.2).** Consider r\u2032(t) \u2208 F initialized as r\u2032(0) = r(0) and evolving as dr\u2032\nThen,\n\ndt = \u2212KT r\u2032.\n\n(20)\nIntuitively, if we can prove in Theorem 3.2 that r\u2032(T ) and \u00afr(T ) are close, then the same should hold for r(T ) and \u00afr(T ) as in\nTheorem 1.2. For convenience, define the operators\nA = Dh(w0)\u2217\n\nand B = Dh(w(T ))\u2217 \u2212 Dh(w0)\u2217.\n\nR0, 8R0).\n\n\u2225r\u2032(T ) \u2212 \u00afr(T )\u2225 \u2264 min(3\u03ba\n\n(21)\n\n(cid:112)\n\nSince the kernels do not vary in time, the closed-form solution is\nr\u2032(t) = e\u2212(A+B)\u2217(A+B)tr(0)\n\nand\n\n\u00afr(t) = e\u2212A\u2217Atr(0)\n\nWe prove that the time evolution operators for r\u2032 and \u00afr are close in operator norm.\n**Lemma 3.3.** For any t \u2265 0, we have \u2225e\u2212(A+B)\u2217(A+B)t \u2212 e\u2212A\u2217At\u2225 \u2264 2\n**Proof of Lemma 3.3.** Define Z(\u03b6) = (A + \u03b6B)\u2217(A + \u03b6B)t. By the fundamental theorem of calculus,\n\nt\u2225B\u2225.\n\n\u221a\n\n\u2225e\u2212(A+B)\u2217(A+B)t \u2212 e\u2212A\u2217At\u2225 = \u2225eZ(1) \u2212 eZ(0)\u2225 =\n\nUsing the integral representation of the exponential map,\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\n(cid:90) 1\n\n0\n\nd\nd\u03b6\n\neZ(\u03b6)d\u03b6\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\n\u2264 sup\n\u03b6\u2208[0,1]\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\nd\nd\u03b6\n\neZ(\u03b6)\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\n.\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\nd\nd\u03b6\n\neZ(\u03b6)\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\n=\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\n(cid:90) 1\n\n0\n\ne(1\u2212\u03c4 )Z(\u03b6)\n\n(cid:19)\n\nZ(\u03b6)\n\n(cid:18) d\nd\u03b6\n\ne\u03c4 Z(\u03b6)d\u03c4\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\n=\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\n(cid:90) 1\n\n0\n\ne(1\u2212\u03c4 )Z(\u03b6)(A\u2217B + B\u2217A + 2\u03b6B\u2217B)e\u03c4 Z(\u03b6)d\u03c4\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\n(22)\n\n(23)\n\n(24)\n\nBy symmetry under transposing and reversing time, it suffices to bound the first term. Since \u2225e\u03c4 Z(\u03b6)\u2225 \u2264 1,\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\n\u2225e(1\u2212\u03c4 )Z(\u03b6)(A + \u03b6B)\u2217\u2225\u2225tB\u2225d\u03c4 \u2264 2t/e\u2225B\u2225 \u2264 2\n\ne(1\u2212\u03c4 )Z(\u03b6)(A + \u03b6B)\u2217Be\u03c4 Z(\u03b6)td\u03c4\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\n(cid:90) 1\n\n(cid:90) 1\n\n\u2264\n\n\u221a\n\n0\n\n0\n\nt\u2225B\u2225\n\n(25)\n\nFinally, let us combine Lemma 3.3 with the weight-change bound in Proposition 3.1 to prove Theorem 3.2. Notice that the\nweight-change bound in Proposition 3.1 implies\n\n\u2225B\u2225 \u2264 Lip(Dh)\u2225w(T ) \u2212 w0\u2225 \u2264 Lip(Dh)(cid:112)T R0/\u03b1.\n\n(26)\n\nSo Lemma 3.3 implies\n\n(27)\nCombining this with \u2225r\u2032(T ) \u2212 \u00afr(T )\u2225 \u2264 \u2225r\u2032(T )\u2225 + \u2225\u00afr(T )\u2225 \u2264 2\n2R0 implies (9). Thus, we have shown Theorem 3.2, which is the\nresult of Theorem 1.2 if we replace r by r\u2032. The actual proof of the theorem handles the time-varying kernel Kt and is in Appendix\nA.\n\n\u2225r\u2032(T ) \u2212 \u00afr(T )\u2225 \u2264 2Lip(Dh)T\n\u221a\n\nR0\u03b1\u22121\u2225r(0)\u2225 = 2\u03ba\u2225r(0)\u2225.\n\n(cid:112)\n\n3.2 Proof Ideas for Theorem 1.3\n\nThe converse in Theorem 1.3 is achieved in the simple case where h(w) = aw + 1\nw0 = 0 and R(y) = 1\n2R0)2, as we show in Appendix B by direct calculation.\n\n\u221a\n\n2 (y \u2212\n\n2 bw2 for a = 1/\n\nT and b = Lip(Dh), and\n\n\u221a\n\n4 Discussion\n\nA limitation of our result is that it applies only to gradient flow, which corresponds to SGD with infinitesimally small step size.\nHowever, larger step sizes are beneficial for generalization in practice, so it would be interesting to understand the validity of the\nNTK approximation in that setting. Another limitation is that our result applies only to the square loss and not to other popular\nlosses such as the cross-entropy loss. Indeed, the known bounds in the setting of general losses require either a \"well-conditioning\"\nassumption or taking \u03b1 exponential in the training time T . Can one prove bounds analogous to Theorem 1.2 for more general losses,\nwith \u03b1 depending polynomially on T , and without conditioning assumptions?\n\nA natural question raised by our bounds in Theorems 1.2 and 1.3 is: how do the dynamics behave just outside the regime where the\nNTK approximation is valid? For models h where Lip(h) and Lip(Dh) are bounded by a constant, can we understand the dynamics\nin the regime where T \u2248 C\u03b1 for some large constant C and \u03b1 \u226b C, at the edge of the lazy training regime?\n\n4",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "",
  "is_publishable": 1,
  "venue": NaN
}
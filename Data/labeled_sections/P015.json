{
  "title": "Overview of Challenges in Trajectory Forecasting and\n3D Perception for Autonomous Driving",
  "abstract": "This document provides a summary of the challenges faced in the domain of\nAutonomous Driving. The dataset incorporated into the study includes 150 minutes\nof labeled Trajectory and 3D Perception data, comprising approximately 80,000\nlidar point clouds and 1000 kilometers of trajectories in urban traffic conditions.\nThe competition is divided into two main segments: (1) Forecasting Trajectories\nand (2) 3D Lidar Object Recognition. Over 200 teams provided their results on the\nleaderboard, and more than 1,000 individuals took part in the workshop.",
  "introduction": "The focus of this paper is to investigate multi-frame perception, prediction, and planning as applied\nto autonomous driving. It serves as a platform to bring together academic and industry experts to\ndiscuss the uses of computer vision in the context of self-driving vehicles.\n\n2 Dataset\n\nThe Apolloscape Dataset is utilized as a research tool designed to advance autonomous driving in\nvarious dimensions, including perception, navigation, prediction, and simulation. This dataset is\ncomprised of labeled street view images and simulation resources that can accommodate user-defined\nstrategies. The dataset includes tasks such as Trajectory Prediction, 3D Lidar Object Detection,\n3D Lidar Object Tracking, lane marking segmentation, online self-positioning, 3D car instance\ncomprehension, Stereo, and Inpainting Dataset. A dedicated online assessment platform and user\ntoolkit are provided for each task.\n\nFor data collection related to Trajectory Prediction and 3D Perception, a data-gathering vehicle\nwas utilized to amass traffic information, including camera-captured images and LiDAR-generated\npoint clouds. Our vehicle operates in urban settings during peak traffic times. The dataset features\ncamera imagery, 3D point cloud data, and paths of traffic agents within the LiDAR\u2019s operational area.\nThis newly created dataset, which includes 150 minutes of sequential information, is extensive and\nconcentrates on urban roadways, with a particular emphasis on 3D perception, prediction, planning,\nand simulation activities involving a variety of traffic agents.\n\n3 Challenge\n\nThis part elaborates on the specifics of the challenges, the metrics for evaluation, and the outcomes\nachieved.\n\n3.1 Trajectory Prediction Challenge\n\nTrajectory information is documented at a rate of 2 frames per second. Each entry in the data\nfile includes the frame identifier, object identifier, object category, object\u2019s position in the global\n\n.\n\n\fcoordinate system along the x, y, and z axes, the object\u2019s dimensions in terms of length, width, and\nheight, and the object\u2019s orientation. Measurements for position and bounding box dimensions are\nprovided in meters. There are five distinct categories for object types: small vehicles are designated\nas 1, large vehicles as 2, pedestrians as 3, motorcyclists and bicyclists as 4, traffic cones as 5, and\nothers as 6.\n\n3.1.1 Evaluation Metric\n\nFor the assessment, the categories of small and large vehicles are merged into a single category\ntermed \u2019vehicle\u2019. The challenge requires using the initial three seconds of data from each sequence as\ninput to forecast the trajectories of objects for the subsequent three seconds. The objects assessed are\nthose present in the final frame of the first three seconds. Subsequently, the discrepancies between\nthe anticipated locations and the actual locations of these objects are calculated.\n\nThe following metrics are used to evaluate the effectiveness of the algorithms:\n\n1. Average Displacement Error (ADE): This metric represents the average Euclidean distance between\nall predicted positions and their corresponding actual positions throughout the forecasting period.\n\n2. Final Displacement Error (FDE): This metric calculates the average Euclidean distance between the\nultimately predicted positions and the actual final positions. Given the varying scales of trajectories\nfor vehicles, pedestrians, and bicyclists, a weighted sum of ADE (WSADE) and a weighted sum of\nFDE (WSFDE) are employed as metrics.\n\nW SADE = Dv \u00b7 ADEv + Dp \u00b7 ADEp + Db \u00b7 ADEb (1)\n\nW SF DE = Dv \u00b7 F DEv + Dp \u00b7 F DEp + Db \u00b7 F DEb (2)\n\nHere, Dv, Dp, and Db are associated with the inverse of the average speeds of vehicles, pedestrians,\nand bicyclists in the dataset, with values set at 0.20, 0.58, and 0.22, respectively.\n\n3.2\n\n3D Detection Challenge\n\nThe dataset for 3D Lidar object detection features LiDAR-scanned point clouds accompanied by\ndetailed annotations. It was gathered in Beijing, China, under diverse conditions of lighting and\ntraffic density. Specifically, the dataset encompasses intricate traffic patterns that include a mix of\nvehicles, cyclists, and pedestrians.\n\n3.2.1 Data Structure\n\nEach annotated file for 3D Lidar object detection represents a one-minute sequence captured at\ntwo frames per second. An entry within each file includes the frame number, object ID, object\nclassification, positions along the x, y, and z axes, object dimensions (length, width, height), and\norientation. Object classifications are consistent with those in the trajectory data. In this evaluation, the\nfirst two categories\u2014small and large vehicles\u2014are considered as a single \u2019vehicle\u2019 class. Positional\ndata is relative, with units in meters, and the heading angle denotes the object\u2019s steering direction.\n\n3.2.2 Evaluation Metric\n\nThe evaluation metric is analogous to the one defined in prior work. The aim of the 3D object\ndetection task is to develop detectors for \u2019vehicle\u2019, \u2019pedestrian\u2019, and \u2019bicyclist\u2019 categories. These\ndetectors should estimate the 3D bounding box (dimensions and position) and provide a detection\nscore or confidence. It is important to note that not all objects within the point clouds are labeled.\nThe performance of 3D object detection is assessed using the mean Average Precision (mAP),\nbased on Intersection over Union (IoU). The evaluation standard aligns with the 2D object detection\nbenchmark, utilizing 3D bounding box overlap. The ultimate metric is the average mAP across\nvehicles, pedestrians, and bicyclists, with IoU thresholds set at 0.7 for cars, and 0.5 for both pedestrians\nand cyclists.\n\n2\n\n\f4 Methods and Teams\n\n4.1 Trajectory prediction\n\nOne team utilized an encoder-decoder framework based on LSTM for predicting trajectories on city\nstreets. To enhance prediction accuracy, they implemented four sequence-to-sequence sub-models\nto capture the distinct movement characteristics of various traffic participants. They produced a\nfuture trajectory for each agent through a three-step process: encoding, perturbation, and decoding.\nInitially, an encoder was employed to embed the past trajectory. Subsequently, they introduced a\n16-dimensional random noise to the encoder\u2019s output to accommodate the multimodal distribution of\nthe data. Finally, they generated the predicted trajectory via a decoder that mirrored the encoder\u2019s\nstructure.\n\nIn addition, they attempted to capture the collective influence among road agents using an interaction\ntechnique. Improving upon the original methodology, they conducted an interaction operation at each\nmoment during the encoding and decoding phases. The interaction module embedded the positions\nof all agents and generated a comprehensive 128-dimensional spatiotemporal representation using\nan LSTM unit. The derived feature was then relayed to the encoders or decoders for the primary\nprediction task. Each encoder or decoder, linked to a particular individual, produced the private\ninteraction within a confined area through an attention operation, utilizing the aforementioned global\nfeature and the agent\u2019s position. Their experimental findings indicated that the interaction module\nenhanced prediction accuracy on the dataset.\n\n4.2\n\n3D Detection\n\nOne team introduced an innovative approach termed sparse-to-dense 3D object detector (STD). STD\nis characterized as a two-stage, point-based detection system. The initial phase involves a bottom-up\nnetwork for generating proposals, where spherical anchors are seeded on each point to encompass\nobjects at various orientations. This spherical anchor design reduces computational load and shortens\ninference time by eliminating the need to account for differently oriented objects during anchor\ncreation. Subsequently, points within these spherical anchors are collected to form proposals for\nadditional refinement. In the second phase, a PointsPool layer is introduced to transform the features\nof proposals from point-based representations to compact grid formats. These dense features are then\nprocessed through a prediction head, which includes two extra fully-connected layers, to derive the\nfinal detection outcomes. A 3D intersection-over-union (IoU) branch is also incorporated into the\nprediction head to estimate the 3D IoU between the final predictions and the ground-truth bounding\nboxes, thereby enhancing localization precision.\n\nDuring the training process, four distinct data augmentation techniques were employed to mitigate\noverfitting. Initially, similar to previous methods, ground-truth bounding boxes with their correspond-\ning interior points were randomly added from different scenes to the existing point cloud, simulating\nobjects in varied settings. Subsequently, each bounding box was randomly rotated based on a uniform\ndistribution and subjected to random translation. Additionally, every point cloud was randomly\nflipped along the x-axis with a 50% probability. Lastly, random rotation and scaling were applied to\neach point cloud using uniformly distributed random variables. In the testing phase, predictions were\nfirst obtained on both the original and the x-axis flipped point clouds, and these results were then\nmerged using Soft-NMS to produce the final predictions.\n\nAnother team\u2019s strategy is based on the PointPillars framework. The network configuration largely\nmirrors that of the original work, with adjustments made to accommodate multiple anchors for\neach class. The substantial variation in the size of objects within each class suggested that a single\nanchor might be inadequate. The k-means algorithm was utilized to create five anchors for each class.\nAnother modification involved deactivating the direction classification in the loss function, as the\nevaluation metric relies on IOU, which is not affected by direction. Detailed settings for each class\nare presented in Table 1.\n\nTo enhance training data, global translation and scaling of the point cloud, along with rotation and\ntranslation for each ground truth, were implemented. Global rotation of the point cloud was omitted\nas it was found to produce less favorable outcomes. The specific parameters for these adjustments are\ndetailed in Table 2.\n\n3\n\n\fTable 1: Detailed settings for each class. MNP indicates the maximum number of points, and MNV\nrepresents the maximum number of voxels.\n\nClass\n\nNumber of anchors\n\nVoxel size\n\nMNP MNV\n\nCar\nBicyclist\nPedestrian\n\n5\n5\n5\n\n[0.28,0.28,32]\n[0.14,0.14,32]\n[0.10,0.10,32]\n\n50\n20\n15\n\n20000\n80000\n80000\n\nTable 2: Augmentation parameters for training data.\n\nGlobal Rotation Global Translation Global Scaling Ground Truth Rotation Ground Truth Translation\n\n[0.2,0.2,0.2]\n\n[0.95,1.1]\n\n[-/20, /20]\n\n[0.25,0.25,0.25]\n\nTest Time Augmentation was employed to enhance performance. For every point cloud, four iterations\nwere generated: the original, and versions flipped along the x-axis, y-axis, and both axes. Each\niteration was processed by the network to obtain bounding box predictions, which were subsequently\nunflipped. Due to the flipping operation, anchors across iterations have a one-to-one correspondence.\nFor each anchor, the corresponding predicted boxes were combined by averaging the location, size,\nand class probability. Redundant boxes were then eliminated using Non-Maximum Suppression\n(NMS).\n\nAnother Team introduced enhancements to the PointPillars method. Their approach incorporated\nresidual learning and channel attention mechanisms into the baseline architecture. The network is\ncomposed of the original Pillar Feature Network, an extended 2D CNN backbone, and a detection\nhead for foreground/background classification and regression. The deeper backbone significantly\nimproves detection accuracy compared to the original PointPillars. A separate network was trained\nfor each class in the Apollo training dataset to perform binary classification, resulting in four distinct\nnetworks. Final predictions were compiled by aggregating all foreground predictions from these\nnetworks.\n\nFor dataset preprocessing, methods from the KITTI dataset were adapted, including positive example\nsampling, global rotation, individual object rotation, and random scaling for each object. However,\nunlike the KITTI approach, global rotation was excluded, and the ranges for scaling and rotation were\nreduced. Additionally, more foreground point clouds were sampled to augment positive examples.\nTable 3 details the specific settings for each class.\n\nTable 3: Detailed settings for each class. MSN indicates the maximum sampling number.\n\nClass\n\nPointcloud Range (m)\n\nPillar Size (m)\n\nAnchor Size (m)\n\nMSN\n\nVehicles\nPedestrian\nMotor&bicyclist\n\nx: -70.8 to 70, y: -67.2 to 67.2, z: -3 to 1\nx: -70.8 to 70, y: -67.2 to 67.2, z: -2.5 to 0.5\nx: -70.8 to 70, y: -67.2 to 67.2, z: -2.5 to 0.5\n\nx: 0.16, y: 0.16, z: 3\nx: 0.2, y: 0.2, z: 3\nx: 0.2, y: 0.2, z: 3\n\nx: 1.6, y: 3.9, z: 1.56\nx: 0.6, y: 1.76, z: 1.73\nx: 0.6, y: 0.8, z: 1.73\n\n15\n15\n15\n\n5 Conclusion and Future Work\n\nThis paper provides a review of the challenges encountered in the domain of Autonomous Driving,\nwith a focus on the analysis of 3D Detection and Trajectory prediction. It is anticipated that this paper\nwill offer contemporary insights into these research areas.\n\nFuture endeavors will aim to refine the open-source tools and dataset for autonomous driving.\nMoreover, additional workshops and challenges are planned to foster the exchange of concepts and to\ncollectively propel the field of autonomous driving research forward.\n\n4",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "",
  "is_publishable": 1,
  "venue": NaN
}
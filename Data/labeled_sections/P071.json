{
  "title": "The Significance of Fillers in Textual Representations\nof Speech Transcripts",
  "abstract": "This paper investigates the role of fillers within text-based representations of speech\ntranscripts. While often ignored in Spoken Language Understanding tasks, we\ndemonstrate that these elements, such as \"um\" or \"uh,\" when incorporated using\ndeep contextualized embeddings, enhance the modeling of spoken language. This\nis further shown through improvements in downstream tasks like predicting a\nspeaker\u2019s stance and their expressed confidence.",
  "introduction": "This paper addresses the critical role of disfluencies, specifically fillers, in spoken language processing.\nDisfluencies, which encompass phenomena like silent pauses, word repetitions, or self-corrections,\nare inherent to spoken language. Fillers, a type of disfluency, often manifest as sounds like \"um\" or\n\"uh,\" serving to bridge pauses during utterances or conversations.\n\nWhile prior research has demonstrated the efficacy of contextualized embeddings pre-trained on\nwritten text for adapting to smaller spoken language corpora, these models typically exclude fillers and\ndisfluencies in pre-processing. This practice is at odds with linguistic research, which considers fillers\nto be informative and integral to spoken language. Existing methods for analyzing fillers primarily\nrely on handcrafted features. Furthermore, pre-trained word embeddings trained on written text\nhave shown poor performance in representing spontaneous speech words like \"uh,\" as their meaning\nvaries significantly in spoken contexts. In this work, we explore the use of deep contextualized word\nrepresentations to model fillers. We assess their value in spoken language tasks without relying on\nmanual feature engineering.\n\nThe core motivation of this study stems from the following observations: First, fillers are essential\nto spoken language. For instance, speakers may employ fillers to signal the linguistic structure of\ntheir utterances, such as difficulties in choosing vocabulary or to indicate a pause in their speech.\nSecond, research has connected fillers and prosodic cues to a speaker\u2019s Feeling of Knowing (FOK)\nor expressed confidence, signifying a speaker\u2019s commitment to a statement. Fillers and prosodic\ncues influence a listener\u2019s perception of a speaker\u2019s expressed confidence, known as the Feeling\nof Another\u2019s Knowing (FOAK). Finally, fillers have been successfully applied in stance prediction,\nwhich gauges a speaker\u2019s subjective attitude.\n\nTherefore, we intend to validate these observations by exploring how to efficiently represent fillers\nautomatically. Our key contributions are: (1) Fillers convey useful information that can be harnessed\nthrough deep contextualized embeddings to improve spoken language modeling and should not be\ndiscarded. We also investigate the best filler representation strategies for Spoken Language Modeling\n(SLM) and examine the learned positional distribution of fillers. (2) In a spontaneous speech corpus\nof monologues, we show that fillers serve as a distinctive feature in predicting both a speaker\u2019s\nperceived confidence and their expressed sentiment.\n\n\f2 Models and Data Description\n\n2.1 Model Description\n\nIn this work, we focus on the two fillers \"uh\" and \"um.\" To generate contextualized word embeddings\nfor fillers, we use Bidirectional Encoder Representations from Transformers (BERT), given its state-\nof-the-art performance in several NLP tasks and its enhanced ability to integrate context compared to\nWord2Vec.\n\n2.1.1 Spoken Language Modeling\n\nWe utilize a masked language modeling (MLM) approach for Spoken Language Modeling. This\ninvolves masking some input words at random and then attempting to predict those masked tokens.\nThis is a standard way of pre-training and fine-tuning BERT. In our case, this method will be used to\nfine-tune a pre-trained BERT model on a spoken language corpus. Each experiment involves a token\nrepresentation strategy i and a pre-processing strategy Si.\n\nThe token representation strategies are essential for our goal of learning the distribution of fillers\nusing BERT. The three token representation strategies are outlined as follows: T1 involves no special\nprocessing for the fillers and BERT is left to use its prior understanding of fillers to model language.\nIn T2, \"uh\" and \"um\" are marked with specific filler tags to distinguish them from other tokens, with\neach filler represented as separate tokens. This strategy encourages BERT to learn new embeddings\nthat emphasize filler context and position. In T3, both fillers are represented as the same token,\nindicating that they carry the same meaning. Table 1 gives a concrete example of this process.\n\n2.1.2 Pre-processing\n\nWe investigate the impact of three pre-processing strategies denoted by S1, S2 and S3. In S1, all\nfillers are removed from the sentences during both training and inference. In S2, fillers are kept\nduring training, but removed during inference. In S3, fillers are preserved during both training and\ninference. For each combination of pre-processing and token representation strategies, we fine-tune\nBERT using the Masked Language Model objective like the original BERT paper. If fine-tuning is\nnot performed the training data of S1 and S2 are equivalent. We evaluate the model performance in\nlanguage modeling using perplexity (ppl).\n\n2.1.3 Confidence and Sentiment Prediction\n\nIn tasks of confidence prediction and sentiment analysis, our objective is to use BERT\u2019s text rep-\nresentations, which include fillers, to predict a confidence/sentiment label. We add a Multi-Layer\nPerceptron (MLP) to BERT, which may have been fine-tuned using MLM. The MLP is trained by min-\nimizing the mean squared error (MSE) loss. These experiments adopt the same token representation\nand pre-processing techniques discussed in Section 2.1.1.\n\n2.2 Data Description\n\nWe use the Persuasive Opinion Mining (POM) dataset which contains 1000 English monologue\nvideos. The speakers recorded themselves giving a movie review. The movies were rated between\n1 (most negative) and 5 stars (most positive). The videos were annotated for high-level attributes\nsuch as confidence, where annotators rated from 1 (not confident) to 7 (very confident). Similarly,\nsentiment was scored by annotators between 1 (strongly negative) to 7 (strongly positive).\n\nThis dataset was chosen for several reasons: (1) The corpus contains manual transcriptions with\nfillers \"uh\" and \"um,\" where approximately 4% of speech consists of fillers. Additionally, sentence\nmarkers are transcribed, with fillers at sentence beginnings if they occur between sentences. (2)\nThe dataset includes monologues, where speakers are aware of an unseen listener, thus we can\nconcentrate on fillers in speaker narratives. (3) The sentiment/stance polarity was clearly defined\nby choosing only reviews that were rated with 1-2 or 5 stars for annotation purposes. (4) FOAK,\nmeasured by confidence labels, has high inter-annotator agreement. More details can be found in\nsupplementary materials. The confidence labels are the root mean square (RMS) values of labels\ngiven by 3 annotators. The sentiment labels are the average of the 3 labels.\n\n2\n\n\fToken. Raw\n\nOutput Tokenizer\n\n(umm) Things that (uhh) you usually wouldn\u2019t find funny were in this movie.\n\n[\u2019umm\u2019, \u2019things\u2019, \u2019that\u2019, \u2019uh\u2019, \u2019you\u2019, \u2019usually\u2019, \u2019wouldn\u2019, \"\u2019\", \u2019t\u2019, \u2019find\u2019, \u2019funny\u2019, \u2019were\u2019, \u2019in\u2019, \u2019this\u2019, \u2019movie\u2019, \u2019.\u2019]\n\n[\u2019umm\u2019, \u2019things\u2019, \u2019that\u2019, \u2019uh\u2019, \u2019you\u2019, \u2019usually\u2019, \u2019wouldn\u2019, \"\u2019\", \u2019t\u2019, \u2019find\u2019, \u2019funny\u2019, \u2019were\u2019, \u2019in\u2019, \u2019this\u2019, \u2019movie\u2019, \u2019.\u2019]\n\n[\u2019[FILLERUMM]\u2019, \u2019things\u2019, \u2019that\u2019, \u2019[FILLERUHH]\u2019, \u2019you\u2019, \u2019usually\u2019, \u2019wouldn\u2019, \"\u2019\", \u2019t\u2019, \u2019find\u2019, \u2019funny\u2019, \u2019were\u2019, \u2019in\u2019, \u2019this\u2019, \u2019movie\u2019, \u2019.\u2019]\n\n[\u2019[FILLER]\u2019, \u2019things\u2019, \u2019that\u2019, \u2019[FILLER]\u2019, \u2019you\u2019, \u2019usually\u2019, \u2019wouldn\u2019, \"\u2019\", \u2019t\u2019, \u2019find\u2019, \u2019funny\u2019, \u2019were\u2019, \u2019in\u2019, \u2019this\u2019, \u2019movie\u2019, \u2019.\u2019]\n\nRaw\n\nT1\n\nT2\n\nT3\n\nTable 1: Filler representation using different token representation strategies\n\n3 Experiments and Analysis\n\n3.1 Fillers Can Be Leveraged to Model Spoken Language\n\nLanguage Modeling with fillers. We examine language model (LM) perplexity using various\npre-processing strategies, using a fixed token representation strategy of T1. The results in Table 2(a)\ncompares S1, S2 and S3. By keeping fillers during both training and inference, the model reaches a\nlower perplexity, with a reduction of at least 10%. Therefore, fillers provide information that BERT\ncan effectively use.\n\nThe fine-tuning procedure improves the language model\u2019s perplexity. Additionally, even without\nfine-tuning, S3 outperforms S1 and S2 by reducing perplexity when fillers are used. This implies that\nBERT has prior knowledge of spoken language and uses the fillers.\n\nConsequently, fillers can reduce uncertainty of BERT for SLM. This is not an intuitive outcome; one\nmight assume that removing fillers during training and inference would decrease perplexity. The\nfact that S3 exceeds other preprocessing methods shows that the Masked Language Model (MLM)\nprocess effectively learns this filler information.\n\nBest token representation: The results presented in Table 2(b) reveal that T1 outperforms other\nrepresentations when fine-tuning. Given the limited data and high BERT embedding dimensionality\n(768), retaining existing representations with T1 is better than learning representations from the\nscratch. Interestingly, T2 and T3 perform similarly. The hypothesis is that the difference between\n\"uh\" and \"um\" lies only in the duration of the pause, which cannot be captured in text. Considering\nthese results, T1 is fixed as the token representation strategy in all subsequent experiments.\n\nLearned positional distribution of fillers: We further test our model\u2019s learning of filler placement.\nWe fine-tune BERT using a filler to determine where the model believes the fillers most likely reside.\nGiven a sentence S with length L, we introduce a mask token after the word j and obtain S*. We then\ncompute the probability of a filler in position j+1.\n\nSpecifically, we calculate P([MASK=filler] | S), as depicted in Figure 1. Then, we plot the average\nprobability of the masked word being a filler given its sentence position in Figure 2. The fine-tuned\nBERT model with fillers predicts a high probability of fillers occurring at the beginning of sentences.\nThis pattern is consistent with filler distribution in the dataset. The fine-tuned BERT without fillers,\npredicts constant low probabilities. Given that we only know sentence boundaries we still manage\nto observe that the model captures a similar positional distribution of fillers that are found in other\nworks.\n\n(a) LM Task\n\n(b) Best token representation\n\n(c) FOAK and Sentiment\n\nFine\n\nSetting Token\n\nPpl\n\nSetting\n\nToken\n\nFOAK\n\n3*w/o\n\n3*w\n\nS1\nS2\nS3\n\nS1\nS2\nS3\n\n3*S3\n\n3*S3\n\nT1\nT1\nT1\n\nT1\nT1\nT1\n\n22\n22\n20\n\n5.5\n5.6\n4.6\n\nT1\nT2\nT3\n\nT1\nT2\nT3\n\n1.47\n1.45\n1.30\n\n1.32\n1.31\n1.24\n\nSent\n\n1.98\n1.75\n1.44\n\n1.39\n1.40\n1.22\n\nTable 2: From left to right, the (a) LM Task, (b) Best token representation, (c) MSE of Confidence\n(FOAK) and the Sentiment (Sent) prediction task. Highlighted results exhibit significant differences\n(p-value < 0.005).\n\n3\n\n\f1.\n\n2\n\n(umm)\n\n|\n\nthought\n\n|\n\nthought\n\n=\n\nthis\n\nthis\n\nmovie\n\nmovie\n\nwas\n\nwas\n\nreally\n\nbad\n\nreally\n\nbad\n\n|\n\n3.\n\nthought\n\nmovie\nTable 3: Predicting the probability of a filler, where 1. Raw input, 2. Pre-processed text with the filler\nremoved, and 3. Illustrates the [MASK] procedure for predicting the probability of a filler at position\n5\n\n[MASK] was\n\nreally\n\nthis\n\nbad\n\n3.2 Fillers are a discriminative feature for FOAK and stance prediction\n\nWe look at the impact of fillers on two downstream tasks: FOAK prediction and sentiment analysis.\nPsycholinguistic studies have found a link between fillers and expressed confidence. Prior work has\nlinked fillers and a speaker\u2019s expressed confidence in the narrow field of QA tasks. Fillers have also\nbeen used to predict stance. In this work, we present data that suggests fillers play a role in predicting\na speaker\u2019s expressed confidence and their stance.\n\nTable 2(c) shows that S3, both with and without fine-tuning, reduces the MSE compared to S1 and\nS2. S1 and S2 have similar MSE since they remove fillers during inference. S2 has a higher MSE,\npossibly due to the mismatch between training and test datasets. This demonstrates that fillers can be\na discriminative feature in FOAK and stance prediction.\n\nDoes using fillers always improve results for spoken language tasks? In the subsection 3.1, we\nobserve that including fillers reduces MLM perplexity. An assumption is that that downstream tasks\nwould also benefit from the inclusion of fillers. However, we notice that when predicting speaker\npersuasiveness, the fillers are not a discriminative feature, following the same procedure as outlined\nin subsubsection 2.1.2.",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "This paper demonstrates that retaining fillers in transcribed spoken language when using deep\ncontextualized representations can improve results in language modeling and downstream tasks\nsuch as FOAK and stance prediction. We also propose and compare several token representation\nand pre-processing strategies for integrating fillers. We plan to extend these results to consider\ncombining textual filler-oriented representations with acoustic representations, and to further analyze\nfiller representation learned during pre-training.\n\n4",
  "is_publishable": 1,
  "venue": NaN
}
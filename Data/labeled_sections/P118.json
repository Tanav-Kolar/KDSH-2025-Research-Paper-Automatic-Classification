{
  "title": "Distant Supervision from Disparate Sources for\nLow-Resource Part-of-Speech Tagging",
  "abstract": "We introduce DSDS: a cross-lingual neural part-of-speech tagger that learns from\ndisparate sources of distant supervision, and realistically scales to hundreds of low-\nresource languages. The model exploits annotation projection, instance selection,\ntag dictionaries, morphological lexicons, and distributed representations, all in a\nuniform framework. The approach is simple, yet surprisingly effective, resulting in\na new state of the art without access to any gold annotated data.",
  "introduction": "Low-resource languages lack manually annotated data to learn even the most basic models such\nas part-of-speech (POS) taggers. To compensate for the absence of direct supervision, work in\ncrosslingual learning and distant supervision has discovered creative use for a number of alternative\ndata sources to learn feasible models:\n\nHowever, only one or two compatible sources of distant supervision are typically employed. In\nreality severely under-resourced languages may require a more pragmatic \u201ctake what you can get\u201d\nviewpoint. Our results suggest that combining supervision sources is the way to go about creating\nviable low-resource taggers.\n\nWe propose a method to strike a balance between model simplicity and the capacity to easily integrate\nheterogeneous learning signals.\n\nsystem is a uniform neural model for POS tagging that learns from disparate sources of distant\nsupervision (DSDS). We use it to combine:\ni) multi-source annotation projection, ii) instance\nselection, iii) noisy tag dictionaries, and iv) distributed word and sub-word representations. We\nexamine how far we can get by exploiting only the wide-coverage resources that are currently readily\navailable for more than 300 languages, which is the breadth of the parallel corpus we employ.\n\nDSDS yields a new state of the art by jointly leveraging disparate sources of distant supervision in an\nexperiment with 25 languages. We demonstrate: i) substantial gains in carefully selecting high-quality\ninstances in annotation projection, ii) the usefulness of lexicon features for neural tagging, and iii)\nthe importance of word embeddings initialization for faster convergence.\n\n2 Method\n\nDSDS is illustrated in Figure 1. The base model is a bidirectional long short-term memory network\n(bi-LSTM)\n\nAnnotation projection. Ever since the seminal work of projecting sequential labels from source to\ntarget languages has been one of the most prevalent approaches to crosslingual learning. Its only\nrequirement is that parallel texts are available between the languages, and that the source side is\nannotated for POS.\n\nWe apply the approach by where labels are projected from multiple sources and then decoded through\nweighted majority voting with word alignment probabilities and source POS tagger confidences. We\n\n\fexploit their widecoverage Watchtower corpus (WTC), in contrast to the typically used Europarl data.\nEuroparl covers 21 languages of the EU with 400k-2M sentence pairs, while WTC spans 300+ widely\ndiverse languages with only 10-100k pairs, in effect sacrificing depth for breadth, and introducing a\nmore radical domain shift. However, as our results show little projected data turns out to be the most\nbeneficial, reinforcing breadth for depth.\n\nWhile selected 20k projected sentences at random to train taggers, we propose a novel alternative:\nselection by coverage. We rank the target sentences by percentage of words covered by word\nalignment from 21 sources and select the top k covered instances for training. In specific, we employ\nthe mean coverage ranking of target sentences, whereby each target sentence is coupled with the\narithmetic mean of the 21 individual word alignment coverages for each of the 21 source-language\nsentences. We show that this simple approach to instance selection offers substantial improvements:\nacross all languages, we learn better taggers with significantly fewer training instances.\n\nDictionaries. Dictionaries are a useful source or distant supervision. There are several ways to\nexploit such information: i) as type constraints during encoding, ii) to guide unsupervised learning,\nor iii) as addiional signal at training. We focus on the latter and evaluate two ways to integrate\nlexical knowledge into neural models, while comparing to the former wo: a) by representing lexicon\nproperties as n-hot vector (e.g., if a word has two properties according to lexicon src, it results\nin a 2-hot vector, if the word is not present in src, a zero vector), with m the number of lexicon\nproperties; b) by embedding the lexical features, i.e., is a lexicon src embedded into an /-dimensional\nspace. We represent as concatenation of all embedded m properies of length [, and a zero vector\notherwise. Tuning on the dev set, we found the second embedding approach to perform best, and\nsimple concatenaion outperformed mean vector representations.\n\nWe evaluate two dictionary sources, motivated by ease of accessibility to many languages: WIK-\nTIONARY, a word type dictionary that maps tokens to one of the 12 Universal POS tags; and\nUNIMORPH, a morphological dictionary that provides inflectional paradigms across 350 languages.\nFor Wiktionary, we use the freely available dictionaries from The size of the dictionaries ranges from\na few thousands (e.g., Hindi and Bulgarian) to 2M (Finnish UniMorph). Sizes are provided in Table\n1, 1st columns. UniMorph covers between 8-38 morphological properties (for English and Finnish,\nrespectively).\n\nWord embeddings. Embeddings are available for many languages. Pre-initialization of offers\nconsistent and considerable performance improvements in our distant supervision setup (Section 4).\nWe use off-the-shelf Polyglot embeddings, which performed consistently better than FastText.",
  "related_work": "Most successful work on low-resource POS tagging is based on projection, tag dictionaries, annotation\nof seed training data or even more recently some combination of these, e.g., via multi-task learning.\nOur paper contributes to this literature by leveraging a range of prior directions in a unified, neural\ntest bed.\n\nMost prior work on neural sequence prediction follows the commonly perceived wisdom that hand-\ncrafted features are unnecessary for deep learning methods. They rely on end-to-end training without\nresorting to additional linguistic resources. Our study shows that this is not the case. Only few prior\nstudies investigate such sources, e.g., for MT and for POS tagging use lexicons, but only as n-hot\nfeatures and without examining the cross-lingual aspect.\n\n4\n\n\fTable 2: Results for languages with missing data sources: WTC projections, Wiktionary (W), or\nUniMorph (U). Test sets (TEST), projection sources (PROJ), and embeddings languages (EMB) are\nindicated. Comparison to TnT trained on PROJ. Results indicated by \u2020use W only.\n\nTEST SETS\n\nLANGUAGE TEST\n\nPROJ\n\nEw TnT TCw n-hot\n\nEw DSDS\n\nBasque (eu)\nBasque (eu)\nEstonian (et)\nSerbian (sr)\nSerbian (sr)\nTamil (ta)\n\nUD\nBible\nCoNLL Bible\nWTC\nUD\nWTC (hr)\nUD\nBible (sr)\nUD\nWTC\nUD\n\n57.5\n57.0\n79.5\n84.0\n77.1\n58.2\n\n61.8\n60.3\n80.6\n84.7\n78.9\n61.2\n\n61.8\n60.3\n\n85.5\n79.4\n\n61.4\n60.3\n\n85.1\n80.5\n\n62.7\n61.3\n\n85.2\n80.7\n\n62.7\n61.3\n81.5\n85.2\n80.7\n\n7 Conclusions\n\nWe show that our approach of distant supervision from disparate sources (DSDS) is simple yet\nsurprisingly effective for low-resource POS tagging. Only 5k instances of projected data paired with\noff-the-shelf embeddings and lexical information integrated into a neural tagger are sufficient to reach\na new state of the art, and both data selection and embeddings are essential components to boost\nneural tagging performance.\n\n5",
  "methodology": "",
  "experiments": "Baselines. We compare to the following weaklysupervised POS taggers: AGIC: Multi-source\nannotation projection with Bible parallel data DAS: The label propagation approach by over Europarl\ndata. GARRETTE: The approach by that works with projections, dictionaries, and unlabeled target\ntext. LI: Wiktionary supervision.\n\nData. Our set of 25 languages is motivated by accessibility to embeddings and dictionaries. In all\nexperiments we work with the 12 Universal POS tags. For development, we use 21 dev sets of the\nUniversal Dependencies 2.1. We employ UD test sets on additional languages as well as the test sets\nof to facilitate comparisons. Their test sets are a mixture of CoNLL and HamleDT test data, and are\nmore distant from the training and development data.\n\nModel and parameters. We extend an off-theshelf state-of-the-art bi-LSTM tagger with lexicon\ninformation. The code is available at: https:// github.com/bplank/bilstm-aux. The parameter l=40\nwas set on dev data across all languages. Besides using 10 epochs, word dropout rate (p=.25) and\n40-dimensional lexicon embeddings, we use the parameters from For all experiments, we average\nover 3 randomly seeded runs, and provide mean accuracy. For the learning curve, we average over 5\nrandom samples with 3 runs each.",
  "results": "Table 1 shows the tagging accuracy for individual languages, while the means over all languages are\ngiven in Figure 2. There are several take-aways.\n\n2\n\n\fData selection. The first take-away is that coverage-based instance selection yields substan-\n\ntially better training data. Most prior work on annotation projection resorts to arbitrary selection;\ninformed selection clearly helps in this noisy data setup, as shown in Figure 2 (a). Training on 5k\ninstances results in a sweet spot; more data (10k) starts to decrease performance, at a cost of runtime.\nTraining on all WTC data (around 120k) is worse for most languages. From now on we consider the\n5k model trained with Polyglot as our baseline (Table 1, column \u201c5k\u201d), obtaining a mean accuracy of\n83.0 over 21 languages.\n\nEmbeddings initialization. Polyglot initialization offers a large boost; on average +3.8% absolute\nimprovement in accuracy for our 5k training scheme, as shown in Figure 2 (b). The big gap in\nlow-resource setups further shows their effectiveness, with up to 10% absolute increase in accuracy\nwhen training on only 500 instances.\n\nLexical information. The main take-away is that lexical information helps neural tagging, and\nembedding it proves the most helpful. Embedding Wiktionary tags reaches 83.7 accuracy on average,\nversus 83.4 for n-hot encoding, and 83.2 for type constraints. Only on 4 out of 21 languages are\ntype constraints better. This is the case for only one language for n-hot encoding (French). The best\napproach is to embed both Wiktionary and Unimorph, boosting performance further to 84.0, and\nresulting in our final model. It helps the most on morphological rich languages such as Uralic.\n\nOn the test sets (Table 4, right) DSDS reaches 87.2 over 8 test languages intersecting and. It reaches\n86.2 over the more commonly used 8 languages of, compared to their 83.4. This shows that our\nnovel \u201csoft\u201d inclusion of noisy dictionaries is superior to a hard decoding restriction, and including\nlexicons in neural taggers helps. We did not assume any gold data to further enrich the lexicons, nor\nfix possible tagset divergences.\n\n5 Discussion\n\nAnalysis. The inclusion of lexicons results in higher coverage and is part of the explanation for the\nimprovement of DSDS; see correlation in Figure 3 (a). What is more interesting is that our model\nbenefits from the lexicon beyond its content: OOV accuracy for words not present in the lexicon\noverall improves, besides the expected improvement on known OOV, see Figure 3 (b).\n\nMore languages. All data sources employed in our experiment are very high-coverage. However, for\ntrue low-resource languages, we cannot safely assume the availability of all disparate information\nsources. Table 2 presents results for four additional languages where some supervision sources\nare missing. We observe that adding lexicon information always helps, even in cases where only\n1k entries are available, and embedding it is usually the most beneficial way. For closely-related\nlanguages such as Serbian and Croatian, using resources for one aids tagging the other, and modern\nresources are a better fit. For example, using the Croatian WTC projections to train a model for\nSerbian is preferable over in-language Serbian Bible data where the OOV rate is much higher.\n\nHow much gold data? We assume not having access to any gold annotated data. It is thus interesting\nto ask how much gold data is needed to reach our performance. This is a tricky question, as training\nwithin the same corpus naturally favors the same corpus data. We test both in-corpus (UD)\n\nand out-of-corpus data (our test sets) and notice an important gap: while in-corpus only 50 sentences\nare sufficient, outside the corpus one would need over 200 sentences. This experiment was done for a\nsubset of 18 languages with both inand out-ofcorpus test data.\n\nFurther comparison. In Table 1 we directly report the accuracies from the original contributions by\nDAS, LI, GARRETTE, and AGIC over the same test data. We additionally attempted to reach the\nscores of LI by running their tagger over the Table 1 data setup. The results are depicted in Figure\n4 as mean accuracies over EM iterations until convergence. We show: i) LI peaks at 10 iterations\nfor their test languages, and at 35 iterations for all the rest. This is in slight contrast to 50 iterations\nthat recommend, although selecting 50 does not dramatically hurt the scores; ii) Our replication falls\n\u02d8223c5 points short of their 84.9 accuracy. There is a large 33-point accuracy gap between the scores\nof, where the dictionaries are large, and the other languages in Figure 4, with smaller dictionaries.\n\nCompared to DAS, our tagger clearly benefits from pre-trained word embeddings, while theirs relies\non label propagation through Europarl, a much cleaner corpus that lacks the coverage of the noisier\nWTC. Similar applies to as they use 1-5M near-perfect parallel sentences. Even if we use much\n\n3\n\n\fTable 1: Results on the development sets and comparison of our best model to prior work. LEX: Size\n(word types) of dictionaries (W: Wiktionary, U: UniMorph). TC: type-constraints using Wiktionary;\n(embedded Wiktionary tags), DSDS: our model with ;. Results indicated by use W only. Best result\nin boldface; in case of equal means, the one with lower std is boldfaced. Averages over language\nfamilies (with two or more languages in the sample, number of languages in parenthesis).\n\nLEX (10%)\n\nDEV SETS (UD2.1)\n\nTEST SETS\n\nW\n\n3\n20\n14\n22\n52\n358\n104\n17\n62\n21\n3\n2\n13\n478\n47\n4\n6\n41\n7\n234\n89\n\nU\n\n47\n\n72\n24\n26\n91\n2,345\n274\n71\n\n12\n26\n13\n410\n18\n26\n132\n211\n4\n324\n67\n\nLANGUAGE\n\nBulgarian (bg)\nCroatian (hr)\nCzech (cs)\nDanish (da)\nDutch (nl)\nEnglish (en)\nFinnish (fi)\nFrench (fr)\nGerman (de)\nGreek (el)\nHebrew (he)\nHindi (hi)\nHungarian (hu)\nItalian (it)\nNorwegian (no)\nPersian (fa)\nPolish (pl)\nPortuguese\nRomanian (ro)\nSpanish (es)\nSwedish (sv)\n\n!\n\nAVG(21)\nAVG(8: DAS)\nAVG(8: LI/AGIC)\n\nGERMANIC (6)\nGERMANIC (4: DAS)\nROMANCE (5)\nROMANCE (3: DAS)\nSLAVIC (4)\nINDO-IRANIAN (2)\nURALIC (2)\n\n5k\n\nTCw n-hot\n\nEw DSDS DAS\n\nLI\n\nGARRETTE AGIC DSDS\n\n88.6\n84.9\n86.6\n89.6\n88.3\n86.5\n81.5\n91.0\n85.0\n80.6\n76.0\n64.6\n75.6\n91.9\n90.9\n42.8\n84.7\n91.4\n83.9\n90.4\n88.9\n\n83.0\n\n88.6\n85.4\n86.6\n89.0\n88.9\n87.4\n81.2\n89.6\n86.4\n85.7\n76.1\n64.6\n75.6\n91.7\n90.9\n43.0\n84.6\n91.5\n83.9\n88.6\n88.9\n\n83.2\n\n88.9\n84.9\n86.9\n89.8\n89.0\n86.8\n81.8\n91.7\n85.5\n80.2\n75.5\n64.8\n75.3\n93.4\n90.9\n43.7\n84.2\n92.3\n84.8\n91.0\n89.6\n\n83.4\n\n89.6\n84.8\n87.6\n90.2\n89.7\n87.3\n82.4\n91.2\n86.0\n80.5\n74.9\n65.4\n75.7\n93.5\n91.0\n43.5\n84.8\n92.9\n85.3\n91.5\n89.9\n\n83.7\n\n89.7\n84.8\n87.2\n90.0\n89.8\n87.3\n82.4\n91.4\n86.7\n80.5\n75.3\n66.2\n77.9\n93.7\n91.5\n59.6\n86.0\n92.2\n86.3\n92.0\n89.9\n\n84.0\n\n88.2\n\n88.6\n\n88.6\n\n89.0\n\n89.2\n\n89.7\n\n89.0\n\n90.6\n\n90.9\n\n91.1\n\n86.2\n53.7\n78.5\n\n86.3\n53.8\n78.4\n\n86.2\n54.3\n78.6\n\n86.7\n54.4\n79.0\n\n86.9\n62.9\n80.1\n\n83.1\n\n78.8\n\n80.7\n\n85.5\n87.1\n52.3\n\n77.9\n76.9\n84.3\n\n87.3\n\n88.7\n76.1\n\n80.8\n80.8\n\n7.7\n67.1\n73.3\n79.0\n\n73.6\n\n76.6\n80.2\n\n67.6\n72.0\n\n76.7\n59.6\n75.1\n83.8\n\n81.4\n75.2\n\n75.5\n75.2\n\n83.9\n78.0\n86.8\n84.5\n83.9\n85.7\n\n88.7\n84.1\n81.1\n\n63.1\n71.3\n92.1\n86.2\n43.6\n84.4\n89.4\n\n91.7\n83.1\n\n86.2\n87.2\n\n83.9\n\n83.2\n79.5\n\n83.3\n86.3\n87.1\n\n82.8\n79.2\n\n85.8\n64.4\n\n86.8\n\n83.5\n\n87.9\n\n84.5\n\n84.2\n80.5\n\n86.4\n86.1\n\n83.4\n\n84.8\n84.9\n\n81.5\n\n85.4\n\n86.3\n\n85.8\n\n86.5\n\n80.7\n\n91.1\n\nsmaller and noisier data sources, DSDS is almost on par: 86.2 vs. 87.3 for the 8 languages from Das\nand , and we even outperform theirs on four languages: Czech, French, Italian, and Spanish.",
  "conclusion": "",
  "is_publishable": 1,
  "venue": NaN
}
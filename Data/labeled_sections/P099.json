{
  "title": "Enhancing LSTM-based Video Narration Through\nText-Derived Linguistic Insights",
  "abstract": "This study delves into how linguistic understanding, extracted from extensive text\ndatasets, can be leveraged to enhance the generation of natural language video\ndescriptions. Specifically, we integrate both a neural language model and distribu-\ntional semantics, trained on large text corpora, into a contemporary LSTM-based\nframework for video description. Our evaluation, conducted on a collection of\nYouTube videos and two substantial movie description datasets, reveals consider-\nable advancements in grammatical correctness, accompanied by subtle improve-\nments in descriptive quality.",
  "introduction": "The capacity to automatically generate natural language (NL) descriptions for videos has numerous\nsignificant applications, such as content-based video retrieval and aiding visually impaired individuals.\nRecent effective approaches, use recurrent neural networks (RNNs), treating the problem as a machine\ntranslation (MT) task, converting from video to natural language. Deep learning methods like RNNs\nrequire extensive training data; however, there\u2019s a shortage of high-quality video-sentence pairs.\nConversely, vast raw text datasets are readily available, exhibiting rich linguistic structure useful\nfor video description. Most work in statistical MT employs a language model, trained on extensive\nmonolingual target language data, and a translation model, trained on restricted parallel bilingual\ndata. This paper investigates methods to incorporate knowledge from language datasets to capture\ngeneral linguistic patterns to improve video description.\n\nThis study integrates linguistic data into a video-captioning model based on Long Short Term Memory\n(LSTM) RNNs, known for state-of-the-art performance. Additionally, LSTMs function effectively\nas language models (LMs). Our initial method (early fusion) involves pre-training the network\nusing plain text prior to training with parallel video-text datasets. Our subsequent two methods,\ninfluenced by current MT research, incorporate an LSTM LM with the existing video-to-text model.\nFurthermore, we explore substituting the standard one-hot word encoding with distributional vectors\nderived from external datasets.\n\nWe present thorough comparisons across these methods, assessing them on a typical YouTube corpus\nand two recently released extensive movie description datasets. The findings indicate notable gains in\ndescription grammaticality (as assessed by crowdsourced human evaluations) and moderate gains in\ndescriptive quality (as determined by human judgements and automated comparisons against human-\ngenerated descriptions). Our main contributions include: (1) numerous approaches to integrate\nknowledge from external text into a current captioning model, (2) comprehensive experiments\ncomparing methods on three large video-caption datasets, and (3) human assessments demonstrating\nthat external linguistic knowledge notably impacts grammar.\n\n\f2 LSTM-based Video Description\n\nWe employ the S2VT video description framework, which we describe briefly here. S2VT adopts a\nsequence-to-sequence approach that maps an input video frame feature sequence to a fixed-dimension\nvector, which is then decoded into a sequence of output words.\n\nAs depicted in the architecture employs a dual-layered LSTM network. The input to the initial LSTM\nlayer is a sequence of frame features extracted from the second-to-last layer (fc7) of a Convolutional\nNeural Network (CNN) after the ReLU operation. This LSTM layer encodes the video sequence. At\neach step, the hidden state is fed into the subsequent LSTM layer. Following the processing of all\nframes, the second LSTM layer is trained to transform this state into a sequence of words. This can be\nthought of as using one LSTM to model visual features and another to model language, conditioned\non the visual data. We modify this structure to incorporate linguistic information during training and\ngeneration. Although our techniques are based on S2VT, they are sufficiently general and could be\napplied to other CNN-RNN based captioning models.\n\n3 Approach\n\nCurrent visual captioning models are trained solely on text from the caption datasets and display some\nlinguistic anomalies stemming from a limited language model and vocabulary. Here, we explore\nseveral methods to integrate prior linguistic knowledge into a CNN/LSTM network for video-to-text\n(S2VT) and assess how well they improve overall description quality.\n\n3.1 Early Fusion\n\nOur early fusion method involves initially pre-training the language-modeling components of the\nnetwork on large raw NL text datasets, before fine-tuning these parameters on video-text paired\ndatasets. An LSTM model can learn the probability of an output sequence given an input. To learn a\nlanguage model, we train the LSTM layer to predict the next word based on the preceding words.\nFollowing the S2VT design, we embed one-hot encoded words into reduced-dimension vectors. The\nnetwork is trained on extensive text datasets, and its parameters are learned using backpropagation\nwith stochastic gradient descent. The weights from this network initialize the embedding and weights\nof the LSTM layers in S2VT, which is then trained on video-text data. This trained LM is also utilized\nas the LSTM LM in both late and deep fusion models.\n\n3.2 Late Fusion\n\nOur late fusion approach draws inspiration from how neural machine translation models incorporate\na trained language model during decoding. At each step of sentence generation, the video caption\nmodel generates a probability distribution over the vocabulary. We then utilize the language model\nto re-score the final output by considering a weighted average of the scores from the LM and the\nS2VT video-description model (VM). Specifically, for output at time step \u2019t\u2019, and given proposal\ndistributions from the video captioning model and the language model, we can calculate the re-scored\nprobability of each new word as:\n\np(yt = y) = \u03b1 \u00b7 pV M (yt = y) + (1 \u2212 \u03b1) \u00b7 pLM (yt = y)\n\n(1)\n\nThe hyper-parameter is tuned on the validation set.\n\n3.3 Deep Fusion\n\nIn the deep fusion approach, we integrate the LM more profoundly in the generation process. We\nachieve this by concatenating the hidden state of the language model LSTM (hLM ) with the hidden\nstate of the S2VT video description model (hV M ) and use the resulting combined latent vector to\npredict the output word. This is similar to the method employed to incorporate language models\nfrom monolingual data for machine translation. However, our method differs in two ways: (1) We\nconcatenate only the hidden states of the S2VT LSTM and language LSTM, without additional\ncontext. (2) We keep the weights of the LSTM language model constant while training the entire\nvideo captioning network. The probability of a predicted word at time step t is:\np(yt|G<t, T ) \u221d exp(WE(hV\n\nt \u2295 WT hLM\n\n) + b)\n\n(2)\n\nt\n\n2\n\n\fwhere V is the visual feature input, W represents the weight matrix, and b stands for biases. We\navoid fine-tuning the LSTM LM to avoid overwriting previously learned weights of a strong language\nmodel. However, the full video caption model is trained to integrate LM outputs while being trained\non captioning data.\n\n3.4 Distributional Word Representations\n\nThe S2VT network, like many image and video captioning models, uses a one-hot encoding for\nwords. During training, the model learns to embed these one-hot words into a 500-dimensional\nspace via linear transformation. This embedding, however, is learned from the limited and possibly\nnoisy caption data. Many techniques exist that leverage large text datasets to learn vector-space\nrepresentations of words, capturing nuanced semantic and syntactic structures. We aim to capitalize\non these to enhance video description. Specifically, we replace the embedding matrix from one-hot\nvectors with 300-dimensional GloVe vectors, pre-trained on 6B tokens from Gigaword and Wikipedia\n2014. We further explore variations where the model predicts both the one-hot word (softmax loss)\nand the distributional vector from the LSTM hidden state using Euclidean loss. The output vector (yt)\nis computed as yt = (Wght + bg), and the loss is:\n\nL(yt, wglove) = ||(Wght + bg) \u2212 wglove||2\n\n(3)\n\nwhere ht is the LSTM output, wglove is the GloVe embedding, and W and b are weights and biases.\nThe network becomes a multi-task model with dual loss functions, which we use to influence weight\nlearning.\n\n3.5 Ensembling\n\nThe loss function of the video-caption network is non-convex and hard to optimize. In practice, using\nan ensemble of trained networks can improve performance. We also present results of an ensemble\ncreated by averaging predictions from the highest performing models.",
  "related_work": "Following the advancements of LSTM-based models in Machine Translation and image captioning,\nvideo description works propose CNN-RNN models that create a vector representation of the video,\nwhich is decoded by an LSTM sequence model to generate a description. Some works also incorporate\nexternal data to improve video description, however, our focus is on integrating external linguistic\nknowledge for video captioning. We explore the use of distributional semantic embeddings and\nLSTM-based language models trained on external text datasets.\n\nLSTMs have proven to be effective language models. Other works have developed an LSTM model\nfor machine translation that incorporates a monolingual language model for the target language,\nachieving improved results. We utilize similar techniques (late fusion, deep fusion) to train an LSTM\nfor video-to-text translation. This model uses large monolingual datasets to enhance RNN-based\nvideo description networks. Unlike other approaches where the monolingual LM is used solely for\nparameter tuning, our approach utilizes the output of the language model as an input for training the\nfull underlying video description network.\n\n4\n\n\fOther recent works propose video description models that focus primarily on improving the video\nrepresentation itself with hierarchical visual pipelines and attention mechanisms. Without the attention\nmechanism their models achieve good METEOR scores on the YouTube dataset. The interesting\naspect is that the contribution of language alone is considerable. Hence, it is important to focus on\nboth aspects to generate better descriptions.",
  "methodology": "",
  "experiments": "4.1 Datasets\n\nOur language model was trained using sentences from Gigaword, BNC, UkWaC, and Wikipedia.\nThe vocabulary contained the 72,700 most frequent tokens, also including GloVe embeddings.\nFollowing evaluation we compare our models on the YouTube dataset, along with two extensive\nmovie description datasets: MPII-MD and M-VAD.\n\n4.2 Evaluation Metrics\n\nWe assess performance using machine translation metrics, METEOR and BLEU, to compare model-\ngenerated descriptions with human-written descriptions. For movie datasets with a single description,\nwe use only METEOR, as it is more robust.\n\n4.3 Human Evaluation\n\nWe also collect human judgments on a random subset of 200 video clips for each dataset through\nAmazon Turk. Each sentence was evaluated by three workers on a Likert scale from 1 to 5 (higher is\nbetter) for relevance and grammar. Grammar evaluations were done without viewing videos. Movie\nevaluation focused solely on grammar due to copyright.\n\n4.4 YouTube Video Dataset Results\n\nThe results show Deep Fusion performed well for both METEOR and BLEU scores. The integration\nof Glove embeddings considerably increased METEOR, and combining both techniques performed\nbest. Our final model is an ensemble (weighted average) of the Glove model and two Glove+Deep\nFusion models trained on external and in-domain COCO sentences. While the state-of-the-art on this\ndataset is achieved using attention to encode the video our work focuses on language modeling.\n\n3\n\n\fModel\n\nMETEOR B-4 Relevance Grammar\n\nS2VT\nEarly Fusion\nLate Fusion\nDeep Fusion\nGlove\nGlove+Deep - Web Corpus\nGlove+Deep - In-Domain\nEnsemble\nHuman\n\n29.2\n29.6\n29.4\n29.6\n30.0\n30.3\n30.3\n31.4\n-\n\n37.0\n37.6\n37.2\n39.3\n37.0\n38.1\n38.8\n42.1\n-\n\n2.06\n-\n-\n-\n-\n2.12\n2.21*\n2.24*\n4.52\n\n3.76\n-\n-\n-\n-\n4.05*\n4.17*\n4.20*\n4.47\n\nTable 1: Results on the YouTube dataset: METEOR and BLEU@4 scores (in %), along with human\nratings (1-5) on relevance and grammar. * denotes a significant improvement over S2VT.\n\nHuman ratings align closely with METEOR scores, indicating modest gains in descriptive quality.\nLinguistic knowledge enhances the grammar of the results. We experimented multiple ways to\nincorporate word embeddings: (1) GloVe input: Using GloVe vectors at the LSTM input performed\nbest. (2) Fine-tuning: Initializing with GloVe and subsequently fine-tuning reduced validation results\nby 0.4 METEOR. (3) Input and Predict: Training the LSTM to accept and predict GloVe vectors, as\ndescribed in Section 3, performed similarly to (1).\n\n4.5 Movie Description Results\n\nModel\n\nMPII-MD\n\nM-VAD\n\nMETEOR Grammar METEOR Grammar\n\nS2VT\nEarly Fusion\nLate Fusion\nDeep Fusion\nGlove\nGlove+Deep\n\n6.5\n6.7\n6.5\n6.8\n6.7\n6.8\n\n2.6\n-\n-\n-\n3.9*\n4.1*\n\n6.6\n6.8\n6.7\n6.8\n6.7\n6.7\n\n2.2\n-\n-\n-\n3.1*\n3.3*\n\nTable 2: Results on the Movie Corpora: METEOR (%) and human grammar ratings (1-5). * indicates\na significant improvement over S2VT.\n\nThe results on the movie datasets show METEOR scores were lower due to single reference translation.\nUsing our architecture, we can see that the capacity of external linguistic information to increase\nMETEOR scores is small yet reliable. Again, human evaluations reveal significant improvements in\ngrammatical accuracy.",
  "results": "",
  "conclusion": "This study investigates methods to integrate linguistic knowledge from text datasets for video\ncaptioning. Our assessments on YouTube videos and two movie description datasets show improved\nresults according to human evaluations of grammar while also modestly improving the descriptive\nquality of sentences. Although the proposed methods are assessed on a particular video-captioning\nnetwork, they are applicable to other video and image captioning models.\n\n5",
  "is_publishable": 1,
  "venue": NaN
}
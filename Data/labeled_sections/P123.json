{
  "title": "Acquiring Cross-Domain Representations for\nContextual Detection Using Extensive Emoji Data",
  "abstract": "This research delves into the application of a vast collection of emoji occurrences\nto acquire versatile representations applicable to diverse domains for the purpose\nof identifying sentiment, emotion, and sarcasm. Natural Language Processing\n(NLP) tasks frequently encounter limitations due to the deficiency of manually\nlabeled data. In the realm of social media sentiment analysis and associated tasks,\nresearchers have thus employed binarized emoticons and specific hashtags as a\nmeans of distant supervision. Our study demonstrates that by broadening distant\nsupervision to include a more varied array of noisy labels, models can achieve\nricher representations. Through emoji prediction on a dataset encompassing 1,246\nmillion tweets, each including one of 64 prevalent emojis, we achieve state-of-\nthe-art results on eight benchmark datasets focusing on sentiment, emotion, and\nsarcasm detection, all with the aid of a singular pre-trained model. Our findings\naffirm that the diversity inherent in our emotional labels leads to an enhancement\nin performance compared to previous distant supervision methods.",
  "introduction": "This paper addresses the challenge that numerous Natural Language Processing (NLP) tasks face due\nto the lack of sufficient manually annotated data. Consequently, emotional expressions that co-occur\nwith text have been utilized for distant supervision in sentiment analysis and related tasks within\nsocial media. This allows models to acquire valuable text representations before directly modeling\nthese specific tasks. For example, state-of-the-art methods for sentiment analysis in social media\nfrequently use positive and negative emoticons to train their models. Similarly, in prior research,\nhashtags like #anger, #joy, #happytweet, #ugh, #yuck, and #fml have been categorized into emotional\nlabels for use in emotion analysis.\n\nThe practice of using distant supervision on noisy labels often leads to enhanced performance in\nthe target task. In this paper, we present evidence that expanding distant supervision to a more\nvaried selection of noisy labels enables models to develop more detailed representations of emotional\ncontent in text. This, in turn, improves performance on benchmark datasets designed for the detection\nof sentiment, emotions, and sarcasm. We further demonstrate that the representations learned by a\nsingle pre-trained model can be successfully generalized across five different domains.\n\nTable 1 showcases example sentences which were scored by our model. For every sentence, the five\nmost probable emojis are displayed, alongside the model\u2019s estimated probabilities.\n\nEmojis do not always function as straightforward labels of emotional content. For instance, a\npositive emoji might clarify an ambiguous sentence or supplement text that might otherwise be\nseen as somewhat negative. While this is true, our results demonstrate that emojis can still be\nused to accurately categorize the emotional content of texts in numerous scenarios. Our DeepMoji\nmodel, for instance, is able to capture various interpretations of the word \u2019love\u2019 and slang terms\nlike \u2019this is the shit\u2019 as having positive connotations (as illustrated in Table 1). To enable others to\nexplore the prediction capabilities of our model, we have made an online demonstration available at\ndeepmoji.mit.edu.\n\n\fOur work makes the following contributions: We demonstrate that a vast number of readily accessible\nemoji occurrences on Twitter can be used to pre-train models for richer emotional representation than\nis typically achieved through distant supervision. We then transfer this learned knowledge to target\ntasks using a novel layer-wise fine-tuning approach. This technique yields significant improvements\nover state-of-the-art methods in areas such as emotion, sarcasm, and sentiment detection. Through\nextensive analyses on the influence of pre-training, our results highlight that the variety present in our\nemoji set plays a crucial role in the transfer learning capabilities of our model. We have made our\npre-trained DeepMoji model publicly available to aid in a range of NLP tasks.",
  "related_work": "The use of emotional expressions as noisy labels in text to address the scarcity of labels is not a new\nconcept. Initially, binarized emoticons served as noisy labels, but subsequent research has utilized\nhashtags and emojis. Previous studies have always manually determined which emotional category\neach emotional expression should belong to. Prior efforts have made use of emotion theories, such as\nEkman\u2019s six basic emotions and Plutchik\u2019s eight basic emotions.\n\nSuch manual categorization necessitates an understanding of the emotional content inherent to each\nexpression, which can be challenging and time-consuming for complex emotional combinations.\nFurthermore, any manual selection and categorization carries the potential for misinterpretations\nand might overlook essential details concerning usage. In contrast, our methodology requires no\nprior knowledge of the corpus and can capture the diverse usage of 64 emoji types (Table 1 presents\nexamples, and Figure 3 shows how the model implicitly organizes emojis).\n\nAn alternative approach to automatically interpreting the emotional content of an emoji involves\nlearning emoji embeddings from the words defining emoji-semantics, as found in official emoji tables.\nIn our study, this approach has two significant limitations: (a) It requires emojis to be present during\ntesting, whereas several domains have limited or no emoji usage. (b) The tables fail to capture the\ndynamic nature of emoji use, such as shifts in an emoji\u2019s intended meaning over time.\n\nKnowledge from the emoji dataset can be transferred to target tasks in several ways. Multi-task\nlearning, which involves training on multiple datasets at once, has been shown to have promising\nresults. However, multi-task learning requires access to the emoji dataset whenever the classifier\nneeds to be adjusted for a new target task. Requiring access to the dataset can be problematic when\nconsidering data access regulations. Data storage issues also arise, as the dataset used in this study\ncomprises hundreds of millions of tweets (see Table 2). Instead, we use transfer learning which does\nnot require access to the original dataset.\n\n3 Method\n\n3.1 Pretraining\n\nIn many instances, emojis function as a stand-in for the emotional content of text. Therefore, pre-\ntraining a model to predict which emojis were initially part of a text can improve performance in the\ntarget task. Social media contains many short texts that use emojis which can be used as noisy labels\nfor pretraining. We used data from Twitter spanning from January 1, 2013, to June 1, 2017, but any\ndata set containing emoji occurrences could be used.\n\nThe pretraining data set uses only English tweets that do not contain URLs. We think the content\nobtained from the URL is important for understanding the emotional content of the text in the tweet.\nBecause of this we expect emojis associated with tweets containing URLs to be noisier labels than\nthose in tweets without URLs, therefore the tweets with URLs have been removed.\n\nProper tokenization is crucial for generalization. All tweets are tokenized word-by-word. Words\ncontaining two or more repeated characters are shortened to the same token (for example, \u2018loool\u2019 and\n\u2018looooool\u2019 are tokenized as the same). We also use a special token for all URLs (which is relevant\nonly for the benchmark datasets), user mentions (for example, \u2018@acl2017\u2019 and \u2018@emnlp2017\u2019 are\ntreated the same), and numbers. To be included in the training set, a tweet must have at least one\ntoken that is not a punctuation mark, emoji, or special token.\n\n2\n\n\fMany tweets repeat the same emoji or contain multiple distinct emojis. To address this in our training\ndata, for each unique emoji type, we save a separate tweet for pretraining, using that emoji type as\nthe label. Regardless of the number of emojis associated with the tweet, we save only a single tweet\nfor the pretraining for each unique emoji type. This pre-processing of data enables the pretraining to\ncapture that multiple kinds of emotional content can be associated with the tweet. It also makes our\npretraining task a single-label classification instead of a more complex multi-label classification.\n\nTo ensure that the pretraining encourages the models to learn a thorough understanding of the\nemotional content of text instead of just the emotional content associated with frequently used emojis,\nwe create a balanced pretraining dataset. The pretraining data is split into training, validation, and test\nsets. The validation and test sets are randomly sampled such that each emoji is represented equally.\nThe remaining data is upsampled to generate a balanced training dataset.\n\n3.2 Model\n\nWith the availability of millions of emoji occurrences, we are able to train expressive classifiers\nwith a limited risk of overfitting. We utilize a variant of the Long Short-Term Memory (LSTM)\nmodel, which has been successful in numerous NLP tasks. Our DeepMoji model uses an embedding\nlayer with 256 dimensions to project each word into a vector space. A hyperbolic tangent activation\nfunction is used to ensure each embedding dimension remains within the range [-1, 1]. To understand\neach word in the context of the text, we use two bidirectional LSTM layers with 1024 hidden units\neach (512 in each direction). Lastly, we employ an attention layer that accepts all these layers as\ninput through skip connections. (Figure 1 presents an illustration).\n\nThe attention mechanism enables the model to determine the importance of each word for the\nprediction task by weighting the words as it creates the text representation. A word like \"amazing\" is\nhighly informative of the emotional meaning of a text and so should be treated accordingly. We use a\nbasic method, taking inspiration from prior work, with a single parameter for each input channel:\n\nei = hiwa ai =\n\nexp(ei)\nj=1 exp(ej)\n\n(cid:80)\n\n(cid:88)\n\nv =\n\naihi\n\n(1)\n\nHere, ht stands for the representation of the word at time step t, and wa is the weight matrix for\nthe attention layer. The attention importance scores for each time step, at, are determined by\nmultiplying the representations by the weight matrix, and then normalizing them to establish a\nprobability distribution across the words. Finally, the text\u2019s representation vector, v, is found using a\nweighted summation over all time steps, with the attention importance scores used as weights. The\nrepresentation vector that comes from the attention layer is a high-level encoding of the whole text.\nThis is used as input into the final Softmax layer for classification. We have found that the addition of\nthe attention mechanism and skip connections enhances the model\u2019s capabilities for transfer learning.\n\nThe only form of regularization used for the pretraining is L2 regularization with a coefficient of\n10\u22126 on the embedding weights. For fine-tuning, further regularization is applied. We implemented\nour model using Theano and have made an easy-to-use version available that utilizes Keras.\n\n3.3 Transfer learning\n\nOur pre-trained model can be fine-tuned for a target task in several ways. Some methods involve\n\u2018freezing\u2019 layers by disabling parameter updates to prevent overfitting. One popular approach is\nto utilize the network as a feature extractor, where all model layers except the final one are frozen\nduring fine-tuning (we will call this the \"last\" approach). An alternative method is to use the pre-\ntrained model for initialization, where the full model is unfrozen (which we will refer to as the \u2018full\u2019\napproach).\n\nWe put forward a new, simple transfer learning approach we are calling \"chain-thaw.\" This approach\nsequentially unfreezes and fine-tunes one layer at a time. It increases accuracy on the target task, but\nrequires more computational power for the fine-tuning process. By separately training each layer,\nthe model can adjust individual patterns across the network while reducing the risk of overfitting. It\nappears that this sequential fine-tuning has a regularizing effect, similar to the layer-wise training\nexplored for unsupervised learning.\n\n3\n\n\fMore specifically, the chain-thaw approach starts by fine-tuning any new layers (often only a Softmax\nlayer) to the target task until the validation set converges. Then, the approach individually fine-tunes\neach layer, starting with the first layer in the network. Lastly, the entire model is trained with all\nlayers. Each time the model converges (as measured on the validation set), the weights are restored to\ntheir optimal setting, preventing overfitting in a similar manner to early stopping. Figure 2 illustrates\nthis process. If only step a) in the figure is performed, this is the same as the \u2018last\u2019 approach, where\nthe existing network is used as a feature extractor. Likewise, only performing step d) is the same as\nthe \u2018full\u2019 approach, where the pre-trained weights are used as the initialization for a fully trainable\nnetwork. While the chain-thaw procedure may seem extensive, it can be implemented with just a few\nlines of code. Also, the added time spent on fine-tuning is not large, when considering the use of\nGPUs on small datasets of manually annotated data which is often the case.\n\nThe chain-thaw approach has the benefit of expanding the vocabulary to new domains with a low\nrisk of overfitting. For a given dataset, up to 10,000 new words from the training set are added to the\nvocabulary.\n\nTable 2 shows the number of tweets in the pretraining dataset associated with each emoji in millions.",
  "methodology": "",
  "experiments": "4.1 Emoji prediction\n\nWe use a raw dataset of 56.6 billion tweets, which is filtered down to 1.2 billion relevant tweets. In\nthe pretraining dataset, a single copy of a tweet is stored for every unique emoji, resulting in a dataset\nwith 1.6 billion tweets. Table 2 shows the distribution of tweets across different emoji types. We used\na validation set and a test set, both containing 640K tweets (10K of each emoji type), to evaluate\nperformance on the pretraining task. The remaining tweets were used for the training set, which was\nbalanced using upsampling.\n\nThe performance of the DeepMoji model on the pretraining task was evaluated, with the results shown\nin Table 3. We use both top 1 and top 5 accuracy for the evaluation as the emoji labels are noisy\nand multiple emojis can potentially be appropriate for a given sentence. For comparison purposes,\nwe also train a version of our DeepMoji model with smaller LSTM layers and a bag-of-words\nclassifier, fastText, which has recently shown competitive results. We use a 256 dimension vector\nfor the fastText classifier, making it almost identical to only using the embedding layer from the\nDeepMoji model. The difference in top 5 accuracy between the fastText classifier (36.2%) and the\nlargest DeepMoji model (43.8%) highlights the difficulty of the emoji prediction task. Since the two\nclassifiers only differ in that the DeepMoji model has LSTM layers and an attention layer between\nthe embedding and the Softmax layer, this difference in accuracy demonstrates the importance of\ncapturing each word\u2019s context.\n\nTable 3 displays the accuracy of classifiers on the emoji prediction task. The value d refers to the\ndimensionality of each LSTM layer and the parameters are given in millions.\n\nModel\n\nParams Top 1\n\nTop 5\n\n-\n12.8\n15.5\n22.4\nTable 1: Accuracy of classifiers on the emoji prediction task. d refers to the dimensionality of each\nLSTM layer. Parameters are in millions.\n\nRandom\nfasttext\nDeepMoji (d=512)\nDeepMoji (d=1024)\n\n1.6%\n12.8% 36.2%\n16.7% 43.3%\n17.0% 43.8%\n\n7.8%\n\n4.2 Benchmarking\n\nWe evaluate our method on 3 distinct NLP tasks using 8 datasets across 5 domains. For fair\ncomparison, DeepMoji is compared to other methods that utilize external data sources in addition\nto the benchmark dataset. We used an averaged F1 measure across classes for evaluating emotion\nanalysis and sarcasm detection, as these consist of unbalanced datasets. Sentiment datasets are\nevaluated using accuracy.\n\n4\n\n\fMany benchmark datasets have an issue with data scarcity, especially in emotion analysis. Many\nstudies that introduce new methods for emotion analysis often evaluate their performance on a single\nbenchmark dataset, SemEval 2007 Task 14, which contains only 1250 data points. There has been\ncriticism regarding the use of correlation with continuous ratings as a measure, making only the\nsomewhat limited binary evaluation possible. We only evaluate the emotions Fear, Joy, Sadness\nbecause the remaining emotions are found in less than 5\n\nTo fully assess our method on emotion analysis, we make use of two other datasets. First, a dataset\nof emotions in tweets about the Olympic Games, created by Sintsova et al. which we convert to\na single-label classification task. Second, a dataset of self-reported emotional experiences from a\nlarge group of psychologists. Because these two datasets have not been evaluated in prior work,\nwe compare against a state-of-the-art approach based on a valence-arousal-dominance framework.\nThe scores extracted using this framework are mapped to the classes in the datasets using logistic\nregression with cross-validation parameter optimization. We have made our preprocessing code\navailable so that these two datasets may be used for future benchmarking in emotion analysis.\n\nWe assessed the performance of sentiment analysis using three benchmark datasets. These small\ndatasets were chosen to highlight the significance of the transfer learning capabilities of the evaluated\nmodels. Two datasets, SS-Twitter and SS-Youtube, are from SentiStrength and follow the relabeling\nas described by prior work to create binary labels. The third dataset is from SemEval 2016 Task4A.\nBecause tweets are often deleted from Twitter, the SemEval dataset has experienced data decay. This\nmakes comparisons across papers difficult. Approximately 15\n\nThe current state of the art in sentiment analysis on social media (and winner of SemEval 2016 Task\n4A) uses an ensemble of convolutional neural networks that are pre-trained on a private dataset of\ntweets with emoticons. This makes it difficult to replicate. As a substitute, we pre-train a model that\nuses the hyperparameters of the largest model in their ensemble on the positive/negative emoticon\ndataset. Using this pretraining as an initialization, we fine-tune the model on the target tasks, utilizing\nearly stopping based on a validation set. We implemented Sentiment-Specific Word Embeddings\n(SSWE), using embeddings available on the authors\u2019 website, but found that it performed worse than\nthe pretrained convolutional neural network, and these results have been excluded.\n\nTable 4 presents a description of the benchmark datasets. Datasets that did not have pre-existing\ntraining/test splits were split by us, and these splits are publicly available. Data from the training set\nwas used for hyperparameter tuning.\n\nIdentifier\n\nStudy\n\nTask\n\nDomain\n\nClasses Ntrain Ntest\n\n(Strapparava and Mihalcea, 2007) Emotion\nSE0714\nEmotion\n(Sintsova et al., 2013)\nOlympic\nEmotion\n(Wallbott and Scherer, 1986)\nPsychExp\nSentiment Tweets\n(Thelwall et al., 2012)\nSS-Twitter\nSentiment Video Comments\n(Thelwall et al., 2012)\nSS-Youtube\nSentiment Tweets\n(Nakov et al., 2016)\nSE1604\nSarcasm\nSCv1\n(Walker et al., 2012)\nSarcasm\nSCv2-GEN (Oraby et al., 2016)\n\nHeadlines\nTweets\nExperiences\n\nDebate Forums\nDebate Forums\n\n3\n4\n7\n2\n2\n3\n2\n2\n\n250\n250\n1000\n1000\n1000\n7155\n1000\n1000\n\n1000\n709\n6480\n1113\n1142\n31986\n995\n2260\n\nTable 2: Description of benchmark datasets. Datasets without pre-existing training/test splits are split\nby us (with splits publicly available). Data used for hyperparameter tuning is taken from the training\nset.\n\nFor sarcasm detection, we used versions 1 and 2 of the sarcasm dataset from the Internet Argument\nCorpus. It should be noted that the results from these benchmarks that are shown elsewhere are not\ndirectly comparable, as only a subset of the data is available online. We establish a state-of-the-art\nbaseline by modeling embedding-based features alongside unigrams, bigrams, and trigrams with\nan SVM. GoogleNews word2vec embeddings are used to compute the embedding-based features.\nCross-validation was used to perform a hyperparameter search for regularization parameters. The\nsarcasm dataset version 2 includes both a quoted text and a sarcastic response, but only the response\nwas used to keep models consistent across the datasets.\n\n5\n\n\fTable 5 displays a comparison across benchmark datasets. The reported values are averages across\n5 runs. Variations refer to the transfer learning approaches that we discussed, and \u2019new\u2019 refers to a\nmodel trained without pretraining.\n\nDataset\nDeepMoji (chain-thaw)\n\nMeasure\n\nState of the art DeepMoji (new) DeepMoji (full) DeepMoji (last)\n\nSE0714\n.37\nOlympic\n.61\nPsychExp\n.57\nSS-Twitter\n.88\nSS-Youtube\n.93\nSE1604\n.58\nSCv1\n.69\nSCv2-GEN\n.75\n\nF1\n\nF1\n\nF1\n\nAcc\n\nAcc\n\nAcc\n\nF1\n\nF1\n\n.34\n\n.50\n\n.45\n\n.82\n\n.86\n\n.51\n\n.63\n\n.72\n\n.21\n\n.43\n\n.32\n\n.62\n\n.75\n\n.51\n\n.67\n\n.71\n\n.31\n\n.50\n\n.42\n\n.85\n\n.88\n\n.54\n\n.65\n\n.71\n\n.36\n\n.61\n\n.56\n\n.87\n\n.92\n\n.58\n\n.68\n\n.74\n\nTable 3: Comparison across benchmark datasets. Reported values are averages across five runs.\nVariations refer to transfer learning approaches with \u2018new\u2019 being a model trained without pretraining.\n\nWe used the Adam optimizer for training, with the gradient norm clipped to 1. For training all new\nlayers, we set the learning rate to 10\u22123 and to 10\u22124 when fine-tuning any pre-trained layers. To\nprevent overfitting on the small datasets, 10\n\nTable 5 demonstrates that the DeepMoji model outperforms the state of the art across all the benchmark\ndatasets and that our new \u2018chain-thaw\u2019 method yields the highest transfer learning performance. The\nresults are averaged across 5 runs to reduce the variance. We confirm statistical significance using\nbootstrap testing with 10,000 samples, our model performance was statistically better than the\nstate-of-the-art across all benchmark datasets (p < 0.001).\n\nOur model exceeds the performance of the state of the art even on datasets that come from different\ndomains than the tweets that the model was pre-trained on. A crucial difference between the\npretraining dataset and the benchmark datasets is the length of the observations. The average number\nof tokens per tweet in the pretraining dataset is 11. Meanwhile, board posts from the Internet\nArgument Corpus version 1 (for example), have an average of 66 tokens, with some posts being much\nlonger.\n\n5 Model Analysis\n\n5.1\n\nImportance of emoji diversity\n\nA key difference between this work and prior research that used distant supervision is the variety in\nnoisy labels. For example, other studies only used positive and negative emoticons as noisy labels.\nOther studies used more nuanced sets of noisy labels, but our set is the most varied known to us. To\ninvestigate the effect of using a diverse set of emojis, we created a subset of our pretraining data that\nincluded tweets with one of 8 emojis, which are similar to the positive/negative emoticons used in\nother work. Because the dataset based on this reduced set of emojis contains 433 million tweets, any\nperformance differences on benchmark datasets are more likely linked to the diversity of the labels\nthan to differences in dataset sizes.\n\nWe trained our DeepMoji model to predict whether tweets contained positive or negative emojis,\nand we evaluated this pre-trained model on benchmark datasets. We call this the DeepMoji-PosNeg\nmodel. To assess the emotional representations learned by the two pre-trained models, we used the\n\u2018last\u2019 transfer learning approach to allow the models to map already learned features to classes in the\n\n6\n\n\ftarget datasets. Table 6 shows that DeepMoji-PosNeg performs worse than DeepMoji across all 8\nbenchmarks. This demonstrates that the diversity of our emoji types enables the model to acquire\nricher representations of emotional content in text, which in turn is more useful for transfer learning.\n\nTable 6 compares benchmarks using a smaller emoji set (Pos/Neg emojis) or a standard architecture\n(standard LSTM). Results for DeepMoji from Table 5 have been added for comparison. The evaluation\nmetrics are the same as in Table 5. Reported values are averages across 5 runs.\n\nDataset\n\nPos/Neg emojis\n\nStandard LSTM DeepMoji\n\n.32\nSE0714\n.55\nOlympic\n.40\nPsychExp\n.86\nSS-Twitter\n.90\nSS-Youtube\n.56\nSE1604\nSCv1\n.66\nSCv2-GEN .72\n\n.35\n.57\n.49\n.86\n.91\n.57\n.66\n.73\n\n.36\n.61\n.56\n.87\n.92\n.58\n.68\n.74\n\nTable 4: Benchmarks using a smaller emoji set (Pos/Neg emojis) or a classic architecture (standard\nLSTM). Results for DeepMoji from Table 5 are added for convenience. Evaluation metrics are as in\nTable 5. Reported values are the averages across five runs.\n\nMany emojis express similar emotional content, but have subtle variations in usage that our model\ncan capture. By using hierarchical clustering on the correlation matrix of the DeepMoji model\u2019s\npredictions on the test set, we can see that the model captures many expected similarities (Figure 3).\nFor example, the model groups emojis into broad categories related to negativity, positivity, or love.\nIt also differentiates within these categories. For example, mapping sad emojis to one subcategory of\nnegativity, annoyed emojis to another subcategory, and angry emojis to a third.\n\n5.2 Model architecture\n\nOur DeepMoji model architecture employs an attention mechanism and skip connections, which assist\nin transferring learned representations to new domains and tasks. Here, we compare the DeepMoji\nmodel architecture to a standard 2-layer LSTM. Both were compared using the \u2018last\u2019 transfer learning\napproach, and all regularization and training parameters were consistent.\n\nTable 6 shows that the DeepMoji model performs better than a standard 2-layer LSTM across all the\nbenchmark datasets. These two architectures performed equally on the pretraining task. This indicates\nthat the DeepMoji model architecture is better for transfer learning, even if it is not necessarily better\nfor a single supervised classification task with an abundance of available data.\n\nWe believe that the improvements in transfer learning can be attributed to two factors: (a) The\nattention mechanism with skip connections provides straightforward access to learned low-level\nfeatures for any time step, making it easy to use this information if needed for a new task. (b) The skip\nconnections improve the gradient flow from the output layer to the early layers in the network. This\nis useful when parameters in early layers are adjusted as a part of transfer learning to small datasets.\nFurther analysis of these factors in future work would allow us to confirm why our architecture\noutperforms a standard 2-layer LSTM.\n\n5.3 Analyzing the effect of pretraining\n\nThe target task\u2019s performance benefits significantly from pretraining, as shown in Table 5. Here,\nwe separate the effects of pretraining into two factors: word coverage and phrase coverage. These\ntwo effects provide regularization to the model, preventing overfitting (the supplementary material\nincludes a visualization of this regularization).\n\nThere are multiple ways of expressing sentiment, emotion, or sarcasm. Because of this, the test set\nmay contain language use not present in the training set. Pretraining helps the target task models\nfocus on low-support evidence by having already seen similar language in the pretraining dataset.\nTo examine this effect, we measure the improvement in word coverage on the test set when using\n\n7\n\n\fpretraining. Word coverage is defined as the percentage of words in the test dataset that were also\nseen in the training/pretraining dataset (as shown in Table 7). One key reason that the \u2018chain-thaw\u2019\napproach outperforms other transfer learning approaches is its ability to tune the embedding layer\nwith a low risk of overfitting. Table 7 shows how adding new words to the vocabulary as part of the\ntuning process increased word coverage.\n\nIt is important to note that word coverage can be misleading in this context. In many small datasets, a\nword may occur only once in the training set. In contrast, all the words in the pretraining vocabulary\nare present in thousands or even millions of observations, enabling the model to learn a good\nrepresentation of the emotional and semantic meaning. Therefore, the benefits of pretraining for word\nrepresentations likely extend beyond the differences seen in Table 7.\n\nTable 7 shows the word coverage on benchmark test sets. This compares the use of only the vocabulary\ngenerated by finding words in the training data (\u2018own\u2019), the pretraining vocabulary (\u2018last\u2019), or a\ncombination of both vocabularies (\u2018full / chain-thaw\u2019).\n\nDataset\n\nOwn\n\nLast\n\nFull / Chain-thaw\n\n41.9% 93.6% 94.0%\nSE0714\n73.9% 90.3% 96.0%\nOlympic\n85.4% 98.5% 98.8%\nPsychExp\n80.1% 97.1% 97.2%\nSS-Twitter\n79.6% 97.2% 97.3%\nSS-Youtube\n86.1% 96.6% 97.0%\nSE1604\nSCv1\n88.7% 97.3% 98.0%\nSCv2-GEN 86.5% 97.2% 98.0%\n\nTable 5: Word coverage on benchmark test sets using only the vocabulary generated by finding words\nin the training data (\u2018own\u2019), the pretraining vocabulary (\u2018last\u2019) or a combination of both vocabularies\n(\u2018full / chain-thaw\u2019).\n\nTo analyze how important capturing phrases and the context of each word are, we evaluated the\naccuracy on the SS-Youtube dataset using a fastText classifier that was pre-trained using the same\nemoji dataset as our DeepMoji model. This fastText classifier is similar to only using the embedding\nlayer from the DeepMoji model. We then evaluated the representations learned by fine-tuning the\nmodels as feature extractors (using the \u2018last\u2019 transfer learning approach). The fastText model achieved\nan accuracy of 63\n\n5.4 Comparing with human-level agreement\n\nTo see how well our DeepMoji classifier performs compared to humans, we created a dataset of\nrandomly selected tweets that were annotated for sentiment. Each tweet was annotated by a minimum\nof 10 English-speaking Amazon Mechanical Turkers (MTurks) who lived in the USA. The tweets\nwere rated on a scale from 1 to 9, with a \u2018Do not know\u2019 option. Guidelines were provided to the\nhuman raters. The tweets were selected to contain only English text and no mentions or URLs, so\nthey could be rated without extra contextual information. Tweets where more than half the evaluators\nchose \u2018Do not know\u2019 were removed (98 tweets).\n\nFor every tweet, we randomly select a single MTurk rating as the \u2018human evaluation.\u2019 We average the\nremaining nine MTurk ratings to make the ground truth. The \u2018sentiment label\u2019 for a given tweet is thus\ndefined as the overall consensus among raters, excluding the randomly selected \u2018human evaluation\u2019\nrating. To ensure clear separation between the label categories, we removed neutral tweets that fell\nwithin the interval [4.5, 5.5] (roughly 29\n\nTable 8 shows that the agreement of the random MTurk rater is 76.1\n\nTable 8 compares the agreement between classifiers and the aggregate opinion of Amazon Mechanical\nTurkers on sentiment prediction of tweets.\n\n8\n\n\fModel\n\nAgreement\n\nRandom\nfastText\nMTurk\nDeepMoji\n\n50.1%\n71.0%\n76.1%\n82.4%\n\nTable 6: Comparison of agreement between classifiers and the aggregate opinion of Amazon Mechan-\nical Turkers on sentiment prediction of tweets.",
  "results": "",
  "conclusion": "We have demonstrated how the abundance of text on social media containing emojis can be used\nto pre-train models. This enables them to acquire representations of emotional content in text. Our\nfindings demonstrate that the diversity of our emoji set is crucial to our method\u2019s performance. This\nwas found by comparing the model performance against an identical model that was pre-trained on a\nsubset of emojis. Our pre-trained DeepMoji model is available for other researchers to use for diverse\nemotion-related NLP tasks.\n\n9",
  "is_publishable": 1,
  "venue": NaN
}
{
  "title": "Premature Termination Strategy for Deep Image Prior",
  "abstract": "Deep Image Prior (DIP) and its variations have demonstrated significant promise in addressing inverse problems in\ncomputational imaging, without the need for separate training data. Often, practical DIP models are significantly\noverparameterized. These models initially capture the intended visual content during the learning phase and\nsubsequently incorporate potential modeling and observational noise, demonstrating a pattern of initial learning\nfollowed by overfitting (ELTO). Consequently, the practical application of DIP depends on an early stopping (ES)\nmechanism capable of identifying this transitional period. Most previous DIP research in computational imaging\nhas focused on demonstrating the models\u2019 potential by reporting peak performance against ground truth, without\nproviding practical methods to achieve near-peak performance without access to ground truth. This paper aims to\novercome this practical limitation of DIP by introducing an efficient ES strategy that reliably identifies near-peak\nperformance across various computational imaging tasks and DIP variants. This ES method, based on the running\nvariance of intermediate reconstructions in DIP, not only surpasses existing methods that are limited to specific\nconditions but also maintains its effectiveness when combined with techniques aimed at reducing overfitting.",
  "introduction": "Inverse problems (IPs) are widespread in the field of computational imaging, encompassing tasks from fundamental image denoising,\nsuper-resolution, and deblurring to complex 3D reconstruction and significant challenges in scientific and medical imaging. Despite\nthe variety of settings, all these problems involve recovering a visual object x from an observation y = f(x), where f represents the\nforward physical process. Usually, these visual IPs are underdetermined, meaning x cannot be uniquely ascertained from y. This\nambiguity is further complicated by potential modeling inaccuracies (such as using a linear f to approximate a nonlinear process)\nand observational noise (like Gaussian or shot noise), represented as y \u02d82248 f(x). To address nonuniqueness and enhance stability\nagainst noise, researchers often integrate a range of problem-specific priors on x when formulating IPs.",
  "related_work": "There are three primary methods to counteract the overfitting of DIP models. The first one is Regularization: Overfitting is lessened\nby limiting the size of G\u02d803b8 to the underparameterization range. Layer-wise weights or the network Jacobian are regularized to\nregulate the network capacity. The total-variation norm or trained denoisers are used as additional regularizers R(G\u02d803b8(z)). To\nprevent overfitting, these techniques need the proper amount of regularization, which varies depending on the kind and degree of\nnoise. They may nevertheless cause overfitting if the regularization level is incorrect. Furthermore, even when they are successful,\nthe performance peak is delayed until the last few iterations, which frequently increases the computing cost by several times. The\nsecond method is Noise modeling: In their optimization objective, sparse additive noise is explicitly represented. Regularizers and\nES criteria are created especially for Gaussian and shot noise. Subgradient techniques using decreasing step size schedules are\nbeing investigated for impulse noise with the \u02d821131 loss, and they have shown some early promise. These techniques are ineffective\noutside of the noise types and levels that they are designed to address, and our understanding of the noise in a particular visual\nIP is often constrained. The third method is Early stopping (ES): Progress is tracked using a ratio of no-reference blurriness and\nsharpness, however, as the authors point out, the criterion is only applicable to their modified DIP models. It is unclear how to apply\nthe noise-specific regularizer and ES criterion to unknown noise types and levels. It is suggested to monitor DIP reconstruction by\ntraining a coupled autoencoder. Although it performs similarly to ours, the additional autoencoder training significantly increases the\noverall processing time. By dividing the elements of y into \"training\" and \"validation\" sets, it is possible to simulate validation-based\nES in supervised learning. However, in IPs, particularly nonlinear ones (such as blind image deblurring (BID), where y \u02d82248 k\n\u02d82217 x and \u02d82217 denotes linear convolution), elements of y may not be i.i.d., which could impair the effectiveness of validation.\nFurthermore, withholding a portion of the observation in y can significantly diminish peak performance.",
  "methodology": "We advocate for the ES approach because, even when effective, regularization and noise modeling techniques frequently fail to\nenhance peak performance; instead, they extend it to the final iterations, potentially requiring ten times more iterations than would be\nnecessary to reach the peak in the original DIP models. Furthermore, both approaches necessitate extensive knowledge of the noise\ntype and level, which is often unavailable for most applications. If their essential models and hyperparameters are not appropriately\nconfigured, overfitting is likely to persist, and ES will still be necessary. This paper introduces a novel ES criterion applicable to\nvarious DIP models, based on monitoring the trend of the running variance in the reconstruction sequence.\n\nDetecting transition by running variance:\nOur lightweight method only involves computing the VAR curve and numerically detecting its valley\u02d82014 the iteration stops once the\nvalley is detected. To obtain the curve, we set a window size parame- ter W and compute the windowed moving variance (WMV). To\nrobustly detect the valley, we introduce a patience number P to tolerate up to P consecutive steps of variance stagnation. Obviously,\nthe cost is dominated by the calculation of variance per step, which is O(W N ) (N is the size of the visual object). In comparison, a\ntypical gradient update step for solving Eq. (2) costs at least \u02d82126(|\u02d803b8|N ), where |\u02d803b8| is the number of parameters in the DNN\nG\u02d803b8. Since |\u02d803b8| is typically much larger than W (default: 100), our running VAR and detection incur very little compu- tational\noverhead.",
  "experiments": "ES-WMV is tested for DIP in a variety of linear and nonlinear IPs, including image denoising, inpainting, demosaicing, super-\nresolution, MRI reconstruction, and blind image deblurring. ES-WMV is also systematically assessed for major DIP variants, such\nas deep decoder, DIP-TV, and GP-DIP, for image denoising. It is shown to be a dependable helper in identifying effective ES\npoints. The specifics of the DIP variants are covered in Appendix A.5. In addition, ES-WMV is contrasted with the primary rival\ntechniques, such as DF-STE, SV-ES, DOP, SB, and VAL. The specifics of the primary ES-based techniques are found in Appendix\nA.6. Reconstruction quality is evaluated using both PSNR and SSIM, and detection performance is shown using PSNR and SSIM\ngaps, which are the differences between our detected and peak values.\n\n4.1\n\nImage Denoising\n\nThe majority of earlier research on DIP overfitting has concentrated on image denoising and often assessed their techniques using\nonly one or two forms of noise with modest noise levels, such as low-level Gaussian noise. We use the traditional 9-image dataset\nfor each noise type, and we create two noise levels\u02d82014low and high\u02d82014for each.\n\n4.2\n\nImage Super-Resolution\n\nIn this task, we try to recover a clean im- age x0 from a noisy downsampled ver- sion y = Dt(x0) + \u02d803f5, where Dt(\u02d800b7) : [0,\n1]3\u02d800d7tH\u02d800d7tW \u02d82192 [0, 1]3\u02d800d7H\u02d800d7W is a down- sampling operator that resizes an im- age by the factor t and \u02d803f5 models\nex- tra additive noise. We consider the fol- lowing DIP-reparametrized formulation . = \u02d82225Dt(G\u02d803b8(z)) \u02d82212 y\u02d822252 min\u02d803b8\n\u02d82113(\u02d803b8) F , where G\u02d803b8 is a trainable DNN parameterized by \u02d803b8 and z is a frozen random seed. Then we conduct experiments\nfor 2\u02d800d7 super- resolution with low-level Gaussian and impulse noise. We test our ES-WMV for DIP and a state-of-the-art zero-shot\nmethod based on pre-trained diffusion model\u02d82014DDNM+ on the standard super-resolution dataset Set14, as shown in Tab. 5, Fig.\n11, and Appendix A.7.9. We note that DDNM+ relies on pre-trained models from large external training datasets, while DIP does\nnot. We observe that (1) Our ES-WMV is again able to detect near-peak performance for most images: the average PSNR gap is\n\u02d82264 1.50 and the average SSIM gap is \u02d82264 0.07; (2) DDNM+ is sensitive to the noise type and level: from Tab. 5, DDNM+ trained\nassuming Gaussian noise level \u02d803c3y = 0.12 outperforms DIP and DIP+ES-WMV when there is Gaus- sian measurement noise at\nthe level \u02d803c3y = 0.12, which is unrealistic in practice, as the noise level is often unknown beforehand. When the noise level is not\nset correctly, e.g., as \u02d803c3y = 0 in the DDNM+ (\u02d803c3y = .00) row of Tab. 5, the performance of DDNM+ is much worse than that of\nDIP and DIP+ES-WMV. Also, for super-resolution with impulse noise, DIP is also a clear winner that leads DDNM+ by a large\nmargin; and (3) in Appendix A.8, we show that DDNM+ may also suffer from the overfitting issue.\n\n4.3 MRI Reconstruction\n\nWe also test ES-WMV on MRI reconstruction, a typical linear IP with a nontrivial forward mapping: y \u02d82248 F(x), where F is the\nsubsampled Fourier operator, and we use \u02d82248 to indicate that the noise encountered in practical MRI imaging may be hybrid (e.g.,\nadditive, shot) and uncertain. Here, we take the 8-fold undersampling and parameterize x using \u02d8201cConv-Decoder\u02d8201d, a variant of\ndeep decoder. Due to the heavy over-parameterization, overfitting occurs and ES is needed.\n\n2\n\n\f4.4 Blind Image Deblurring\n\nIn BID, a blurry and noisy image is given, and the goal is to recover a sharp and clean image. The blur is mostly caused by motion\nand/or op- tical non-ideality in the camera, and the forward process is often modeled as y = k \u02d82217 x + n, where k is the blur\nkernel, n models additive sensory noise, and \u02d82217 is linear convolution to model the spa- tial uniformity of the blur effect. BID\nis a very challenging visual IP due to bilin- earity: (k, x) 7\u02d82192 k \u02d82217 x. Recently, researchers have tried to use DIP models to\nsolve BID by modeling k and x as two separate DNNs, i.e., min\u02d803b8k,\u02d803b8x \u02d82225y \u02d82212 G\u02d803b8k (zk) \u02d82217 G\u02d803b8x(zx)\u02d822252 2 +\n\u02d803bb\u02d82225\u02d82207G\u02d803b8x (zx)\u02d822251/\u02d82225\u02d82207G\u02d803b8x (zx)\u02d822252, where the regular- izer is to promote sparsity in the gradient domain\nfor the reconstruction of x, as stan- dard in BID. We follow previous work and choose a multilayer perceptron (MLP) with softmax\nactivation for G\u02d803b8k , and the canonical DIP model (CNN-based encoder-decoder architecture) for G\u02d803b8x(zx). We change their\nregularizer from the original \u02d82225\u02d82207G\u02d803b8x (zx)\u02d822251 to the current, as their original formulation is tested only at a very low\nnoise level \u02d803c3 = 10\u02d822125 and no overfitting is observed. We set the test with a higher noise level \u02d803c3 = 10\u02d822123, and find that its\noriginal formulation does not work.",
  "results": "Table 1: Summary of performance of our DIP+ES-WMV and competing methods on image denoising and blind image deblurring\n(BID). \u02d82713: working reasonably well (PSNR \u02d82265 2dB less of the original DIP peak); -: not working well (PSNR \u02d82264 2dB less of\nthe original DIP peak): N/A: not applicable (i.e., we do not perform comparison due to certain reasons). Note that DF-STE, DOP,\nand SB are based on modified DIP models.\n\nImage denoising\n\nSpeckle\nShot\nGaussian\nImpulse\nLow High\nLow High\nLow High\nLow High\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n-\n-\n-\n-\n-\n-\n-\n-\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\n\u02d82713\nN/A \u02d82713\n\u02d82713 N/A\n\u02d82713\nN/A\nN/A\n\u02d82713 N/A\nN/A \u02d82713\nN/A\nN/A\nN/A\nN/A\n\u02d82713 N/A\n\u02d82713\nN/A\nN/A\nN/A\nN/A\nN/A\n\nBID\n\nReal world\nLow\n\u02d82713\nN/A\nN/A\n-\nN/A\nN/A\nN/A\n\nHigh\n\u02d82713\nN/A\nN/A\n-\nN/A\nN/A\nN/A\n\nDIP+ES-WMV (Ours)\nDIP+NR-IQMs\nDIP+SV-ES\nDIP+VAL\nDF-STE\nDOP\nSB\n\nTable 2: ES-WMV (our method) on real-world image denoising for 1024 images: mean and (std) on the images. (D: detected)\n\nSSIM Gap\n\nMSE\n\u02d821131\nHuber\n\n\u02d82113 (loss)\n\nPSNR (D)\n\nPSNR Gap\n\nSSIM (D)\n\n34.04 (3.68)\n33.92 (4.34)\n33.72 (3.86)\n\n0.92 (0.83)\n0.92 (0.59)\n0.95 (0.73)\n\n0.92 (0.07)\n0.93 (0.05)\n0.92 (0.06)\n\n0.02 (0.04)\n0.02 (0.02)\n0.02 (0.03)\n\nTable 3: Wall-clock time (secs) of DIP and three ES methods per epoch on NVIDIA Tesla K40 GPU : mean and (std). The total wall\nclock time should contain both DIP and a certain ES method.\n\nDIP\n\nSV-ES\n\nES-WMV\n\nES-EMV\n\n0.448 (0.030)\n\n13.027 (3.872)\n\n0.301 (0.016)\n\n0.003 (0.003)\n\nThe results of our experiments are summarized in the tables above. Table 1 shows the performance of our DIP+ES-WMV method\nagainst competing methods for image denoising and BID. Table 2 reports the performance of ES-WMV on real-world image\ndenoising for 1024 images. Table 3 compares the wall-clock time of DIP and three ES methods per epoch. Table 4 compares\nES-WMV and SB for image denoising on the CBSD68 dataset. Table 5 compares ES-WMV for DIP and DDNM+ for 2\u02d800d7 image\nsuper-resolution. Table 6 shows the performance of ConvDecoder on MRI reconstruction. Table 7 compares BID detection between\nES-WMV and VAL on the Levin dataset. Table 8 compares DIP with ES-WMV vs. DOP on impulse noise. Table 9 compares\nES-WMV for DIP and DDNM+ for denoising images with medium-level Gaussian and impulse noise. Table 10 compares detection\nperformance between DIP with ES-WMV and DIP with ES-EMV for real image denoising on 1024 images. Table 11 compares\ndetection performance between DIP with ES-WMV and DIP with ES-EMV for real image denoising on the PolyU dataset. Table 12\nshows the performance of DIP with ES-WMV for image inpainting.\n\n3\n\n\fTable 4: Comparison between ES-WMV and SB for image denoising on the CBSD68 dataset with varying noise level \u02d803c3. The\nhigher PSNR detected and earlier detection are better, which are in red: mean and (std).\n\n\u02d803c3 = 15\n\n\u02d803c3 = 25\n\n\u02d803c3 = 50\n\nPSNR\n\nEpoch\n\nPSNR\n\nEpoch\n\nPSNR\n\nEpoch\n\nWMV 28.7(3.2)\n29.0(3.1)\n\nSB\n\n3962(2506)\n4908(1757)\n\n27.4(2.6)\n27.3(2.2)\n\n3068(2150)\n5099(1776)\n\n24.2(2.3)\n23.0(1.0)\n\n1548(1939)\n5765(1346)\n\nTable 5: Comparison of ES-WMV for DIP and DDNM+ for 2\u02d800d7 image super-resolution with low-level Gaussian and impulse\nnoise: mean and (std). The highest PSNR and SSIM for each task are in red. In particular, we set the best hyperparameter for\nDDNM+ (\u02d803c3y = 0.12), which is unfair for the DIP + ES-WMV combination as we fix its hyperparameter setting.\n\nPSNR\n\nSSIM\n\nGaussian\n\nImpulse\n\nGaussian\n\nImpulse\n\nDIP (peak)\nDIP + ES-WMV\nDDNM+ (\u02d803c3y = .12)\nDDNM+ (\u02d803c3y = .00)\n\n22.88 (1.58)\n22.11 (1.90)\n25.37 (2.00)\n16.91 (0.42)\n\n28.28 (2.73)\n26.77 (3.76)\n18.50 (0.68)\n16.59 (0.34)\n\n0.61 (0.09)\n0.54 (0.11)\n0.74 (0.11)\n0.31 (0.09)\n\n0.88 (0.06)\n0.86 (0.06)\n0.50 (0.08)\n0.49 (0.06)",
  "conclusion": "This paper introduces an innovative ES detection approach, ES-WMV, along with its variant, ES-EMV, which has demonstrated\nrobust performance across a range of visual IPs and different DIP variations. In contrast to most competing ES methods that are\nspecific to certain types of noise or DIP models and have limited applicability, our method exhibits broad effectiveness. While\nthere is a method with comparable performance, it significantly increases processing time. Another method, validation-based ES,\nperforms well in simple denoising tasks but falls short in more complex nonlinear IPs like BID.\n\n4\n\n\fTable 6: ConvDecoder on MRI reconstruction for 30 cases: mean and (std). (D: Detected)\n\nPSNR(D)\n\nPSNR Gap\n\nSSIM(D)\n\nSSIM Gap\n\n32.63 (2.36)\n\n0.23 (0.32)\n\n0.81 (0.09)\n\n0.01 (0.01)\n\nTable 7: BID detection comparison between ES-WMV and VAL on the Levin dataset for both low-level and high-level noise: mean\nand (std).Higher PSNR is in red and higher SSIM is in blue. (D: Detected)\n\nLow Level\n\nHigh Level\n\nPSNR(D)\n\nSSIM(D)\n\nPSNR(D)\n\nSSIM(D)\n\nWMV 28.54(0.61)\n18.87(1.44)\nVAL\n\n0.83(0.04)\n0.50(0.09)\n\n26.41(0.67)\n16.69(1.39)\n\n0.76(0.04)\n0.44(0.10)\n\nTable 8: DIP with ES-WMV vs. DOP on impulse noise: mean and (std). (D: Detected)\n\nLow Level\nPSNR SSIM\n\nHigh Level\nPSNR SSIM\n\nDIP-ES\nDOP\n\n31.64 (5.69) 0.85 (0.18)\n32.12 (4.52) 0.92 (0.07)\n\n24.74 (3.23) 0.67 (0.19)\n27.34 (3.78) 0.86 (0.10)\n\nTable 9: Comparison of ES-WMV for DIP and DDNM+ for denoising images with medium-level Gaussian and impulse noise: mean\nand (std). The highest PSNR and SSIM for each task are in red. In particular, we set the best hyperparameter for DDNM+ (\u02d803c3y =\n0.18), which is unfair for the DIP + ES-WMV combination as we fix its hyperparameter setting.\n\nPSNR\n\nSSIM\n\nGaussian\n\nImpulse\n\nGaussian\n\nImpulse\n\nDIP (peak)\nDIP + ES-WMV\nDDNM+ (\u02d803c3y = .18)\nDDNM+ (\u02d803c3y = .00)\n\n24.63 (2.06)\n23.61 (2.67)\n26.93 (2.25)\n15.66 (0.39)\n\n37.75 (3.32)\n36.87 (4.29)\n22.29 (3.00)\n15.52 (0.43)\n\n0.68 (0.06)\n0.60 (0.13)\n0.78 (0.07)\n0.25 (0.10)\n\n0.96 (0.10)\n0.96 (0.10)\n0.62 (0.12)\n0.30 (0.10)\n\nTable 10: Detection performance comparison between DIP with ES-WMV and DIP with ES-EMV for real image denoising on 1024\nimages from the RGB track of NTIRE 2020 Real Image Denoising Challenge: mean and (std). Higher PSNR and SSIM are in red.\n(D: Detected)\n\nPSNR(D)-WMV PSNR(D)-EMV SSIM(D)-WMV SSIM(D)-EMV\n\nDIP (MSE)\nDIP (\u02d821131)\nDIP (Huber)\n\n34.04 (3.68)\n33.92 (4.34)\n33.72 (3.86)\n\n34.96 (3.80)\n34.83 (4.35)\n34.72 (4.04)\n\n0.92 (0.07)\n0.93 (0.05)\n0.92 (0.06)\n\n0.93 (0.07)\n0.94 (0.05)\n0.93 (0.06)\n\nTable 11: Detection performance comparison between DIP with ES-WMV and DIP with ES-EMV for real image denoising on the\nPolyU dataset: mean and (std). Higher PSNR and SSIM are in red. (D: Detected)\n\nPSNR(D)-WMV PSNR(D)-EMV SSIM(D)-WMV SSIM(D)-EMV\n\nDIP (MSE)\nDIP (\u02d821131)\nDIP (Huber)\n\n36.83 (3.07)\n36.20 (2.81)\n36.76 (2.96)\n\n37.32 (3.82)\n36.43 (3.22)\n37.21 (3.19)\n\n0.98 (0.02)\n0.97 (0.02)\n0.98 (0.02)\n\n0.98 (0.03)\n0.97 (0.02)\n0.98 (0.02)\n\n5\n\n\fTable 12: DIP with ES-WMV for image inpainting: mean and (std). PSNR gaps below 1.00 are colored as red; SSIM gaps below\n0.05 are colored as blue. (D: Detected)\n\nPSNR(D)\n\nPSNR Gap\n\nSSIM(D)\n\nSSIM Gap\n\nBarbara\nBoat\nHouse\nLena\nPeppers\nC.man\nCouple\nFinger\nHill\nMan\nMontage\n\n21.59 (0.03)\n21.91 (0.10)\n27.95 (0.33)\n24.71 (0.30)\n25.86 (0.22)\n25.26 (0.09)\n21.40 (0.44)\n20.87 (0.04)\n23.54 (0.08)\n22.92 (0.25)\n26.16 (0.33)\n\n0.20 (0.03)\n1.16 (0.18)\n0.48 (0.10)\n0.37 (0.18)\n0.23 (0.05)\n0.23 (0.14)\n1.21 (0.53)\n0.24 (0.17)\n0.25 (0.11)\n0.46 (0.11)\n0.38 (0.26)\n\n0.67 (0.00)\n0.68 (0.00)\n0.89 (0.01)\n0.80 (0.00)\n0.84 (0.01)\n0.82 (0.00)\n0.63 (0.01)\n0.77 (0.00)\n0.70 (0.00)\n0.70 (0.01)\n0.86 (0.01)\n\n0.00 (0.00)\n0.03 (0.01)\n0.01 (0.00)\n0.01 (0.00)\n0.02 (0.00)\n0.01 (0.00)\n0.04 (0.02)\n0.01 (0.01)\n0.00 (0.00)\n0.01 (0.00)\n0.03 (0.01)\n\n6",
  "is_publishable": 1,
  "venue": NaN
}
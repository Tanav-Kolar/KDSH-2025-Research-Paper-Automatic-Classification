{
  "title": "Collaborative Clothing Segmentation and\nIdentification Through Image Analysis",
  "abstract": "This research introduces a comprehensive clothing co-parsing system designed\nto analyze a collection of clothing images, which are unsegmented but include\ndescriptive tags. The system aims to segment these images into meaningful config-\nurations. The proposed method uses a two-stage, data-driven approach. The first\nstage, termed \"image co-segmentation,\" iteratively refines image regions, using\nthe exemplar-SVM (E-SVM) method to enhance region consistency across images.\nThe second stage, \"region co-labeling,\" utilizes a multi-image graphical model\nwhere segmented regions serve as nodes. This incorporates contextual information\nabout clothing, such as item placement and interactions, which can be solved using\nthe efficient Graph Cuts algorithm. The system\u2019s performance is tested on the\nFashionista dataset and a newly developed dataset called CCP, which contains 2098\nhigh-resolution street fashion images. The results show a segmentation accuracy of\n90.29% and 88.23% and a recognition rate of 65.52% and 63.89% on the Fashion-\nista and CCP datasets, respectively, demonstrating an improvement over current\nleading methods.",
  "introduction": "The growth of online clothing sales has increased the demand for accurate clothing recognition and\nretrieval technologies. This has led to the development of several vision-based solutions. A key\nchallenge in these systems is the detailed, pixel-level labeling of clothing, which is often resource-\nintensive. However, image-level tags from user data offer a viable alternative. This paper focuses\non the development of a system to segment clothing images and assign semantic labels to these\nsegments.\n\nThe main contribution of this work is an effective system for parsing groups of clothing images and\nproviding precise pixel-level annotations. The system addresses the following significant challenges:\n\n\u2022 Clothes exhibit a wide variety of styles and textures, making them difficult to segment and\n\nidentify using only basic visual features.\n\n\u2022 Variations in human poses and the way clothes can obscure themselves complicate the\n\nrecognition process.\n\n\u2022 The existence of numerous, highly specific clothing categories, such as over 50 in the\nFashionista dataset, far more than in existing co-segmentation systems which typically\nhandle fewer categories.\n\nTo overcome these challenges, the system employs two sequential stages: image co-segmentation\nto isolate distinct clothing regions and region co-labeling to identify different clothing items, as\nillustrated below. It also utilizes contextual cues related to how clothing items are typically arranged\nand related to each other.\n\nThe co-segmentation phase refines regions across images using the E-SVM method. Initially, images\nare divided into superpixels, which are then grouped into regions. Many of these regions may not be\n\n.\n\n\fmeaningful due to the diversity of clothing and human poses. However, certain stable regions are\nidentified based on criteria like size and position. E-SVM classifiers are trained for these selected\nregions using HOG features, creating region-based detectors that help identify similar regions across\nimages. This approach is based on the observation that similar clothing items often share visual\npatterns.\n\nThe co-labeling phase uses a data-driven approach, constructing a multi-image graph where regions\nare treated as nodes. Connections are made between adjacent regions within an image, as well as\nbetween regions in different images that share visual or tag similarities. This strategy allows for\ncollective label assignment, leveraging similarities across images. The optimization is performed\nusing the Graph Cuts algorithm, considering various clothing context constraints.",
  "related_work": "Previous research on clothing and human segmentation has often focused on creating detailed models\nto handle the diversity in clothing styles and appearances. Some of the classic work used And-Or\ngraph templates to model and parse clothing configurations. Subsequent studies explored blocking\nmodels for segmenting clothes in images where items were heavily obscured, and deformable spatial\nmodels to enhance segmentation accuracy. Recent approaches have used shape-based human models\nor combined pose estimation with supervised region labeling, achieving notable results. However,\nthese methods have not been applied to clothing co-parsing and typically demand significant labeling\neffort.\n\nResearch on image/object co-labeling, which jointly processes a set of images containing similar\nobjects, has been explored. Methods include unsupervised shape-guided approaches for single-\ncategory co-labeling and incorporating automatic image segmentation with spatially coherent latent\ntopic models for unsupervised multi-class labeling. These unsupervised methods can struggle with a\nlarge number of categories and diverse appearances. Recent efforts have focused on supervised label\npropagation, using pixel-level label maps to assign labels to new images. However, these methods are\noften limited by the need for detailed annotations and rely on pixel-level correspondences, which\nmay not be effective for clothing parsing.",
  "methodology": "This research introduces a probabilistic model for the co-parsing of clothing images. The input\nconsists of a set of clothing images, denoted as I = {Ii}N\ni=1, each associated with tags Ti. Each\nimage Ii is represented by a set of superpixels, Ii = {sj}M\nj=1, which are subsequently grouped into\ncoherent regions. Each image is associated with four additional variables:\n\n(a) Regions {rk}K\nk=1, each comprising a set of superpixels.\n(b) Garment labels for each region, denoted as \u2113k \u2208 T , where k = 1, . . . , K.\n(c) E-SVM weights wk trained for each selected region.\n(d) Segmentation propagations C = (x, y, m), where (x, y) is the location and m is the\nsegmentation mask of an E-SVM, indicating that mask m can be propagated to position\n(x, y) of Ii.\n\nThe objective is to optimize parameters by maximizing the posterior probability:\n\n{L\u2217, R\u2217, W \u2217, C \u2217} = arg max P (L, R, W, C|I)\n\nThis probability can be factorized into co-labeling and co-segmentation components:\n\nP (L, R, W, C|I) \u221d P (L|R, C) \u00d7\n\nN\n(cid:89)\n\ni=1\n\nP (Ri|Ci, Ii)P (Wi|Ri)P (Ci|Wi, Ii)\n\nThe optimization process involves two phases: clothing image co-segmentation and co-labeling.\n\n2\n\n\fIn the co-segmentation phase, optimal regions are obtained by maximizing P (R|C, I). A superpixel\ngrouping indicator oj \u2208 {1, . . . , K} is introduced, indicating the region to which superpixel sj\nbelongs. Each region rk is defined as rk = {sj|oj = k}. The probability P (R|C, I) is defined as:\n\n\uf8ee\n\n\uf8f9\n\nP (R|C, I) =\n\n(cid:89)\n\ni\n\n\uf8f0P (ri|C, I)\n\n(cid:89)\n\nsj \u2208Ii\n\nP (oj|C, Ii)\n\n(cid:89)\n\n(m,n)\n\nP (om, on, sm, sn|C)\n\n\uf8fb\n\nThe unary potential P (oj, sj) indicates the probability of superpixel sj belonging to a region, and the\npairwise potential P (om, on, sm, sn|C) encourages smoothness between neighboring superpixels.\n\nCoherent regions are selected to train E-SVMs by maximizing P (W |R):\n\nP (W |R) =\n\n(cid:89)\n\nP (wk|rk) \u221d\n\n(cid:89)\n\nexp{\u2212E(wk, rk) \u2212 \u03d5(rk)}\n\nk\nwhere \u03d5(rj) indicates whether rj has been chosen for training E-SVM, and E(wk, rk) is the convex\nenergy function of E-SVM.\n\nk\n\nFinally, P (Ci|Wi, Ii) is defined based on the responses of E-SVM classifiers, maximized by selecting\nthe top k detections of each E-SVM as segmentation propagations.\n\nIn the co-labeling phase, a multi-image graphical model is used to assign a garment tag to each\nregion:\n\n\uf8ee\n\nP (L|R, C) \u221d\n\nN\n(cid:89)\n\nK\n(cid:89)\n\ni\n\nk\n\n\uf8f0P (\u2113ik, ri)\n\n(cid:89)\n\n(m,n)\n\nP (\u2113im, \u2113in, ri, rj)\n\n(cid:89)\n\n(u,v)\n\nQ(\u2113iu, \u2113iv, ru, rv|C)\n\n\uf8fb\n\n\uf8f9\n\nwhere P (\u2113ik, ri) is the singleton potential, P (\u2113im, \u2113in, ri, rj) is the interior affinity model, and\nQ(\u2113iu, \u2113iv, ru, rv|C) is the exterior affinity model.\n\n3.1 Unsupervised Image Co-Segmentation\n\nThe co-segmentation process involves iteratively refining regions, E-SVM weights, and segmentation\npropagations.\n\nSuperpixel Grouping: A linear programming problem is formulated to determine the number of\nregions automatically:\n\n(cid:88)\n\n(cid:88)\n\narg min\n\nd(se1, se2)oe +\n\nh(c)oc\n\nwhere d(se1, se2) is the dissimilarity between superpixels and h(c) measures the consistency of\ngrouping superpixels covered by an E-SVM mask.\n\ne\n\nc\u2208C\n\nTraining E-SVMs: The energy function for training E-SVMs is:\n\nE(wk, rk) =\n\n\u03bb1\n2\n\n||wk||2 +\n\n(cid:88)\n\nsj \u2208rk\n\nmax(0, 1 \u2212 wT\n\nk f (sj)) + \u03bb2\n\n(cid:88)\n\nsn\u2208N E\n\nmax(0, 1 + wT\n\nk f (sn))\n\nSegmentation Propagation: The E-SVM response is calibrated using a logistic distribution:\n\nSE(f ; w) =\n\n1\n1 + exp(\u2212\u03b1E(wT f \u2212 \u03b2E))\n\n3.2 Contextualized Co-Labeling\n\nIn this phase, a multi-image graphical model connects all images, incorporating two types of clothing\ncontexts. The singleton potential is defined as:\n\nP (\u2113k, rk) = sig(S(f (rk), \u2113k)) \u00b7 G\u2113k (Xk)\n\nwhere S(f (rk), \u2113k) is the appearance model score and G\u2113k (Xk) is the location context.\nThe interior affinity model is:\n\nP (\u2113im, \u2113in, rm, rn) = \u03d5(\u2113im, \u2113in, rm, rn) \u00b7 U (\u2113im, \u2113in)\n\nand the exterior affinity model is:\n\nQ(\u2113iu, \u2113iv, ru, rv|C) = G\u2113iu(Xu) \u00b7 G\u2113iv (Xv) \u00b7 \u03d5(\u2113iu, \u2113iv, ru, rv)\n\n3",
  "experiments": "The framework is evaluated on two datasets: Clothing Co-Parsing (CCP) and Fashionista. CCP\nincludes 2,098 high-resolution fashion photos with extensive variations in human appearance and\nclothing styles. The Fashionista dataset contains 158,235 fashion photos, with a subset of 685 images\nannotated at the superpixel level.\n\n4.1 Quantitative Evaluation\n\nThe method is compared with three state-of-the-art methods: PECS, Bi-layer Sparse Coding (BSC),\nand Semantic Texton Forest (STF). Performance is measured using average Pixel Accuracy (aPA)\nand mean Average Garment Recall (mAGR).\n\nTable 1: Clothing parsing results (%) on the Fashionista and CCP datasets.\n\n2*Methods\n\nFashionista\n\nCCP\n\nOurs-full\nPECS\nBSC\nSTF\nOurs-1\nOurs-2\nOurs-3\nBaseline\n\naPA mAGR\n\naPA mAGR\n\n90.29\n89.00\n82.34\n68.02\n89.69\n88.55\n84.44\n77.63\n\n65.52\n64.37\n33.63\n43.62\n61.26\n61.13\n47.16\n9.03\n\n88.23\n85.97\n81.61\n66.85\n87.12\n86.75\n85.43\n77.60\n\n63.89\n51.25\n38.75\n40.70\n61.22\n59.80\n42.50\n15.07\n\nThe proposed method outperforms BSC, STF, and PECS on both datasets, demonstrating the effec-\ntiveness of the iterative co-segmentation and co-labeling phases.",
  "results": "",
  "conclusion": "This paper presents a framework for jointly parsing a collection of clothing images using image-level\ntags. The framework includes a new dataset of high-resolution street fashion photos with detailed\nannotations. The experiments show that the proposed method is effective and performs favorably\ncompared to existing methods. Future work will focus on improving inference by iterating between\nthe two phases and exploring parallel implementations for large-scale applications.\n\n4",
  "is_publishable": 1,
  "venue": NaN
}
{
  "title": "A Vehicle Motion Prediction Approach for the 2021\nShifts Challenge",
  "abstract": "This paper details the solution developed for the 2021 Shifts Challenge, which\nfocused on robustness and uncertainty in real-world distributional shifts. The\ncompetition sought methods for addressing motion prediction in cross-domain\nscenarios. A key issue is the variance between input and ground truth data distribu-\ntions, known as the domain shift problem. The method proposed features a novel\narchitecture utilizing a self-attention mechanism and a specifically designed loss\nfunction. Ultimately, this approach achieved 3rd place in the competition.",
  "introduction": "This paper examines the crucial issue of prediction in autonomous driving. Predicting vehicle\ntrajectories to generate control commands is essential for avoiding collisions. While deep learning\nhas shown promise in specific domains, real-world conditions, such as varying environments, weather,\nand driver behaviors, create challenges for models trained on single datasets. These models may not\nperform well across diverse datasets.\n\nThe 2021 Shifts Challenge concentrated on prediction tasks across different domains. The goal was\nto predict 25 timestamps of trajectories from given raster images. To address this, a new architecture\nwas developed using insights from current research. The feature extractor was modified using NFNet\nfor stability, and a self-attention layer was included to enhance time-related predictions. The loss\nfunction was also adjusted for improved robustness, leading to a 3rd place ranking with 8.637 R-AUC\nCNLL in the competition.\n\n2 Our Solution\n\nThis section explains the solution for the domain-shift problem through the design of new model\narchitectures. The domain-shift problem arises when training and validation datasets come from\ndifferent distributions. Given input raster images X that contain the first 5 seconds of vehicle data, the\nobjective is to predict the last 5 seconds of trajectories Y for the objects. These images include details\nabout the positions, orientations, accelerations, and velocities of dynamic objects. The proposed\nmodel has two main parts: (1) a new backbone model and feature extractor, and (2) a revised loss\nfunction for better performance.\n\n[width=0.8]./Recurrentmodel.png\n\nFigure 1: Base Model Architecture: The baseline model uses the backbone model to extract features\nand utilizes recurrent model to generate prediction according to latent vectors.\n\n2.1 Baseline Model\n\nThe competition provided two baseline models and used an ensemble method to improve robustness.\nBoth Behavior Cloning (BC) and Deep Imitation Model (DIM) use convolutional backbones to\n\n.\n\n\fconvert raster image data into a latent vector, and then apply an autoregressive model to predict\nvehicle paths based on the latent vector. BC models the autoregressive likelihood as a single-variate\nGaussian, while DIM uses a multivariate normal distribution. After assessing the performance of BC\nand DIM, BC was selected as the baseline due to its better performance. The BC method is broken\ndown into two components: the feature extraction backbone and the recurrent model.\n\nFeature Extraction Backbone Using the input raster image X, a feature extraction backbone and a\nself-attention layer (described below) are used to encode both spatial and temporal information about\ndynamic objects into a latent embedding.\n\nZ = f (X)\n\n(1)\n\nThe baseline applies MobileNetV1 as its backbone. MobileNetV2 and MobileNetV3 were also\nconsidered but produced worse results, likely due to the simplicity of input data and model complexity.\nUltimately, the NFNet was chosen as the backbone (feature extractor) because of its training stability.\n\nSelf-Attention Layer To further refine the raster image features, a self-attention layer was in-\ncorporated. Self-attention, a key part of the Transformer model, allows for the consideration of\nlong-range dependencies and global information. The feature map was divided into pixel groups, and\nself-attention was used to aggregate pixel-wise information.\n\nRecurrent Model The GRU model was selected for the recurrent component due to superior\nperformance compared to other models. Using the embedding from feature extraction as hidden\nstates, the recurrent model makes predictions recursively. Given the embedding Zt at time t, with the\noutput vector Y0 as zero vector, the recurrent model g is used to generate predictions:\n\nZt = gencoder(Yt\u22121, Zt\u22121)\n\nYt = gdecoder(Yt\u22121, Zt)\n\n(2)\n\n(3)\n\nWhere Yt \u2208 RB\u00d7T \u00d72 represents the vehicle\u2019s position on a 2D bird\u2019s-eye-view map, and Zt \u2208 RB\u00d7K\nrepresents the hidden vector. B and T refer to the batch and time dimensions, respectively.\n\n2.2 Loss Function\n\nThe model was initially trained using negative log-likelihood (NLL) loss. However, because of the\ninadequate performance of the model on Average Distance Error (ADE) and Final Distance Error\n(FDE), these metrics were added to minimize the distance between predicted and actual positions.\n\nN LL(Y ) = \u2212log(p(Y ))\n\nLoss = \u2212 log(p(Y ; \u03b8)) + \u03b31||Y \u2212 \u02c6Y || + \u03b32||Yf \u2212 \u02c6Yf ||\n\n(4)\n\n(5)\n\nHere, p(Y ; \u03b8) indicates the probability of a predicted trajectory Y based on model parameters \u03b8. Yf\nrepresents the trajectory\u2019s final location. In the equation, the first component is the original loss, the\nsecond is the ADE loss, and the last is the FDE loss.\n\n2.3 Ensemble Method\n\nTo improve performance, the Robust Imitative Planning (RIP) method was employed to combine\nseveral models.",
  "related_work": "",
  "methodology": "",
  "experiments": "3.1 Dataset and Evaluation\n\nDataset The dataset provided by Yandex Self-Driving Group was utilized for motion prediction. The\ntraining set contains 27036 scenes, and the testing set contains 9569 scenes. The dataset for the Shifts\n\n2\n\n\fVehicle Motion Prediction includes 600000 scenes that vary in season, weather, location and time of\nday.\n\nEvaluations metrics The evaluation used three metrics: Average Distance Error (ADE), Final\nDistance Error (FDE), and Negative log-likelihood (NLL). ADE measures the sum of squared errors\nbetween predicted and actual positions at each time step. FDE calculates the sum of squared errors of\nthe final positions. NLL measures the unlikelihood of predicted trajectories matching the actual ones.\n\n3.2\n\nImplementation Details\n\nModels were trained on a single V100 machine for one day, with a batch size of 512 and a learning\nrate of 1e-4. Input feature maps were resized to 128 x 128. The AdamW optimizer and gradient\nclipping with a value of 1.0 was used.\n\n3.3 Ablation Study and Comparison Results\n\nAblation Study Table 1 displays the results of the ablation study. The baselines selected were\nDIM and BC. Various backbones, including EfficientNet, NFNet, and MobileNet, were compared,\nbut models with more parameters performed worse. This result suggests that simpler models are\nsufficient for extracting raster image information. Adding a self-attention mechanism improved the\nresults. Finally, incorporating ADE and FDE loss further improved performance, as shown in Table\n1. Although the DIM method resulted in the lowest Negative Log Likelihood(NLL), it was not as\ncompetitive as other models. Therefore, the DIM model was not chosen to pursue performance.\n\nTable 1: Ablation Study on Shift Vehicle Motion Prediction Dataset\n\nMethod\n\nADE\u2193 In Domain FDE\u2193\n\nNLL\u2193\n\nADE\u2193 Out of Domain FDE\u2193\n\nNLL\u2193\n\nDIM + MobileNetV2(baseline)\nBC + MobileNetV2(baseline)\nBC + NFNet18\nBC + NFNet50\nBC + NFNet18 + Attention\nBC + NFNet50 + Attention\nBC + NFNet18 + ADE Loss\nBC + NFNet18 + Attention + ADE Loss\n\n2.450\n1.632\n1.225\n1.360\n1.174\n1.155\n1.197\n1.139\n\n5.592\n3.379\n2.670\n2.963\n2.549\n2.504\n2.55\n2.488\n\n-84.724\n-42.980\n-53.149\n-50.605\n-56.199\n-56.291\n-54.047\n-55.208\n\n2.421\n1.519\n1.300\n1.392\n1.325\n1.265\n1.299\n1.227\n\n5.639\n3.230\n2.893\n3.066\n2.852\n2.770\n2.821\n2.714\n\n-85.134\n-46.887\n-53.130\n-51.317\n-54.476\n-54.730\n-53.056\n-54.282\n\nComparison Results After verifying the base model\u2019s effectiveness, the aggregation model, RIP, was\nused along with the Worst Case Method (WCM). The WCM method samples multiple predictions\nper model and picks the one with the lowest confidence for more reliable results. Table 2 shows the\ncompetition results, where our model outperformed baselines in weighted sums of ADE and FDE.\nHowever, the MINADE and MINFDE results were not as strong. Overall, this approach secured 3rd\nplace.\n\nTable 2: Quantitative Result of Top3 Final Submission: CNLL represents the weighted sum of NLL;\nWADE represents the weighted sum of ADE; WFDE represents the weighted sum of FDE;\n\nRank\n\nMethod\n\nScore (R-AUC CNLL) CNLL\u2193 WADE\u2193 WFDE\u2193 MINADE\u2193 MINFDE\u2193\n\n-\n1\n2\n3\n\nbaseline\nSBteam\nAlexey & Dmitry\nOurs\n\n10.572\n2.571\n2.619\n8.637\n\n65.147\n15.676\n15.599\n61.864\n\n1.082\n1.850\n1.326\n1.017\n\n2.382\n4.433\n3.158\n2.264\n\n0.824\n0.526\n0.495\n0.799\n\n1.764\n1.016\n0.936\n1.719",
  "results": "",
  "conclusion": "In this challenge focused on distributional shifts, we introduced a novel base model architecture,\nwhich combined with an ensemble method, yielded competitive results. Other state-of-the-art methods\nwere implemented, and results were compared with analysis. The robustness of the provided ensemble\nmethod was verified. This methodology resulted in the third prize in the competition.\n\n3",
  "is_publishable": 1,
  "venue": NaN
}
{
  "title": "Enhanced Image Compression Through Advanced\nResidual Network Architectures",
  "abstract": "This manuscript provides an in-depth explanation of the methodology developed\nfor a recent image compression challenge. The method primarily incorporates two\ninnovative aspects: the application of advanced residual networks for enhanced\ncompression and the utilization of sub-pixel convolution techniques for efficient\nup-sampling during decompression. The efficacy of these methodologies, which\nachieved a high Multiscale Structural Similarity Index (MS-SSIM) of 0.972 under\na strict bit rate constraint of 0.15 bits per pixel (bpp) while maintaining reasonable\ncomputational demands during the evaluation stage.",
  "introduction": "Image compression remains a crucial research area within the field of signal processing, aiming to\nfacilitate more efficient data storage and transfer. Conventional image compression algorithms, like\nthe various JPEG standards, often employ manually designed encoder/decoder frameworks. However,\nwith the emergence of novel image formats and the proliferation of high-resolution mobile devices,\nthere is a growing recognition that existing standards may not represent the most effective or universal\nsolutions for image compression.\n\nRecently, deep learning-based techniques have shown a surge of progress in the image compression\ndomain. Some of these methods employ generative models, trained adversarially, to effectively learn\nthe underlying distribution of images, resulting in impressive subjective quality even at exceptionally\nlow bit rates. Other works utilize recurrent neural networks to iteratively compress residual informa-\ntion, enabling progressive coding which allows for multiple quality levels within a single compression\noperation. Further advancements have been made by focusing on relaxing quantization constraints\nand improving entropy modeling, leading to enhanced performance compared to established image\ncompression methods.\n\nNevertheless, identifying an optimal network structure presents a formidable challenge across various\nmachine learning applications, including image compression. This paper primarily discusses two\nimportant aspects of network design for image compression. The first concerns the selection of kernel\nsize, a parameter that significantly influences compression effectiveness in traditional algorithms.\nMotivated by its impact in classical methods, this paper presents experiments that use different filter\nsizes to prove that larger kernel sizes contribute to improved coding efficiency. Building upon this, a\nstrategy is presented that utilizes a deep residual learning approach, allowing for the maintenance\nof a broad receptive field while utilizing a reduced number of parameters. This approach not only\ndecreases the model\u2019s overall size but also substantially enhances its performance. Additionally,\nthe architecture of up-sampling operations within the decoder plays a pivotal role in determining\nthe quality of reconstructed images and the presence of artifacts. This issue, extensively studied in\nthe context of super-resolution, involves various implementations for up-sampling layers, such as\ninterpolation, transposed convolution, and sub-pixel convolution. This work compares two commonly\nused up-sampling methods, transposed and sub-pixel convolutions, to demonstrate their relative\nperformance in the context of image compression.\n\n.",
  "related_work": "",
  "methodology": "The fundamental network architectures employed in this research are based on prior works that\nhave demonstrated state-of-the-art compression performance. The network is structured as a pair\nof autoencoders. The primary autoencoder is responsible for optimizing the rate-distortion tradeoff\ninherent in image compression. The loss function can be expressed as:\n\nJ = \u03bbd(x, \u02c6x) + R(\u02c6y)\n\n(1)\n\nwhere \u03bb is a parameter that balances the importance of rate and distortion. The secondary autoencoder\nhandles the encoding of side information, which is used to model the probability distribution of the\ncompressed data. A Gaussian scale mixture approach is utilized to develop an image-adaptive entropy\nmodel, with scale parameters conditioned on a hyperprior.\n\n2.1 From Small Kernel Size to Large Kernel Size\n\nIn traditional image compression techniques, the size of transform filters significantly affects coding\nefficiency, especially for high-definition videos. Initially, transform sizes were small, but as the field\nprogressed, there was a gradual shift towards larger sizes to better capture spatial correlations and\nsemantic details. The experiments detailed in this paper, using a standard dataset, explore the impact\nof different filter sizes in both the main and auxiliary autoencoders. Table 1 indicates that for the\nBaseline architecture, larger kernel sizes lead to better rate-distortion outcomes. Similarly, Table\n2 demonstrates comparable improvements for the HyperPrior architectures. Table 3 reveals that\nemploying large kernels in the auxiliary autoencoder does not enhance rate-distortion performance\nand may even negatively impact it. This is likely due to the small size of the compressed codes, which\nmakes smaller kernels sufficient for effective encoding. An excessive number of trainable parameters\ncan hinder the learning process.\n\nTable 1: The effect of kernel size on Baseline on Kodak, optimized by MSE with \u03bb = 0.015.\n\nMethod\n\nPSNR MS-SSIM Rate\n\nBaseline-3\nBaseline-5\nBaseline-9\n\n32.160\n32.859\n32.911\n\n0.9742\n0.9766\n0.9776\n\n0.671\n0.641\n0.633\n\nTable 2: The effect of kernel size on HyperPrior on Kodak, optimized by MSE with \u03bb = 0.015.\n\nMethod\n\nPSNR MS-SSIM Rate\n\nHyperPrior-3\nHyperPrior-5\nHyperPrior-9\n\n32.488\n32.976\n33.005\n\n0.9742\n0.9757\n0.9765\n\n0.543\n0.518\n0.512\n\nTable 3: The effect of kernel size in the auxiliary autoencoder on Kodak, optimized by MS-SSIM\nwith \u03bb = 5.\n\nMethod\n\nPSNR MS-SSIM Rate\n\nHyperPrior-9-Aux-5\nHyperPrior-9-Aux-9\n\n26.266\n26.236\n\n0.9591\n0.9590\n\n0.169\n0.171\n\n2.2 From Shallow Network to Deep Residual Network\n\nIn terms of receptive field coverage, a sequence of four 3x3 kernels can encompass the same area as\na single 9x9 kernel but with a reduced parameter count. Initial attempts to substitute a large kernel\nwith multiple 3x3 filters encountered convergence issues during training. To address this, shortcut\nconnections were incorporated between adjacent 3x3 kernels. The resultant deep residual network\n\n2\n\n\farchitecture for image compression is denoted as ResNet-3x3(4), signifying that a stack of four 3x3\nkernels achieves an equivalent receptive field to a 9x9 kernel. To minimize parameter overhead,\nGDN/IGDN activation functions are applied only once within each residual unit when the output\ndimensions change. For the remaining convolutional layers, parameter-free Leaky ReLU activations\nare employed to introduce non-linearity. As indicated in Table 4, ResNet-3x3(4) surpasses both\nResNet-3x3(3) and Hyperprior-9 in terms of performance.\n\nTable 4: Comparison of residual networks and upsampling operations on Kodak, optimized by\nMS-SSIM with \u03bb = 5.\n\nMethod\n\nPSNR MS-SSIM Rate\n\nHyperprior-9\nResNet-3x3(3)\nResNet-3x3(4)-TConv\nResNet-3x3(4)-SubPixel\n\n26.266\n26.378\n26.457\n26.498\n\n0.9591\n0.9605\n0.9611\n0.9622\n\n0.1690\n0.1704\n0.1693\n0.1700\n\n2.3 Upsampling Operations at Decoder Side\n\nThe encoder-decoder structure is characterized by its symmetrical design. While down-sampling at\nthe encoder is typically achieved using strided convolution filters, up-sampling at the decoder can be\nimplemented through various methods, such as bicubic interpolation, transposed convolution, and\nsub-pixel convolution. Considering the importance of rapid end-to-end learning, bicubic interpolation\nwas excluded, and a comparison was made between the two widely used up-sampling techniques:\ntransposed convolution (TConv) and sub-pixel convolution (SubPixel). To implement sub-pixel\nconvolution, the channel count is expanded fourfold, followed by the application of a depth-to-space\noperation. The results presented in Table 4 demonstrate that sub-pixel convolution filters offer slight\nimprovements in both PSNR and MS-SSIM compared to transposed convolution filters.",
  "experiments": "For the training process, 256x256 image patches were extracted from a large-scale image dataset. A\nbatch size of 8 was employed, and training was conducted for up to 2 million iterations to ensure\nstable convergence. Optimization was performed using the Adam optimizer, with an initial learning\nrate of 1 x 10<sup>-4</sup>, reduced to 1 x 10<sup>-5</sup> for the final 80,000 iterations.\n\nTwo primary strategies were implemented. The first strategy, termed \"Wide Bottleneck,\" involves\nincreasing the model\u2019s capacity by expanding the number of filters. Since increasing filters in large\nfeature maps significantly increases computational cost (FLOPs), the filter count was only raised in\nthe encoder\u2019s final layer, from 128 to 192. This results in a minor FLOPs increase, as detailed in Table\n5. While Bottleneck192 effectively reduces the bit rate, it also leads to some quality degradation\ncompared to Bottleneck128.\n\nTable 5: The effect of wide bottleneck on Kodak dataset.\n\nMethod\n\nPSNR MS-SSIM Rate\n\nResNet-3x3(4)-Bottleneck128\nResNet-3x3(4)-Bottleneck192\n\n26.498\n26.317\n\n0.9622\n0.9619\n\n0.1700\n0.1667\n\nThe second strategy is \"Rate Control.\" For achieving a target bit rate, two models are trained at\ndistinct bit rates by adjusting the \u03bb parameter. This allows for adaptive selection during encoding to\napproach the target bit rate while maximizing MS-SSIM, as shown in Table 6. A single bit is added\nto the bitstream to indicate the model used for decoding, without increasing decoder complexity.",
  "results": "Table 7 summarizes the compression performance of the proposed methods on a validation dataset.\n\n3\n\n\fTable 6: Rate control on validation dataset.\n\nMethod\n\nResNet-3x3(4)-Bottleneck192\nResNet-3x3(4)-Bottleneck192\n\n\u03bb\n\n5\n10\n\nPSNR MS-SSIM Rate\n\n29.708\n30.710\n\n0.9697\n0.9765\n\n0.1369\n0.1816\n\nTable 7: Results on validation dataset.\n\nEntry\n\nDescription\n\nPSNR MS-SSIM Rate\n\nKattolab\nKattolab\nKattolab\nKattolabv2\nKattolabSSIM ResNet-3x3(4)-SubPixel + Wide Bottleneck + Rate Control\n\nHyperPrior-9\nHyperPrior-9 + Rate Control\nResNet-3x3(4)-TConv + Rate Control\nResNet-3x3(4)-SubPixel+ Rate Control\n\n28.902\n29.102\n29.315\n29.300\n29.211\n\n0.9674\n0.9701\n0.9716\n0.9720\n0.9724\n\n0.134\n0.150\n0.150\n0.150\n0.150\n\nWhile deep residual networks enhance coding gain, they also lead to a substantial increase in model\nsize. This section analyzes the parameter count and model complexity in terms of floating-point\noperations per second (FLOPs) for various architectures. Specifically, using the HyperPrior-9\narchitecture as an example, Table 8 provides a layer-wise breakdown of model size. The number of\nparameters and FLOPs are calculated as follows:\n\nP ara = (h \u00d7 w \u00d7 Cin + 1) \u00d7 Cout\nF LOP s = P ara \u00d7 H \u2032 \u00d7 W \u2032\n\n(2)\n(3)\n\nwhere h \u00d7 w represents the kernel size, H \u2032 \u00d7 W \u2032 denotes the output dimensions, and Cin and Cout\nare the number of input and output channels, respectively. The +1 term is omitted when no bias\nis used. Quantization and leaky-ReLU are parameter-free. GDN operates across channels but not\nspatial positions, resulting in a parameter count of (Cin + 1) \u00d7 Cout. The total FLOPs for GDN\nand inverse GDN calculations are minimal. This analysis primarily focuses on the backbone of\nconvolutional layers, so the FLOPs of GDN, inverse GDN, and factorized prior are not included in the\ncomparison. Table 9 presents a comparison of different architectures, with the last column showing\nthe relative FLOPs using Baseline-5 as the reference. The proposed models achieve improved coding\nperformance with relatively low computational complexity.",
  "conclusion": "This manuscript details the proposed deep residual learning framework and sub-pixel convolution\ntechnique for image compression, forming the foundation of the submitted entries: Kattolab, Katto-\nlabv2, and KattolabSSIM. The results demonstrate that these approaches achieve a high MS-SSIM of\n0.972 under a bit rate constraint of 0.15 bpp, while maintaining a moderate level of computational\ncomplexity during the validation phase.\n\n4\n\n\fTable 8: The model size analysis of HyperPrior-9.\n\nLayer\nFLOPs\n\nKernel\n\nChannel\n\nOutput\n\nPara\n\nh w Cin\n\nCout\n\nH x W\n\n128\n\n128 x 128\n\n31232\n\n128\n\n128\n\n128\n\n128\n\n128\n\n128\n\n128\n\n192\n\n256\n\n640\n\n512\n\n256\n\n128\n\n128\n\n64 x 64\n\n1327232\n\n32 x 32\n\n1327232\n\n16 x 16\n\n1327104\n\n99072\n\n16 x 16\n\n147584\n\n8 x 8\n\n4 x 4\n\n409728\n\n409728\n\n5888\n\n8 x 8\n\n409728\n\n16 x 16\n\n614592\n\n16 x 16\n\n442624\n\n16 x 16\n\n164480\n\n16 x 16\n\n328192\n\n16 x 16\n\n131072\n\n32 x 32\n\n1327232\n\n64 x 64\n\n1327232\n\n128\n\n128 x 128\n\n1327232\n\n3\n\n256 x 256\n\n31107\n\n11188291\n\n!\n\nconv1\n5.12 x 10<sup>9</sup>\nconv2\n5.44 x 10<sup>7</sup>\nconv3\n1.36 x 10<sup>7</sup>\nconv4\n3.40 x 10<sup>6</sup>\nGDN/IGDN\n-\nHconv1\n3.78 x 10<sup>6</sup>\nHconv2\n2.62 x 10<sup>6</sup>\nHconv3\n6.56 x 10<sup>5</sup>\nFactorizedPrior\n-\nHTconv1\n2.62 x 10<sup>6</sup>\nHTconv2\n1.57 x 10<sup>7</sup>\nHTconv3\n1.13 x 10<sup>7</sup>\nlayer1\n4.21 x 10<sup>6</sup>\nlayer2\n8.40 x 10<sup>6</sup>\nlayer3\n3.36 x 10<sup>6</sup>\nTconv1\n1.36 x 10<sup>7</sup>\nTconv2\n5.44 x 10<sup>7</sup>\nTconv3\n2.17 x 10<sup>10</sup>\nTconv4\n2.04 x 10<sup>7</sup>\n\nTotal\n3.88 x 10<sup>10</sup>\n\n9\n\n9\n\n9\n\n9\n\n3\n\n5\n\n5\n\n5\n\n5\n\n3\n\n9\n\n9\n\n9\n\n9\n\n9\n\n9\n\n9\n\n9\n\n3\n\n5\n\n5\n\n5\n\n5\n\n3\n\n9\n\n9\n\n9\n\n9\n\n3\n\n128\n\n128\n\n128\n\n128\n\n128\n\n128\n\n128\n\n128\n\n192\n\n256\n\n640\n\n512\n\n128\n\n128\n\n128\n\n128\n\n5\n\n\fTable 9: The model complexity of different architectures.\n\nMethod\n\nBaseline-3\nBaseline-5\nBaseline-9\nHyperPrior-3\nHyperPrior-5\nHyperPrior-9\nResNet-3x3(3)\nResNet-3x3(4)\nResNet-3x3(4)-SubPixel\nResNet-3x3(4)-SubPixel-Bottleneck192\n\nPara\n\n997379\n2582531\n8130563\n4055107\n5640259\n11188291\n5716355\n6684931\n8172172\n11627916\n\nFLOPs\n\nRelative\n\n4.25 x 10<sup>9</sup>\n1.18 x 10<sup>10</sup>\n3.82 x 10<sup>10</sup>\n4.78 x 10<sup>9</sup>\n1.23 x 10<sup>10</sup>\n3.88 x 10<sup>10</sup>\n1.75 x 10<sup>10</sup>\n2.43 x 10<sup>10</sup>\n2.50 x 10<sup>10</sup>\n2.56 x 10<sup>10</sup>\n\n0.36\n1.00\n3.24\n0.40\n1.04\n3.28\n1.48\n2.06\n2.12\n2.17\n\n6",
  "is_publishable": 1,
  "venue": NaN
}
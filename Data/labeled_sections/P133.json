{
  "title": "Discontinuous Constituent Parsing as Sequence\nLabeling",
  "abstract": "This paper reduces discontinuous parsing to sequence labeling. It first shows that\nexisting reductions for constituent parsing as labeling do not support discontinuities.\nSecond, it fills this gap and proposes to encode tree discontinuities as nearly ordered\npermutations of the input sequence. Third, it studies whether such discontinuous\nrepresentations are learnable. The experiments show that despite the architectural\nsimplicity, under the right representation, the models are fast and accurate.",
  "introduction": "Discontinuous constituent parsing studies how to generate phrase-structure trees of sentences coming\nfrom non-configurational languages, where non-consecutive tokens can be part of the same grammati-\ncal function (e.g. nonconsecutive terms belonging to the same verb phrase). Figure 1 shows a German\nsentence exhibiting this phenomenon. Discontinuities happen in languages that exhibit free word\norder such as German or Guugu Yimidhirr, but also in those with high rigidity, e.g. English, whose\ngrammar allows certain discontinuous expressions, such as wh-movement or extraposition. This\nmakes discontinuous parsing a core computational linguistics problem that affects a wide spectrum\nof languages.\n\nThere are different paradigms for discontinuous phrase-structure parsing, such as chart-based parsers,\ntransitionbased algorithms or reductions to a problem of a different nature, such as dependency\nparsing. However, many of these approaches come either at a high complexity or low\n\nspeed, while others give up significant performance to achieve an acceptable latency.\n\nRelated to these research aspects, this work explores the feasibility of discontinuous parsing under\nthe sequence labeling paradigm, inspired by work on fast and simple continuous constituent parsing.\nWe will focus on tackling the limitations of their encoding functions when it comes to analyzing\ndiscontinuous structures, and include an empirical comparison against existing parsers.\n\nContribution (i) The first contribution is theoretical: to reduce constituent parsing of free word order\nlanguages to a sequence labeling problem. This is done by encoding the order of the sentence as\n(nearly ordered) permutations. We present various ways of doing so, which can be naturally combined\nwith the labels produced by existing reductions for continuous constituent parsing. (ii) The second\ncontribution is a practical one: to show how these representations can be learned by neural transducers.\nWe also shed light on whether general-purpose architectures for NLP tasks can effectively parse\nfree word order languages, and be used as an alternative to adhoc algorithms and architectures for\ndiscontinuous constituent parsing.",
  "related_work": "Discontinuous phrase-structure trees can be derived by expressive formalisms such as Multiple\nContext Free Grammmars (MCFGs) or Linear Context-Free Rewriting Systems (LCFRS). MCFGs\nand LCFRS are essentially an extension of Context-Free Grammars (CFGs) such that non-terminals\ncan link to non-consecutive spans. Traditionally, chart-based parsers relying on this paradigm\n\n\fcommonly suffer from high complexity. Let k be the block degree, i.e. the number of nonconsecutive\nspans than can be attached to a single non-terminal; the complexity of applying CYK (after binarizing\nthe grammar) would be O(n3k), which can be improved to O(n2k+2) if the parser is restricted to\nwell-nested LCFRS, and discusses how for a standard discontinuous treebank, k 3 (in contrast to\nk = 1 in CFGs). Recently, presents a chart-based parser for k = 2 that can run in O(n3), which is\nequivalent to the running time of a continuous chart parser, while covering 98\n\nDifferently, it is possible to rely on the idea that discontinuities are inherently related to the location\nof the token in the sentence. In this sense, it is possible to reorder the tokens while still obtaining a\ngrammatical sentence that could be parsed by a continuous algorithm. This is usually achieved with\ntransition-based parsing algorithms and the swap transition which switches the topmost elements in\nthe stack. For instance, uses this transition to adapt an easy-first strategy for dependency parsing to\ndiscontinuous constituent parsing. In a similar vein, builds on top of a fast continuous shift-reduce\nconstituent parser, and incorporates both standard and bundled swap transitions in order to analyze\ndiscontinuous constituents. system produces derivations of up to a length of n2 n + 1 given a\nsentence of length n. More efficiently, presents a transition system which replaces swap with a gap\ntransition. The intuition is that a reduction does not need to be always applied locally to the two\ntopmost elements in the stack, and that those two items can be connected, despite the existence of a\ngap between them, using non-local reductions. Their algorithm ensures an upper-bound of n(n1)2\ntransitions. With a different optimization goal, removed the traditional reliance of discontinuous\nparsers on averaged perceptrons and hand-crafted features for a recursive neural network approach\nthat guides a swap-based system, with the capacity to generate contextualized representations. replace\nthe stack used in transition-based systems with a memory set containing the created constituents.\nThis model allows interactions between elements that are not adjacent, without the swap transition, to\ncreate a new (discontinuous) constituent. Trained on a 2 stacked BiLSTM transducer, the model is\nguaranteed to build a tree with in 4n-2 transitions, given a sentence of length n.\n\nA middle ground between explicit constituent parsing algorithms and this paper is the work based on\ntransformations. For instance, convert constituent trees into a nonlinguistic dependency representation\nthat is learned by a transition-based dependency parser, to then map its output back to a constituent tree.\nA similar approach is taken by, but they proposed a more compact representation that leads to a much\nreduced set of output labels. Other authors such as propose a two-step approach that approximates\ndiscontinuous structure trees by parsing context-free grammars with generative probabilistic models\nand transforming them to discontinuous ones. cast discontinuous phrase-structure parsing into a\nframework that jointly performs supertagging and non-projective dependency parsing by a reduction\nto the Generalized Maximum Spanning Arborescence problem. The recent work by can be also\nframed within this paradigm. They essentially adapt the work by and replace the averaged perceptron\nclassifier with pointer networks, adressing\n\nIn this context, the closest work to ours is the reduction proposed by, who cast continuous constituent\nparsing as sequence labeling. In the next sections we build on top of their work and: (i) analyze why\ntheir approach cannot handle discontinuous phrases, (ii) extend it to handle such phenomena, and (iii)\ntrain functional sequence labeling discontinuous parsers.\n\n3 Preliminaries\n\nLet w = [w0, w1, ..., w|w|1] be an input sequence of tokens, and T|w| the set of (continuous)\nconstituent trees for sequences of length |w|; define an encoding function : T|w| \u2192 L|w| to map\ncontinuous constituent trees into a sequence of labels of the same length as the input. Each label, li\nL, is composed of three components li = (ni, xi, ui):\n\n\u2022 ni encodes the number of levels in the tree in common between a word wi and wi+1. To obtain a\nmanageable output vocabulary space, ni is actually encoded as the difference ni ni1, with n1 = 0. We\ndenote by abs(ni) the absolute number of levels represented by ni. i.e. the total levels in common\nshared between a word and its next one.\n\n\u2022 xi represents the lowest non-terminal symbol shared between wi and wi+1 at level abs(ni).\n\n\u2022 ui encodes a leaf unary chain, i.e. nonterminals that belong only to the path from the terminal wi to\nthe root. Note that cannot encode this information in (ni, xi), as these components always represent\ncommon information between wi and wi+1.\n\n2\n\n\fIncompleteness for discontinuous phrase structures proved that is complete and injective for continu-\nous trees. However, it is easy to prove that its validity does not extend to discontinuous trees, by using\na counterexample. Figure 3 shows a minimal discontinuous tree that cannot be correctly decoded.\n\nThe inability to encode discontinuities lies on the assumption that wi+1 will always be attached to a\nnode belonging to the path from the root to wi (ni is then used to specify the location of that node in\nthe path). This is always true in continuous trees, but not in discontinuous trees, as can be seen in\nFigure 3 where c is the child of a constituent that does not lie in the path from S to b.\n\n4 Encoding nearly ordered permutations\n\nNext, we fill this gap to address discontinuous parsing as sequence labeling. We will extend the\nencoding to the set of discontinuous constituent trees, which we will call T|w|. The key to do this\nrelies on a well-known property: a discontinuous tree t T|w| can be represented as a continuous one\nusing an in-order traversal that keeps track of the original indexes (e.g. the trees at the left and the\nright in Figure 4). We will call this tree the (canonical) continuous arrangement of t, (t) T|w|.\n\nThus, if given an input sentence we can generate the position of every word as a terminal in (t), the\nexisting encodings to predict continuous trees as sequence labeling could be applied on (t). In essence,\nthis is learning to predict a permutation of w. As introduced in \u00a72, the concept of location of a token\nis not a stranger in transition-based discontinuous parsing, where actions such as swap switch the\nposition of two elements in order to create a discontinuous phrase. We instead propose to explore\nhow to handle this problem in end-to-end sequence labeling fashion, without relying on any parsing\nstructure nor a set of transitions.\n\nTodo so, first we denote by \u03c4 : {0, . . . , |w| \u2212 1} \u2192 {0, . . . , |w| \u2212 1} the permutation that maps the\nposition i of a given wi in w into its position as a terminal node in \u03c9(t). From this, one can derive\n\u03c4 \u22121, a function that encodes a permutation of w in such a way that its phrase structure does not have\ncrossing branches. For continuous trees, \u03c4 and \u03c4 \u22121 are identity permutations. Then, we extend the\ntree encoding function \u03a6 to T|w| \u2192 L\u2032\n|w| where l \u2208 L\u2032 is enriched with a fourth component pi such\nthat l = (ni, xi, ui, pi), where pi is a discrete symbol such that the sequence of pi\u2019s encodes the\npermutation \u03c4 (typically, each pi will be an encoding of \u03c4 (i), i.e., the position of wi in the continuous\narrangement, although this need not be true in all encodings, as will be seen below).\n\n.\n\n.\n\nThe crux of defining a viable encoding for discontinuous parsing is then in how we encode tau as\na sequence of values pi, for i = 0 .\n|w| 1. While the naive approach would be the identity\nencoding (pi = tau(i)), we ideally want an encoding that balances minimizing sparsity (by minimizing\ninfrequently-used values) and maximizing learnability (by being predictable). To do so, we will look\nfor encodings that take advantage of the fact that discontinuities in attested syntactic structures are\nmild , i.e., in most cases, tau (i + 1) = tau (i) + 1. In other words, permutations tau corresponding to\nreal syntactic trees tend to be nearly ordered permutations. Based on these principles, we propose\nbelow a set of concrete encodings, which are also depicted on an example in Figure 4. All of them\nhandle multiple gaps (a discontinuity inside a discontinuity) and cover 100\n\nAbsolute-position: For every token wi, pi = \u03c4 (i) only if wi \u0338= \u03c4 (i). Otherwise, we use a special\nlabel INV, which represents that the word is a fixed point in the permutation, i.e., it occupies the same\nplace in the sentence and in the continuous arrangement.\n\nRelative-position If i != tau(i), then pi = i tau(i). otherwise, we again use the INV label.\n\nLehmer code In combinatorics, let n = [0, ..., n 1] be a sorted sequence of objects, a Lehmer code\nis a sequence sigma = [sigma0, ...sigman1] that encodes one of the n! permutations of n, namely .\nThe idea is intuitive: let ni+1 be the subsequence of objects from n that remain available after we\nhave permuted the first i objects to achieve the permutation , then sigmai+1 equals the (zero-based)\nposition in ni+1 of the next object to be selected. For instance, given n = [0, 1, 2, 3, 4] and a valid\npermutation = [0, 1, 3, 4, 2], then sigma = [0, 0, 1, 1, 0]. Note that the identity permutation would be\nencoded as a sequence of zeros.\n\nIn the context of discontinuous parsing and encoding pi, n can be seen as the input sentence w\nwhere pi(w) is encoded by sigma. The Lehmer code is particularly suitable for this task in terms\nof compression, as in most of the cases we expect (nearly) ordered permutations, which translates\ninto the majority of elements of sigma being zero. However, this encoding poses some potential\n\n3\n\n\fLabel Component\n\nTIGER Labels NEGRA DPTB\n\nni\nti\nui\npi as absolute-position\npi as relative-position\npi as Lehmer\npi as inverse Lehmer\npi as pointer-based\npi as pointer-based simplified\n\n22\n93\n15\n129\n105\n39\n68\n122\n81\n\n19\n56\n4\n110\n90\n34\n57\n99*\n65\n\n34\n137\n56\n98\n87\n27\n61\n110*\n83*\n\nTable 1: Number of values per label component, merging the training and dev sets (gold setup). *are\ncodes that generate one extra label with predicted PoS tags (this variability depends on the used\nPoS-tagger).\n\nHyperparameter\n\nValue\n\nBiLSTM size\n# BiLSTM layers\noptimizer\nloss\nlearning rate\ndecay (linear)\nmomentum\ndropout\nword embs\nPoS tags emb size\ncharacter emb size\nbatch size training\ntraining epochs\nbatch size test\n\n800\n2\nSGD\ncat. cross-entropy\n0.2\n0.05\n0.9\n0.5\nLing et al. (2015)\n20\n30\n8\n100\n128\n\nTable 2: Main hyper-parameters for the training of the BiLSTMs, both for the gold and predicted\nsetups\n\nlearnability problems. The root of the problem is that sigmai does not necessarily encode tau(i), but\ntau(j) where j is the index of the word that occupies the ith position in the continuous arrangement\n(i.e., j = tau 1(i)). In other words, this encoding is expressed following the order of words in the\ncontinuous arrangement rather than the input order, causing a non-straightforward mapping between\ninput words and labels. For instance, in the previous example, sigma2 does not encode the location of\nthe object n2 = 2 but that of n3 = 3.\n\nLehmer code of the inverse permutation To ensure that each pi encodes tau(i), we instead interpret\npi as meaning that should fill the (pi + 1)th currently remaining blank in a sequence sigma that is\ninitialized as a sequence of blanks, i.e. sigma = [,,...,] .F orinstance, letn = [0, 1, 2, 3, 4]be\nPointer-based encoding When encoding tau(i), the previous encodings generate the position for the\ntarget word, but they do not really take into account the left-to-right order in which sentences are\nnaturally read, nor they are linguistically inspired. In particular, informally speaking, in human lin-\n\nFinally, in Table 11 we list the number of parameters for each of the transducers trained on the pointer-\nbased encoding. For the rest of the encodings, the models have a similar number of parameters, as the\nonly change in the architecture is the small part involving the feed-forward output layer that predicts\nthe label component pi.\n\nMore in detail, for BiLSTMs and vanilla Trans-\n\nformers, the word embeddings are pre-trained FastText embeddings with 100 dimensions for English\nand 60 for German, and the PoS tags are represented by an embedding layer of 20 dimensions.\n\n4\n\n\fHyperparameter\n\nValue (gold setup)\n\nValue (pred setup)\n\n8\n6\n800\n0.4\nSGD\nCross-entropy\n0.003\n0.0\n0.0\n\n8\n6\n800\n0.4\nSGD\nCross-entropy\n0.004*\n0.0\n0.0\nPrevious Works\n20\n136/132batch size training\n\nAtt. heads\nAtt. layers\nHidden size\nHidden dropout\noptimizer\nloss\nlearning rate\ndecay (linear)\nmomentum\nword embs\nPoS tags emb size\ncharacter emb size\n8\n400\ntraining epochs\nbatch size test\n128\nTable 3: Main hyper-parameters for training the Transformer encoders\n\n400\n128\n\n20\n8\n\nModel\n\nParameters\n\nPointer-based BiLSTM\nPointer-based Transformer\nPointer-based DistilBERT\nPointer-based BERT base\nPointer-based BERT large\nTable 4: Number of parameters per model.\n\n13.9 M\n23.4 M\n73 M\n108 M\n330 M\n\nAdditionally we use a char-based LSTM with a hidden layer of 100/132 dimensions (English/German).\nFor both approaches, a linear layer followed by a softmax is used to predict every label component.\n\nFor BERT and DistilBERT we use the default fine-tuning parameters. We use Adam as optimizer and\ncross entropy as the loss function. The learning rate and other hyper-parameters are left as default\nin the transformers library, except for the number of training epochs (we train them for at most 30\nepochs), and the batch size, which is adjusted depending on the memory required by the model (e.g. 8\nfor BERT and 32 for DistilBERT). For the BERT-large model, due to the limitations in GPU memory,\nwe have to reduce the training batch size to 1, and use a smaller learning rate of 1e-5.",
  "methodology": "",
  "experiments": "Setup For English, we use the discontinuous Penn Treebank (DPTB) by. For German, we use TIGER\nand NEGRA. We use the splits by which in turn follow the splits for the NEGRA treebank, the splits\nfor TIGER, and the standard splits for (D)PTB (Sections 2 to 21 for training, 22 for development and\n23 for testing). See also Appendix A.5 for more detailed statistics. We consider gold and predicted\nPoS tags. For the latter, the parsers are trained on predicted PoS tags, which are generated by a\n2stacked BiLSTM, with the hyper-parameters used to train the parsers. The PoS tagging accuracy (\n\nMetrics We report the F-1 labeled bracketing score for all and discontinuous constituents, using\ndiscodop and the proper.prm parameter file. Model selection is based on overall bracketing F1score.\n\n5.1 Results\n\nTable 2 shows the results on the dev sets for all encodings and transducers. The tendency is clear\nshowing that the pointer-based encodings obtain the best results. The pointer-based encoding with\nsimplified PoS tags does not lead however to clear improvements, suggesting that the models can learn\nthe sparser original PoS tags set. For the rest of encodings we also observe interesting tendencies. For\ninstance, when running experiments using stacked BiLSTMs, the relative encoding performs better\n\n5\n\n\fthan the absolute one, which was somehow expected as the encoding is less sparse. However, the\ntendency is the opposite for the Transformer encoders (including BERT and DistilBERT), especially\nfor the case of discontinuous constituents. We hypothesize this is due to the capacity of Transformers\nto attend to every other word through multihead attention, which might give an advantage to encode\nabsolute positions over BiLSTMs, where the whole left and right context is represented by a single\nvector. With respect to the Lehmer and Lehmer of the inverse permutation encodings, the latter\nperforms better overall, confirming the bigger difficulties for the tested sequence labelers to learn\nLehmer, which in some cases has a performance even close to the naive absolute-positional encoding\n(e.g.\nfor TIGER using the vanilla Transformer encoder and BERT). As introduced in \u00a74, we\nhypothesize this is caused by the non-straightforward mapping between words and labels (in the\nLehmer code the label generated for a word does not necessarily contain information about the\nposition of such word in the continuous arrangement).\n\nIn Table 3 we compare a selection of our models against previous work using both gold and predicted\nPoS tags. In particular, we include: (i) models using the pointer-based encoding, since they obtained\nthe overall best performance on the dev sets, and (ii) a representative subset of encodings (the absolute\npositional one and the Lehmer code of the inverse permutation) trained with the best performing\ntransducer. Additionally, for the case of the (English) DPTB, we also include experiments using a\nbert-large model, to shed more light on whether the size of the networks is playing a role when it\ncomes to detect discontinuities. Additionally, we report speeds on CPU and GPU. The experiments\nshow that the encodings are learnable, but that the model\u2019s power makes a difference. For instance, in\nthe predicted setup BILSTMs and vanilla Transformers perform in line with predeep learning models\n, DistilBERT already achieves a robust performance, close to models such as and BERT transducers\nsuffice to achieve results close to some of the strongest approaches, e.g.. Yet, the results lag behind\nthe state of the art. With respect to the architectures that performed the best the main issue is that\nthey are the bottleneck of the pipeline. Thus, the computation of the contextualized word vectors\nunder current approaches greatly decreases the importance, when it comes to speed, of the chosen\nparsing paradigm used to generate the output trees (e.g. chart-based versus sequence labeling).\n\nFinally, Table 4 details the discontinuous performance of our best performing models.\n\nDiscussion on other applications It is worth noting that while we focused on parsing as sequence\nlabeling, encoding syntactic trees as labels is useful to straightforwardly feed syntactic information\nto downstream models, even if the trees themselves come from a non-sequence-labeling parser. For\nexample, use the sequence labeling encoding of to provide syntactic information to a semantic role\nlabeling model. Apart from providing fast and accurate parsers, our encodings can be used to do the\nsame with discontinuous syntax.",
  "results": "",
  "conclusion": "We reduced discontinuous parsing to sequence labeling. The key contribution consisted in predicting\na continuous tree with a rearrangement of the leaf nodes to shape discontinuities, and defining\nvarious ways to encode such a rearrangement as a sequence of labels associated to each word, taking\nadvantage of the fact that in practice they are nearly ordered permutations. We tested whether those\nencodings are learnable by neural models and saw that the choice of permutation encoding is not\ntrivial, and there are interactions between encodings\n\n6",
  "is_publishable": 1,
  "venue": NaN
}
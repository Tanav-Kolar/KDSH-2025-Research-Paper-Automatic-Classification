{
  "title": "Equivariant Adaptation of Large Pretrained Models",
  "abstract": "This paper explores the adaptation of video alignment to improve multi-step infer-\nence. Specifically, we first utilize VideoCLIP to generate video-script alignment\nfeatures. Afterwards, we ground the question-relevant content in instructional\nvideos. Then, we reweight the multimodal context to emphasize prominent features.\nFinally, we adopt GRU to conduct multi-step inference. Through comprehensive\nexperiments, we demonstrate the effectiveness and superiority of our method.",
  "introduction": "This paper addresses the critical task of assisting users in navigating unfamiliar events for specific\ndevices by providing step-by-step guidance using knowledge acquired from instructional videos.\nDue to the substantial disparity among specific tasks, the integration of multimodal input, and the\ncomplexity of multi-step inference, this is still a challenging task.\n\nSeveral studies have been proposed to address this task. For instance, one study proposes a Question-\nto-Actions (Q2A) Model, which employs vision transformer (ViT) and BERT to extract visual and\ntextual features, respectively. Moreover, attention mechanisms are leveraged to anchor question-\nrelevant information in instructional videos. Another study proposes a two-stage Function-centric\napproach, which segments both the script and video into function clips instead of sentences or\nframes. Additionally, they substitute BERT with XL-Net for text encoding. Despite the advancements\nachieved through these techniques, all of them adopt the unaligned pretrained encoders to extract\nvisual and textual features, leading to significant semantic gaps between modalities, thereby hindering\nbetter results.\n\nTo alleviate the negative effects of modalities unalignment, in this paper, we leverage pretrained\nvideo-text models to achieve instructional video-text alignment, facilitating a more robust grounding\nof question-relevant knowledge for multi-step inference. We build the pipeline with four steps:\nInstructional Video Alignment, Question-Aware Grounding, Multimodal Context Reweighting and\nMulti-Step Inference. Specifically, we employ pretrained VideoCLIP for generating video-script\nalignment features, which are beneficial to cross-modal grounding. Subsequently, we anchor the\nquestion-relevant content in instructional videos by the combination of hard and soft grounding.\nAfterwards, we leverage additive attention to adjust the weighting of the multimodal context to\nemphasize the salient features. Finally, we employ GRU for performing multi-step inference. We\nreduce the proportion of teacher forcing linearly to bridge the gap between training and inference,\nwhich boosts the multi-step inference.\n\n2 Problem Definition\n\nIn this section, we formulate the problem of AQTC.\n\nGiven an instructional video, which contains numerous frames and scripts, AI assistant extracts\nrelevant information from the video in accordance with the user\u02d82019s question q. Then, it deduces\nthe correct answer ai j based on the image U as perceived by the user, from the candidate answer set\nAnsi = ai 1, ai 2, ..., ai n in i-th step. Following previous work, we segment the video into several clips\nbased on scripts. Each clip illustrates one specific function of the device in video. We concatenate\n\n.\n\n\fthese clips to form the visual function sequence as [F v 2 , ..., F v 1 , F v m] and the textual function\nsequence as [F t 1, F t 2, ..., F t m], where F v i comprises all frames of the i-th function\u02d82019s clip,\nand F t i contains all script sentences of the i-th function\u02d82019s clip. To adapt AI assistant to the\nuser\u02d82019s view, following previous work, we mask the referenced button related to candidate answers\nin user images U , denoted as bk.\n\n3 Method\n\nIn this section, we will introduce the details of our method. Our method consists of four steps:\nInstructional Video Alignment, Question-Aware Grounding, Multimodal Context Reweighting and\nMulti-Step Inference.\n\n3.1\n\nInstructional Video Alignment\n\nTo align the videos and the text for better cross-modal understanding, we leverage pretrained Video-\nCLIP to generate the features of instructional videos. For the video part, we initially utilize pretrained\nS3D to generate an embedding for each second of the video, with a frame rate of 30 frames per second.\nNext, to represent each function within the videos, we utilize the pretrained visual transformer from\nVideoCLIP to process the embeddings generated by S3D in each function. Then, we apply average\npooling over the processed sequence of embeddings to form the video embedding Vi corresponding\nto a given visual function F v i . For the text part, we use the pretrained textual transformer of\nVideoCLIP to encode the scripts of a textual function F t i . Similarly, we employ average pooling to\naggregate the processed sequence of text, generating the text embedding Ti of a given textual function\nF t i . Finally, we obtain the video feature sequence [V1, V2, ..., Vm] and the text feature sequence\n[T1, T2, ..., Tm] of the given function sequence.\n\nBesides, we also utilize VideoCLIP to encode the questions q, the answer ai j and the masked button\nimage bk. We duplicate the images 30 times to ensure consistent video encoding. We get the question\nfeature Q, answer feature Ai j and visual button feature Bk.\n\n3.2 Question-Aware Grounding\n\nOwing to the extensive pretraining of VideoCLIP on a vast collection of videos, the features of videos\nand text are cross-modal aligned. Therefore, we can utilize the question Q to ground the video and\ntext feature sequence directly. Specifically, we leverage three grounding mechanisms: soft, hard and\ncombined grounding. Soft grounding employs attention to learn the similarity between the question\nfeature Q and the video feature sequence [V1, V2, ..., Vm] directly. And, it uses another attention\nnetwork to compute the similarity between the question feature Q and the text feature sequence [T1,\nT2, ..., Tm]. Soft grounding adopts the similarity from two attention networks to perform a weighted\naverage of the two feature sequences, respectively. Instead of relying on deep learning methods, hard\ngrounding follows previous work, which uses TF-IDF model to calculate the similarity between the\nquestion q and each textual function F t i from textual function sequence [F t 1, F t 2, ..., F t m].\nThen, it uses the similarities as the weights to compute the averages of the video feature sequence\n[V1, V2, ..., Vm] and the text feature sequence [T1, T2, ..., Tm], respectively. Besides, the combined\ngrounding utilizes soft grounding and hard grounding simultaneously. Then, the two features from\ntwo grounding methods are averaged. Ultimately, we obtain the aggregated question-aware video\nfeature V and text feature T .\n\n3.3 Multimodal Context Reweighting\n\nAfter obtaining multimodal question-aware context features from instructional videos, we need to\nmodel the answers to determine the correct one. Specifically, we utilize the gate network to fuse\nthe candidate answer feature Ai j with the corresponding button feature Bk, which generates the\nmultimodal answer feature \u02d802c6Ai j. We concatenate these multimodal contexts into a sequence [V,\nT, Q, \u02d802c6Ai j] for each candidate answer. Due to the varying importance of different context features\nin determining the correct answers, we utilize additive attention to reweight the multimodal context\nand get the fused feature. Finally, the fused feature is processed using a two-layer MLP to obtain the\ncandidate answer context feature C i j.\n\n2\n\n\f3.4 Multi-Step Inference\n\nOwing to the requirement for multi-step guidance in order to respond to the given questions, it is\nessential for models to perform multi-step inference. Following previous work, we utilize GRU to\ninfer the current correct answer by incorporating historical knowledge. Specifically, we feed the\nprevious hidden state H i\u02d822121 and the contextual features C i j of candidate answers in Ansi into\nthe GRU. Then, the resulting current hidden state H i j for each candidate answer in Ansi is utilized\nto predict the correct answer in the i-th step. We adopt a two-layer MLP and the softmax function\non the concatenated current hidden states [H i 1, H i 2, ..., H i n] to generate the probability of the\ncorrect answer. Cross entropy is used to compute the loss. While previous works utilize the state of\nthe ground truth as the historical state of the next step H i. This causes a huge gap between training\nand inference. To bridge this gap, we reduce the reliance on teacher forcing linearly. In other words,\nwe choose the hidden state of the most probable answer predicted by models as the historical state of\nthe next step H i, when a sample is selected for autoregressive training.",
  "related_work": "",
  "methodology": "",
  "experiments": "4.1 Dataset and Implementation Details\n\nWe use AssistQ train@22 and test@22 sets to train and validate. And we test our model on the\nAssistQ test@23 dataset.\nIn our experiments, we use Adam optimizer with a learning rate 10\u02d822124. The batch size is set to 16,\nthe maximum training epoch is 100, and we adopt early stopping. We randomly select 5\n\n4.2 Performance Evaluation\n\nWe present the performance evaluation on the test dataset in Table 1a. We find that our method\noutperforms baseline methods. This superiority can be attributed to our utilization of a video-text\naligned pretrained encoder for feature extraction. The aligned features are beneficial to multi-step\ninference. Furthermore, our method exhibits improved performance when the results are ensembled.\n\nTable 1: Performance evaluation and impact of pretrain features.\n\nMethods\n\nR@1 (%) R@3 (%)\n\nQ2A\nQuestion2Function\nOurs\nOurs (Ensemble)\n\n67.5\n62.6\n75.4\n78.4\n\n89.2\n87.5\n91.8\n93.8\n\nMethods\n\nR@1 (%) R@3 (%)\n\nViT+XL-Net\nVideoCLIP (Ours)\n\n86.6\n91.8\nTable 2: (b) Impact of pretrain features.\n\n63.9\n75.4\n\n4.3 Ablation Study\n\nPretrain Feature To validate the efficacy of video-text aligned features, we conduct the ablation\nstudy, which adopts ViT for processing the visual features and XL-Net for processing the text features.\nAs shown in Table 1b, we observe that the performance of method that uses the unaligned features\ndrops sharply.\n\nGrounding Methods To validate the effectiveness of various grounding methods, we use different\ngrounding techniques to train this model. The result is presented in Table 2. We find that the model\nachieves optimal performance when the text grounding leverages combined grounding and the video\ngrounding utilizes soft grounding.\n\n3\n\n\fText Grounding Video Grounding R@1 (%) R@3 (%)\n\nSoft\nHard\nSoft\nHard\n\nSoft\nSoft\nHard\nHard\n\n75.4\n75.1\n73.8\n71.8\n\n91.8\n89.2\n90.5\n89.8\n\nTable 3: Impact of grounding methods.\n\nMethods\n\nR@1 (%) R@3 (%)\n\nOurs\nw/o reweighting\nw/o SSL\n\n75.4\n72.1\n72.5\n\n91.8\n89.5\n92.1\n\nTable 4: (a) Impact of the reweighting mechanism and SSL.\n\nReweighting Mechanism We show the result of the model without attention reweighting in Table 3a.\nWe observe a considerable decrease in performance for the model lacking attention reweighting. This\nis because the attention reweighting can discern and prioritize the most informative features within\ncomplex multimodal contexts.\n\nMulti-Step Inference We evaluate different multi-step inference strategies, as demonstrated in Table\n3b. We find that the performance of TeacherForcing is inferior to that of the Linear Decay strategy,\nwhich is employed by our approach. This is because TeacherForcing widens the gap between training\nand inference. We also observe that Linear Decay outperforms AutoRegression. This is because\nteacher forcing is beneficial in preventing models from accumulating mistakes during the early stages\nof training.\n\nSSL The performance of the w/o SSL model exhibits a significant drop, as shown in Table 3a.",
  "results": "",
  "conclusion": "In this paper, we present a solution aimed at enhancing video alignment to achieve more effective\nmulti-step inference for the AQTC challenge. We leverage VideoCLIP to generate alignment features\nbetween videos and scripts. Subsequently, we identify and highlight question-relevant content\nwithin instructional videos. To further improve the overall context, we assign weights to emphasize\nprominent features. Lastly, we employ GRU for conducting multi-step inference. Besides, we conduct\nexhaustive experiments to validate the effectiveness of our method.\n\n4\n\n\fMethods\n\nR@1 (%) R@3 (%)\n\nLinear Decay (Ours)\nAutoRegression\nTeacherForcing\n\n75.4\n74.4\n74.1\n\n91.8\n91.1\n88.5\n\nTable 5: (b) Impact of multi-step inference strategies.\n\n5",
  "is_publishable": 1,
  "venue": NaN
}
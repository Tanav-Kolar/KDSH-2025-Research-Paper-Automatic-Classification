{
  "title": "Privacy Evaluation in Tabular Synthetic Data:\nCurrent Approaches and Future Directions",
  "abstract": "This paper examines the present methods for quantifying the level of privacy\nprotection offered by tabular synthetic data (SD). Currently, there is no standardized\napproach for measuring the degree of privacy protection these datasets offer. This\ndiscussion contributes to the development of SD privacy standards, encourages\ninterdisciplinary discourse, and aids SD researchers in making well-informed\nchoices concerning modeling and assessment.\n\n1\n\nIntroduction and Relation to Prior Research\n\nSynthetic data (SD) has emerged as a powerful tool for enhancing privacy, preserving the analytic\nutility of data while decoupling it from real individuals. However, the wide variety of SD generation\napproaches makes the degree of privacy protection they offer difficult to assess. Therefore, this paper\noutlines the typical technical assessment frameworks for individual privacy in SD sets. This increases\ninterdisciplinary awareness of privacy in SD and helps SD researchers make informed modeling and\nassessment choices.\n\nWhile several surveys mention privacy as a use case for SD, they do not cover its assessment in a\ndetailed way. In addition, reviews of privacy in AI fail to mention SD, and surveys, reviews, and\nexperimental comparisons of SD techniques often do not focus on privacy metrics. Furthermore,\nlegal analyses of SD are scarce and do not address quantitative methods for privacy assessment on a\ncase-by-case basis.\n\n2 Definitions and Notation\n\nTo the best of our knowledge, there is currently no widely accepted definition of SD. We present\nDefinition 2.1, which is consistent with the approach by Jordon et al.\n\nDefinition 2.1. (Synthetic data) Synthetic data (SD) are data generated through a purpose-built\nmathematical model or algorithm (the \"generator\"), intended to solve a set of data science tasks.\n\nWe let D denote a database describing data subjects with attributes A(D). Rows d \u2208 D are |A(D)|-\ntuples, with a value v(d, a) for each attribute a \u2208 A(D). An attribute a \u2208 A(D) is categorical if\nits domain is finite and numerical if its domain is a subset of R. We use the terms row and record\ninterchangeably. We denote by G a generator, and \u02c6D \u223c G(D) to represent a synthetic dataset \u02c6D\nobtained from generator G trained on D. Seed-based generators are a specific type of generators that\nproduce a unique synthetic record denoted by G(d) for every real record d. This is different from\nmost models (e.g., GANs, VAEs) which probabilistically represent overall dataset properties and\nproduce synthetic data by sampling from the obtained distribution.\n\n.\n\n\f3 Synthetic Data Privacy Risks\n\nThree significant risks identified in prior works serve as a basis for a proper anonymization. These\nare: singling out, linkability, and inference. Privacy risks in SD can occur due to various factors,\nwhich include:\n\n\u2022 Model and data properties: Improperly trained generators may overfit, memorizing and\nreproducing training data rather than inferring them stochastically. Records that emerge\nin isolation with little variability in their attribute values are difficult to generalize. As\nsuch, datasets containing outliers or sparse data are more at risk of memorization than more\nhomogeneous sets. Such datasets are also more susceptible to singling-out.\n\n\u2022 The approach to data synthesis: Most generators create stochastic models of datasets,\ncreating synthetic records via sampling. This detaches real data subjects from synthetic\nrecords. However, some methods create a single synthetic record for each real record. This\napproach poses greater risk as it retains the link between a subject and its data.\n\n\u2022 Mode collapse: GANs can focus on the minimal information necessary to deceive the\ndiscriminator, failing to capture the nuances and variations of the real data. In such cases,\nthe SD resembles a small selection of real data subjects well, but not the entire population.\nThis causes data clutter around specific real records, leaking their information.\n\n\u2022 The threat model: A threat model describes the information an adversary leverages besides\nthe SD. This can range from no access to the generator, to full knowledge including\nmodel parameters. Threat models also include scenarios where an adversary uses auxiliary\ninformation and can be:\n\n\u2013 No box: the adversary only has access to the SD.\n\u2013 Black box: the adversary also has limited generator access (no access to the model\n\nclass or parameters, but access to the model\u02d82019s input-output relation).\n\n\u2013 White box: the adversary has full generator access (model class and parameters).\n\u2013 Uncertain box: the adversary has stochastic model knowledge (model class and knowl-\n\nedge that parameters come from a given probability distribution).\n\n\u2013 Any of the aforementioned, along with auxiliary information, which is formalized in\n\nthe definition of auxiliary information in Definition 3.1.\n\nDefinition 3.1. Let D be a dataset with attributes A(D). An adversary has auxiliary information if\nthey know the values of a subset A\u2032 of attributes of some subset D\u2032 of records.\n\n4 Mathematical Privacy Properties\n\n4.1 Differential Privacy\n\nDifferential privacy (DP) is a property of information-releasing systems where data is not released\ndirectly, but via a processed version. The system is considered DP if the released information does\nnot significantly change when one record is removed from the dataset.\n\nDefinition 4.1. (Differential Privacy) A randomized algorithm M is (\u03f5, \u03b4)-differentially private\n((\u03f5, \u03b4)-DP) if, for all S \u2286 A(P ):\n\nP [M (D) \u2208 S] \u2264 e\u03f5 \u00b7 P [M (D\u2032) \u2208 S] + \u03b4,\nfor all databases D, D\u2032 such that \u2203d \u2208 D : D\u2032 = D \\ {d}. Generators are information-releasing\nsystems and can therefore be DP. Suppose there are two real datasets, D and D\u2032, with D\u2032 = D \\ {d}.\nA generator G is considered DP if a data controller with access to \u02c6D \u223c G cannot infer if G was\ntrained on D or D\u2032. Approaches to train generators with built-in mechanisms to guarantee DP can be\nfound in the literature. In this context, DP is a property of generators, not of the synthetic data they\nproduce.\n\n4.2 k-Anonymity\n\nPrivacy risks persist, even if identifying attributes are removed. Combinations of attribute values may\nstill be used to single out an individual. The notion of k-anonymity was introduced to address these\n\n2\n\n\frisks. A dataset is k-anonymous if at least k individuals share each combination of attribute values.\nFurther restrictions such as l-diversity, t-closeness, and (\u03b1, k)-anonymity have been introduced to\noffer additional protection.\n\nSynthetic data based on autoregressive models can implement k-anonymity directly into the generation\nprocess. For example, pruning in decision trees can guarantee that each combination of attribute\nvalues is sampled at least k times in mathematical expectation. Unlike DP, k-anonymity is a property\nof synthetic datasets, not the algorithms producing them.\n\n4.3 Plausible Deniability\n\nA degree of plausible deniability is inherent in synthetic datasets, as their records do not pertain to\nreal data subjects. Two approaches have emerged to formalize this notion, with one most relevant to\nseed-based synthetic data.\n\nDefinition 4.2. (Plausible deniability) Let D be a dataset and G be a generator that converts any record\nd \u2208 D into a corresponding synthetic record \u02c6d = G(d). For any dataset D where |D| > k, and any\nrecord \u02c6d such that \u02c6d = G(d1) for d1 \u2208 D, we say that \u02c6d is releasable with (k, \u03b3)-plausible deniability\nif there exist at least k \u2212 1 distinct records d2, ..., dk \u2208 D \\ {d1} such that for all i, j \u2208 {1, 2, ..., k}:\n\nP [d = G(di)] \u2248\u03b3 P [d = G(dj)]\n\nIn other words, a generator producing synthetic records from a seed has PD if, for each synthetic\nrecord produced from a particular seed, k other seeds could have resulted in roughly the same\n(quantified through \u03b3) synthetic record. Like DP, and unlike k-anonymity, PD is a property of\n(seed-based) generators, though it is related to both.\n\n5 Statistical Privacy Indicators\n\n5.1\n\nIdentical Records, Distances, and Nearest Neighbors\n\nMost indicators quantify the frequency of synthetic records being identical or suspiciously similar to\nreal records. Unlike DP and PD, these indicators measure properties of synthetic datasets, not their\ngenerators. The proportion of synthetic records that match real records is called the identical match\nshare (IMS). The IMS has been generalized to similarity metrics, and further to Nearest neighbor\n(NN)-based methods. These can be classified based on the following properties, summarized in Table\n3 of Appendix C:\n\n\u2022 Similarity metrics. Table 2 of Appendix C contains an overview of commonly invoked\n\nmeasures.\n\n\u2022 Metric evaluation. Because structured datasets can have a mix of different datatypes, metric\nevaluation is complex. Several approaches exist, such as binning numeric attributes; com-\nbining multiple metrics; ignoring specific attributes; or evaluating distances in embedding\nspaces.\n\n\u2022 Evaluated distances. For a given synthetic record \u02c6d \u2208 \u02c6D, we can find its closest real record\nd \u2208 D. The distance between these records is the synthetic to real distance (SRD) of \u02c6d, and\nis denoted as SRD( \u02c6d):\n\nSRD( \u02c6d) := min\nd\u2208D\n\nDist( \u02c6d, d)\n\n\u2200 \u02c6d \u2208 \u02c6D.\n\nSimilarly, the smallest synthetic-to-synthetic (SSD), real-to-synthetic (RSD), and real-to-real\ndistance (RRD) can be defined.\n\n\u2022 Use of holdout sets. To compute the RRD, the real data D can be partitioned into two subsets\nD1 and D2. For a real record d1 \u2208 D1, the RRD is the smallest distance to any record\nd2 \u2208 D2 :\n\nRRD(d1) := min\nd2\u2208D2\n\nDist(d1, d2)\n\n\u2200d1 \u2208 D1.\n\nThis provides a baseline for SD comparison.\n\n3\n\n\f\u2022 Statistics. The distance to the closest record (DCR) compares the SRD and RRD distributions.\nStatistical properties are expressed through the proportions of \"suspiciously close\" synthetic\nrecords. Measures used for this include medians, means, and standard deviations. Small\npercentiles are also often invoked when analyzing the distance distribution.\n\n5.2 Other Statistical Indicators\n\nThe targeted correct attribution probability (TCAP) is an indicator of parameter inference attack\nsuccess rates. It measures how often synthetic parameter values correspond to real values in l-diverse\nequivalence classes. Furthermore, there are several probabilistic techniques to quantify the risks by\nusing real hold-out sets as baselines. Maximum mean discrepancy (MMD) can be also used as a\nprivacy metric to test if the generator overfits.\n\n6 Computer Scientific Experimental Privacy Assessment\n\nComputer-scientific privacy assessment involves performing privacy attacks using synthetic data.\nThe effectiveness of these attacks is used to measure the degree of protection SD provides. Attack\nframeworks, as classified in Table 4 of Appendix D, are based on threat models and the following\nfactors:\n\n\u2022 Attack Frameworks. These include Vulnerable Record Discovery (VRD), which identifies\nsynthetic records that are the result of overfitting generators. Other frameworks include\nModel inversion, membership inference attacks (MIAs), and shadow modeling, which can\nall compromise confidentiality.\n\n\u2022 Attack Mechanisms. Nearest Neighbors (NN) is one such attack mechanism, where an\nadversary infers missing attribute values based on its k synthetic nearest neighbors. Machine\nlearning (ML) techniques are another approach, where classifiers are trained to re-identify\nreal data subjects. Additionally, information theory (IT) measures, such as Shannon entropy\nand mutual information, are sometimes used to identify records that may be more likely to\nbe memorized by the generator.\n\n\u2022 Baselines and Effectiveness Estimation. The efficacy of a model can be measured in a few\ndifferent ways. Absolute metrics include the probability with which records can be singled\nout, and the proportion of real records that can be re-identified. A random baseline approach\nuses random guessing to determine how effective an attack is. In a control baseline, the real\ndata is split into a training set and a control set. A model is trained on the training set, and\nthen the estimated success rate of attacks is compared on the training and control data sets.\nAnother approach involves the deliberate insertion of secrets in training data or in the SD\nafter generation.\n\n6.1 Relation to WP29 Attack Types\n\n\u2022 Singling out. VRD attacks directly implement singling-out attacks, identifying outlier SD\nrecords. MIAs can also model singling out, where an adversary quantifies the likelihood of\na unique real record\u2019s attribute combination.\n\n\u2022 Linkage. NN-based attacks usually require auxiliary information and can be interpreted as\nlinkage attacks. Anonymeter and information theory based VRD are the only methods that\nexplicitly model linkage attacks.\n\n\u2022 Inference. NN-based attacks and MIA can be seen as inference attacks.\n\n7 Discussion\n\n7.1 The Assessment Frameworks\n\nMathematical privacy properties, such as differential privacy (DP), do not offer a clear choice of the\nrequired parameters (\u03f5, \u03b4). Large parameter values offer weak privacy guarantees, and a given \u03f5 can\nresult in different degrees of protection depending on the application. DP may still be vulnerable to\nlinkage and inference attacks, giving a false sense of security, and is a property of generators and\n\n4\n\n\fnot their synthetic data. The difficulty with k-anonymity is that implementing it causes considerable\ninformation loss and is an NP-hard problem. Furthermore, k-anonymity was shown to offer sufficient\nprotection only when the utility of the data is completely removed. In addition, k-anonymity is a\nproperty of synthetic data, and not the methods to produce them. Plausible deniability (PD) is only\napplicable to seed-based methods. It shares properties with both DP and k-anonymity, making a\nrecord protected if it can be confused with other records.\n\nStatistical privacy indicators are difficult to interpret, with many options and decision points, such as\nthe choice of similarity metric. Statistical indicators measure properties of the synthetic data, and not\ntheir generators.\n\nComputer-scientific experiments allow for flexible modeling using various threat models, and can\ninclude properties of both synthetic data and their generators. However, they require more data and\ncomputation than mathematical properties.\n\n7.2 Relation to Synthetic Data Risks\n\nAll assessment frameworks address the issue of generator memorization. Mathematical properties\nfocus on the uniqueness of records. DP measures the impact of individual training records, with\noutliers having large impacts, and both k-anonymity and PD focus on limiting the uniqueness of\nrecords. Distance-based indicators are sensitive to outliers, because synthetic neighbors of outliers\nhave small SRDs, while the RRD of corresponding real outliers is large. Furthermore, some methods\nexplicitly search for outliers.\n\nThere are currently no studies that assess whether seed-based generators inherently pose greater risks\nthan other generators.\n\n7.3 Suggestions for Future Research\n\nFor the future research directions we identify are:\n\n\u2022 Standardizing privacy assessment: More interdisciplinary research is required to develop\nan inclusive understanding of synthetic data. Standards should be developed for research\nfindings to be more easily interpreted, and there should be a consensus formed over whether\nprivacy is a property of synthetic datasets, the generators, or both.\n\n\u2022 Synergies between assessments: A comparison between mathematical, statistical, and\nempirical approaches would be useful to evaluate their consistency, and to identify their\nindividual merits and weaknesses. Experiments should use open-source generators and\npublicly available datasets. It would also be useful to include information regarding the used\nmetrics, and the use of a holdout set, and the statistical interpretation of the results.\n\n\u2022 Outlier protection: Future research should investigate methods for outlier protection through\nbinning and aggregating attributes or using innovative techniques. It would also be beneficial\nto see how outlier detection can be used to guide vulnerable record discovery.\n\n\u2022 Incorporating privacy into generators: While DP is used in some generators, the same\nis not true for all privacy metrics and empirical privacy methods. Future research should\nfocus on incorporating these, by integrating metrics in loss functions, or by combinatorial\noptimization.\n\n\u2022 Assessment for advanced data formats: More work is needed to assess privacy in relational\ndatasets that have information contained in multiple, interconnected tables. In particular,\nprofiling attacks, which re-identify subjects based on behavioral patterns, may play a key\nrole in the assessment of relational databases.\n\n\u2022 Distribution-level confidentiality: There is a need for frameworks that assess the confiden-\n\ntiality of overall dataset properties.\n\nA A Proof of Theorem 2.1\n\nProof. Let x \u2208 Ai. Then, \u03c3i(x) = 0, and for all b \u2208 O where bi = 0, wb(x) = 0. Thus,\n\nF (x) =\n\n(cid:88)\n\nwb(x)Gb(x)\n\nb\u2208O,bi=1\n\n5\n\n\fIf bi = 1, then Gb(x) \u2208 Bi, and therefore F (x) is also in Bi due to the convexity of Bi.\n\nB B Example on Synthetic Datasets\n\nFigure 2 depicts an example of applying our safe predictor to a notional regression problem with 1-D\ninput and outputs, and one input-output constraint. The unconstrained network has a single hidden\nlayer of dimension 10 with ReLU activations, followed by a fully connected layer. The safe predictor\nshares this structure with constrained predictors, G0 and G1, but each predictor has its own fully\nconnected layer. The training uses a sampled subset of points from the input space and the learned\npredictors are shown for the continuous input space.\n\nFigure 3 shows an example of applying the safe predictor to a notional regression problem with a 2-D\ninput and 1-D output and two overlapping constraints. The unconstrained network has two hidden\nlayers of dimension 20 with ReLU activations, followed by a fully connected layer. The constrained\npredictors G00, G10, G01 and G11 share the hidden layers and have an additional hidden layer of size\n20 with ReLU followed by a fully connected layer. Again, training uses a sampled subset of points\nfrom the input space and the learned predictors are shown for the continuous input space.\n\nC C Details of VerticalCAS Experiment\n\nC.1 Safeability Constraints\n\nThe \u201csafeability\u201d property from previous work can be encoded into a set of input-output constraints.\nThe \"safeable region\" for a given advisory is the set of input space locations where that advisory can\nbe chosen, for which future advisories exist that will prevent a NMAC. If no future advisories exist,\nthe advisory is \"unsafeable\" and the corresponding input region is the \"unsafeable region\". Figure 5\nshows an example of these regions for the CL1500 advisory.\n\nThe constraints we enforce in our safe predictor are: x \u2208 Aunsafeable,i \u21d2 Fi(x) < maxj Fj(x),\n\u2200i. To make the output regions convex, we approximate by enforcing Fi(x) = minj Fj(x), for all\nx \u2208 Aunsafeable,i.\n\nC.2 Proximity Functions\n\nWe start by generating the unsafeable region bounds. Then, a distance function is computed between\npoints in the input space (vO \u2212 vI , h, \u03c4 ), and the unsafeable region for each advisory. These are not\ntrue distances, but are 0 if and only if the data point is within the unsafeable set. These are then used\nto produce proximity functions. Figure 5 shows examples of the unsafeable region, distance function,\nand proximity function for the CL1500 advisory.\n\nC.3 Structure of Predictors\n\nThe compressed policy tables for ACAS Xu and VerticalCAS use neural networks with six hidden\nlayers with a dimension of 45, and ReLU activation functions. We used the same architecture for the\nunconstrained network. For constrained predictors, we use a similar architecture, but share the first\nfour layers for all predictors. This provides a common learned representation of the input space, while\nallowing each predictor to adapt to its constraints. Each constrained predictor has two additional\nhidden layers and their outputs are projected onto our convex approximation of the safe output region,\nusing Gb(x) = minj Gj(x) \u2212 \u03f5. In our experiments, we used \u03f5 = 0.0001.\n\nWith this construction, we needed 30 separate predictors to enforce the VerticalCAS safeability\nconstraints. The number of nodes for the unconstrained and safe implementations were 270 and\n2880, respectively. Our safe predictor is smaller than the original look-up tables by several orders of\nmagnitude.\n\nC.4 Parameter Optimization\n\nWe use PyTorch for defining our networks and performing parameter optimization. We optimize both\nthe unconstrained network and our safe predictor using the asymmetric loss function, guiding the\n\n6\n\n\fnetwork to select optimal advisories while accurately predicting scores from the look-up tables. Each\ndataset is split using an 80/20 train/test split with a random seed of 0. The optimizer is ADAM, with\na learning rate of 0.0003, a batch size of 216, and training for 500 epochs.\n\n7",
  "introduction": "",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "",
  "is_publishable": 1,
  "venue": NaN
}
{
  "title": "End-to-End Neural Discourse Deixis Resolution in\nDialogue",
  "abstract": "We adapt a span-based entity coreference model to the task of end-to-end discourse\ndeixis resolution in dialogue, specifically by proposing extensions to their model\nthat exploit task-specific characteristics. The resulting model, dd-utt, achieves\nstate-of-the-art results on the four datasets.",
  "introduction": "Discourse deixis (DD) resolution, also known as abstract anaphora resolution, is an under-investigated\ntask that involves resolving a deictic anaphor to its antecedent. A deixis is a reference to a discourse\nentity such as a proposition, a description, an event, or a speech act. DD resolution is arguably\nmore challenging than the extensively-investigated entity coreference resolution task. Recall that in\nentity coreference, the goal is to cluster the entity mentions in narrative text or dialogue, which are\ncomposed of pronouns, names, and nominals, so that the mentions in each cluster refer to the same\nreal-world entity. Lexical overlap is a strong indicator of entity coreference, both among names (e.g.,\n\u201cPresident Biden\u201d, \u201cJoe Biden\u201d) and in the resolution of nominals (e.g., linking \u201cthe president\u201d to\n\u201cPresident Biden\u201d). DD resolution, on the other hand, can be viewed as a generalized case of event\ncoreference involving the clustering of deictic anaphors, which can be pronouns or nominals, and\nclauses, such that the mentions in each cluster refer to the same real-world proposition/event/speech\nact, etc. An example of DD resolution in which the deictic anaphor \u201cthe move\u201d refers to Salomon\u2019s\nact of issuing warrants on shares described in the preceding sentence. DD resolution is potentially\nmore challenging than entity coreference resolution because (1) DD resolution involves understanding\nclause semantics, which are arguably harder to encode than noun phrase semantics; and (2) string\nmatching plays little role in DD resolution, unlike in entity coreference.\n\nWe focus on end-to-end DD resolution in dialogue. While the deictic anaphors in dialogue are also\ncomposed of pronouns and nominals, the proportion of pronominal deictic anaphors in dialogue is\nmuch higher than that in narrative text. For instance, the percentage of pronominal deictic anaphors\nrises to 93\n\nSince DD resolution can be cast as a generalized case of event coreference, a natural question is:\nhow successful would a state-of-the-art entity coreference model be when applied to DD resolution?\nRecently, a re-implementation of a span-based entity coreference model has been applied to resolve\nthe deictic anaphors in the DD track after augmenting it with a type prediction model. Not only\ndid they achieve the highest score on each dataset, but they beat the second-best system, which is a\nnon-span-based neural approach combined with hand-crafted rules, by a large margin. These results\nsuggest that a span-based approach to DD resolution holds promise.\n\nOur contributions are three-fold. First, we investigate whether task-specific observations can be\nexploited to extend a span-based model originally developed for entity coreference to improve\nits performance for end-to-end DD resolution in dialogue. Second, our extensions are effective\nin improving model performance, allowing our model to achieve state-of-the-art results. Finally,\nwe present an empirical analysis of our model, which, to our knowledge, is the first analysis of a\nstate-of-the-art span-based DD resolver.\n\n\fTable 1: Statistics on the datasets.\n\nTotal\n#docs\n\nTotal\n#sents\n\nTotal\n#turns\n\nAvg.\n#sents\n\nAvg. #toks\nper sent\n\nAvg.\n#turns\n\nAvg. Avg.\n#ante\n#ana\n\nAvg.\n#speakers\nper doc\n\nARRAU train\nLIGHT dev\nLIGHT test\nAMI dev\nAMI test\nPers. dev\nPers. test\nSwbd. dev\nSwbd. test\n\n552\n20\n21\n7\n3\n21\n28\n11\n22\n\n22406\n908\n923\n4139\n1967\n812\n1139\n1342\n3652\n\n-\n280\n294\n2828\n1463\n431\n569\n715\n1996\n\n40.6\n45.4\n44.0\n591.3\n655.7\n38.7\n40.7\n122.0\n166.0\n\n15.5\n12.7\n12.8\n8.2\n9.3\n11.3\n11.1\n11.2\n9.6\n\n-\n14.0\n14.0\n404.0\n487.7\n20.5\n20.3\n65.0\n90.7\n\n2.9\n3.1\n3.8\n32.9\n39.3\n4.5\n4.4\n11.5\n12.0\n\n4.8\n4.2\n4.6\n42.0\n47.3\n4.5\n4.8\n15.9\n14.7\n\n-\n2.0\n2.0\n4.0\n4.0\n2.0\n2.0\n2.0\n2.0",
  "related_work": "Broadly, existing approaches to DD resolution can be divided into three categories, as described\nbelow.\n\n\u2022 Rule-based approaches. Early systems that resolve deictic expressions are rule-based.\nSpecifically, they use predefined rules to extract anaphoric mentions, and select antecedent\nfor each extracted anaphor based on the dialogue act types of each candidate antecedent.\n\u2022 Non-neural learning-based approaches. Early non-neural learning-based approaches to\nDD resolution use hand-crafted feature vectors to represent mentions. A classifier is then\ntrained to determine whether a pair of mentions is a valid antecedent-anaphor pair.\n\n\u2022 Deep learning-based approaches. Deep learning has been applied to DD resolution. For\ninstance, a Siamese neural network is used, which takes as input the embeddings of two\nsentences, one containing a deictic anaphor and the other a candidate antecedent, to score\neach candidate antecedent and subsequently rank the candidate antecedents based on these\nscores. In addition, motivated by the recent successes of Transformer-based approaches\nto entity coreference, a Transformer-based approach to DD resolution has recently been\nproposed, which is an end-to-end coreference system based on SpanBERT. Their model\njointly learns mention extraction and DD resolution and has achieved state-of-the-art results.\n\n3 Corpora\n\nWe use the DD-annotated corpora provided as part of the shared task. For training, we use the\nofficial training corpus from the shared task, ARRAU, which consists of three conversational sub-\ncorpora (TRAINS-93, TRAINS-91, PEAR) and two non-dialogue sub-corpora (GNOME, RST).\nFor validation and evaluation, we use the official development sets and test sets from the shared\ntask. The shared task corpus is composed of four well-known conversational datasets: AMI, LIGHT,\nPersuasion, and Switchboard. Statistics on these corpora are provided in Table 1.\n\n4 Baseline Systems\n\nWe employ three baseline systems.\n\nThe first baseline, coref-hoi, is a re-implementation of a widely-used end-to-end entity coreference\nmodel. The model ranks all text spans of up to a predefined length based on how likely they\ncorrespond to entity mentions. For each top-ranked span z, the model learns a distribution P (y) over\nits antecedents y \u2208 Y(z), where Y(z) includes a dummy antecedent \u03f5 and every preceding span:\n\nP (y) =\n\nes(z,y)\ny\u2032\u2208Y(z) es(z,y\u2032)\n\n(cid:80)\n\n(1)\n\nwhere s(x, y) is a pairwise score that incorporates two types of scores: (1) sm(\u00b7), which indicates\nhow likely a span is a mention, and (2) sc(\u00b7) and sa(\u00b7), which indicate how likely two spans refer to\n\n2\n\n\fthe same entity (sc(z, \u03f5) = sa(z, \u03f5) = 0 for dummy antecedents):\n\ns(z, y) = sm(x) + sm(y) + sc(z, y) + sa(z, y)\nsm(\u00b7) = FFNNm(gz)\n\nsc(z, y) = gT\nsa(z, y) = FFNNa([gx, gy, gx \u2299 gy, \u03d5(x, y)])\n\nx Wcgy\n\n(2)\n(3)\n\n(4)\n(5)\n\nwhere gx and gy are the vector representations of x and y, Wc is a learned weight matrix for bilinear\nscoring, FFNN(\u00b7) is a feedforward neural network, and \u03d5(\u00b7) encodes features. Two features are used,\none encoding speaker information and the other the segment distance between two spans.\n\nThe second baseline, UTD_NLP, is the top-performing system in the DD track of the shared task.\nIt extends coref-hoi with a set of modifications. Two of the most important modifications are:\n(1) the addition of a sentence distance feature to \u03d5(\u00b7), and (2) the incorporation into coref-hoi\na type prediction model, which predicts the type of a span. The possible types of a span i are:\nANTECEDENT (if i corresponds to an antecedent), ANAPHOR (if i corresponds to an anaphor),\nand NULL (if it is neither an antecedent nor an anaphor). The types predicted by the model are then\nused by coref-hoi as follows: only spans predicted as ANAPHOR can be resolved, and they can only\nbe resolved to spans predicted as ANTECEDENT.\n\nThe third baseline, coref-hoi-utt, is essentially the first baseline except that we restrict the candidate\nantecedents to be utterances. This restriction is motivated by the observation that the antecedents of\nthe deictic anaphors in the datasets are all utterances.\n\n5 Model\n\nNext, we describe our resolver, dd-utt, which augments coref-hoi-utt with 10 extensions.\n\nE1. Modeling recency. Unlike in entity coreference, where two coreferent names (e.g., \u201cJoe Biden\u201d,\n\u201cPresident Biden\u201d) can be far apart from each other in the corresponding document (because names\nare non-anaphoric), the distance between a deictic anaphor and its antecedent is comparatively smaller.\nTo model recency, we restrict the set of candidate antecedents of an anaphor to be the utterance\ncontaining the anaphor as well as the preceding 10 utterances, the choice of which is based on our\nobservation of the development data, where the 10 closest utterances already cover 96\u201399% of the\nantecedent-anaphor pairs.\n\nE2. Modeling distance. While the previous extension allows us to restrict our attention to candidate\nantecedents that are close to the anaphor, it does not model the fact that the likelihood of being\nthe correct antecedent tends to increase as its distance from the anaphor decreases. To model this\nrelationship, we subtract the term \u03b31Dist(x, y) from s(x, y) (see Equation (1)), where Dist(x, y) is\nthe utterance distance between anaphor x and candidate antecedent y and \u03b31 is a tunable parameter\nthat controls the importance of utterance distance in the resolution process. Since s(x, y) is used to\nrank candidate antecedents, modeling utterance distance by updating s(x, y) will allow distance to\nhave a direct impact on DD resolution.\n\nE3. Modeling candidate antecedent length. Some utterances are pragmatic in nature and do not\nconvey any important information. Therefore, they cannot serve as antecedents of deictic anaphors.\nExamples include \u201cUmm\u201d, \u201cAhhhh... okay\u201d, \u201cthat\u2019s right\u201d, and \u201cI agree\u201d. Ideally, the model can\nidentify such utterances and prevent them from being selected as antecedents. We hypothesize that\nwe could help the model by modeling such utterances. To do so, we observe that such utterances\ntend to be short and model them by penalizing shorter utterances. Specifically, we subtract the term\nLength(y) from s(x, y), where Length(y) is the number of words in candidate antecedent y and \u03b32\n\u03b32\nis a tunable parameter that controls the importance of candidate antecedent length in resolution.\n\n1\n\nE4. Extracting candidate anaphors. As mentioned before, the deictic anaphors in dialogue are\nlargely composed of pronouns. Specifically, in our development sets, the three pronouns \u201cthat\u201d,\n\u201cthis\u201d, and \u2018it\u2019 alone account for 74\u201388% of the anaphors. Consequently, we extract candidate deictic\nanaphors as follows: instead of allowing each span of length n or less to be a candidate anaphor, we\nonly allow a span to be a candidate anaphor if its underlying word/phrase has appeared at least once\nin the training set as a deictic anaphor.\n\n3\n\n\fE5. Predicting anaphors. Now that we have the candidate anaphors, our next extension involves\npredicting which of them are indeed deictic anaphors. To do so, we retrain the type prediction model\nin UTD_NLP, which is a FFNN that takes as input the (contextualized) span representation gi of\ncandidate anaphor i and outputs a vector oti of dimension 2 in which the first element denotes the\nlikelihood that i is a deictic anaphor and the second element denotes the likelihood that i is not a\ndeictic anaphor. i is predicted as a deictic anaphor if and only if the value of the first element of oti is\nbigger than its second value:\n\noti = FFNN(gi)\nti = arg max\n\nx\u2208{A,N A}\n\noti(x)\n\n(6)\n(7)\n\nwhere A (ANAPHOR) and NA (NON-ANAPHOR) are the two possible types. Following UTD_NLP,\nthis type prediction model is jointly trained with the resolution model. Specifically, we compute the\ncross-entropy loss using oti, multiply it by a type loss coefficient \u03bb, and add it to the loss function of\ncoref-hoi-utt. \u03bb is a tunable parameter that controls the importance of type prediction relative to DD\nresolution.\n\nE6. Modeling the relationship between anaphor recognition and resolution. In principle, the\nmodel should resolve a candidate anaphor to a non-dummy candidate antecedent if it is predicted\nto be a deictic anaphor by the type prediction model. However, type prediction is not perfect, and\nenforcing this consistency constraint, which we will refer to as C1, will allow errors in type prediction\nto be propagated to DD resolution. For example, if a non-deictic anaphor is misclassified by the type\nprediction model, then it will be (incorrectly) resolved to a non-dummy antecedent. To alleviate\nerror propagation, we instead enforce C1 in a soft manner. To do so, we define a penalty function p1,\nwhich imposes a penalty on span i if C1 is violated (i.e., a deictic anaphor is resolved to the dummy\nantecedent), as shown below:\n\np1(i) =\n\n(cid:26)0\n\nif arg maxy\u2208Y s(i, y) = \u03f5 and ti = NA\n\noti(A) \u2212 oti(N A) otherwise\n\n(8)\n\nIntuitively, p1 estimates the minimum amount to be adjusted so that span i\u2019s type is not ANAPHOR.\nWe incorporate pi into the model as a penalty term in s (Equation (1)). Specifically, we redefine\ns(i, \u03f5) as shown below:\n\ns(i, \u03f5) = s(i, \u03f5) \u2212 [\u03b33p1(i)]\n(9)\nwhere \u03b33 is a positive constant that controls the hardness of C1. The smaller \u03b33 is, the softer C1 is.\nIntuitively, if C1 is violated, s(i, \u03f5) will be lowered by the penalty term, and the dummy antecedent\nwill less likely be selected as the antecedent of i.\n\nE7. Modeling the relationship between non-anaphor recognition and resolution. Another\nconsistency constraint that should be enforced is that the model should resolve a candidate anaphor\nto the dummy antecedent if it is predicted as a non-deictic anaphor by the type prediction model. As\nin Extension E6, we will enforce this constraint, which we will refer to as C2, in a soft manner by\ndefining a penalty function p2, as shown below:\n\np2(i) =\n\n(cid:26)oti(N A) \u2212 oti(A)\n\n0\n\nif arg maxy\u2208Y s(i, y) \u0338= \u03f5 and ti = NA\notherwise\n\n(10)\n\nThen we redefine s(i, j) when j \u0338= \u03f5 as follows:\n\ns(i, j) = s(i, j) \u2212 [\u03b34p2(i)]\n(11)\nwhere \u03b34 is a positive constant that controls the hardness of C2. Intuitively, if C2 is violated, s(i, j)\nwill be lowered by the penalty term, and j will less likely be selected as the antecedent of i.\n\nE8. Encoding candidate anaphor context. Examining Equation (1), we see that s(x, y) is computed\nbased on the span representations of x and y. While these span representations are contextualized,\nthe contextual information they encode is arguably limited. As noted before, most of the deictic\nanaphors in dialogue are pronouns, which are semantically empty. As a result, we hypothesize that\nwe could improve the resolution of these deictic anaphors if we explicitly modeled their contexts.\nSpecifically, we represent the context of a candidate anaphor using the embedding of the utterance in\nwhich it appears and add the resulting embedding as features to both the bilinear score sc(x, y) and\nthe concatenation-based score sa(x, y):\nsc(x, y) = gT\nsa(x, y) = FFNNa([gx, gy, gx \u2299 gy, gs, \u03d5(x, y)])\n\nx Wcgy + gT\n\n(12)\n(13)\n\ns Wagy\n\n4\n\n\fTable 2: Lists of filtered words.\n\nFilling words\n\nyeah, okay, ok, uh, right, so, hmm, well, um, oh, mm,\nyep, hi, ah, whoops, alright, shhhh, yes, ay, hello,\naww, alas, ye, aye, uh-huh, huh, wow, www, no, and,\nbut, again, wonderful, exactly, absolutely, actually, sure\nthanks, awesome, gosh, ooops\n\nReporting verbs\n\ncommand, mention, demand, request, reveal, believe,\nguarantee, guess, insist, complain, doubt, estimate,\nwarn, learn, realise, persuade, propose, announce,\nadvise, imagine, boast, suggest, remember, claim,\ndescribe, see, understand, discover, answer, wonder,\nrecommend, beg, prefer, suppose, comment, think,\nargue, consider, swear, ask, agree, explain, report,\nknow, tell, decide, discuss, repeat, invite, reply,\nexpect, forget, add, fear, hope, say, feel, observe,\nremark, confirm, threaten, teach, forbid, admit,\npromise, deny, state, mean, instruct\n\nwhere Wc and Wa are learned weight matrices, gs is the embedding of the utterance s in which\ncandidate anaphor x appears, and \u03d5(x, y) encodes the relationship between x and y as features.\n\nE9. Encoding the relationship between candidate anaphors and antecedents. As noted in\nExtension E8, \u03d5(x, y) encodes the relationship between candidate anaphor x and candidate antecedent\ny. In UTD_NLP, \u03d5(x, y) is composed of three features, including two features from coref-hoi-utt\n(i.e., the speaker id and the segment distance between x and y) and one feature that encodes the\nutterance distance between them. Similar to the previous extension, we hypothesize that we could\nbetter encode the relationship between x and y using additional features. Specifically, we incorporate\nan additional feature into \u03d5(x, y) that encodes the utterance distance between x and y. Unlike the one\nused in UTD_NLP, this feature aims to more accurately capture proximity by ignoring unimportant\nsentences (i.e., those that contain only interjections, filling words, reporting verbs, and punctuation)\nwhen computing utterance distance. The complete list of filling words and reporting verbs that we\nfilter can be found in Table 2.\n\nE10. Encoding candidate antecedents. In coref-hoi-utt, a candidate antecedent is simply encoded\nusing its span representation. We hypothesize that we could better encode a candidate antecedent\nusing additional features. Specifically, we employ seven features to encode a candidate antecedent y\nand incorporate them into \u03d5(x, y): (1) the number of words in y; (2) the number of nouns in y; (3)\nthe number of verbs in y; (4) the number of adjectives in y; (5) the number of content word overlaps\nbetween y and the portion of the utterance containing the anaphor that precedes the anaphor; (6)\nwhether y is the longest among the candidate antecedents; and (7) whether y has the largest number of\ncontent word overlap (as computed in Feature #5) among the candidate antecedents. Like Extension\nE3, some features implicitly encode the length of a candidate antecedent. Despite this redundancy,\nwe believe the redundant information could be exploited by the model differently and may therefore\nhave varying degrees of impact on it.\n\n6 Evaluation\n\n6.1 Experimental Setup\n\nEvaluation metrics. We obtain the results of DD resolution using the Universal Anaphora Scorer.\nSince DD resolution is viewed as a generalized case of event coreference, the scorer reports perfor-\nmance in terms of CoNLL score, which is the unweighted average of the F-scores of three coreference\nscoring metrics, namely MUC, B3, and CEAFe. In addition, we report the results of deictic anaphor\nrecognition. We express recognition results in terms of Precision (P), Recall (R) and F-score, con-\n\n5\n\n\fTable 3: Resolution and recognition results on the four test sets.\n\nLIGHT AMI\n\nResolution\nPers.\n\nSwbd. Avg. LIGHT AMI\n\nRecognition\nPers.\n\nUTD_NLP\ncoref-hoi\ncoref-hoi-utt\ndd-utt\n\n42.7\n42.7\n42.3\n48.2\n\n35.4\n30.7\n35.0\n43.5\n\n39.6\n49.7\n53.3\n54.9\n\n35.4\n35.4\n34.1\n47.2\n\n38.3\n39.6\n41.2\n48.5\n\n70.1\n70.9\n70.3\n71.3\n\n61.0\n49.3\n52.4\n56.9\n\n69.9\n67.8\n71.0\n71.4\n\nSwbd. Avg.\n\n68.1\n61.9\n60.6\n65.2\n\n67.3\n62.5\n63.6\n66.2\n\nTable 4: Parameter values enabling dd-utt to achieve the best CoNLL score on each development set.\n\nLIGHT AMI\n\nPers.\n\nSwbd.\n\nType loss coef. \u03bb\n\u03b31\n\u03b32\n\u03b33\n\u03b34\n\n800\n1\n1\n5\n5\n\n800\n1\n1\n10\n5\n\n800\n1\n1\n10\n5\n\n800\n1\n1\n5\n5\n\nsidering an anaphor correctly recognized if it has an exact match with a gold anaphor in terms of\nboundary.\n\nModel training and parameter tuning. For coref-hoi and coref-hoi-utt, we use SpanBERTLarge as\nthe encoder and reuse the hyperparameters with the only exception of the maximum span width: for\ncoref-hoi, we increase the maximum span width from 30 to 45 in order to cover more than 97% of\nthe antecedent spans; coref-hoi-utt we use 15 as the maximum span width, which covers more than\n99% of the anaphor spans in the training sets. For UTD_NLP, we simply take the outputs produced\nby the model on the test sets and report the results obtained by running the scorer on the outputs. For\ndd-utt, we use SpanBERTLarge as the encoder. Since we do not rely on span enumerate to generate\ncandidate spans, the maximum span width can be set to any arbitrary number that is large enough\nto cover all candidate antecedents and anaphors. In our case, we use 300 as our maximum span\nwidth. We tune the parameters (i.e., \u03bb, \u03b31, \u03b32, \u03b33, \u03b34) using grid search to maximize CoNLL score on\ndevelopment data. For the type loss coefficient, we search out of {0.2, 0.5, 1, 200, 500, 800, 1200,\n1600}, and for \u03b3, we search out of {1, 5, 10}.\nAll models are trained for 30 epochs with a dropout rate of 0.3 and early stopping. We use 1 \u00d7 10\u22125\nas our BERT learning rate and 3 \u00d7 10\u22124 as our task learning rate. Each experiment is run using a\nrandom seed of 11 and takes less than three hours to train on an NVIDIA RTX A6000 48GB.\n\nTrain-dev partition. Since we have four test sets, we use ARRAU and all dev sets other than\nthe one to be evaluated on for model training and the remaining dev set for parameter tuning. For\nexample, when evaluating on AMItest, we train models on ARRAU, LIGHTdev, Persuasiondev and\nSwitchboarddev and use AMIdev for tuning.\n\n6.2 Results\n\nRecall that our goal is to perform end-to-end DD resolution, which corresponds to the Predicted\nevaluation setting in the shared task.\n\nOverall performance. Recognition results (expressed in F-score) and resolution results (expressed\nin CoNLL score) of the three baselines and our model on the four test sets are shown in Table 3,\nwhere the Avg. columns report the macro-averages of the corresponding results on the four test\nsets, and the parameter settings that enable our model to achieve the highest CoNLL scores on the\ndevelopment sets are shown in Table 4. Since coref-hoi and coref-hoi-utt do not explicitly identify\ndeictic anaphors, we assume that all but the first mentions in each output cluster are anaphors when\ncomputing recognition precision; and while UTD_NLP (the top-performing system in the shared\ntask) does recognize anaphors, we still make the same assumption when computing its recognition\nprecision since the anaphors are not explicitly marked in the output (recall that we computed results\nof UTD_NLP based on its outputs).\n\n6\n\n\fWe test the statistical significance among the four models using two-tailed Approximate Random-\nization. For recognition, the models are statistically indistinguishable from each other w.r.t. their\nAvg. score (p < 0.05). For resolution, dd-utt is highly significantly better than the baselines w.r.t.\nAvg. (p < 0.001), while the three baselines are statistically indistinguishable from each other. These\nresults suggest that (1) dd-utt\u2019s superior resolution performance stems from better antecedent selec-\ntion, not better anaphor recognition; and (2) the restriction of candidate antecedents to utterances in\ncoref-hoi-utt does not enable the resolver to yield significantly better resolution results than coref-hoi.\n\nPer-anaphor results. Next, we show the recognition and resolution results of the four models on the\nmost frequently occurring deictic anaphors in Table 5 after micro-averaging them over the four test\nsets. Not surprisingly, \u201cthat\u201d is the most frequent deictic anaphor on the test sets, appearing as an\nanaphor 402 times on the test sets and contributing to 68.8% of the anaphors. This is followed by \u201cit\u201d\n(16.3%) and \u201cthis\u201d (4.3%). Only 8.9% of the anaphors are not among the top four anaphors.\n\nConsider first the recognition results. As can be seen, \u201cthat\u201d has the highest recognition F-score\namong the top anaphors. This is perhaps not surprising given the comparatively larger number of\n\u201cthat\u201d examples the models are trained on. While \u201cit\u201d occurs more frequently than \u201cthis\u201d as a deictic\nanaphor, its recognition performance is lower than that of \u201cthis\u201d. This is not surprising either: \u201cthis\u201d,\nwhen used as a pronoun, is more likely to be deictic than \u201cit\u201d, although both of them can serve as\na coreference anaphor and a bridging anaphor. In other words, it is comparatively more difficult to\ndetermine whether a particular occurrence of \u201cit\u201d is deictic. Overall, UTD_NLP recognizes more\nanaphors than the other models.\n\nNext, consider the resolution results. To obtain the CoNLL scores for a given anaphor, we retain all\nand only those clusters containing the anaphor in both the gold partition and the system partition and\napply the official scorer to them. Generally, the more frequently occurring an anaphor is, the better\nits resolution performance is. Interestingly, for the \u201cOthers\u201d category, dd-utt achieves the highest\nresolution results despite having the lowest recognition performance. In contrast, while UTD_NLP\nachieves the best recognition performance on average, its resolution results are among the worst.\n\nResults of the four resolvers (UTD_NLP, coref-hoi, coref-hoi-utt, and dd-utt) on the CODI-CRAC\n2021 shared task test sets in terms of MUC, B3, and CEAFe scores are reported in Table. Their\nmention extraction results in terms of recall (R), precision (P), and F-score (F) are provided in Table.\n\ndd-utt achieves the best CoNLL scores on all four datasets, via achieving the best MUC, B3, and\nCEAFe F-scores. In terms of MUC F-score, the performance difference between dd-utt and the\nsecond best resolver on each dataset is substantial (2.2%-14.9% points). These results suggest that\nbetter link identification, which is what the MUC F- score reveals, is the primary reason for the\nsuperior performance of dd-utt. Moreover, Persuasion appears to be the easiest of the four datasets,\nas this is the dataset on which three of the four resolvers achieved the highest CoNLL scores. Note\nthat Persuasion is also the dataset on which the differences in CoNLL score between dd-utt and the\nother resolvers are the smallest. These results seem to suggest that the performance gap between\ndd-utt and the other resolvers tends to widen as the difficulty of a dataset increases.\n\nIn terms of anaphor extraction results in Table, dd-utt lags behind UTD_NLP on two datasets, AMI\nand Switchboard, in terms of F-score. Nevertheless, the anaphor extraction precision achieved by\ndd-utt is often one of the highest in each dataset.\n\n7 Further Analysis\n\nAn example is analyzed. In this example, dd-utt successfully extracts the anaphor \"that\" and resolves\nit to the correct antecedent, \"Losing one decimal place, that is okay\". UTD_NLP fails to extract \"that\"\nas a deictic anaphor. While coref-hoi correctly extracts the anaphor, it incorrectly selects \"You want\nyour rating to be a two?\" as the antecedent. From a cursory look at this example, one could infer that\nthis candidate antecedent is highly unlikely to be the correct antecedent since it is 10 utterances away\nfrom the anaphor. As for coref-hoi-utt, the resolver successfully extracts the anaphor but incorrectly\nselects \"Its just two point five for that one\" as the antecedent, which, like the antecedent chosen by\ncoref-hoi, is farther away from the anaphor than the correct antecedent. Coref-hoi and coref-hoi-utt\nfail to identify the correct antecedent because they do not explicitly model distance and therefore may\nnot have an idea about how far a candidate antecedent is from the anaphor under consideration. The\n\n7\n\n\fTable 5: Resolution results on the test sets.\n\nMUC\n\nP\n\nR\n\nF\n\nP\n\n44.6\n37.2\n36.5\n52.4\n\n45.5\n21.7\n25.5\n41.2\n\n45.5\n48.6\n50.0\n56.7\n\n35.2\n31.5\n30.6\n46.3\n\n31.3\n36.3\n37.6\n41.3\n\n21.2\n30.5\n33.1\n39.8\n\n20.3\n42.3\n49.6\n48.0\n\n21.3\n30.4\n29.3\n43.4\n\n36.8\n36.7\n37.6\n46.2\n\n28.9\n25.4\n28.8\n40.5\n\n28.1\n45.2\n49.8\n52.0\n\n26.5\n31.0\n29.9\n44.8\n\n56.2\n48.9\n46.7\n62.0\n\n52.4\n28.7\n34.6\n48.9\n\n65.0\n57.5\n56.8\n63.8\n\n52.3\n40.9\n39.5\n54.9\n\nB3\n\nR\n\n37.0\n42.0\n42.3\n41.6\n\n29.5\n36.3\n39.0\n42.8\n\n30.2\n45.9\n51.7\n49.9\n\n30.4\n34.0\n32.7\n44.5\n\nCEAFe\n\nCoNLL\n\nF\n\nP\n\nR\n\nF\n\n44.6\n45.2\n44.4\n49.8\n\n37.8\n32.1\n36.7\n45.6\n\n41.2\n51.1\n54.1\n56.0\n\n38.5\n37.1\n35.8\n49.2\n\n55.3\n58.2\n55.3\n69.0\n\n44.9\n39.0\n43.4\n54.4\n\n61.0\n66.2\n64.4\n72.1\n\n50.5\n51.4\n49.5\n63.4\n\n40.5\n38.5\n38.0\n37.6\n\n35.1\n31.0\n36.1\n37.5\n\n41.8\n44.0\n49.4\n46.9\n\n34.9\n30.2\n29.2\n38.3\n\n46.7\n46.3\n45.0\n48.7\n\n39.4\n34.6\n39.4\n44.4\n\n49.6\n52.9\n55.9\n56.8\n\n41.3\n38.0\n36.7\n47.7\n\n42.7\n42.7\n42.3\n48.2\n\n35.4\n30.7\n35.0\n43.5\n\n39.6\n49.7\n53.3\n54.9\n\n35.4\n35.4\n34.1\n47.2\n\nLIGHT\nUTD_NLP\ncoref-hoi\ncoref-hoi-utt\ndd-utt\n\nAMI\nUTD_NLP\ncoref-hoi\ncoref-hoi-utt\ndd-utt\n\nPersuasion\nUTD_NLP\ncoref-hoi\ncoref-hoi-utt\ndd-utt\n\nSwitchboard\nUTD_NLP\ncoref-hoi\ncoref-hoi-utt\ndd-utt\n\nadditional features that dd-utt has access to, including those that encode sentence distance as well as\nthose that capture contextual information, may have helped dd-utt choose the correct antecedent.\n\nA: You want your rating to be a two?\nA: Is that what you\u2019re saying?\nB: Yeah, I just got it the other way.\nB: Uh in Yep, I just got\nA: Okay.\nA: So, I\u2019ll work out the average for that again at the end.\nA: It\u2019s very slightly altered. Okay, and we\u2019re just waiting for your rating.\nB: two point five\nC: Its just two point five for that one.\nA: Two point five, okay.\nD: Yeah.\nA: Losing one decimal place, that is okay.\n\n8 Error Analysis\n\nDD anaphora recognition precision errors. A common type of recognition precision errors involves\nmisclassifying a coreference anaphor as a deictic anaphor. Consider the first example in Figure 2, in\nwhich the pronoun \"that\" is a coreference anaphor with \"voice recognition\" as its antecedent but is\nmisclassified as a deictic anaphor with the whole sentence as its antecedent. This type of error occurs\nbecause virtually all of the frequently occurring deictic anaphors, including \"that\", \"it\", \"this\", and\n\"which\", appear as a coreference anaphor in some contexts and as a deictic anaphor in other contexts,\nand distinguishing between the two different uses of these anaphors could be challenging.\nDD anaphor recognition recall errors. Consider the second example in Figure 2, in which \"it\" is a\ndeictic anaphor that refers to the boldfaced utterance, but dd-utt fails to identify this and many other\noccurrences of \"it\" as deictic, probably because \"it\" is more likely to be a coreference anaphor than a\ndeictic anaphor: in the dev sets, 80% of the occurrences of \"it\" are coreference anaphors while only\n5% are deictic anaphors.\nDD resolution precision errors. A major source of DD resolution precision errors can be attributed\n\n8\n\n\fTable 6: Mention extraction results on the test sets.\n\nLIGHT\n\nAMI\n\nPersuasion\n\nP\n\nR\n\nF\n\nP\n\nR\n\nF\n\nP\n\nR\n\nF\n\nOverall\nUTD_NLP\ncoref-hoi\ncoref-hoi-utt\ndd-utt\n\nAnaphor\nUTD_NLP\ncoref-hoi\ncoref-hoi-utt\ndd-utt\n\nAntecedent\nUTD_NLP\ncoref-hoi\ncoref-hoi-utt\ndd-utt\n\n65.2\n62.9\n59.3\n72.6\n\n71.4\n71.8\n68.2\n81.0\n\n50.8\n52.7\n49.4\n63.9\n\n46.9\n49.5\n50.0\n46.9\n\n68.8\n70.0\n72.5\n63.8\n\n27.7\n34.8\n33.9\n34.8\n\n54.6\n55.4\n54.2\n57.0\n\n70.1\n70.9\n70.3\n71.3\n\n35.8\n41.9\n40.2\n45.1\n\n60.2\n40.5\n43.9\n57.8\n\n58.0\n42.2\n46.4\n57.9\n\n66.0\n38.3\n41.0\n57.7\n\n39.1\n42.7\n45.2\n46.6\n\n64.4\n59.3\n60.2\n55.9\n\n20.5\n30.4\n34.2\n39.8\n\n47.4\n41.5\n44.5\n51.6\n\n61.0\n49.3\n52.4\n56.9\n\n31.3\n33.9\n37.3\n47.1\n\n72.3\n68.6\n66.2\n73.9\n\n76.7\n72.9\n71.3\n77.9\n\n59.6\n63.9\n60.7\n69.5\n\n41.6\n52.0\n57.6\n54.7\n\n64.2\n63.4\n70.7\n65.9\n\n21.2\n42.5\n46.6\n45.2\n\n52.8\n59.2\n61.6\n62.8\n\n69.9\n67.8\n71.0\n71.4\n\n31.3\n51.0\n52.7\n54.8\n\nSwitchboard\n\nP\n\nR\n\nF\n\nOverall\nUTD_NLP\ncoref-hoi\ncoref-hoi-utt\ndd-utt\n\nAnaphor\nUTD_NLP\ncoref-hoi\ncoref-hoi-utt\ndd-utt\n\nAntecedent\nUTD_NLP\ncoref-hoi\ncoref-hoi-utt\ndd-utt\n\n64.4\n55.3\n53.3\n66.9\n\n65.7\n63.0\n61.9\n67.5\n\n60.8\n46.3\n43.3\n66.2\n\n42.2\n41.2\n39.6\n49.6\n\n70.7\n60.8\n59.3\n63.1\n\n21.5\n27.2\n25.5\n40.0\n\n51.0\n47.2\n45.5\n57.0\n\n68.1\n61.9\n60.6\n65.2\n\n31.7\n34.3\n32.1\n49.8\n\nto the model\u2019s failure in properly understanding the context in which a deictic anaphor appears.\nConsider the third example in Figure 2, in which \"that\" is a deictic anaphor that refers to the boldfaced\nutterance. While dd-utt correctly identifies \"that\" as a deictic anaphor, it erroneously posits the\nitalicized utterance as its antecedent. This example is interesting in that without looking at the\nboldfaced utterance, the italicized utterance is a plausible antecedent for \"that\" because \"I am not\nsurprised to hear that at all\" can be used as a response to almost every statement. However, when\nboth the boldfaced utterance and the italicized utterance are taken into consideration, it is clear that\nthe boldfaced utterance is the correct antecedent for \"that\" because winning over seven awards for\nsome charitable work is certainly more surprising than seeing a place bring awareness to the needs of\nthe young. Correctly resolving this anaphor, however, requires modeling the emotional implication of\nits context.\nA: The design should minimize R_S_I and be easy to locate and we were still slightly ambivalent as\nto whether to use voice recognition there, though that did seem to be the favored strategy, but there\nwas also, on the sideline, the thought of maybe having a beeper function.\nA: Sounds like a blessed organization.\nB: Yes, it does.\nA: Did you know they\u2019ve won over 7 different awards for their charitable work?\n\n9\n\n\fA: As a former foster kid, it makes me happy to see this place bring such awareness to the issues and\nneeds of our young.\nB: I am not surprised to hear that at all.",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "An end-to-end discourse deixis resolution model that augments Lee et al.\u2019s (2018) span-based entity\ncoreference model with 10 extensions is presented. The resulting model achieved state-of- the-art\nresults on the CODI-CRAC 2021 datasets.\n\n10",
  "is_publishable": 1,
  "venue": NaN
}
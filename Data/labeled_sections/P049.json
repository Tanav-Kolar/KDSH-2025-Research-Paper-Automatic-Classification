{
  "title": "Improving Model Generalization Using a Single Data\nSample for Semantic Adaptation",
  "abstract": "The limited capacity of deep networks to generalize beyond their training dis-\ntribution presents a significant challenge in semantic segmentation. Traditional\napproaches have operated under the assumption of a fixed model post-training,\nwith parameters remaining constant during testing. This research introduces a\nself-adaptive methodology for semantic segmentation that modifies the inference\nmechanism to accommodate each input sample individually. This adaptation in-\nvolves two principal operations. First, it refines the parameters of convolutional\nlayers based on the input image, employing a consistency-based regularization.\nSecond, it modifies the Batch Normalization layers by dynamically blending the\ntraining distribution with a reference distribution extracted from a single test sam-\nple. Although these techniques are individually recognized in the field, their\ncombined application establishes new benchmarks in accuracy for generalization\nfrom synthetic to real-world data. The empirical evidence from this study indicates\nthat self-adaptation can effectively enhance deep network generalization to out-\nof-domain data, serving as a valuable complement to the established methods of\nmodel regularization during training.",
  "introduction": "State-of-the-art models in semantic segmentation exhibit a notable deficiency in robustness when\nconfronted with out-of-distribution data, where the distributions of training and testing sets diverge.\nWhile numerous studies have examined this challenge, with a predominant focus on image classifica-\ntion, it has been observed that Empirical Risk Minimization (ERM), which presumes independent and\nidentically distributed training and testing samples, remains remarkably competitive. This contrasts\nwith the evident advancements in domain adaptation for both image classification and semantic\nsegmentation. The domain adaptation setup, however, typically requires access to an unlabeled\ntest distribution during training. In the generalization scenario considered here, only a single test\nsample is accessible during inference, and no information sharing must occur between subsequent\ntest samples.\n\nThis study investigates the generalization challenge in semantic segmentation, specifically from\nsynthetic data to real-world scenarios, by employing an adaptive approach. Unlike prior research\nthat has concentrated on modifying model architecture or training procedures, this work revises the\nstandard inference procedure using a technique derived from domain adaptation methods. Termed\nself-adaptation, this technique utilizes a self-supervised loss function to facilitate adaptation to\nindividual test samples through a limited number of parameter updates. In addition to these loss-based\nupdates, self-adaptation incorporates feature statistics from the training data with those of the test\nsample within the Batch Normalization layers.",
  "related_work": "This research contributes to the ongoing investigation into the generalization capabilities of semantic\nsegmentation models and is related to explorations of feature normalization and online learning.\n\n.\n\n\fIn contrast to previous studies that focused on training strategies and model design, this study\nspecifically examines the inference process during test time. Prior research has attempted to improve\ngeneralization by augmenting synthetic training data with styles transferred from real images, or\nby utilizing a classification model trained on real images to ensure feature proximity between\nmodels via distillation, often seeking layer-specific learning rates. Some approaches have added\ninstance normalization (IN) layers heuristically to the network. Recent studies have sought to extract\ndomain-invariant feature statistics through instance-selective whitening loss or frequency-based\ndomain randomization. Others have aimed to learn style-invariant representations using causal\nframeworks or have augmented single-domain data to simulate a multi-source scenario to increase\nsource domain diversity. Some techniques involve swapping channel-wise statistics in feature\nnormalization layers and learning adapter functions to adjust the mean and variance based on the\ninput. Another method enforces consistency of output logits across multiple images of the same class.\nTo improve generalization in federated learning, researchers have explored training clients locally\nwith sharpness-aware minimization and averaging stochastic weights. However, these methods either\nassume access to a distribution of real images during training or require modifications to the network\narchitecture. The technique presented in this work does not require either, making it applicable\npost-hoc to already trained models to improve their generalization.\n\nBatch Normalization (BN) and other normalization techniques have been increasingly associated\nwith model robustness. The most common methods, including BN, Layer Normalization (LN), and\nInstance Normalization (IN), also impact the model\u2019s expressive capacity, which can be further\nimproved by combining these techniques within a single architecture. In domain adaptation, some\nstudies use source-domain statistics during training and replace them with target-domain statistics\nduring inference. Recent work has explored combining source and target statistics during inference,\nweighted by the number of samples they aggregate. Others propose using batch statistics from the\ntarget domain during inference instead of training statistics from the source domain. This study\ncomplements these findings by demonstrating improved generalization of semantic segmentation\nmodels.\n\nSeveral previous studies have updated model parameters during inference, particularly in object\ntracking where the object detector must adapt to the changing appearance of the tracked instance.\nConditional generative models have been employed to learn from single image samples for super-\nresolution and scene synthesis. Recently, this principle has been extended to improve the robustness\nof image classification models, though the self-supervised tasks developed for image classification do\nnot always extend well to dense prediction tasks like semantic segmentation. Recent research has\nproposed more suitable alternatives for self-supervised loss in domain adaptation, and several works\nhave developed domain-specific approaches for medical imaging or first-person vision.\n\nMost of the related works focus on domain adaptation in image classification, typically assuming\naccess to multiple samples from the target distribution during training. This work addresses semantic\nsegmentation in the domain generalization setting, requiring only a single datum from the test set. In\nthis context, simple objectives like entropy minimization improve baseline accuracy only moderately.\nIn contrast, the self-adaptation method presented here, which uses pseudo-labels to account for\nprediction uncertainty, proves significantly more effective. The task is distinct from few-shot learning,\nwhere the model may adapt during testing using a small annotated set of samples. Here, no such\nannotation is available; the model adjusts to the test sample in an unsupervised manner, without\nrequiring proxy tasks or prior knowledge of the test distribution.",
  "methodology": "In traditional inference, the parameters of the segmentation model are assumed to remain fixed. In\ncontrast, adaptive systems are capable of learning to specialize to their environment. Analogously,\nthis study allows the segmentation model to update its parameters during inference. It is important to\nnote that this setup differs from domain adaptation scenarios, as the updated parameters are discarded\nafter processing each sample, aligning with the principles of domain generalization.\n\nThe proposed approach creates mini-batches of images for each test sample using data augmentation.\nStarting with the original test image, a set of N augmented images is generated through multi-scaling,\nhorizontal flipping, and grayscaling. These images form a mini-batch that is processed by the CNN.\nThe resulting softmax probabilities are transformed back to the original pixel space using inverse\n\n2\n\n\faffine transformations, producing multiple predictions for each pixel. The mean of these probabilities\nis computed along the mini-batch dimension for each class and pixel on the spatial grid.\n\nA threshold value is computed from the maximum probability of every class to create a class-\ndependent threshold. For each pixel, the class with the highest probability is extracted. Low-\nconfidence predictions are ignored by setting pixels with a softmax probability below the threshold to\nan ignore label, while the remaining pixels use the dominant class as the pseudo-label. This pseudo\nground truth is used to fine-tune the model for a set number of iterations using gradient descent with\nthe cross-entropy loss. After this self-adaptation process, a single final prediction is produced using\nthe updated model weights. The weights are then reset to their initial values before processing the\nnext test sample, ensuring that the model does not accumulate knowledge about the entire target data\ndistribution.\n\nBatch Normalization (BN) has become an integral part of modern CNNs. Although originally\ndesigned to improve training convergence, it is now recognized for its role in model robustness,\nincluding domain generalization. During training, BN computes the mean and standard deviation\nacross the batch and spatial dimensions. The normalized features are derived using these statistics.\nAt test time, it is common practice to normalize feature values with running estimates of the mean\nand standard deviation across training batches, rather than using test-batch statistics. This is referred\nto as train BN (t-BN).\n\nIn the context of out-of-distribution generalization, the running statistics derived from the source data\ncan differ substantially from those computed using target images, a problem known as covariate shift.\nDomain adaptation methods often mitigate this issue by replacing source running statistics with those\nof the target, a technique known as Adaptive Batch Normalization (AdaBN). Recent studies have\nalso explored prediction-time BN (p-BN), which uses the statistics of the current test batch instead of\nrunning statistics from training.\n\nThis study assumes the availability of only a single target sample during inference. Alternatives like\nAdaBN and p-BN are not directly applicable in this scenario. Instance Normalization (IN) layers\ncould replace BN layers, but this might lead to covariate shift issues, as sample statistics may only\napproximate the complete test distribution. Additionally, such a replacement could interfere with the\nstatistics of activations in intermediate layers.\n\nSelf-adaptive normalization (SaN) is proposed as a solution. It combines the inductive bias from\nthe source domain\u2019s running statistics with statistics extracted from a single test instance. The\nsource mean and variance are averaged with sample statistics from the target domain, weighted by\na parameter 1. This parameter represents the shift from the source domain ( 1 = 0) to a reference\ndomain ( 1 = 1). During inference, new mean and variance are computed using this weighted average,\nand these are used to normalize the features of the single test sample. This approach does not affect\nthe behavior of BN layers during training and applies only during testing.",
  "experiments": "In this study, the evaluation protocol is revised to adhere to principles of robustness and generalization.\nThe supplier has access to two data distributions: the source data for model training and a validation\nset for model validation. The generalization ability of the model is assessed on three distinct target\nsets, providing an estimate of the expected model accuracy for out-of-distribution deployment. The\ndatasets used are restricted to traffic scenes for compatibility with previous research.\n\nSource data for model training comes from two synthetic datasets, GTA and SYNTHIA, which offer\nlow-cost ground truth annotation and exhibit visual discrepancies with real imagery. The validation set\nused is WildDash, which is understood to be of limited quantity but bears a closer visual resemblance\nto potential target domains. The model is evaluated on three target domains: Cityscapes, BDD, and\nIDD, chosen for their geographic diversity and differences in data acquisition. The average accuracy\nacross these target domains estimates the expected model accuracy. Additionally, the Mapillary\ndataset is used for comparison with previous works, although it does not disclose the geographic\norigins of individual samples.\n\nThe framework is implemented in PyTorch, and the baseline model is DeepLabv1 without CRF post-\nprocessing. The models are trained on the source domains for 50 epochs using an SGD optimizer with\na learning rate of 0.005, decayed polynomially. Data augmentation techniques include random-size\n\n3\n\n\fcrops, random aspect ratio adjustments, random horizontal flipping, color jitter, random blur, and\ngrayscaling.\n\nExperiments were conducted to investigate the influence of the parameter 1 in Self-adaptive Nor-\nmalization (SaN) on segmentation accuracy and the Intersection over Union (IoU) for both source\ndomains (GTA, SYNTHIA) and all main target domains (Cityscapes, BDD, IDD). The optimal 1 was\ndetermined based on the IoU on the WildDash validation set. The segmentation accuracy with this\noptimal 1 was reported, showing that SaN improves the mean IoU over both the established t-BN\nbaseline and the more recent p-BN. The improvement was consistent across different backbone archi-\ntectures and target domains. Additionally, model calibration, measured by the expected calibration\nerror (ECE), was found to improve with SaN, which was competitive with the MC-Dropout method\nand showed complementary effects when used jointly.\n\nSelf-adaptation was compared to Test-Time Augmentation (TTA), which involves augmenting test\nsamples with flipped and grayscaled versions at multiple scales and averaging the predictions. Self-\nadaptation outperformed TTA by a clear margin, aligning with reported ECE scores and demonstrating\nthat self-adaptation effectively uses calibrated confidence to generate reliable pseudo-labels.\n\nSelf-adaptation was compared with state-of-the-art domain generalization methods, showing consis-\ntent improvements over carefully tuned baselines, regardless of backbone architecture or source data.\nThe method outperformed previous methods without modifying the model architecture or training\nprocess, altering only the inference procedure.\n\nA comparison with Tent, which also updates model parameters at test time but minimizes entropy\ninstead of using pseudo-labels, showed that self-adaptation outperformed Tent substantially. This\nwas demonstrated by training HRNet-W18 on GTA and comparing the IoU on Cityscapes, where\nself-adaptation achieved a 7.5% improvement in IoU.\n\nThe influence of the number of iterations for self-adaptation was investigated, showing that self-\nadaptation balances accuracy and inference time by adjusting iteration numbers and layer choices.\nIt was found to be more efficient and accurate than model ensembles. Self-adaptation can trade off\naccuracy vs. runtime by using fewer update iterations or updating fewer upper network layers.\n\nHyperparameter sensitivity analysis revealed that self-adaptation is robust to the choice of hyperpa-\nrameters 1, 8, and 7. The optimal values were determined using the validation set, and the model\naccuracy declined moderately with deviations from these values. Qualitative results showed that\nself-adaptation improves segmentation quality and reduces pathological failure modes.\n\nThe integration of self-adaptation with state-of-the-art architectures like DeepLabv3+, HRNet-W18,\nHRNet-W48, and UPerNet with a Swin-T backbone demonstrated substantial improvements in\nsegmentation accuracy across all target domains. Evaluation on the ACDC dataset, which includes\nadverse weather conditions, showed that self-adaptation outperformed the baseline by 13.57% on\naverage.\n\nAdditional qualitative results and failure cases were discussed, showing that self-adaptation can\nstruggle with cases of mislabeling regions with incorrect but semantically related classes. However,\nthese failure cases were relatively rare, and the majority of image samples benefited from self-\nadaptation, with accuracy improvements of up to 35% IoU compared to the baseline.",
  "results": "The empirical results demonstrate that self-adaptive normalization (SaN) consistently enhances\nsegmentation accuracy in out-of-distribution scenarios. For instance, when training on the GTA\ndataset and testing on Cityscapes, BDD, and IDD, SaN improved the mean IoU by 4.1% with ResNet-\n50 and 5.1% with ResNet-101 compared to the t-BN baseline. Furthermore, SaN outperformed the\nmore recent p-BN method, showing improvements irrespective of the backbone architecture and\nthe target domain tested. In terms of calibration quality, measured by the expected calibration error\n(ECE), SaN not only improved the baseline but also showed competitiveness with the MC-Dropout\nmethod, even exhibiting complementary effects when both methods were combined.\n\nSelf-adaptation was found to outperform traditional Test-Time Augmentation (TTA) across both\nsource domains (GTA, SYNTHIA) and three target domains (Cityscapes, BDD, IDD). Despite TTA\nimproving the baseline, self-adaptation provided a clear and consistent margin of 2.19% IoU on\n\n4\n\n\faverage. This aligns with the reported ECE scores, demonstrating that self-adaptation effectively\nexploits the calibrated confidence of predictions to yield reliable pseudo-labels.\n\nIn comparison to state-of-the-art domain generalization methods, self-adaptation showed substantial\nimprovements even over carefully tuned baselines. It outperformed methods like DRPC and FSDR\non most benchmarks, despite these methods using individual models for each target domain and\nresorting to target domains for hyperparameter tuning. Self-adaptation achieved superior segmentation\naccuracy without requiring access to a distribution of real images for training or modifying the model\narchitecture, unlike previous methods such as ASG, CSG, DRPC, and IBN-Net.\n\nThe study also compared self-adaptation with Tent, which updates model parameters at test time\nby minimizing entropy. Self-adaptation, which constructs pseudo-labels based on well-calibrated\npredictions, substantially outperformed Tent. Specifically, when training HRNet-W18 on GTA and\nevaluating on Cityscapes, self-adaptation achieved a 7.5% improvement in IoU compared to Tent\nunder a comparable computational budget.\n\nFurther analysis revealed that self-adaptation provides a flexible mechanism for trading off accuracy\nand runtime by varying the number of update iterations and the layers to adjust. It was found to be\nmore efficient and accurate than model ensembles. Hyperparameter sensitivity analysis indicated that\nself-adaptation is robust to the choice of hyperparameters, with optimal values determined using the\nvalidation set.\n\nQualitative results demonstrated that self-adaptation visibly improves segmentation quality, reducing\nartifacts and mislabeling compared to the baseline. The method\u2019s effectiveness was consistent across\ndifferent architectures, including DeepLabv3+, HRNet-W18, HRNet-W48, and UPerNet with a\nSwin-T backbone, showing substantial improvements in segmentation accuracy on all target domains.",
  "conclusion": "The traditional learning principle of Empirical Risk Minimization (ERM) assumes independent and\nidentically distributed training and testing data, which often results in models that are not robust to\ndomain shifts. To address this, a self-adaptive inference process was introduced, bypassing the need\nfor explicit assumptions about the test distribution. This study also outlined four principles for a\nrigorous evaluation process in domain generalization, adhering to best practices in machine learning\nresearch.\n\nThe analysis demonstrated that even a single sample from the test domain can significantly improve\nmodel predictions. The self-adaptive approach showed substantial accuracy improvements without\naltering the training process or model architecture, unlike previous works. These results suggest\nthat self-adaptive techniques could be valuable in other application domains, such as panoptic\nsegmentation or monocular depth prediction.\n\nWhile the presented self-adaptation method is not yet real-time, it offers a favorable trade-off between\naccuracy and computational cost compared to model ensembles. Future research could explore\nreducing the latency of self-adaptive inference through adaptive step sizes, higher-order optimization,\nor low-precision computations. Overall, this work demonstrates the potential of self-adaptation to\nenhance model generalization and robustness in various applications.\n\n5",
  "is_publishable": 1,
  "venue": NaN
}
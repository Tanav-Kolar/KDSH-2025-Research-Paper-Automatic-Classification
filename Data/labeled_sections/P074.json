{
  "title": "Agriculture-Vision Challenge 2022 \u2013 The Runner-Up\nSolution for Agricultural Pattern Recognition via\nTransformer-based Models",
  "abstract": "This paper explores the adaptation The Agriculture-Vision Challenge is one of\nthe most famous and competitive challenges for global researchers to break the\nboundary between computer vision and agriculture sectors, aiming at agricultural\npattern recognition from aerial images. In this paper, we propose our solution to\nthe third Agriculture-Vision Challenge. We leverage a data pre-processing scheme\nand several Transformer-based models as well as data augmentation techniques to\nachieve a mIoU of 0.582, accomplishing the 2nd place in this challenge.",
  "introduction": "This paper addresses the critical Computer vision applications in agricultural domain has become\none of hot topics nowadays, especially using remote sensing satellite images and aerial images. With\nthe rapid development of deep learning methods, numerous research studies have proposed pioneer\nand practical solutions to various computer vision problems in agriculture. Aside from fruitful\nresearch achievements, various algorithm challenges have been held at top-tier conferences for\nglobal researchers in recent years, in order to explore more effective algorithms to solve the specific\nproblems. The Agriculture-Vision Challenge is one of most famous and competitive challenges in\nthis inter-disciplinarity study. It aims at applying computer vision algorithms to agricultural pattern\nrecognition from high-resolution aerial images. This year, holds the 3rd Agriculture-Vision Challenge,\nand we form our team to participate in this contest.",
  "related_work": "This section reviews",
  "methodology": "This section details of In this section, we elaborate on the given datasets, the pre-processing method,\nthe proposed deep learning-based framework, and the test-time augmentation (TTA) strategy.\n\n3.1 Description of Dataset\n\nThe challenge this year provides the entire Agriculture-Vision dataset. It contains 94,986 aerial\nfarmland images collected throughout 2019 across the U.S. Each image has a size of 512\u00d7512 pixels\nand has 4 channels (RGB and NIR). A total of 9 label classes are manually labeled for every image.\nTable 1 shows the given amount of images in each class. Note that many images have multiple labels,\nand even have overlapped labels (one pixel has multiple labels).\n\nAlthough the amount of the given training data is considerable, we still generate more data following\nthe data augmentation scheme of the winner solution last year. They conducted an image mosaic\n\n.\n\n\fscheme to enable the model to have multi-scale views during the training. To fit the model input\nsize, we create two new datasets using mosaicked images with down-sampling 2X (2 times) and\ndown-sampling 3X. The down-sampling dataset has the same image size of 512\u00d7512 pixels that the\nrecognition model can share the same network architecture among 1X, 2X, and 3X imagery.\n\n3.2 Data Pre-Processing\n\nWe observe that the image counts in each category are uneven. For example, the image count of the\nbackground class is 25 times larger than the water class. To tackle the unbalance issue, we try to\nsample more images in the few-shot classes. The re-sampled image counts are listed in Table 1.\n\nTable 1: Information of the given and resampled datasets for training and validation categories.\n\nClass Index\n\nClass Name\n\nOriginal Amount (Train/Val) Resampled Amount (Train/Val)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\nBackground\nDouble Plant\nDrydown\nEndrow\nNutrient Deficiency\nPlanter Skip\nWater\nWaterway\nWeed Cluster\n\n56944 / 18334\n6234 / 2322\n16806 / 5800\n4481 / 1755\n13308 / 3883\n2599 / 1197\n2155 / 987\n3899 / 696\n11111 / 2834\n\n75121 / 13642\n10961 / 2294\n19320 / 3383\n8544 / 1858\n14859 / 2610\n5361 / 1015\n4132 / 721\n6024 / 1109\n14423 / 2773\n\n3.3 Framework\n\nFig. 1 shows our deep learning-based framework. SegFormer is a Transformer-based efficient\nsegmentation model. It designs a hierarchical Transformer encoder with multi-level feature outputs.\nUnlike other cumbersome decoders, SegFormer\u2019s decoder adopts MLP layers to aggregate multi-scale\nfeature outputs from different layers. One of the key advantages of SegFormer is that its model size\nis relatively small but the performance keeps outstanding. Therefore, SegFormer is suitable for this\nchallenge due to the model size parameter limit of 150M.\n\nSegFormer provides six versions with various settings of Transformer encoders, leading to different\nmodel sizes. These six models are named from B0 to B5, with the increased model size. To follow\nthe policy, we select Mix Transformer (MiT) B3 and Mix Transformer B2 as our training models.\nTheir model size information can be found in Table 7 \u201cMix Transformer Encoder\u201d. After obtaining\nthe individual inference result from each model, the model ensemble is performed to predict the final\nsegmentation results.\n\n3.4 Test-Time Augmentation\n\nSince our models are trained with 1X, 2X, and 3X down-sampling imagery, we conduct the same\nprocessing on the test dataset. In addition to the scale augmentation, we include image rotation and\nflip.",
  "experiments": "",
  "results": "This section presents the results\n\n4.1 Evaluation Metric\n\nThe required evaluation metric is the average Intersection over Union metric (mIoU), which is defined\nas Eq. 1 to measure the performance.\n\nmIoU =\n\n1\nc\n\nc\n(cid:88)\n\ni=1\n\nArea(Pc \u2229 Tc)\nArea(Pc \u222a Tc)\n\n2\n\n(1)\n\n\fwhere c is the number of label classes (8 foreground classes + 1 background class for this challenge);\nPc and Tc are the predicted label mask and ground truth label mask of the class c, respectively.\n\n4.2 Experiment Results\n\nTable 2 presents our results, the baseline provided by the host Agriculture-Vision organizers, and\nthe results of other methods. Note that other baselines evaluate their performance on the validation\nset due to the unavailable test set. As we can see, while our single model baselines are competitive\nwith other baselines, our proposed method effectively improves the single model performance. Even\nthough some single models have peak performance in some classes (0.778 for \u201cBackground\u201d and\n0.782 for \u201cWater\u201d), our model ensemble enjoys the merits of multiple single models\u2019 strength to\nachieve the mIoU of 0.582. It also shows that our ensemble results significantly outperform other\nbaselines and our implementation of various single models.\n\nTable 2: Performance comparisons among various models. The bold font of numeric results indicates\nthe best performance on the test set. BG: Background; DP: Double Plant; D: Drydown; E: Endrow;\nND: Nutrient Deficiency; PS: Planter Skip; W: Water; WW: Waterway; WC: Weed Cluster. The\nnumber in the parentheses following the class name refers to the class index.\n\nModels\n\nmIoU BG(0) DP(1) D(2)\n\nE(3) ND(4)\n\nPS(5) W(6) WW(7) WC(8)\n\nAgriculture-Vision baseline(RGBN)\nMiT-B3(RGBN)\nMiT-B5(RGB)\nMiT-B5(RGBN)\n\nHRNet-W48+OCR(RGB baseline)\nMiT-B3(RGB baseline)\nMiT-B2(RGBN+Our method)\nMiT-B3(RGBN+Our method)\nModel Ensemble(RGBN+Our method)",
  "conclusion": "0.285\n0.371\n0.370\n0.373\n\n0.574\n0.609\n0.585\n0.618\n\n(Other methods, on the val set)\n0.743\n0.768\n0.755\n0.762\n\n0.217\n0.245\n0.227\n0.246\n(Our implementation, on the test set)\n0.233\n0.325\n0.476\n0.452\n0.481\n\n0.567\n0.557\n0.632\n0.640\n0.646\n\n0.316\n0.395\n0.483\n0.471\n0.485\n\n0.717\n0.720\n0.778\n0.773\n0.777\n\n0.434\n0.454\n0.464\n0.490\n\n0.413\n0.448\n0.554\n0.563\n0.582\n\n0.389\n0.424\n0.313\n0.428\n\n0.269\n0.364\n0.570\n0.569\n0.573\n\n0.336\n0.413\n0.414\n0.420\n\n0.283\n0.330\n0.403\n0.442\n0.471\n\n0.736\n0.692\n0.802\n0.813\n\n0.718\n0.687\n0.768\n0.782\n0.779\n\n0.344\n0.269\n0.401\n0.437\n\n0.289\n0.293\n0.410\n0.463\n0.547\n\n0.283\n0.299\n0.304\n0.318\n\n0.326\n0.358\n0.466\n0.475\n0.479\n\nThis paper presents a novel method In this paper, we propose our solution to the 3rd Agriculture-\nVision Challenge. For data usage, we perform data pre-processing and test data augmentation schemes.\nSeveral SegFormer models are leveraged. We finally accomplish a mIoU of 0.582, achieving the 2nd\nplace in this challenge.\n\nFuture Directions. The potential applications of our proposed algorithm include crop type identifica-\ntion in precision agriculture, agricultural asset estimation and agricultural insurance product design\nin the Environmental, Social, and Governance (ESG) domain. These future directions can illuminate\nthe revitalization of rural areas and facilitate the service of inclusive finance in an eco-friendly way.\n\n3",
  "is_publishable": 1,
  "venue": NaN
}
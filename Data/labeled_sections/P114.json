{
  "title": "An Empathetic AI Painter: A System for\nComputational Creativity Through Embodied\nConversational Interaction",
  "abstract": "This paper presents an investigation into the computational modeling of the creative\nprocess of a portrait artist, focusing on the incorporation of human traits like per-\nsonality and emotions into the artistic process. The system includes an empathetic\nconversational component to discern the dominant personality traits of the user,\nand this information is then utilized by a generative AI portraiture module to create\na personalized stylization of the user\u2019s portrait. The paper details the system and\nthe outcomes of real-time interactions from a demonstration session.",
  "introduction": "The incorporation of human traits in the creation of artworks has consistently held significant\nimportance. Although there are differences between art and science regarding their goals and\ntoolsets, these distinctions blur when artists use scientific understanding to inform their work and\nscience examines art to comprehend the human experience. The idea of leveraging established\npsychological insights into human traits such as personality and emotion to guide the creation,\ncritique, and informing of artwork is not novel. Traditional portrait artists employ their understanding\nof human perception and vision to create portraits from life or photographs. This process includes the\narrangement of the environment, placement of the subject, and an interview to grasp their mental\nand physical characteristics. Artists also aim to convey their individual painting style while trying\nto express personal and universal ideas. An artist has several options in themes, brush style, color\nplan, edge and line plan, abstraction style, and emotional narrative to achieve the finished artwork.\nComputational creativity and generative art offer fresh avenues for modeling scientific knowledge\nto replicate this process and deepen our grasp of human creativity. This study uses AI techniques\nto begin emulating this artistic procedure. The Empathic AI Painter system seeks to discover novel\napproaches to balance diverse aesthetic and conceptual aspects.\n\n2 System Description\n\nThe Empathic Painter System is created to mimic the interaction between a live portrait artist and\na person, referred to as the sitter. It aims to understand the sitter\u2019s traits, such as personality and\nemotions, to create a unique portrait by selecting the appropriate abstraction techniques, color palette,\nand style that correspond to those traits. The system operates in a two-stage process; the first stage\ninvolves capturing the characteristics of the sitter, followed by the second stage, which uses the\ncaptured traits to generate a stylized artistic representation of their portrait. The initial stage of\ncapturing the personality of the sitter occurs during the conversation with an embodied conversational\nagent, using empathetic interaction methods. This system utilizes the M-Path conversational agent,\nwhich has been developed previously. The M-Path system was modified for this demonstration to\nconduct an interview based on the Big-5 personality questionnaire to categorize the sitter into one\nof the established personality dimensions. This data is then used to map the personality traits to a\nparticular artistic style. The mapping is transferred to the Generative AI Portrait Stylization system in\n\n.\n\n\fthe second stage, which creates an artistic portrait. The interaction process includes several steps.\nFirst, a portrait of the sitter is captured under controlled lighting conditions, and a unique ID is\nassigned after consent is provided for participation and use of the portrait. The sitter is then given\ninformation about the M-Path system with instructions about how to interact. The sitter initiates\nthe interaction until a complete conversation is concluded and the agent informs the sitter that the\ninteraction has ended. The M-Path system uses the data collected to classify the sitter\u2019s personality\ninto a specific dimension. This dimension is then used by the Generative AI Portraiture system\nto create a personalized portrait style. The generated portraits are showcased on a monitor for all\nparticipants and the crowd to observe and assess.\n\n2.1 Big-5 Personality Mapping\n\nThe five-factor model of personality is also known as the \"Big-5 Personality Model\" and is designed\nas a categorization to capture the variations in personality traits among individuals. This model\nclassifies personality variations across five dimensions: extraversion, openness, conscientiousness,\nneuroticism, and agreeableness. Each of these dimensions encompasses a wide range of psychological\nfunctions, which are composed of more specific traits. Extraversion pertains to the extent to which\npeople are dominant, talkative, assertive, active, energetic and enthusiastic. Openness characterizes\npeople who are curious, creative, innovative, imaginative, reflective, cultured, curious, original,\nbroad-minded, intelligent, and artistically sensitive, seeking new experiences and exploring novel\nideas. Conscientiousness indicates an individual\u2019s level of hard work, persistence, organization,\nand motivation in achieving their goals. Individuals high in conscientiousness tend to be organized,\nplan-oriented, and determined. Neuroticism, also referred to as Emotional Stability, represents\ndifferences in emotional stability and adjustment. Individuals scoring high on neuroticism tend\nto experience negative emotions, such as anxiety, depression, impulsiveness, self-consciousness,\nvulnerability, anger, hostility and worry. Agreeableness is linked to likability, conformity, friendliness,\nand social compliance. Individuals with high scores in agreeableness are characterized as trusting,\ncaring, forgiving, altruistic, flexible, gullible, good-natured, soft-hearted, cooperative and tolerant.\nThis model is based on factor analysis of descriptive words of human behavior. The questionnaire\nused is a shortened version of the Revised NEO Personality Inventory, which has 120 questions\nand takes 45 minutes to complete. For the online demonstration, one statement for each dimension\nwas used, where the whole conversational interaction could be completed in under 5 minutes. Each\nquestion is further modified to align with the conversation setup in the demonstration environment.\n\nDimension\n\nQuestion\n\nHow do you like the conference so far, is it interesting to you?\n\nOpenness\nConscientiousness Don\u2019t you think the conferences are always a bit chaotic?\nExtraversion\nAgreeableness\nNeuroticism\n\nDo you normally talk and interact with a lot of people?\nHow about agents? Do you trust me in sharing how you feel?\nHow do you feel about your portrait being displayed on the screen?\n\nTable 1: The questions used for the personality dimensions.\n\nThe answers to these questions are evaluated for their polarity and then mapped onto two-factor\ndimensions for personality adjectives. The mapping model is the Abridged Big Five Circumplex\nModel, in which facets of the Big Five dimensions are mapped as combinations of two factors. The\nAB5C mapping contains descriptive personality terms for each of the resulting 90 combinations,\nwhere the most distinctive trait of an individual is used to select the column, and the second most\ndistinctive trait selects the row. These traits may be either negative or positive. The mapping from\nBig-5 traits to the Generative AI portrait styles was provided by art experts who independently\nmapped the styles to the Big-5 categories and reached an agreement.\n\n2.2 Empathic Conversational Avatar\n\nThe starting point of interaction is the empathetic conversational agent, M-Path, which was developed\nusing a framework based on a computational model of empathy. M-Path is a human-like avatar\ncapable of initiating and maintaining an emotional conversation, based on the predetermined goal of\nthe dialogue. The interaction involves a face-to-face conversation with a human interaction partner,\n\n2\n\n\fsimilar to a video-conference with audio and visual input and output. The agent processes the\nreal-time inputs in terms of their linguistic and affective properties to generate empathetic verbal\nand non-verbal behavior. The main objective of the interaction is to complete the modified Big-5\nquestionnaire to categorize the partner\u2019s personality and send it to the generative art system. The\nsystem has three distinct modules: a perceptual module, a behavior controller and a behavior manager.\nThe perceptual module gathers the video and audio signals when the conversation partner is speaking.\nThis process was triggered with a push-to-talk system. M-Path enters a listening state when the\nuser speaks. During the listening state, speech and facial expressions are processed in real-time for\nspeech and emotion recognition. The video input is used in the facial emotion recognition module,\nwhich uses an OpenCV face-recognition algorithm to identify the face. Emotions are categorized\nusing a CNN model, trained on the CK+ Dataset, into 6 basic emotion categories. The speech\ninput is sent to the speech-to-text module which uses a service to get streaming speech recognition.\nSentiment analysis evaluates the text for its polarity using the SO-CAL Sentiment Analyzer, which\nwas trained on the NRC-Canada lexicon. The text is sent to the decision-making module for creating\nconversational responses. This process continues until the partner finishes speaking, which concludes\nthe listening state. The information is then sent to the decision-making module, and the agent enters a\nthinking state. The behavior controller module creates goal-directed verbal and non-verbal responses\nin all states of the conversation: listening, thinking, and speaking. This is done by analyzing the user\u2019s\nemotional response from the listening state. The conversation begins with the user\u2019s greeting and\nfinishes when the agent receives suitable answers to the personality survey questions. The listening,\nthinking, and speaking states of the agent loop until the user is categorized. During the listening\nstage, the agent shows a non-verbal affect matching response and backchanneling behavior. Affect\nmatching is a facial expression that mirrors the user\u2019s facial expressions in real-time, chosen by\nempathy mechanisms. Backchanneling is created by a nodding behavior when pauses are detected\nin the user\u2019s speech. These behaviors are combined to create an empathic listening behavior. After\nthe conversation with the participant ends, the final text received and the user\u2019s overall sentiment are\nsent to the Dialogue Manager (DM), and ultimately to the Empathy Mechanisms (EM). The DM\ncompletes the Big-5 personality questionnaire to assign a personality category. The EM ensures that\nthe DM generates empathetic responses while reaching its goal. The DM gathers the appropriate\nemotional response from the EM to generate an emotionally appropriate verbal reaction to the user,\nfollowed by a survey-related coping response, and then the next survey question. The system uses the\nscikit-learn library in Python for the TF-IDF vectorizer model, and the NLTK Lemmatizer. A second\nmodel is created by fine-tuning BERT for the classification of user responses according to sentiment\nand the Big-5 questionnaire answers. The Big-5 questionnaire answers are collected to select the\nmost dominant personality dimensions of the user, based on their probability values and polarity. The\nBig-5 mapping is used to select a category for the user, with adjectives. This categorization is then\nsent to the generative art cycle to produce a personalized portrait. After each response is generated\nby the dialogue manager, it is sent to the behavior manager to be performed by the conversational\nagent during the speaking state. To achieve a natural conversation, the system continuously produces\nnon-verbal and verbal behaviors. Lip movements, facial expressions, head gestures, body gestures,\nand posture are synchronized with the agent\u2019s speech. The animation is sent as a BML message to\nthe Smartbody character animation platform, to display the generated behaviors.\n\n2.3 Generative AI Portraiture System\n\nThe stylistic rendering of the portraits is generated by the generative art component of the system.\nThe portrait goes through three processing phases. The first phase preprocesses the original portrait\nby using an AI tool to separate the foreground from the background, which will be used to stylize\nthe portrait. Then, the light and color balance of the face are adjusted to achieve a lighting effect,\nwhere one side of the face is dramatically shown. The next phase uses this image and the personality\ncategory as inputs to a modified Deep Dream (mDD) system with multiple passes on the image to\ncreate the base style. While most DD systems use pre-trained networks with object recognition data,\nthe modified system uses artistic paintings and drawings as training data. The system has a dataset of\n160,000 labeled and categorized paintings from 3000 artists. A method called hierarchical tight style\nand tile was developed to overcome the problem that most artists create fewer than 200 paintings\nin their lifetimes. In the last phase, the source image from the previous phase is further enhanced\nusing the personality category. The ePainterly system combines Deep Style techniques as a surface\ntexture manipulator, and a series of Non-Photorealistic Rendering (NPR) techniques like particle\nsystems, color palette manipulation, and stroke engine techniques. This iterative process enhances\n\n3\n\n\fthe portrait, and the final result is shown in an online gallery. The ePainterly module is an expansion\nof the Painterly painting system, which models the cognitive processes of artists based on years of\nresearch. The NPR subclass of stroke-based rendering is used as the final part of the process to realize\nthe internal mDD models with stroke-based output. This additional step reduces noise artifacts from\nthe mDD output, creates cohesive stroke-based clustering, and a better distributed color space.",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "The Empathic AI Painter was presented at a conference demonstration session. Forty-two participants\ntested the system, with 26 of them completing the portrait-taking and interaction. Each conversation\nwith the M-Path system took approximately 5 minutes. The performance of the M-Path system was\nevaluated individually. On average, 84.72\n\n4",
  "is_publishable": 1,
  "venue": NaN
}
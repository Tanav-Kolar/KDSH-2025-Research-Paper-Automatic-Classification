{
  "title": "A PyTorch-Based Approach for Variational Learning\nwith Disentanglement",
  "abstract": "This paper presents the Disentanglement-PyTorch library, which has been devel-\noped to assist in the research, application, and assessment of novel variational\nalgorithms. This modular library allows for independent and reliable experimen-\ntation across diverse variational methodologies, through the decoupling of neural\narchitectures, the dimensionality of the latent space, and training algorithms. Fur-\nthermore, the library manages training schedules, logging, and the visualization\nof reconstructions and traversals in the latent space. It also provides evaluation\nof the encodings using various disentanglement metrics. Currently, the library\nincludes implementations of the following unsupervised algorithms: VAE, \u03b2-VAE,\nFactor-VAE, DIP-I-VAE, DIP-II-VAE, Info-VAE, and \u03b2-TCVAE. Additionally,\nconditional approaches such as CVAE and IFCVAE are also supported. This library\nwas utilized in some Disentanglement Challenge, where it achieved a 3rd rank in\nboth the first and second phases of the competition.",
  "introduction": "In the field of representation learning, two primary paths can be identified. One path concentrates\non learning transformations that are specific to a given task, often optimized for particular domains\nand applications. The other path involves learning the inherent factors of variation, in a manner\nthat is both disentangled and task-invariant. The task of unsupervised disentanglement of latent\nfactors, where changes in a single factor shift the latent encoding in a single direction, represents\nan unresolved problem in representation learning. Disentangled representations offer significant\nadvantages across various domains of machine learning including few-shot learning, reinforcement\nlearning, transfer learning, and semi-supervised learning. This work introduces a library developed\nusing the functionalities of the PyTorch framework. This library has been designed to facilitate the\nresearch, implementation, and evaluation of new variational algorithms, with a specific emphasis\non representation learning and disentanglement. This library was created in conjunction with the\nDisentanglement Challenge of NeurIPS 2019. The Disentanglement-PyTorch library is publicly\navailable under the GNU General Public License.\n\n2 Library Features\n\n2.1 Supported Algorithms and Objective Functions\n\n2.1.1 Unsupervised Objectives\n\nThe library currently offers implementations of the following unsupervised variational algorithms:\nVAE, \u03b2-VAE, \u03b2-TCVAE, Factor-VAE, Info-VAE, DIP-I-VAE, and DIP-II-VAE. The algorithms\nare incorporated as plug-ins to the variational Bayesian framework. They are specified by their\nrespective loss terms. Consequently, if the loss terms from two learning algorithms (e.g., A and B)\nare compatible, they can be integrated into the objective function by setting the appropriate flag. This\nallows researchers to combine loss terms that optimize for related objectives.\n\n.\n\n\f2.1.2 Conditional and Attribute-variant Objectives\n\nThe library provides support for conditional methods such as CVAE, where extra known attributes (i.e.,\nlabels) are utilized in both the encoding and decoding procedures. It also offers support for IFCVAE.\nThis is a method that enforces certain latent factors to encode known attributes through a set of positive\nand negative discriminators in a supervised manner. The library\u2019s modular construction allows the\nuse of any of the previously mentioned unsupervised loss terms in conjunction with conditional and\ninformation factorization techniques. This allows for the encouragement of disentanglement across\nattribute-invariant latents.\n\n2.2 Neural Architectures\n\nThe neural architectures and the dimensionality of the data and latent spaces can be configured and\nare independent from the training algorithm. This design enables the independent investigation of\nnew architectures for encoder and decoder networks, as well as support for diverse data domains.\n\n2.3 Evaluation of Disentanglement\n\nTo evaluate the quality of the learned representations, we use an existing implementation of disen-\ntanglement metrics. Thanks to an external library, the following metrics are supported: BetaVAE,\nFactorVAE, Mutual Information Gap (MIG), Interventional Robustness Score (IRS), Disentanglement\nCompleteness and Informativeness (DCI), and Separated Attribute Predictability (SAP).\n\n2.4 Miscellaneous Features\n\n2.4.1 Controlled Capacity Increase\n\nIt has been demonstrated that gradually relaxing the information bottleneck during training improves\ndisentanglement without compromising reconstruction accuracy. The capacity, which is defined as\nthe distance between the prior and the latent posterior distributions and represented with the variable\nC, is incrementally increased throughout training.\n\n2.4.2 Reconstruction Weight Scheduler\n\nTo prevent convergence at points with high reconstruction loss, training can be initialized with a greater\nfocus on reconstruction. The emphasis can be progressively shifted toward the disentanglement term\nas training proceeds.\n\n2.4.3 Dynamic Learning Rate Scheduling\n\nThe library supports all types of learning rate schedulers. Researchers are encouraged to use the\ndynamic learning rate scheduling to reduce the rate gradually. This should be done when the average\nobjective function over the epoch ceases its decreasing trend.\n\n2.4.4 Logging and Visualization\n\nThe library utilizes a tool to log the training process and visualizations. It allows the visualization\nof condition traversals, latent factor traversals, and output reconstructions in both static images and\nanimated GIFs.\n\n3 Experiments and Results\n\nThe \u03b2-TCVAE algorithm yielded the most effective disentanglement outcomes on the mpi3d real\ndataset during the second phase of the disentanglement challenge. Given the limited 8-hour timeframe\nallocated for training, the model was pre-trained on the mpi3d toy dataset. The model was trained\nusing the Adam optimizer for a total of 90,000 iterations, with a batch size of 64. The \u03b2 value for the\n\u03b2-TCVAE objective function was set at 2. The learning rate was initially set to 0.001. It was reduced\nby a factor of 0.95 when the objective function reached a plateau. The capacity parameter, C, was\nincreased gradually from 0 to 25. The dimensionality of the z-space was set to 20.\n\n2\n\n\fThe encoder comprised 5 convolutional layers. The number of kernels increased gradually from 32 to\n256. The encoder concluded with a dense linear layer. This layer was used to estimate the posterior\nlatent distribution as a parametric Gaussian. The decoder network included one convolutional layer.\nThis was followed by 6 deconvolutional (transposed convolutional) layers. The number of kernels\ngradually decreased from 256 down to the number of channels in the image space. ReLU activations\nwere used for all layers, except for the final layers of both the encoder and decoder networks.\n\nThe performance of the model on unseen objects from the mpi3d realistic and mpi3d real datasets is\nshown in Table 1. The model consistently performed better on the mpi3d realistic and mpi3d real\ndatasets. This is despite the fact that the model was only pre-trained using the mpi3d toy dataset.\n\nTable 1: Results of the best configurations of \u03b2-TCVAE on DCI, FactorVAE, SAP, MIG, and IRS\nmetrics.\n\nMethod\n\nDataset\n\nDCI\n\nFactorVAE\n\nSAP\n\nMIG\n\nIRS\n\n\u03b2-TCVAE mpi3d realistic\n\u03b2-TCVAE mpi3d real\n\n0.3989\n0.4044\n\n0.3614\n0.5226\n\n0.1443\n0.1592\n\n0.2067\n0.2367\n\n0.6315\n0.6423",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "The Disentanglement-PyTorch library offers a modular platform for studying, implementing, and\nassessing algorithms for disentanglement learning. It incorporates implementations of several well-\nknown algorithms, along with a variety of evaluation metrics. This makes it a valuable resource for\nthe research community.\n\nAppendix A. Latent Factor Traversal\n\n[width=0.8]latenttraversalf igure\n\nFigure 1: Latent factor traversal of the trained \u03b2-TCVAE model on a random sample of the mpi3d\nrealistic dataset. The disentanglement is not complete as some features are encoded in the same latent\nfactor. A latent space of size 20 was used, however, changes in the other 13 latent factors had no\neffect on the reconstruction; thus, these feature-invariant factors were not included for brevity.\n\n3",
  "is_publishable": 1,
  "venue": NaN
}
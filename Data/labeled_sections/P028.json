{
  "title": "Do You See What I Mean? Visual Resolution of\nLinguistic Ambiguities",
  "abstract": "Understanding language goes hand in hand with the ability to integrate com-\nplex contextual information obtained via perception. We present a novel task for\ngrounded language understanding: disambiguating a sentence given a visual scene\nwhich depicts one of the possible interpretations of that sentence. To this end, we\nintroduce a new multimodal corpus containing ambiguous sentences, representing\na wide range of syntactic, semantic and discourse ambiguities, coupled with videos\nthat visualize the different interpretations for each sentence. We address this task\nby extending a vision model which determines if a sentence is depicted by a video.\nWe demonstrate how such a model can be adjusted to recognize different interpre-\ntations of the same underlying sentence, allowing to disambiguate sentences in a\nunified fashion across the different ambiguity types.",
  "introduction": "Ambiguity is one of the defining characteristics of human languages, and language understanding\ncrucially relies on the ability to obtain unambiguous representations of linguistic content. While\nsome ambiguities can be resolved using intra-linguistic contextual cues, the disambiguation of many\nlinguistic constructions requires integration of world knowledge and perceptual information obtained\nfrom other modalities.\n\nWe focus on the problem of grounding language in the visual modality, and introduce a novel task\nfor language understanding which requires resolving linguistic ambiguities by utilizing the visual\ncontext in which the linguistic content is expressed. This type of inference is frequently called for in\nhuman communication that occurs in a visual environment, and is crucial for language acquisition,\nwhen much of the linguistic content refers to the visual surroundings of the child.\n\nOur task is also fundamental to the problem of grounding vision in language, by focusing on\nphenomena of linguistic ambiguity, which are prevalent in language, but typically overlooked when\nusing language as a medium for expressing understanding of visual content. Due to such ambiguities,\na superficially appropriate description of a visual scene may in fact not be sufficient for demonstrating\na correct understanding of the relevant visual content. Our task addresses this issue by introducing a\ndeep validation protocol for visual understanding, requiring not only providing a surface description\nof a visual activity but also demonstrating structural understanding at the levels of syntax, semantics\nand discourse.\n\nTo enable the systematic study of visually grounded processing of ambiguous language, we create\na new corpus, LAVA (Language and Vision Ambiguities). This corpus contains sentences with\nlinguistic ambiguities that can only be resolved using external information. The sentences are paired\nwith short videos that visualize different interpretations of each sentence. Our sentences encompass a\nwide range of syntactic, semantic and dis-\n\ncourse ambiguities, including ambiguous prepositional and verb phrase attachments, conjunctions,\nlogical forms, anaphora and ellipsis. Overall, the corpus contains 237 sentences, with 2 to 3\ninterpretations per sentence, and an average of 3.37 videos that depict visual variations of each\nsentence interpretation, corresponding to a total of 1679 videos.\n\n\fUsing this corpus, we address the problem of selecting the interpretation of an ambiguous sentence\nthat matches the content of a given video. Our approach for tackling this task extends the sentence\ntracker. The sentence tracker produces a score which determines if a sentence is depicted by a\nvideo. This earlier work had no concept of ambiguities; it assumed that every sentence had a single\ninterpretation. We extend this approach to represent multiple interpretations of a sentence, enabling\nus to pick the interpretation that is most compatible with the video.",
  "related_work": "Previous language and vision studies focused on the development of multimodal word and sentence\nrepresentations as well as methods for describing images and videos in natural language. While these\nstudies handle important challenges in multimodal processing of language and vision, they do not\nprovide explicit modeling of linguistic ambiguities.\n\nPrevious work relating ambiguity in language to the visual modality addressed the problem of word\n\nsense disambiguation. However, this work is limited to context independent interpretation of individ-\nual words, and does not consider structure-related ambiguities. Discourse ambiguities were previously\nstudied in work on multimodal coreference resolution. Our work expands this line of research, and\naddresses further discourse ambiguities in the interpretation of ellipsis. More importantly, to the best\nof our knowledge our study is the first to present a systematic treatment of syntactic and semantic\nsentence level ambiguities in the context of language and vision.\n\nThe interactions between linguistic and visual information in human sentence processing have been\nextensively studied in psycholinguistics and cognitive psychology. A considerable fraction of this\nwork focused on the processing of ambiguous language, providing evidence for the importance of\nvisual information for linguistic ambiguity resolution by humans. Such information is also vital\nduring language acquisition, when much of the linguistic content perceived by the child refers to their\nimmediate visual environment. Over time, children develop mechanisms for grounded disambiguation\nof language, manifested among others by the usage of iconic gestures when communicating ambigu-\nous linguistic content. Our study leverages such insights to develop a complementary framework that\nenables addressing the challenge of visually grounded disambiguation of language in the realm of\nartificial intelligence.\n\n3 Task\n\nWe provide a concrete framework for the study of language understanding with visual context by\nintroducing the task of grounded language disambiguation. This task requires to choose the correct\nlinguistic representation of a sentence given a visual context depicted in a video. Specifically, provided\nwith a sentence, n candidate interpretations of that sentence and a video that depicts the content of\nthe sentence, one needs to choose the interpretation that corresponds to the content of the video.\n\nTo illustrate this task, consider the example, where we are given the sentence \u201cSam approached the\nchair with a bag\u201d along with two different linguistic interpretations. In the first in-\n\nterpretation, which corresponds to parse 1(a), Sam has the bag. In the second interpretation associated\nwith parse 1(b), the bag is on the chair rather than with Sam. Given the visual context from figure\n1(c), the task is to choose which interpretation is most appropriate for the sentence.\n\n4 Approach Overview\n\nTo address the grounded language disambiguation task, we use a compositional approach for determin-\ning if a specific interpretation of a sentence is depicted by a video. a sentence and an accompanying\ninterpretation encoded in first order logic, give rise to a grounded model that matches a video against\nthe provided sentence interpretation.\n\nThe model is comprised of Hidden Markov Models (HMMs) which encode the semantics of words,\nand trackers which locate objects in video frames. To represent an interpretation of a sentence, word\nmodels are combined with trackers through a cross-product which respects the semantic representation\nof the sentence to create a single model which recognizes that interpretation.\n\n2\n\n\fGiven a sentence, we construct an HMM based representation for each interpretation of that sentence.\nWe then detect candidate locations for objects in every frame of the video. Together the re-\n\nforestation for the sentence and the candidate object locations are combined to form a model which\ncan determine if a given interpretation is depicted by the video. We test each interpretation and report\nthe interpretation with highest likelihood.\n\n5 Corpus\n\nTo enable a systematic study of linguistic ambiguities that are grounded in vision, we compiled\na corpus with ambiguous sentences describing visual actions. The sentences are formulated such\nthat the correct linguistic interpretation of each sentence can only be determined using external,\nnon-linguistic, information about the depicted activity. For example, in the sentence \u201cBill held the\ngreen chair and bag\u201d, the correct scope of \u201cgreen\u201d can only be determined by integrating additional\ninformation about the color of the bag. This information is provided in the accompanying videos,\nwhich visualize the possible interpretations of each sentence. Figure 2 presents the syntactic parses\nfor this example along with frames from the respective videos. Although our videos contain visual\nuncertainty, they are not ambiguous with respect to the linguistic interpretation they are presenting,\nand hence a video always corresponds to a single candidate representation of a sentence.\n\nThe corpus covers a wide range of well\n\nknown syntactic, semantic and discourse ambiguity classes. While the ambiguities are associated\nwith various types, different sentence interpretations always represent distinct sentence meanings,\nand are hence encoded semantically using first order logic. For syntactic and discourse ambiguities\nwe also provide an additional, ambiguity type specific encoding as described below.\n\n\u2022 Syntax Syntactic ambiguities include Prepositional Phrase (PP) attachments, Verb Phrase\n(VP) attachments, and ambiguities in the interpretation of conjunctions. In addition to\nlogical forms, sentences with syntactic ambiguities are also accompanied with Context Free\nGrammar (CFG) parses of the candidate interpretations, generated from a deterministic CFG\nparser.\n\n\u2022 Semantics The corpus addresses several classes of semantic quantification ambiguities, in\nwhich a syntactically unambiguous sentence may correspond to different logical forms. For\neach such sentence we provide the respective logical forms.\n\n\u2022 Discourse The corpus contains two types of discourse ambiguities, Pronoun Anaphora and\nEllipsis, offering examples comprising two sentences. In anaphora ambiguity cases, an\nambiguous pronoun in the second sentence is given its candidate antecedents in the first\nsentence, as well as a corresponding logical form for the meaning of the second sentence. In\nellipsis cases, a part of the second sentence, which can constitute either the subject and the\nverb, or the verb and the object, is omitted. We provide both interpretations of the omission\nin the form of a single unambiguous sentence, and its logical form, which combines the\nmeanings of the first and the second sentences.\n\nTable 2 lists examples of the different ambiguity classes, along with the candidate interpretations of\neach example.\n\nThe corpus is generated using Part of Speech (POS) tag sequence templates. For each template, the\nPOS tags are replaced with lexical items from the corpus lexicon, described in table 3, using all the\nvisually applicable assignments. This generation process yields an overall of 237 sentences,\n\nof which 213 sentences have 2 candidate interpretations, and 24 sentences have 3 interpretations.\nTable 1 presents the corpus templates for each ambiguity class, along with the number of sentences\ngenerated from each template.\n\nThe corpus videos are filmed in an indoor environment containing background objects and pedestrians.\nTo account for the manner of performing actions, videos are shot twice with different actors. Whenever\napplicable, we also filmed the actions from two different directions (e.g. approach from the left,\nand approach from the right). Finally, all videos were shot with two cameras from two different\nview points. Taking these variations into account, the resulting video corpus contains 7.1 videos\nper sentence and 3.37 videos per sentence interpretation, corresponding to a total of 1679 videos.\n\n3\n\n\fTable 1: POS templates for generating the sentences in our corpus. The rightmost column represents\nthe number of sentences in each category. The sentences are produced by replacing the POS tags\nwith all the visually applicable assignments of lexical items from the corpus lexicon shown in table 3.\n\nAmbiguity\n\nTemplates\n\n4*Syntax\n\nPP\nVP\n\nConjunction\n\nTotal\n\nSemantics\n\nLogical Form\n\n2*Discourse Anaphora\n\nEllipsis\n\nTotal\n\nTotal\n\nNNP V DT [JJ] NN1 IN DT [JJ] NN2.\nNNP1 V [IN] NNP2 V [JJ] NN.\nNNP1 [and NNP2] V DT JJ NN1 and NN2\nNNP V DT NN1 or DT NN2 and DT NN3.\n\nNNP1 and NNP2 V a NN.\nSomeone V the NNS.\n\nNNP V DT NN1 and DT NN2. It is JJ.\nNNP1 V NNP2. Also NNP3.\n\n#\n\n48\n60\n\n40\n\n148\n\n35\n\n36\n18\n\n54\n\n237\n\nThe average video length is 3.02 seconds (90.78 frames), with in an overall of 1.4 hours of footage\n(152434 frames).\n\nA custom corpus is required for this task because no existing corpus, containing either videos or\nimages, systematically covers multimodal ambiguities. Datasets aim to control for more aspects of\nthe videos than just the main action being performed but they do not provide the range of ambiguities\ndiscussed here. The closest dataset is that of as it controls for object appearance, color, action,\nand direction of motion, making it more likely to be suitable for evaluating disambiguation tasks.\nUnfortunately, that dataset was designed to avoid ambiguities, and therefore is not suitable for\nevaluating the work described here.\n\n6 Model\n\nTo perform the disambiguation task, we extend the sentence recognition model which represents\nsentences as compositions of words. Given a sentence, its first order logic interpretation and a\nvideo, our model produces a score which determines if the sentence is depicted by the video. It\nsimultaneously tracks the participants in the events described by the sentence while recognizing the\nevents themselves. This al-\n\nlows it to be flexible in the presence of noise by integrating top-down information from the sentence\nwith bottom-up information from object and property detectors. Each word in the query sentence is\nrepresented by an HMM, which recognizes tracks (i.e. paths of detections in a video for a specific\nobject) that satisfy the semantics of the given word. In essence, this model can be described as having\ntwo layers, one in which object tracking occurs and one in which words observe tracks and filter\ntracks that do not satisfy the word constraints.\n\nGiven a sentence interpretation, we construct a sentence-specific model which recognizes if a video\ndepicts the sentence as follows. Each predicate in the first order logic formula has a corresponding\nHMM, which can recognize if that predicate is true of a video given its arguments. Each variable has\na corresponding tracker which attempts to physically locate the bounding box corresponding to that\nvariable in each frame of a\n\nvideo. This creates a bipartite graph: HMMs that represent predicates are connected to trackers that\nrepresent variables. The trackers themselves are similar to the HMMs, in that they comprise a lattice\nof potential bounding boxes in every frame. To construct a joint model for a sentence interpretation,\nwe take the cross product of HMMs and trackers, taking only those cross products dictated by the\nstructure of the formula corresponding to the desired interpretation. Given a video, we employ an\nobject detector to generate candidate detections in each frame, construct trackers which select one of\nthese detections in each frame, and finally construct the overall model from HMMs and trackers.\n\n4\n\n\fTable 2: An overview of the different ambiguity types, along with examples of ambiguous sentences\nwith their linguistic and visual interpretations. Note that similarly to semantic ambiguities, syntactic\nand discourse ambiguities are also provided with first order logic formulas for the resulting sentence\ninterpretations. Table 4 shows additional examples for each ambiguity type, with frames from sample\nvideos corresponding to the different interpretations of each sentence.\n\nAmbiguity\nPP\n\nExample\nClaire left the green chair with a\nyellow bag.\n\nVP\n\nClaire looked at Bill picking up\na chair.\n\nConjunction\n\nClaire held a green bag and\nchair.\n\nClaire held the chair or the bag\nand the telescope.\n\nLogical Form\n\nSomeone moved the two chairs.\n\nAnaphora\n\nEllipsis\n\nSam picked up the bag and the\nchair. It is yellow.\nSam left Bill. Also Clark.\n\nLinguistic interpretations\nClaire [left the green chair] [with\na yellow bag].\nClaire left [the green chair with\na yellow bag].\nClaire looked at [Bill [picking up\na chair]].\nClaire [looked at Bill] [picking\nup a chair].\nClaire held a [green [bag and\nchair]].\nClaire held a [[green bag] and\n[chair]].\nClaire held [[the chair] or [the\nbag and the telescope]].\nClaire held [[the chair or the bag]\nand [the telescope]].\nchair(x), move(Claire,\nmove(Bill, x)\nchair(x), chair(y), x \u0338= y,\nmove(Claire, x),\nmove(Bill, y)\nchair(x), chair(y), x \u0338= y,\nperson(u),\nmove(u, x), move(u, y)\nchair(x), chair(y), x \u0338= y,\nperson(u), person(v)\nu \u0338= v, move(u, x), move(v, y)\nIt = bag\nIt = chair\nSam left Bill and Clark.\nSam and Clark left Bill.\n\nx),\n\nVisual setups\nThe bag is with Claire.\nThe bag is on the chair.\n\nBill picks up the chair.\nClaire picks up the chair.\n\nThe chair is green.\nThe chair is not green.\n\nClaire holds the chair.\nClaire holds the chair and the\ntelescope.\n\nClaire and Bill move the same\nchair.\nClaire and Bill move different\nchairs.\nOne person moves both chairs.\nEach chair moved by a different\nperson.\n\nThe bag is yellow.\nThe chair is yellow.\nSam left Bill and Clark.\nSam and Clark left Bill.\n\nTable 3: The lexicon used to instantiate the templates in table 1 in order to generate the corpus.\n\nSyntactic Category Visual Category Words\n\nNouns\nVerbs\nPrepositions\nAdjectives\n\nObjects, People\nActions\nSpacial Relations with, left of, right of, on\nVisual Properties\n\nyellow, green\n\nchair, bag, telescope, someone, proper names\npick up, put down, hold, move (transitive), look at, approach, leave\n\nProvided an interpretation and its corresponding formula composed of P predicates and V variables,\nalong with a collection of object detections, bf rame\ndetection index, in each frame of a video of\nlength T the model computes the score of the videosentence pair by finding the optimal detection\nfor each participant in every frame. This is in essence the Viterbi algorithm, the MAP algorithm for\nHMMs, applied to finding optimal object detections jf rame\nvariable for each participant, and the optimal\nstate kf rame\npredicate for each predicate HMM, in every frame. Each detection is scored by its confidence\nfrom the object detector, f and each object track is scored by a motion coherence metric g which\n\ni\n\n5\n\n\fdetermines if the motion of the track agrees with the underlying optical flow. Each predicate,\n\nV\n(cid:88)\n\nv=1\n\nmax\ni1...iV\nk1...kP\n\n(cid:32)\n\nF (b1\ni1\nv\n\n) +\n\nT\n(cid:88)\n\nt=2\n\ng(bt\n\nit\u22121\nv\n\n, bt\nit\nv\n\n(cid:33)\n)\n\n+\n\n(cid:33)\n\n(1)\n\nP\n(cid:88)\n\nT\n(cid:88)\n\np=1\n\nt=1\n\n(cid:32)\n\nlog hp(kt\n\np, b\u03b8p(1)\nit\n\u03b8p (1)\n\n, b\u03b8p(2)\nit\n\u03b8p (2)\n\n) +\n\nT\n(cid:88)\n\nt=2\n\nlog ap(kt\u22121\n\np\n\n, kt\np)\n\np, is scored by the probability of observing a particular detection in a given state hp, and by the\nprobability of transitioning between states ap. The structure of the formula and the fact that multiple\npredicates often refer to the same variables is recorded by \u03b8, a mapping between predicates and their\narguments. The model computes the MAP estimate as:\n\nfor sentences which have words that refer to at most two tracks (i.e.\ntransitive verbs or binary\npredicates) but is trivially extended to arbitrary arities. Figure 3 provides a visual overview of the\nmodel as a cross-product of tracker models and word models.\n\nOur model extends the approach of in several ways. First, we depart from the dependency based\nrepresentation used in that work, and recast the model to encode first order logic formulas. Note\nthat some complex first order logic formulas cannot be directly encoded in the model and require\nadditional inference steps. This extension enables us to represent ambiguities in which a given\nsentence has multiple logical interpretations for the same syntactic parse.\n\nSecond, we introduce several model components which are not specific to disambiguation, but are\nrequired to encode linguistic constructions that are present in our corpus and could not be handled by\nthe model of. These new components are the predicate \u201cnot equal\u201d, disjunction, and conjunction. The\nkey addition among these components is support for the new predicate \u201cnot equal\u201d, which enforces\nthat two tracks, i.e. objects, are distinct from each other. For example, in the sentence \u201cClaire and Bill\nmoved a chair\u201d one would want to ensure that the two movers are distinct entities. In earlier work,\nthis was not required because the sentences tested in that work were designed to distinguish objects\nbased on constraints rather than identity. In other words, there might have been two different people\nbut they were distinguished in the sentence by their actions or appearance. To faithfully recognize\nthat two actors are moving the chair in the earlier example, we must ensure that they are disjoint\nfrom each other. In order to do this we create a new HMM for this predicate, which assigns low\nprobability to tracks that heavily overlap, forcing the model to fit two different actors in the previous\nexample. By combining the new first order logic based semantic representation in lieu of a syntactic\nrepresentation with a more expressive model, we can encode the sentence interpretations required to\nperform the disambiguation task.\n\nFigure 3(left) shows an example of two different interpretations of the above discussed sentence\n\u201cClaire and Bill moved a chair\u201d. Object trackers, which correspond to variables in the first order\nlogic representation of the sentence interpretation, are shown in red. Predicates which constrain the\npossible bindings of the trackers, corresponding to predicates in the representation of the sentence, are\nshown in blue. Links represent the argument structure of the first order logic formula, and determine\nthe cross products that are taken between the predicate HMMs and tracker lattices in order to form\nthe joint model which recognizes the entire interpretation in a video.\n\nThe resulting model provides a single unified formalism for representing all the ambiguities in table\n2. Moreover, this approach can be tuned to different levels of specificity. We can create models that\nare specific to one interpretation of a sentence or that are generic, and accept multiple interpretations\nby eliding constraints that are not com-\n\nmon between the different interpretations. This allows the model, like humans, to defer deciding on a\nparticular interpretation or to infer that multiple interpretation of the sentence are plausible.\n\n7 Experimental Results\n\nWe tested the performance of the model described in the previous section on the LAVA dataset\npresented in section 5. Each video in the dataset was pre-processed with object detectors for humans,\nbags, chairs, and telescopes. We employed a mixture of CNN and DPM detectors, trained on held\nout sections of our corpus. For each object class we generated proposals from both the CNN and\n\n6\n\n\fthe DPM detectors, and trained a scoring function to map both results into the same space. The\nscoring function consisted of a sigmoid over the confidence of the detectors trained on the same held\nout portion of the training set. As none of the disambiguation examples discussed here rely on the\nspecific identity of the actors, we did not detect their identity. Instead, any sentence which contains\nnames was automatically converted to one which contains arbitrary \u201cperson\u201d labels.\n\nThe sentences in our corpus have either two or three interpretations. Each interpretation has one or\nmore associated videos where the scene was shot from a different angle, carried out either by different\nactors, with different objects, or in different directions of motion. For each sentence-video pair, we\nperformed a 1-out-of-2 or 1-out-of-3 classification task to determine which of the interpretations of\nthe corresponding sentence best fits that video. Overall chance performance on our dataset is 49.04%,\nslightly lower than 50% due to the 1out-of-3 classification examples.\n\nThe model presented here achieved an accuracy of 75.36% over the entire corpus averaged across\nall error categories. This demonstrates that the model is largely capable of capturing the underlying\ntask and that similar compositional crossmodal models may do the same. For each of the 3 major\nambiguity classes we had an accuracy of 84.26% for syntactic ambiguities, 72.28% for semantic\nambiguities, and 64.44% for discourse ambiguities.\n\nThe most significant source of model failures are poor object detections. Objects are often rotated\nand presented at angles that are difficult to recognize. Certain object classes like the telescope\n\nare much more difficult to recognize due to their small size and the fact that hands tend to largely\nocclude them. This accounts for the degraded performance of the semantic ambiguities relative to the\nsyntactic ambiguities, as many more semantic ambiguities involved the telescope. Object detector\nperformance is similarly responsible for the lower performance of the discourse ambiguities which\nrelied much more on the accuracy of the person detector as many sentences involve only people\ninteracting with each other without any additional objects. This degrades performance by removing a\nhelpful constraint for inference, according to which people tend to be close to the objects they are\nmanipulating. In addition, these sentences introduced more visual uncertainty as they often involved\nthree actors.\n\nThe remaining errors are due to the event models. HMMs can fixate on short sequences of events\nwhich seem as if they are part of an action, but in fact are just noise or the prefix of another action.\nIdeally, one would want an event model which has a global view of the action, if an object went up\nfrom the beginning to the end of the video while a person was holding it, it\u2019s likely that the object was\nbeing picked up. The event models used here cannot enforce this constraint, they merely assert that\nthe object was moving up for some number of frames; an event which can happen due to noise in the\nobject detectors. Enforcing such local constraints instead of the global constraint of the motion of the\nobject over the video makes joint tracking and event recognition tractable in the framework presented\nhere but can lead to errors. Finding models which strike a better balance between local information\nand global constraints while maintaining tractable inference remains an area of future work.",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "We present a novel framework for studying ambiguous utterances expressed in a visual context. In\nparticular, we formulate a new task for resolving structural ambiguities using visual signal. This is a\nfundamental task for humans, involving complex cognitive processing, and is a key challenge for\nlanguage acquisition during childhood. We release a multimodal corpus that enables to address this\ntask, as well as support further investigation of ambiguity related phenomena in visually grounded\nlanguage processing. Finally, we\n\npresent a unified approach for resolving ambiguous descriptions of videos, achieving good perfor-\nmance on our corpus.\n\nWhile our current investigation focuses on structural inference, we intend to extend this line of work\nto learning scenarios, in which the agent has to deduce the meaning of words and sentences from\nstructurally ambiguous input. Furthermore, our framework can be beneficial for image and video\nretrieval applications in which the query is expressed in natural language. Given an ambiguous query,\nour approach will enable matching and clustering the retrieved results according to the different query\ninterpretations.\n\n7",
  "is_publishable": 1,
  "venue": NaN
}
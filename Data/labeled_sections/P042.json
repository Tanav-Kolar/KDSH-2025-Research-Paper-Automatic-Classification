{
  "title": "DeepSim: A Semantic Approach to Image Registration\nEvaluation",
  "abstract": "This paper introduces a novel semantic similarity metric designed for image regis-\ntration. Current metrics, such as Euclidean distance or normalized cross-correlation,\nprimarily focus on aligning intensity values, which presents challenges when deal-\ning with low contrast or noise. Our approach utilizes learned, dataset-specific\nfeatures to guide the optimization of learning-based registration models. In com-\nparisons with existing unsupervised and supervised methods across various image\nmodalities and applications, our method demonstrates consistently superior regis-\ntration accuracy and faster convergence. Additionally, its learned noise invariance\nresults in smoother transformations on lower-quality images.",
  "introduction": "This paper delves into the significant area of deformable registration, an essential preprocessing\nstep in medical imaging. The primary objective is to ascertain anatomical correspondences between\nimages and determine geometric transformations, denoted as \u03a6, for their alignment. The majority\nof algorithmic and deep learning-based techniques achieve alignment by optimizing a similarity\nmeasure, D, and a \u03bb-weighted regularizer, R, which are combined to form a loss function:\n\nL(I, J, \u03a6) = D(I \u25e6 \u03a6, J) + \u03bbR(\u03a6).\n\n(1)\n\nThe alignment is critically evaluated by the similarity metric, D, which significantly impacts the\nfinal outcome. Common pixel-based metrics, such as Euclidean distance (MSE) and patch-wise\nnormalized cross-correlation (NCC), are used in both algorithmic and deep learning approaches to\nimage registration. Typically, a similarity measure for a particular task is selected from a small set of\nmetrics, with no certainty that any of them is suitable for the data.\n\nThe limitations of pixel-based similarity metrics have been extensively studied in the image generation\nfield, where the adoption of deep similarity metrics, designed to emulate human visual perception, has\nenhanced the generation of highly realistic images. Because registration models are also generative,\nwe anticipate that employing these similarity metrics could also improve registration results. However,\ncurrent methods that use learned similarity metrics for image registration require ground truth\ntransformations, or they restrict the input to the registration model.\n\nWe propose a data-driven similarity metric for image registration that relies on aligning semantic\nfeatures. Our metric uses learned semantic filters specific to the dataset, which are then used to train\na registration model. We have validated our method using three biomedical datasets characterized by\nvarying image modalities and applications. Across all datasets, our approach achieves consistently\nhigh registration accuracy, even outperforming metrics that use supervised information. Our models\nalso demonstrate quicker convergence and learn to overlook noisy image patches, leading to more\nconsistent transformations on lower-quality data.\n\n.\n\n\f2 A Deep Similarity Metric for Image Registration\n\nTo align areas with comparable semantic content, we propose a similarity metric based on the\nconsensus of semantic feature representations between two images. These semantic feature maps\nare generated by a feature extractor, trained through a surrogate segmentation task. To capture\nthe alignment of both localized, specific features and more abstract, global ones, we compute the\nsimilarity across multiple layers of abstraction.\nGiven a set of feature-extracting functions, Fl : R\u2126\u00d7C \u2192 R\u2126l\u00d7Cl , for L layers, we define:\n\nDeepSim(I \u25e6 \u03a6, J) =\n\nL\n(cid:88)\n\nl=1\n\n1\n|\u2126l|\n\n(cid:88)\n\np\u2208\u2126l\n\nFl(I \u25e6 \u03a6)p \u00b7 Fl(J)p\n\u2225Fl(I \u25e6 \u03a6)p\u2225\u2225Fl(J)p\u2225\n\n(2)\n\nwhere Fl(J)p denotes the l-th layer feature extractor applied to image J at spatial coordinate p. It is\nrepresented as a vector of Cl output channels, and the spatial size of the l-th feature map is denoted\nas |\u2126l|. The metric is influenced by the pixel\u2019s neighborhood, since Fl uses convolutional filters with\nan expanding receptive area. Note that the formulation, using cosine similarity, mirrors the classic\nNCC metric, which can be interpreted as the squared cosine-similarity between two zero-mean patch\ndescription vectors.\n\nTo improve registration, the functions Fl(\u00b7) should extract features that are semantically relevant\nto the registration task, while ignoring noise and artifacts. This is achieved by training the feature\nextractor on an additional segmentation task, since segmentation models excel at learning pertinent\nkernels while also achieving invariance to features like noise that are not predictive. The convolutional\nfilters obtained act as feature extractors for DeepSim.",
  "related_work": "",
  "methodology": "",
  "experiments": "We evaluated registration models trained with DeepSim against baseline metrics such as MSE,\nNCC, NCCsup (NCC using supervised information), and VGG (a VGG-based metric used in image\ngeneration, similar to our approach). The model architecture is shown in Figure 1. For both\nregistration and segmentation, we used U-nets. The registration network predicts the transformation\n\u03a6 based on two input images, I and J. The spatial transformer module applies \u03a6 to obtain the\nmorphed image I \u25e6 \u03a6. The loss function is as in Eq. 1; we chose the diffusion regularizer for R and\nfine-tuned the hyperparameter \u03bb on the validation sets.\n\nTo demonstrate the broad applicability of our method across various registration tasks, we assessed it\nusing three datasets of both 2D and 3D images with different image modalities: T1-weighted Brain-\nMRI scans, human blood cells from the Platelet-EM dataset, and cell tracking from the PhC-U373\ndataset. Each dataset was divided into training, validation, and testing subsets.",
  "results": "Table 1: Quantitative comparison of similarity metrics. Stars indicate p-test significance level. Effect\nsize given by Cohen\u2019s d.\n\nBrain-MRI\n\nPlatelet-EM PhC-U373\n\nMSE\nNCC\nNCCsup\nVGG\nDeepSim\n\n0.70\n0.71\u2021\n0.72\u2021\n0.71\u2021\n0.75\n\n0.98\u2021\n0.98\u2021\n0.98\u2021\n0.98\u2021\n0.99\n\n0.98\n0.98\n0.98\n0.98\n0.99\n\n\u2021 indicates p<0.001 statistical significance with effect size > 0.8.\n\nRegistration Accuracy Convergence: We evaluated the mean S\u00f8rensen-Dice coefficient on the\nunseen test set (Table 1) and tested the statistical significance of the results using the Wilcoxon\nsigned-rank test for paired samples. The null hypothesis for each similarity metric was that the model\n\n2\n\n\ftrained with DeepSim would perform better. Statistical significance levels were set at p\u2217 = 0.05,\np\u2217\u2217 = 0.01, and p\u2217\u2217\u2217 = 0.001. Additionally, we used Cohen\u2019s d to measure the effect size. Models\ntrained with our proposed DeepSim were ranked highest on both the Brain-MRI and Platelet-EM\ndatasets, exhibiting strong statistical significance. In the PhC-U373 dataset, all models achieved a\nhigh dice-overlap exceeding 0.97. DeepSim converged faster than the baseline models, particularly\nduring the initial training epochs.\n\nQualitative Examples Transformation Grids: We display the fixed and moving images, I and\nJ, along with the transformed image I \u25e6 \u03a6, for each similarity metric model in Figure 2(a), and a\nmore detailed view of a noisy patch from the Platelet-EM dataset in Figure 2(b). The transformation\nis shown using grid-lines, which were transformed from an evenly spaced grid. We observed\nconsiderably distorted transformation fields in noisy image areas in models trained with the baselines.\nSpecifically, models trained with NCC and NCCsup demonstrated highly irregular transformations,\ndespite the careful adjustment of the regularization hyperparameter. The model trained with DeepSim\nshowed greater invariance to noise.\n\n5 Discussion and Conclusion\n\nRegistration models trained with DeepSim show substantial registration accuracy across multiple\ndatasets, which improves downstream medical analysis and diagnostics. The reliability of our\nproposed metric reduces the need for testing multiple traditional metrics. Instead of experimentally\ndetermining whether MSE or NCC best captures the properties of a dataset, DeepSim can be used to\nlearn the appropriate features from the data.\n\nThe analysis of noisy patches in Figure 2(b) highlights an inherent resistance to noise. Pixel-based\nsimilarity metrics are influenced by artifacts, leading to excessively detailed transformation fields,\nwhich DeepSim does not exhibit. Although smoother transformation fields can be achieved for\nall metrics by increasing the regularizer, this would negatively affect the registration precision of\nanatomically important areas. Accurate registration of noisy, low-quality images allows for shorter\nacquisition times and reduced radiation in medical applications.\n\nDeepSim is a general metric that can be applied to image registration across all modalities and\nanatomies. Beyond the presented datasets, good results on low-quality data suggest that DeepSim\ncould improve registration accuracy in lung CT and ultrasound imaging, where details are difficult to\nidentify, and image quality is often compromised. Furthermore, DeepSim is not restricted to deep\nlearning; algorithmic image registration follows a comparable optimization structure where similarity-\nbased loss is minimized through gradient descent methods. Applying DeepSim in algorithmic\nmethods can improve their performance by aligning deep, semantic feature embeddings.\n\n6 Broader Impact\n\nThe widespread applications of medical image registration significantly amplify the broader impact\nof our work. Some of the typical applications include neuroscience, CT imaging of the lungs and\nabdomen, as well as the fusion and combination of different modalities.\n\nThe use of deep learning for image registration, while capable of achieving remarkable outcomes\nacross many different applications, often necessitates the training of models using specialized\nhardware over extended periods. This energy-intensive task may raise carbon emissions, which are\na major contributor to climate change. By introducing a method that learns a semantic similarity\nmetric directly from data, we hope to eliminate the need for excessive testing of other loss functions.\nThis can reduce the number of model configurations tested during the development of deep learning\nmethods, thus contributing to a lower environmental impact within the image registration community.\n\n3",
  "conclusion": "",
  "is_publishable": 1,
  "venue": NaN
}
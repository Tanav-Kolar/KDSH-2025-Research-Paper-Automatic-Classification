{
  "title": "Scene Comprehension Through Image Analysis with\nan Extensive Array of Categories and Context at the\nScene Level",
  "abstract": "This research introduces a unique approach to scene parsing that is nonparametric,\nwhich enhances the precision and expands the scope of foreground categories within\nimages of scenes. Initially, the accuracy of label likelihood at the superpixel level\nis improved by combining likelihood scores from multiple probabilistic classifiers.\nThis method improves classification accuracy and enhances the representation of\ncategories that are less frequently represented. The second advancement involves\nthe integration of semantic context into the parsing procedure by utilizing global\nlabel costs. Instead of relying on sets derived from image retrieval, the technique\ndescribed assigns a comprehensive likelihood estimate to each label, which is\nsubsequently incorporated into the overall energy function. The effectiveness\nof the system is assessed using two expansive datasets, SIFTflow and LMSun.\nThe system demonstrates performance that is at the forefront of the field on the\nSIFTflow dataset and achieves outcomes that are close to setting new records on\nthe LMSun dataset.",
  "introduction": "The task of scene parsing involves assigning semantic labels to every pixel within an image of a\nscene. Algorithms for image parsing attempt to categorize different types of scenes, both indoors and\noutdoors, such as a shoreline, a roadway, an urban environment, and an airport. Numerous systems\nhave been developed to categorize each pixel in an image semantically. A significant obstacle for\nimage parsing methods is the considerable variability in recognition rates across different types of\nclasses. Background classes, which usually cover a significant area of the image\u2019s pixels, often have a\nuniform look and are identified with great accuracy. Foreground classes, which usually take up fewer\npixels in the image, have changeable forms and might be hidden or set up in various ways. These\nkinds of classes represent noticeable parts of the image that frequently grab a viewer\u2019s attention.\nHowever, their recognition rates are often much lower than those of background classes, making\nthem frequent examples of unsuccessful recognition.\n\nImpressive results have been obtained by parametric scene parsing techniques on datasets with a\nlimited number of labels. Nevertheless, for considerably bigger datasets with a lot of labels, using\nthese techniques becomes more challenging because of the increased demands on learning and\noptimization.\n\nNonparametric image parsing techniques have recently been introduced to tackle the growing variety\nof scene types and semantic labels effectively. These methods usually begin by reducing the com-\nplexity of the problem from individual pixels to superpixels. Initially, a set of images is selected,\nconsisting of training images that bear the closest visual resemblance to the image being queried.\nThe potential labels for a specific image are limited to those found in the selected set of images.\nSubsequently, the probability scores for the classification of superpixels are determined by matching\nvisual characteristics. Ultimately, context is applied by reducing an energy function that includes both\nthe expense of the data and information on how often classes appear together in nearby superpixels.\n\n.\n\n\fA shared difficulty encountered by nonparametric parsing methods is the phase of image retrieval.\nEven though image retrieval helps narrow down the number of labels to think about, it\u2019s seen as a\nvery important step in the process. There\u2019s no opportunity to correct the mistake if the correct labels\nare not among the images that were retrieved. It has been reported that mistakes in retrieval are the\nmain reason for most unsuccessful cases.\n\nA novel nonparametric image parsing algorithm is proposed in this work, aiming for enhanced\noverall precision and improved identification rates for classes that are less commonly represented. An\nefficient system is developed that can adapt to an ever-growing quantity of labels. The contributions\nmade are outlined as follows:\n\n1. Superpixel label likelihood scores are improved by merging classifiers. The system merges\nthe output probabilities from several classification models to generate a more equitable score for\neach label at every superpixel. The weights for merging the scores are determined by employing a\nlikelihood normalization technique on the training set in an automated manner. 2. Semantic context is\nintegrated within a probabilistic structure. To prevent the removal of important labels that cannot be\nretrieved later, a retrieval set is not structured. Instead, label costs are utilized, which are determined\nfrom the global contextual relationships of labels in analogous scenes, to obtain enhanced parsing\noutcomes.\n\nThe system developed achieves top-tier per-pixel recognition accuracy on two extensive datasets:\nSIFTflow, which includes 2688 images with 33 labels, and LMSun, which has 45576 images with\n232 labels.",
  "related_work": "Several techniques for scene parsing, both parametric and nonparametric, have been suggested. The\nnonparametric systems that try to cover a wide range of semantic classes are very similar to the\nmethod. Different methods are used to improve the overall effectiveness of nonparametric parsing.\nThe authors merge region-parsing with outputs from per-exemplar SVM detectors. Object masks\nare transferred by per-exemplar detectors into the test image for segmentation. Their method greatly\nimproves overall accuracy, but it requires a lot of computer power. It\u2019s hard to scale because data\nterms need to be calibrated using a batch of fine training in a leave-one-out way, which is hard to do.\nSuperpixels from rare classes are specifically added to the retrieval set to make them more visible.\nThe authors filter the list of labels for a test image by doing an image retrieval step, and query time\nis used to add more samples to rare classes. The way superpixels are classified, how rare classes\nare recognized, and how semantic context is applied are all different in this system. By combining\nclassification costs from different contextual models, a more balanced set of label costs is produced,\nwhich promotes the representation of foreground classes. Instead of using image retrieval, global\nlabel costs are used in the inference step.\n\nThe value of semantic context has been thoroughly investigated in numerous visual recognition\nalgorithms. Context has been employed to enhance the overall labeling performance through a\nfeedback mechanism in nonparametric scene parsing systems. Initial labeling of superpixels in a\nquery image is utilized to modify the training set by adjusting for recognized background classes,\nthereby enhancing the visibility of uncommon classes. The objective is to enhance the image retrieval\nset by reintroducing segments of uncommon classes. A semantic global descriptor is generated.\nImage retrieval is enhanced by merging the semantic descriptor with the visual descriptors. Context\nis added by creating global and local context descriptors based on classification likelihood maps. The\nmethod described differs from these methods as it does not employ context at each superpixel when\ncalculating a global context descriptor. Instead, contextual information across the entire image is\ntaken into account.\n\nContextually relevant outcomes are produced by deducing label correlations in comparable scene\nimages. Additionally, there is no retrieval set that needs to be enriched. Rather, the global context is\nstructured within a probabilistic framework, where label costs are calculated across the whole image.\nFurthermore, the global context is executed in real time without any preliminary training. Another\nmethod of image parsing that doesn\u2019t use retrieval sets is where image labeling is done by moving\nannotations from a graph of patch matches across image sets. But this method needs a lot of memory,\nwhich makes it hard to scale for big datasets.\n\n2\n\n\fThe presented method draws inspiration from the combination of classifier techniques in machine\nlearning, which have demonstrated the ability to enhance the capabilities of individual classifiers.\nSeveral fusion methods have been effectively applied in various fields of computer vision, including\ndetecting faces, annotating images with multiple labels, tracking objects, and recognizing characters.\nNonetheless, the classifiers that make up these systems and the ways they are combined are very\ndifferent from the framework, and the other methods have only been tested on small datasets.\n\n3 Baseline Parsing Pipeline\n\nThis section provides a summary of the basic image parsing system, which is composed of three\nstages: feature extraction, label likelihood estimation at superpixels, and inference.\n\nAfterward, contributions are presented: enhancing likelihoods at superpixels and calculating label\ncosts for global context at the scene level.\n\n3.1 Segmentation and Feature Extraction\n\nTo reduce the complexity of the task, the image is partitioned into superpixels. Extraction of\nsuperpixels from images begins by employing an efficient graph-based method. For each superpixel,\n20 distinct types of local features are extracted to characterize its shape, appearance, texture, color, and\nposition, adhering to established methods. In addition to these features, Fisher Vector (FV) descriptors\nare extracted at each superpixel using an established library. Computation of 128-dimensional dense\nSIFT feature descriptors is performed on five different patch sizes (8, 12, 16, 24, 30). A dictionary\ncomprising 1024 words is constructed. Subsequently, the FV descriptors are retrieved and Principal\nComponent Analysis (PCA) is applied to decrease their dimensionality to 512. Each superpixel is\nrepresented by a feature vector that has 2202 dimensions.\n\n3.2 Label Likelihood Estimation\n\nThe features obtained in the prior stage are utilized to determine label probabilities for each superpixel.\nUnlike conventional approaches, the possible labels for a test image are not restricted. Instead, the\ndata term for the likelihood of each class label c C is computed, where C represents the total number\nof classes in the dataset. The normalized cost D(l<sub>si</sub> = c|s<sub>i</sub>) of assigning\nlabel c to superpixel s<sub>i</sub> is given by:\n\nD(lsi = c|si) = 1 \u2212\n\n1\n1 + e\u2212Lunbal(si,c)\n\n(1)\n\nwhere L<sub>unbal</sub>(s<sub>i</sub>, c) is the log-likelihood ratio score of label c, given by\nL<sub>unbal</sub>(s<sub>i</sub>, c) = 1/2 log(P(s<sub>i</sub>|c)/P(s<sub>i</sub>|\u00acc)), where\n\u00acc = C c is the set of all labels except c, and P(s<sub>i</sub>|c) is the likelihood of superpixel\ns<sub>i</sub> given c. A boosted decision tree (BDT) model is trained to obtain the label likelihoods\nL<sub>unbal</sub>(s<sub>i</sub>, c). For implementation, a publicly accessible boostDT library\nis utilized. During this phase, the BDT model is trained using every superpixel in the training set,\nwhich constitutes an imbalanced distribution of class labels C.\n\n3.3 Smoothing and Inference\n\nThe optimization challenge is formulated as a maximum a posteriori (MAP) estimation to determine\nthe ultimate labeling L through Markov Random Field (MRF) inference. Using only the estimated\nlikelihoods from the preceding section to categorize superpixels leads to imprecise classifications.\nIncorporating a smoothing term V(l<sub>s<sub>i</sub></sub>, l<sub>s<sub>j</sub></sub>) into\nthe MRF energy function aims to address this problem by penalizing adjacent superpixels with\nsemantically incongruous labels. The goal is to minimize the following energy function:\n\nE(L) =\n\n(cid:88)\n\nsi\u2208S\n\nD(lsi = c|si) + \u03bb\n\n(cid:88)\n\n(i,j)\u2208A\n\nV (lsi , lsj )\n\n(2)\n\n3\n\n\fwhere A represents the set of neighboring superpixel indices and V(l<sub>s<sub>i</sub></sub>,\nl<sub>s<sub>j</sub></sub>) denotes the penalty for assigning labels l<sub>s<sub>i</sub></sub>\nand l<sub>s<sub>j</sub></sub> to two adjacent pixels, calculated from occurrences in the training\nset combined with the constant Potts model following established methods.\nis the smoothing constant.\nInference is conducted using the -expansion method with established code.\n\n4\n\nImproving Superpixel Label Costs\n\nAlthough foreground objects typically stand out the most in a picture of a scene, parsing algorithms\nfrequently misclassify them. For instance, in an image of a city street, a person would usually first\nspot the individuals, signs, and vehicles before they would see the structures and the street. However,\nbecause of two primary factors, scene parsing algorithms frequently misclassify foreground regions\nas belonging to the surrounding background. Initially, in the superpixel classification phase, any\nclassifier would naturally prefer classes that are more prevalent to reduce the overall training error.\nSecondly, during the MRF smoothing phase, a lot of the superpixels that were accurately identified as\nforeground objects are smoothed out by the background pixels around them.\n\nIt is suggested that the label likelihood score at each superpixel be improved to obtain a more precise\nparsing output. Various classifiers are designed that provide supplementary information regarding\nthe data. Subsequently, all the developed models are merged to produce a unified conclusion. An\noverview of the method for merging classifiers is displayed in Figure 1. During the testing phase,\nthe label likelihood scores from all the BDT models are combined to generate the final scores for\nsuperpixels.\n\n4.1 Fusing Classifiers\n\nThe proposed method is inspired by ensemble classifier methods, which train several classifiers and\nmerge them to enhance decision-making. These methods are especially helpful when the classifiers\nare distinct. In other words, the decrease in error is connected to the lack of correlation between the\nmodels that were trained. This means that the total error is decreased if the classifiers misclassify\ndifferent data points. Furthermore, it has been demonstrated that for large datasets, dividing the\ntraining set yields superior results compared to dividing the feature space.\n\nIt has been observed that the classification error for a particular class is correlated with the average\nnumber of pixels it covers in the scene images, as indicated by the blue line in Figure 2. This is in\nline with what earlier methods found, which is that the rate of classification error is related to how\noften classes show up in the training set. However, it goes beyond that by taking into account how\noften the classes appear at the image level, which is meant to solve the problem of less-represented\nclasses being smoothed out by a background class that is nearby.\n\nTo achieve this, three BDT models are trained using the following training data criteria: (1) a balanced\nsubsample of all classes C in the dataset, (2) a balanced subsample of classes that occupy an average\nof less than z\n\nThe goal of these decisions is to lessen the correlation between the trained BDT models, as seen in\nFigure 2. The balanced classifiers are able to correctly identify some of the less-represented classes,\nbut they make more mistakes on the more-represented classes. The unbalanced classifier, on the\nother hand, mostly misclassifies the less-represented classes. Combining the likelihoods from all\nthe classifiers leads to an improved overall decision that enhances the representation of all classes\n(Figure 1). It was noticed that the addition of more classifiers did not enhance performance for any of\nthe datasets.\n\nThe ultimate expense of allocating a label c to a superpixel s<sub>i</sub> can subsequently be\nexpressed as the amalgamation of the likelihood scores of all classifiers:\n\nD(lsi = c|si) = 1 \u2212\n\n1\n1 + e\u2212Lcomb(si,c)\n\n(3)\n\nwhere L<sub>comb</sub>(s<sub>i</sub>, c) represents the combined likelihood score obtained by\nthe weighted sum of the scores from all classifiers:\n\n4\n\n\fLcomb(si, c) =\n\n(cid:88)\n\nj=1,2,3,4\n\nwj(c)Lj(si, c)\n\n(4)\n\nwhere L<sub>j</sub>(s<sub>i</sub>, c) is the score from the j<sup>th</sup> classifier, and\nw<sub>j</sub>(c) is the normalized weight of the likelihood score of class c in the j<sup>th</sup>\nclassifier.\n\n4.2 Normalized Weight Learning\n\nThe weights w\n\nw < sub > j < /sub > (c)]arelearnedf orallclassesCinof f linesettingsusingthetrainingset.T heweightsarecalculatedindependentlyf oreachclassif ier.T heweight\u02dcw < sub > j < /sub > (c)f orclasscinthej < sup > th < /sup > classif ierisdeterminedbyaveragingtheratioof thetotallikelihoodsf orclassctothetotallikelihoodsf orallclassesc < sub > i < /sub > Ccacrossallsuperpixelss < sub > i < /sub > S :\n\n\u02dcwj(c) =\n\n1\n|Cj|\n\n(cid:80)\n\n(cid:80)\n\nsi\u2208S Lj(si, c)\n\nci\u2208C\\c\n\n(cid:80)\n\nsi\u2208S Lj(si, ci)\n\n(5)\n\nwhere |C<sub>j</sub>| denotes the quantity of classes encompassed by the j<sup>th</sup> classifier\nand not covered by any other classifier with a fewer number of classes.\n\nThe normalized weight w<sub>j</sub>(c) of class c can then be computed as: w<sub>j</sub>(c) =\n~w<sub>j</sub>(c) / <sub>j=1,2,3,4</sub>(~w<sub>j</sub>(c)). Normalizing the output likelihoods\nin this way improves the likelihood that all classifiers will be taken into account in the outcome, with\na focus on classes that are less represented.\n\n5 Scene-Level Global Context\n\nWhen working with scene parsing challenges, including the scene\u2019s semantics in the labeling process\nis beneficial. For example, if a scene is known to be a beach scene, labels such as sea, sand, and sky\nare expected to be found with a much greater probability than labels like car, building, or fence. The\ninitial labeling results of a test image are used in estimating the likelihoods of all labels c C. The\nlikelihoods are estimated globally over an image, i.e., there is a unique cost per label per image. The\nglobal label costs are then incorporated into a subsequent MRF inference stage to enhance the results.\n\nThe presented method, in contrast to previous methods, does not restrict the number of labels to\nthose found in the retrieval set. Instead, it utilizes the set to calculate the likelihood of class labels\nin a k-nn manner. The likelihoods are normalized by counts over the entire dataset and smoothed\nto provide an opportunity for labels not present in the retrieval set. The likelihoods are also used in\nMRF optimization, not for reducing the number of labels.\n\n5.1 Context-Aware Global Label Costs\n\nIt is proposed that semantic context be incorporated by using label statistics instead of global visual\nfeatures. The reasoning behind this decision is that sorting by global visual characteristics often\ndoesn\u2019t find images that are similar at the scene level. For instance, a highway scene might be\nmistaken for a beach scene if road pixels are incorrectly classified as sand. Nonetheless, when given a\nreasonably accurate initial labeling, sorting by label statistics finds images that are more semantically\nrelated. This helps to eliminate outlier labels and find labels that are absent in a scene.\n\nFor a given test image I, minimizing the energy function in equation 2 produces an initial labeling\nL of the superpixels in the image. If C is the total number of classes in the dataset, let T C be the\nset of unique labels which appear in L, i.e. T = t | s<sub>i</sub> : l<sub>s<sub>i</sub></sub> = t,\nwhere s<sub>i</sub> is a superpixel with index i in the test image, and l<sub>s<sub>i</sub></sub>\nis the label of s<sub>i</sub>. Semantic context is exploited in a probabilistic framework, where the\nconditional distribution P(c|T) is modeled over class labeling C given the initial global labeling of an\nimage T. P(c|T) c C is computed in a K-nn fashion:\n\nP (c|T ) =\n\n1 + n(c, KT )\nn(c, S)\n\n1 + n(\u00acc, KT )\n|S|\n\n(6)\n\n5\n\n\fwhere K<sub>T</sub> is the K-neighborhood of initial labeling T, n(c, X) is the number of superpixels\nwith label c in X, n(\u00acc, X) is the number of superpixels with all labels except c in X, and |S| is the\ntotal number of superpixels in the training set. The likelihoods are normalized and a smoothing\nconstant of value 1 is added.\n\nTo obtain the neighborhood K<sub>T</sub>, training images are ranked by their distance to the\nquery image. The distance between two images is determined by the weighted size of the intersection\nof their class labels, which intuitively shows that the neighbors of T are images that share many labels\nwith those in T. A different weight is assigned to each class in T in a manner that gives preference to\nclasses that are less represented.\n\nThe algorithm operates in three stages, as depicted in Figure 3. It begins by (1) assigning a weight\n<sub>t</sub> to each class t T, which is inversely proportional to the number of superpixels in the\ntest image with label t: <sub>t</sub> = 1 - n(t,I)/|I|, where n(t, I) is the number of superpixels in the\ntest image with label l<sub>s<sub>i</sub></sub> = t, and |I| is the total number of superpixels in the\nimage. Then, (2) training images are ranked by the weighted size of intersection of their class labels\nwith the test image. Finally, (3) the global label likelihood L<sub>global</sub>(c) = P(c|T) of each\nlabel c C is computed using equation 6.\n\nCalculating the label costs is performed in real-time for a query image, without the need for any\noffline batch training. The method enhances the overall precision by utilizing solely the true labels of\ntraining images, without incorporating any global visual characteristics.\n\n5.2\n\nInference with Label Costs\n\nOnce the likelihoods L<sub>global</sub>(c) of each class c C are obtained, a label cost H(c) =\n-log(L<sub>global</sub>(c)) can be defined. The final energy function becomes:\n\nE(L) =\n\n(cid:88)\n\nsi\u2208S\n\nD(lsi = c|si) + \u03bb\n\n(cid:88)\n\n(i,j)\u2208A\n\nV (lsi, lsj ) +\n\n(cid:88)\n\nc\u2208C\n\nH(c)\u03b4(c)\n\n(7)\n\nwhere (c) is the indicator function of label c:\n\n\u03b4(c) =\n\n(cid:26)1\n0\n\nif \u2203si : lsi = c\notherwise\n\n(8)\n\nEquation 7 is solved using -expansion with the extension method to optimize label costs. Optimizing\nthe energy function in equation 7 effectively minimizes the number of unique labels in a test image to\nthose with low label costs, i.e., those most relevant to the scene.",
  "methodology": "",
  "experiments": "The experiments were conducted on two extensive datasets: SIFTflow and LMSun. SIFTflow consists\nof 2,488 training images and 200 test images. All images are of outdoor scenes, sized 256x256 with\n33 labels. LMSun includes both indoor and outdoor scenes, with a total of 45,676 training images\nand 500 test images. Image sizes range from 256x256 to 800x600 pixels with 232 labels.\n\nThe same evaluation metrics and train/test splits as in previous methods are employed. The per-pixel\naccuracy (the percentage of pixels in test images that were correctly labeled) and per-class recognition\nrate (the average of per-pixel accuracies of all classes) are reported. The following variants of the\nsystem are evaluated: (i) baseline, as described in section 3, (ii) baseline (with balanced BDT), which\nis the baseline approach using a balanced classifier, (iii) baseline + FC (NL fusion), which is the\nbaseline in addition to the fusing classifiers with normalized-likelihood (NL) weights in section 4, and\n(iv) full, which is baseline + fusing classifiers + global costs. To show the effectiveness of the fusion\nmethod (section 4.2), the results of (v) baseline + FC (average fusion), which is fusing classifiers by\naveraging their likelihoods, and (vi) baseline + FC (median fusion), which is fusing classifiers by\ntaking the median of their likelihoods are reported. Results of (vii) full (without FV), which is the\nfull system without using the Fisher Vector features are also reported.\n\n6\n\n\fx = 5 is fixed (section 4.1), a value that was obtained through empirical evaluation on a small subset\nof the training set.\n\n6.1 Results\n\nThe results are compared with state-of-the-art methods on SIFTflow in Table 1. K = 64 top-ranked\ntraining images have been set for computing the global context likelihoods (section 5.1). The full\nsystem achieves 81.7\n\nTable 1: Comparison with state-of-the-art per-pixel and per-class accuracies (%) on the SIFTflow\ndataset.\n\nMethod\n\nPer-pixel\n\nPer-class\n\nLiu et al.\nFarabet et al.\nFarabet et al. balanced\nEigen and Fergus\nSingh and Kosecka\nTighe and Lazebnick\nTighe and Lazebnick\nYang et al.\nBaseline\nBaseline (with balanced BDT)\nBaseline + FC (NL fusion)\nBaseline + FC (average fusion)\nBaseline + FC (median fusion)\nFull without Fisher Vectors\nFull\n\n76.7\n78.5\n74.2\n77.1\n79.2\n77.0\n78.6\n79.8\n78.3\n76.2\n80.5\n78.6\n77.3\n77.5\n81.7\n\nN/A\n29.5\n46.0\n32.5\n33.8\n30.1\n39.2\n48.7\n33.2\n45.5\n48.2\n46.3\n46.8\n47.0\n50.1\n\nTable 2 compares the performance of the same variants of the system with the state-of-the-art methods\non the large-scale LMSun dataset. LMSun is more challenging than SIFTflow in terms of the number\nof images, the number of classes, and the presence of both indoor and outdoor scenes. Accordingly,\na larger value of K = 200 in equation 6 is used. The method achieves near-record performance in\nper-pixel accuracy (61.2\n\nTable 2: Comparison with state-of-the-art per-pixel and per-class accuracies (%) on the LMSun\ndataset.\n\nMethod\n\nPer-pixel\n\nPer-class\n\nTighe and Lazebnick\nTighe and Lazebnick\nYang et al.\nBaseline\nBaseline (with balanced BDT)\nBaseline + FC (NL fusion)\nBaseline + FC (average fusion)\nBaseline + FC (median fusion)\nFull without Fisher Vectors\nFull\n\n54.9\n61.4\n60.6\n57.3\n45.4\n60.0\n60.5\n59.2\n58.2\n61.2\n\n7.1\n15.2\n18.0\n9.5\n13.8\n14.2\n11.4\n14.7\n13.6\n16.0\n\nThe performance of the system is analyzed when varying the number of trees T for training the BDT\nmodel (section 4.1), and the number of top training images K in the global label costs (section 5.1).\nFigure 4 shows the per-pixel accuracy (on the y-axis) and the per-class accuracy (on the x-axis) as a\nfunction of T for a variety of K\u2019s. Increasing the value of T generally produces better classification\nmodels that better describe the training data. At T 400, performance levels off. As shown, the\nglobal label costs consistently improve the performance over the baseline method with no global\ncontext. Using more training images (higher K) improves the performance through considering more\nsemantically relevant scene images. However, performance starts to decrease for very high values of\nK (e.g., K = 1000) as more noisy images start to be added.\n\n7\n\n\fFigure 5 shows the per-class recognition rate for the baseline, combined classifiers, and the full\nsystem on SIFTflow. The fusing classifiers technique produces more balanced likelihood scores that\ncover a wider range of classes. The semantic context step removes outlier labels and recovers missing\nlabels, which improves the recognition rates of both common and rare classes. Recovered classes\ninclude field, grass, bridge, and sign. Failure cases include extremely rare classes, e.g. cow, bird,\ndesert, and moon.\n\n6.2 Running Time\n\nThe runtime performance was analyzed for both SIFTflow and LMSun (without feature extraction)\non a four-core 2.84GHz CPU with 32GB of RAM without code optimization. For the SIFTflow\ndataset, training the classifier takes an average of 15 minutes per class. The training process is run\nin parallel. The training time highly depends on the feature dimensionality. At test time, superpixel\nclassification is efficient, with an average of 1 second per image. Computing global label costs takes\n3 seconds. Finally, MRF inference takes less than one second. MRF inference is run twice for the\nfull pipeline. LMSun is much larger than SIFTflow. It takes 3 hours for training the classifier, less\nthan a minute for superpixel classification per image, less than 1 minute for MRF inference, and 2\nminutes for global label cost computation.\n\n6.3 Discussion\n\nThe presented scene parsing method is generally scalable as it does not require any offline training\nin a batch fashion. However, the time required for training a BDT classifier increases linearly with\nincreasing the number of data points. This is challenging with large datasets like LMSun. Randomly\nsubsampling the dataset has a negative impact on the overall precision of the classification results.\nAlternative approaches of mining discriminative data points that better describe each class are planned\nto be investigated. The system still faces challenges in trying to recognize very less-represented\nclasses in the dataset (e.g., bird, cow, and moon). This could be handled via better contextual models\nper query image.",
  "results": "",
  "conclusion": "A novel scene parsing algorithm has been presented that enhances the overall labeling precision,\nwithout neglecting foreground classes that are significant to human viewers. By merging likelihood\nscores from various classification models, the strengths of individual models have been successfully\namplified, thus enhancing both the per-pixel and per-class accuracy. To prevent the removal of\naccurate labels through image retrieval, global context has been integrated into the parsing process\nusing a probabilistic framework. The energy function has been expanded to incorporate global label\ncosts that produce a more semantically relevant parsing output. Experiments have demonstrated\nthe superior performance of the system on the SIFTflow dataset and comparable performance to\nstate-of-the-art methods on the LMSun dataset.\n\n8",
  "is_publishable": 1,
  "venue": NaN
}
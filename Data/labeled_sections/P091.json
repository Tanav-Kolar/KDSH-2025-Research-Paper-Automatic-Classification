{
  "title": "An Investigation into Named Entity Recognition for\nCall Center Transcripts to Ensure Privacy Law\nCompliance",
  "abstract": "This study explores the application of Named Entity Recognition (NER) on a\nnovel form of user-generated text, specifically call center conversations. These\ndialogues present unique challenges, blending the complexities of spontaneous\nspeech with issues specific to conversational Automatic Speech Recognition (ASR),\nsuch as inaccuracies. By employing a custom corpus with manual annotations,\ntraining contextual string embeddings, and implementing a BiLSTM-CRF model,\nwe achieve results that are on par with the state-of-the-art for this new task.",
  "introduction": "This paper addresses the crucial need to identify and handle sensitive personal information within\ncall center transcripts, which are generated as a result of speech recognition systems. Although these\ntranscripts are typically redacted for Payment Card Industry (PCI) compliance, they still often contain\na caller\u2019s name and internal ID number, which can be useful for quality assurance. However, new\nprivacy laws, such as the General Data Protection Regulation (GDPR) in the EU, establish stringent\nguidelines concerning data collection, storage, and an individual\u2019s right to withdraw consent for\ndata usage. To adhere to these regulations without losing the data\u2019s value, it is essential to pinpoint\nnon-public personal and personally identifiable information (NPI/PII) in call transcripts.\n\nWe utilize Named Entity Recognition (NER) to locate instances of NPI/PII within the transcripts,\nremove them, and replace them with appropriate tags that denote the type of removed data. For\ninstance, a transcript such as \"This is john doe reference number 12345\" would be transformed into\n\"This is [NAME] reference number [NUMBER]\". This task is distinctive to call centers for several\nreasons. First, these transcripts consist of natural human conversations, which have many common\nproblems of user-generated content such as incomplete sentences and unusual words. Furthermore,\ntranscript text is produced by Automatic Speech Recognition (ASR) systems, which are susceptible to\nerrors, as will be described in Section 3.1. Even though modern ASR systems are usually reliable, the\nsource audio is from phone calls, which is often low quality and contains background noise. The poor\naudio quality leads to incorrect ASR, producing ungrammatical sentences. This makes understanding\nthe call semantics and identifying features essential to NER systems more difficult. Moreover, call\ntranscripts frequently lack capitalization, numeric digits, and proper punctuation, which are crucial\nfeatures for classic NER methods. Also, traditional NER systems are inadequate for handling emails,\naddresses, or spellings, which makes it difficult to use pre-trained NER models.\n\nIn this paper, we apply the current best neural network architecture for sequence labeling, a BiLSTM-\nCRF, to the task of identifying NPI and PII in call transcripts. We match the state-of-the-art perfor-\nmance on standard datasets by using our model with annotated data and custom contextual string\nembeddings.",
  "related_work": "Named Entity Recognition has become a focus in the field of Natural Language Processing (NLP),\nparticularly since the Message Understanding Conferences (MUCs) in the 1990s. The CoNLL2003\nshared task in 2003 concentrated on language-independent NER and popularized feature based\nsystems. The OntoNotes corpus, released in 2006, has been vital to the progress of NER research.\n\nFollowing the CoNLL task, Conditional Random Field (CRF) based models became the most\nsuccessful, which requires that features be manually produced. Current research utilizes neural\nnetworks to generate these features. Bidirectional Long Short Term Memory models with a CRF layer\n(BiLSTM-CRF) have been used successfully on CoNLL2000 and CoNLL2003 datasets. A BiLSTM-\nCNN-CRF has been used for NER on the CoNLL2003 dataset, producing superior results. Similar\nresults were achieved by a BiLSTM-CNN with features from word embeddings and the lexicon.\nEmbeddings have been used for both words and entity types to create more robust models. Flair, with\ncharacter-based embeddings and a pooling approach, has set the state of the art. Crossweigh uses\nFlair embeddings to address mishandled annotations.\n\nIn 2006, the word confidence scores from ASR systems were used as a feature for NER. Similar\nexperiments were done on French radio and TV audio. Neither of those used natural conversation,\nand the quality of the audio was superior, making ASR a more accurate task.\n\n2.1 Conversations are Different: The Twitter Analogy\n\nMuch of the past research has used newswire datasets. While newswire data is expected to conform\nto standard text conventions, call center transcripts do not have these conventions. This presents a\nproblem for the usual approaches to NER and is further complicated by our poor audio quality.\n\nSpeaker 1: Thank you for calling our company how may i help you today.\nSpeaker 2:\n\nId like to pay my bill.\n\nTable 1: An example of turns of a conversation, where each person\u2019s line in the dialogue represents\ntheir turn. This output matches the format of our data described in Section 3.\n\nThe most similar research area to this is work on Twitter data. Similar to our transcripts, tweets are\nuser-generated and may not have conventional grammar or spelling. Initial research tackled this\nproblem with a K-nearest neighbors model combined with a CRF. A model combining a multi-step\nneural network with a CRF output layer achieved first place in the 2017 Workshop on Noisy User-\ngenerated Text (W-NUT). The success of pooled contextualized string embeddings was also shown\nwith this data. We use prior work on tweets to direct our model creation for call center data.\n\n3 Data\n\nOur dataset includes 7,953 training, 500 validation, and 534 test samples. Each sample represents\na complete speaker turn from a debt collection call center. A speaker turn is defined as a complete\ntranscription from one speaker before another speaker starts, as shown in Table 1. The training set is\na random sample of turns from 4 months of call transcripts. The transcripts were generated using a\nproprietary speech recognition system, which outputs all lowercase transcripts without punctuation\nor numeric digits. We used spaCy to convert each turn to a document that begins with a capital letter\nand ends with a period, as this is the default for spaCy. In order to make use of entities, a Sentencizer\nmodule was added, which defaults to this capitalization and period structure.\n\n3.1 Data Annotation\n\nWe created a schema for annotating the training and validation data with different types of NPI/PII,\nwhich are shown in Table 2.\n\nInitial annotations were performed using Doccano. The annotators were trained in NPI/PII recognition,\nand were instructed to err on the side of caution in unclear instances. Ambiguity often came from\nerrors in the ASR model. The lack of audio meant it was sometimes unclear if \"I need oak leaves\"\nwas actually \"Annie Oakley\". The opposite was also true such as when \"Brilliant and wendy jeff to\n\n2\n\n\fEntity Type Description\n\nFirst and last name of a customer or agent\n\nNUMBERS A sequence of numbers related to a customer\u2019s information (e.g. phone numbers or internal ID number)\nNAME\nCOMPANY The name of a company\nADDRESS\nEMAIL\nSPELLING\n\nA complete address, including city, state, and zip code\nAny email address\nLanguage that clarifies the spelling of a word (e.g. \"c as in cat\")\nTable 2: A brief description of our annotation schema.\n\nprocess the refund\" was actually \"Brilliant and when did you want to process the refund\". Emails\nwere also difficult, as errors in ASR made it difficult to determine the bounds of the email address.\nAlso, the transcripts were pre-redacted for PCI compliance. This redaction can obscure important\ndata, for example, sometimes a customer ID is redacted as part of the PCI redaction process. To\nlessen false negatives, we use context to include the [redacted] tag as part of the numbers sequence\nwhen possible. No steps to clean the transcripts were taken; the natural noise in the data was left for\nthe model to interpret.\n\nDue to limitations with spaCy and the complexity of nested entities, we only allowed one annotation\nper word in the dataset. This means, for instance, that \"c a t as in team at gmail dot com\" would be\nlabeled either as SPELLING[0:6] EMAIL[6:] or as EMAIL[0:] with the indices corresponding to the\nposition of words in the text. This ultimately results in a lower count of SPELLING entities, because\nthese are often part of EMAIL or ADDRESS entities, which influences our analysis in Section 6.\n\n4 Model Design\n\nWe utilized a standard BiLSTM-CRF model in PyTorch, adapted from a GitHub repository. We wrote\nour own main.py to use our spaCy preprocessing, and adapted the code to handle batch processing.\nAfter preprocessing, we trained the model on the training set and used the validation set for model\ntuning. All numbers in this paper are reported on the test set. A visualization of our model is shown\nin Figure 1.",
  "methodology": "",
  "experiments": "5.1 Basic Hyperparameter Tuning\n\nWe used a grid search algorithm to maximize model performance. The word embedding layer uses\nFastText embeddings trained on the client\u2019s call transcripts. This aids in mitigating the impacts of\npoor ASR, and this will be explored in Sections 5.2 and 5.3. The grid search included the parameters:\nepochs (a sampled distribution between 5 and 50), the size of a dropout layer (between 0 and 0.5,\nwith 0.1 intervals of search), the number of hidden layers (between 5 and 20 in increments of 5), and\nthe encoding type used in the output of the CRF (BIO, BILOU, IO). The other hyperparameters were\na learning rate of .001, a batch size of 1, 30 nodes in each fully connected layer, and the inclusion of\nbias in each layer. The experiments were run in parallel on a virtual machine with 16 CPUs and 128\nGB of memory. Each experiment took a few hours to run.\n\nTo understand the performance of the model, we broke down the measurements of precision, recall,\nand F1 by entity type. Table 3 shows these results for the best model configuration. This model used\n46 epochs, a dropout rate of 0.2, 5 hidden layers, and a BIO encoding.\n\n5.2 Training Word Embeddings\n\nMost past research has fine-tuned existing word embeddings, but the task of mitigating misrecognition\nseemed more complex than domain adaptation. To lessen the impact of the errors, we understand that\nfrequent misrecognitions appear in contexts similar to the intended word. A custom model gives a\nmisrecognized word a vector similar to the word it should be and not to the other meaning it has. The\nimportance of domain specific word embeddings when using ASR data has been shown in research.\n\n3\n\n\fWe ran our best performing model with the 300 dimensional GloVe 6b word embeddings. Our\nembeddings were trained on roughly 216 million words. The results from the best epoch of this\nmodel (16) are shown in Table 3.\n\n2*Entity Type\n\nO\nNUMBERS\nNAME\nCOMPANY\nADDRESS\nEMAIL\nSPELLING\n\nMicro Average\n\nPrecision\n\nRecall\nCustom GloVe Custom GloVe Custom GloVe\n\nF1\n\n89.8\n95.6\n89.6\n98.8\n70.6\n0\n45.8\n\n89.2\n\n84.2\n88.7\n92.1\n99.5\n0.3\n07.1\n34\n\n85.6\n\n81.7\n85.4\n91.1\n72.9\n75.0\n0\n52.4\n\n79.6\n\n76.6\n82.9\n88.7\n64.3\n18.7\n03.1\n40.5\n\n74.0\n\n85.6\n90.1\n90.3\n83.9\n72.7\n0\n48.9\n\n84.1\n\n80.2\n85.7\n90.3\n78.1\n23\n04.4\n37.0\n\n79.4\n\nTable 3: The performance by entity type of the BiLSTM-CRF model on the held out test set. This table\ncompares the results of our custom embeddings model (\"Custom\") against the GloVe embeddings\n(\"GloVe\").\n\n5.3 Using Flair\n\nPrevious experiments highlighted the importance of custom word embeddings to account for mis-\nrecognition in call center transcripts. Here, we test the performance of Flair and its contextual string\nembeddings.\n\nWe begin by training custom contextual string embeddings based on the results of the first experiments.\nWe use the same corpus as in Section 5.1. The tutorial on the Flair GitHub page was used with the\nfollowing parameters: hidden size: 1024, sequence length: 250, mini batch size: 100. We use the\nnewline to indicate a document change, and each turn as a separate document for consistency. The\nmodel\u2019s validation loss stabilized after epoch 4, and the best version of the model was used.\n\nWe conduct experiments using Flair\u2019s SequenceTagger with default parameters and a hidden size of\n256.\n\nFlair uses only the custom trained Flair embeddings.\n\nFlair + FastText uses the custom trained Flair embeddings and the custom trained FastText embeddings\nusing Flair\u2019s StackedEmbeddings.\n\nFlairmean pooling uses only the custom trained Flair embeddings within Flair\u2019s PooledFlairEmbed-\nding. Mean pooling was used.\n\nFlairmean pooling + FastText uses PooledFlairEmbeddings with mean pooling and the custom trained\nFastText embeddings using Flair\u2019s StackedEmbeddings.\n\nThese results are shown in Table 4.\n\nEntity\n\nFlair Flair + FastText Flairmean pooling Flairmean pooling + FastText\n\nO\nNUMBERS\nCOMPANY\nADDRESS\nEMAIL\nSPELLING\n\n98.3\n83.1\n81.1\n87.5\n58.8\n55.0\n\n98.5\n87.9\n80.7\n94.1\n50.0\n57.1\n\n98.2\n87.7\n80.7\n61.5\n73.3\n55.8\n\n98.5\n86.2\n80.3\n94.1\n66.7\n57.9\n\n97.3\nMicro Average\nTable 4: The F1 scores on the test set for each entity type for each Flair embedding experiment.\n\n97.5\n\n97.7\n\n97.7\n\n4\n\n\f6 Discussion\n\nTable 3 shows that using custom embeddings is beneficial over using GloVe embeddings, with the\nexception of the EMAIL category. The Flair embeddings show a large improvement over other word\nembeddings; however all four varieties of Flair models have nearly identical Micro Average F1s. The\nbest performing Flair models are those that use both the custom contextualized string embeddings\nand the custom FastText embeddings.\n\nAcross all of the models in this paper, EMAIL and SPELLING consistently performed worse than\nother categories. This is due to the overlap in their occurrences and their variable appearance. The\ncustom embeddings model often identified parts of an email correctly but labeled some aspects, such\nas a name, as NAME followed by EMAIL instead of labeling the whole thing as EMAIL. SPELLING\noften appears within an EMAIL entity. Due to the previously discussed limitations, the SPELLING\nentity had a limited presence in our training data, with many EMAIL and ADDRESS entities\ncontaining examples of SPELLING. All models frequently misidentified EMAIL as SPELLING and\nvice versa. Additionally, the test data had a number of turns that consisted of only SPELLING, which\nwas poorly represented in training. The Flairmean pooling model outperforms the other models in\nEMAIL by a large margin.\n\nThe results in Table 4 highlight that the NUMBERS category contains strings that appear frequently\nin the text. There are a finite number of NUMBER words in our corpus (those numeric words along\nwith many instances of \"[redacted]\"), and the numbers of interest in our dataset appear in very similar\ncontexts and do not often get misrecognized. The COMPANY entity performs well for similar\nreasons; when the model was able to identify the company name correctly, it was often in a common\nerror form and in a known context. The model\u2019s failures can be attributed to the training data because\nthe company name is a proper noun that is not in standard ASR language models, including the one\nwe used. Thus, it is often misrecognized since the language model has higher probabilities assigned to\ngrammatically correct phrases that have nothing to do with the company name. This causes variability\nin appearance, which means that not every version of the company name was present in our training\nset.\n\nInteresting variability also occurred in ADDRESS entities. Both models that used Flair and FastText\nembeddings strongly outperformed the models that used only Flair, and standard Flair embeddings\nstrongly outperformed the Pooled Flair embeddings. Neither version of the Flair-only model identified\naddresses in which numbers were shown as \"[redacted]\" but both models that utilized FastText had\nno issue with these instances.\n\n7 Conclusion and Future Work\n\nThrough the use of a BiLSTM-CRF model, paired with custom-trained Flair embeddings, we achieve\nstate-of-the-art NER performance on a new call center conversation dataset with distinct entity types.\nWe also show the importance of training word embeddings that fully capture the intricacies of the\ntask. Although we cannot release our data for privacy, we have shown that existing state-of-the-art\ntechniques can be applied to less common datasets and tasks. Future work will include evaluating\nthe model with call transcripts from other industries. We would also like to explore how well these\ntechniques work on other user-generated conversations like chats and emails.\n\n5",
  "results": "",
  "conclusion": "",
  "is_publishable": 1,
  "venue": NaN
}
{
  "title": "Multi-Agent Systems Control Using Graph Neural\nNetworks with Model-Based Reinforcement Learning",
  "abstract": "Multi-agent systems (MAS) play a crucial role in the advancement of machine\nintelligence and its applications. To explore complex interactions within MAS\nsettings, we introduce a novel \"GNN for MBRL\" model. This model employs a\nstate-space Graph Neural Network alongside Model-based Reinforcement Learning\nto tackle MAS tasks, such as Billiard-Avoidance and Autonomous Driving. The\nprocess involves using a GNN model to predict the future states and paths of several\nagents. Subsequently, a Model Predictive Control, enhanced by the Cross-Entropy\nMethod (CEM), is used to guide the ego-agent\u2019s action planning, facilitating\nsuccessful completion of MAS tasks.",
  "introduction": "1.1 Purpose\n\nVision-based approaches have been extensively researched in various reinforcement learning (RL)\nareas, including mastering video games directly from raw pixels, managing simulated autonomous\nvehicles using complex image observations, and carrying out robotic tasks like grasping using state\nrepresentations derived from complicated visual data. However, it has been shown that RL from\ncomplex observations such as raw pixels is time-consuming and needs a lot of samples. Additionally, it\nis widely acknowledged that learning policies from physical state-based characteristics is significantly\nmore effective and straightforward than learning from visual pixels. Therefore, this research focused\non learning control policies from states and exploring the use of a graph neural network (GNN)\ndynamics model to predict future states in multi-agent systems. We then utilized a Cross-Entropy\nMethod (CEM)-optimized model-based controller for motion planning of the ego-agent, which\nenabled successful execution of specific MAS missions. These include multi-billiard avoidance and\nself-driving car scenarios.\n\n1.2 Background\n\nInspired by a state-space model for videos that reasons about multiple objects and their positions,\nvelocities, and interactions, our project seeks to develop a \"GNN for MBRL\" model. This model is\nbased on a multi-billiard simulator for sample-efficient model-based control in MAS tasks involving\nmany interacting agents. Autonomous driving, a complicated multi-agent system, requires the\nego-agent to consider the situations of surrounding agents when conducting motion planning. The\ngym-carla can be used for further study in this context. We begin by developing and testing our\n\"GNN for MBRL\" model on a MAS billiard avoidance scenario to investigate the possibilities of\nGNNs and model-based RL. We aim to transfer the framework to real-world self-driving applications.\n\nGraph Neural Networks. GNNs have been proposed to create node and edge representations in\ngraph data, achieving remarkable success in applications such as recommendation systems, social\nnetwork prediction, and natural language processing. Recognizing the capabilities of GNNs in\nphysical systems, we can utilize GNN-based reasoning to represent objects as nodes and relations\nas edges, which allows for an effective approach to analyzing objects and relations. A successful\n\n.\n\n\ft zp(zt|zt\u22121)\n\nexample of a GNN is the state-space model, STOVE, which combines a GNN dynamics model for\ninference with an image reconstruction model to accelerate training and improve the unsupervised\nlearning of physical interactions. Thus, the state-space predictive model is the result of combining\nthese two components:\nzp(xt|zt\u22121) = zp(z0)zp(z1|z0) (cid:81)\nwhere x is the image observation and z represents object states. The latent positions and velocities of\nmultiple agents act as the connection between the two components. The model uses simple uniform\nand Gaussian distributions to initialize the states. The STOVE model is trained on video sequences\nby maximizing the evidence lower bound (ELBO). STOVE has also extended their video model\ninto reinforcement learning (RL) tasks for planning. Empirical evidence demonstrates that an actor\nusing Monte-Carlo tree search (MCTS) on top of STOVE is comparable to model-free techniques,\nsuch as Proximal Policy Optimization (PPO), while needing a fraction of the samples. Inspired by\nthese RL experiments, we apply the GNN model directly to states rather than complex visual data\nto improve sample efficiency and predict agents\u2019 future states. This is then combined with another\nmodel-based RL approach, such as Model Predictive Control (MPC). In the experiment, we train\nthe GNN dynamics model using ground truth states of video sequence data for multi-agent systems\ninstead of visual data.\n\nModel-based Reinforcement Learning. Model-based RL is considered a solution to the high\nsample complexity of model-free RL. This method typically includes two primary steps: (1) creating\na dynamics model that predicts future states based on present states and actions, and (2) using a\nplanning method to learn a global policy and act in the environment effectively. The STOVE model\nuses Monte-Carlo tree search (MCTS) to develop a policy based on the world model, which acts as a\nplanning simulator, and found that MCTS combined with STOVE could outperform the model-free\nPPO algorithm in a multi-billiards avoidance task.\n\n2 Method\n\n2.1 Framework\n\nThe \"GNN for MBRL\" method consists of two primary stages: (1) a GNN dynamics model training\nphase, using offline recorded video sequences or low-dimensional states for video prediction, and\n(2) a motion planning phase using CEM-based Model Predictive Control (MPC). This involves a\nfeedback control algorithm with a Cross-Entropy Method optimizer to interact with the billiard\nenvironment. The aim is to plan effective actions for the ego-agent in order to avoid collisions.\n\nThere are two different cases in the GNN dynamics training stage. The \"Action-conditioned case\"\nfollows the STOVE model-based control approach, training GNN with an object reconstruction model\non visual data. The \"Supervised RL case\" is designed for RL tasks directly on low-level states. Both\ncases involve training GNN dynamics models for predicting future multi-agent states. Subsequently,\nthe trained model is integrated into the model-based RL section to control the ego agent for motion\nplanning.\n\nAfter training, MPC uses a model to predict future outputs of a process. It handles multi-input multi-\noutput (MIMO) systems with constraints and incorporates future reference information to improve\nperformance. Therefore, we established a continuous version of the multi-billiard environment for\ndata collection. It is possible to combine the previously trained GNN model with MPC to assess if\nthis method can successfully address MAS tasks.\n\n2.2 Data Generation\n\nSTOVE proposed an object-aware physics prediction model based on billiard simulations, including\navoidance, billiards, gravity, and multi-billiards modes. We wrapped them into a gym environment\nstyle, \"gym-billiard,\" which can be easily used by Python API, aiding researchers in understanding\nthis system and creating efficient algorithms.\n\nOur project focuses on the avoidance billiard scenario, where the red ball is the ego-object and the\nRL agent controls it to avoid collisions. In STOVE, the ego-ball has nine actions: movement in eight\ndirections and staying at rest. A negative reward is given when the red ball hits another. We obtained\nthe avoidance sequences datasets using the \"generate_billiards_w_actions\" function. 1000 sequences\n\n2\n\n\fof length 100 for training and 300 sequences of length 100 for testing were generated using a random\naction selection policy. The pixel resolution was 32*32 with the ball mass set to 2.0.\n\nWe changed the environment to use continuous actions for agents, where the red ball is controlled by\n2-dimensional numpy values ranging in (-2, 2), representing the acceleration in x and y directions.\nSimilar to the discrete setting, continuous datasets were produced with random actions from a uniform\ndistribution within (-2, 2). These datasets included the image observations, actions, states, dones,\nand rewards. The average rewards for the continuous mode are lower, indicating more frequent\ninteractions between the balls.\n\nTable 1: Basic comparisons of the continuous and discrete datasets.\n\nData Mode Action space Actions dtype Average Rewards of training data Average Rewards of testing data\n\nDiscrete\nContinuous\n\n9\n2\n\nOne-hot mode\nNumpy array\n\n-17.276\n-18.93\n\n-16.383\n-18.71\n\n2.3 GNN Dynamics Model Training\n\nWe used supervised learning to train on ground-truth states rather than high-dimensional image data.\nThe aim is to improve sample efficiency, then combine the trained model with CEM-optimized MPC\nfor predicting future states. Two cases were trained on both Discrete and Continuous datasets: (1) the\nAction-conditioned case, which makes predictions based on state and action and predicts reward, and\n(2) the Supervised RL case, where real states including positions and velocities were used as the input\nfor GNN dynamics model. The model can learn to predict future states of multiple agents instead of\nfirst extracting the states from visual data with a separate model. Training was performed for 500\nepochs, and the model parameters were saved. Training time for the Supervised condition was less\nthan the Action-conditioned case. The GNN model could work on both action space 2 and 9 discrete\nactions without changing the GNN network architecture, resulting in a unified training framework.\n\n2.4 GNN with MBRL\n\nFollowing traditional Model-based RL, the trained GNN model was combined with CEM-optimized\nMPC to assess performance on the continuous gym-billiard avoidance task. The saved GNN model\npredicts future states, and the MPC searches for optimal actions for the ego-agent. The search is\nrepeated after each step to account for prediction errors and rewards, incorporating feedback from the\nenvironment. We also conducted experiments on discrete datasets using MCTS and saved videos.\nFor the continuous case, the GNN dynamics model was combined with CEM optimized MPC and\ncompared with random and ground truth scenarios.",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "3.1 GNN Training Results\n\nThe datasets generated from the gym-billiard API environment, including image observations, actions,\nstates, dones, and rewards, were stored in pickle files. GNN dynamics models were trained in two\nconditions: (1) Action-conditioned case, which used video sequences with a visual reconstruction\nmodel, and (2) Supervised RL case, which used real states as input for the GNN dynamics model.\nBoth conditions were trained for 500 epochs. The Supervised condition took less time to train than\nthe Action-conditioned case. A notable finding is that the GNN model worked equally well for both\naction space 2 and 9 discrete actions without changing the original GNN architecture. This allows for\nunified training for both the Discrete and Continuous billiard avoidance environments. After training,\nmodel parameters were saved in the \"checkpoints\" folder. \"gifs\" folder stores the videos, and \"states\"\ncontains state and reward files.\n\nIn the Action-conditioned case, the reward MSE loss decreases for both continuous and discrete\nconditions. However, the continuous reward error decreased from 0.48 to 0, while the discrete one\ndropped from 0.16 to 0. The ELBO increased significantly from 450 to 3600. Position and velocity\nprediction errors decreased during training. The continuous position error was close to the discrete,\n\n3\n\n\fbut the velocity error showed a greater difference. The continuous V_error dropped from 0.65 to 0.05,\nwhile the discrete one decreased from 0.07 to 0.01. The four metrics met the criteria for a reasonable\nGNN dynamics model for the subsequent RL task. In the Supervised RL case, the model directly\ninputs the ground truth states and actions for GNN training to predict future states. Reconstruction\nerrors were always zero since no image reconstruction was used on the true states. The discrete case\nshowed a better performance compared to the continuous case with respect to the \"Prediction_error\".\nThe continuous loss remained stable for \u201cTotal_error\u201d, while the discrete loss showed a downtrend\nbefore stabilizing. Generated rollout videos indicated that the ego-red ball performed reasonably\nwell in avoiding collisions. Thus, the trained Supervised RL model can be used for the following\nmodel-based RL phase.\n\n3.2 GNN with MBRL\n\nIn the \"Model-based Control\" framework, we used MCTS on discrete datasets to generate qualitative\nvideos. We changed the mass of the ball agents to 1.0 and 2.0 and trained two GNN dynamics models.\nDuring MCTS, the GNN model predicted future sequences for 100 parallel environments with the\nlength of 100, using a maximal rollout depth of 10. We then calculated the mean collision rate and\nsaved 100 videos to show the ego-ball\u2019s interaction with other agents, which demonstrates improved\ncollision avoidance and lower collision rates.\n\nFor the continuous datasets, we combined the trained GNN model into the CEM optimized MPC\nmethod and compared it with random and ground truth cases. The GNN model made accurate\npredictions based on the current states by checking the code, changing the cuda device and data type.\nWe computed the \"reward,\" the average collisions per epoch, for each method.\n\nTable 2: Reward (Collision Rate) for two envs with different baselines.\n\nEnvs Epochs Horizons GNN_MPC\n\nRandom\n\nGround_truth\n\nm=1\nm=1\nm=2\nm=2\n\n100\n50\n100\n50\n\n50\n100\n50\n100\n\n0.0558+0.0012\n0.0565+0.0008\n0.0648+0.001\n0.0455+0.0008\n\n0.2790+0.025\n0.3543+0.0445\n0.2420+0.0178\n0.2690+0.0350\n\n0.0707+0.066\n0.0408+0.0392\n0.0505+0.0480\n0.0612+0.0575\n\nThe \"random\" case used randomly generated actions, while \"ground_truth\" used the true interaction\nenvironment for generating next states. The \"m=1\" version task differed slightly from \"m=2\" as the\n\"m=1\" model was trained on the old continuous datasets, making the red ball movement less flexible.\nThe collision rates in \"GNN_MPC\" were lower than \"Random\" and close to \"ground_truth\". The\nperformance of our proposed method was better than random cases, and the results of \"GNN_MPC\"\nwere close to the \"Ground_truth\" case, which indicated that the trained GNN dynamics model predicts\nthe future states of multi-object systems as well as the ground truth interactive environment.\n\n4 Conclusions\n\nWe introduced the \"GNN for MBRL\" concept, combining a graph neural network (GNN) dynamics\nmodel with CEM-optimized Model Predictive Control (MPC) on a gym-billiard avoidance MAS task.\nWe also conducted experiments on the \"Action-conditioned\" case with MCTS using discrete datasets\nand explored the \"Supervised RL\" GNN dynamics model with CEM-optimized MPC on continuous\ndatasets. The proposed model predicted video sequences well and controlled the ego-agent to address\nRL tasks, which may be applied to complex multi-agent systems like the gym-carla autonomous\ndriving environment.\n\n4",
  "conclusion": "",
  "is_publishable": 1,
  "venue": NaN
}
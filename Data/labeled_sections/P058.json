{
  "title": "Enhanced Vocabulary Handling in Recurrent Neural Networks\nThrough Positional Encoding",
  "abstract": "This research presents a counterintuitive discovery: positional encoding, a high-dimensional representation of\ntemporal indices, improves the learning capabilities of recurrent neural networks (RNNs). While positional encod-\ning is well-known for its crucial role in enabling Transformer networks to process sequential data, its application\nto RNNs, which inherently manage temporal information, seems unnecessary. However, our experiments with\nsynthetic benchmarks demonstrate that incorporating positional encoding into RNNs enhances their performance,\nparticularly when dealing with extensive vocabularies that result in numerous low-frequency tokens. A detailed\nanalysis reveals that these infrequent tokens introduce instability to the gradients of standard RNNs, and positional\nencoding effectively counteracts this instability. These findings highlight a previously unrecognized benefit of\npositional encoding, extending its utility beyond its conventional function as a temporal marker for Transformers.",
  "introduction": "Since their introduction, Transformer neural networks have become the preferred method for processing and generating time series\ndata, surpassing traditional models like recurrent neural networks (RNNs). A significant distinction between these two types of\nmodels lies in their approach to encoding temporal information, which refers to the sequence of individual data points, or tokens,\nwithin the time series. RNNs encode this information by sequentially updating their internal state based on both the current input\nand the preceding state. Conversely, Transformers do not inherently possess a mechanism to represent the order of data points; thus,\nthey depend on an external system known as positional encoding to provide this temporal context.\n\nPositional encoding offers a high-dimensional representation of the temporal indices associated with input data. Its most common\nimplementation involves the use of sinusoidal waves with predetermined frequencies. This method \"timestamps\" input tokens\nby adding or concatenating these encoding vectors to the corresponding input embeddings. In contrast to RNNs, the temporal\nrepresentation provided by positional encoding remains unchanged by input values until processed collectively by a network.\n\nAlthough positional encoding has often been viewed as a substitute for the temporal processing capabilities of RNNs when used\nwith Transformers, the two are not inherently incompatible. Inputs to RNNs can be augmented with position-encoding vectors,\ndespite this appearing redundant. The presence of autonomous activities in biological neurons, like neural oscillations, is believed to\nbe significant in time perception and other perceptual processes, as well as in motor control.\n\nThis study, therefore, investigates the application of positional encoding to the inputs of RNNs using synthetic benchmarks. The\nresults demonstrate that positional encoding helps RNNs manage a more diverse set of discrete inputs, effectively handling a larger\nvocabulary, compared to those without positional encoding.\n\nThe contributions of this research are outlined as follows:\n\n\u2022 Challenges in training RNNs with extensive vocabularies are shown through carefully designed benchmark tasks. This\nissue, despite its potential implications for practical applications, has not been previously identified or has received minimal\nattention.\n\n\u2022 The identified training challenges for RNNs with large vocabularies are explained by gradient instability caused by\n\ninfrequent tokens, which are inevitable when expanding vocabulary size.\n\n\u2022 A new effectiveness of positional encoding is revealed by combining it with RNNs, showing it mitigates the large-vocabulary\n\nissue by stabilizing RNN gradients against the disruptions caused by infrequent tokens.\n\n\f2 Related Studies\n\n2.1 Theoretical and Empirical Computational Power of (Vanilla) RNNs\n\nMathematically, RNNs are recognized as Turing-complete, meaning they can simulate any Turing machine if their weights have\nunlimited precision and are perfectly tuned. Even RNNs with random recurrent and input-to-hidden weights, known as reservoir\ncomputers, can achieve universal approximation if their hidden-to-output weights are idealized. These theoretical insights have\ndriven the use of RNNs in processing complex time series like human languages and weather patterns.\n\nHowever, in practical scenarios, RNN weights are limited by finite precision and must be optimized based on a finite set of data\nobservations. These constraints place limitations on the actual capabilities of RNNs. For instance, empirical RNNs cannot store an\ninfinite number of observations in their memory, and the stored information degrades over time. This issue of memory duration has\nbeen a focal point for researchers, leading to extensive exploration of RNN architectures that can retain memory for longer periods.\n\nMore recently, the focus of research on extending memory retention has moved towards continuous-time models. Instead of\nrepresenting the memory of an input sequence through discrete-time changes in a latent state, these models approximate the input\nhistory using a linear combination of orthogonal polynomials in continuous-time space. The coefficients of these polynomials\nprovide a finite-dimensional representation of the input sequence, known as the High-Order Polynomial Projection Operator (HiPPO),\nand the dynamics of these coefficients can be described by an ordinary differential equation (ODE). This concept of continuous-time\nmemory representation has been further developed into neural state-space models by replacing the fixed state matrix in the ODE\nwith a learnable one, while restricting its structure to a diagonal matrix plus a row-rank matrix. Notably, with further refinements,\nthe latest state-space model has achieved language modeling performance that rivals that of Transformer-based models.\n\n2.2 Positional Encoding\n\nPositional encoding serves as a high-dimensional representation of the temporal structures present in input data. The primary need\nfor this type of representation arises from Transformers, which, unlike RNNs, do not have an inherent mechanism for representing\nthe order of inputs. Consequently, input tokens to a Transformer are \"time-stamped\" by adding or concatenating a position-encoding\nvector.\n\nIn the initial implementation of the Transformer, token positions were encoded using sinusoidal waves of various predefined\nfrequencies. While this original encoding method is effective for a wide range of tasks, researchers have also explored other\npossibilities. For instance, the well-known BERT pretraining for natural language processing used learnable embeddings to encode\ntoken positions. Some research has also indicated that combining sinusoidal and learnable encoding can enhance model performance.\nAnother approach involves encoding the distance between tokens rather than the time elapsed since the beginning of the sequence.\n\nBeyond Transformers, positional encoding is utilized to represent elapsed time in diffusion processes. Furthermore, the effec-\ntiveness of positional encoding is not restricted to temporal information; previous studies in three-dimensional mesh/point-cloud\nmodeling have shown that sinusoidal transformation of spatial data improves model performance compared to using raw coordinate\nrepresentations.\n\nDespite the extensive use of positional encoding across various areas of machine learning, its application to pure RNNs remains\nlargely unexplored. To the author\u2019s knowledge, only two studies have previously investigated position-encoded RNNs. Karanikolos\nand Refanidis (2019) found that a position-encoded LSTM outperformed a standard LSTM as well as a shallow Transformer in\ntext summarization tasks. In another study, which predates the introduction of sinusoidal positional encoding in the deep learning\ncommunity, Vincent-Lamarre et al. (2016) demonstrated that oscillatory signals at random frequencies enhanced the performance\nof a random RNN (i.e., reservoir computer) in a timing task, evaluating the model\u2019s memory duration by its ability to generate a\nsmoothed output pulse after a specific time interval from an onset signal.\n\nSimilarly, the time index in time series data has rarely been directly used by RNNs, likely due to its perceived redundancy alongside\nthe functionality of RNNs. As an exception, Neil et al. (2016) introduced a periodic gating mechanism for updating the state and\nmemory cell of LSTM. This periodic gating was scheduled based on a triangular wave interspersed with a plateau at the floor value\n(= 0.0; the frequency, phase, and duration of the wave phase were learnable parameters).\n\n3 Methods\n\n3.1 Task\n\nThe impact of positional encoding on RNNs was examined using a reverse-ordering task. In this task, RNNs were trained to\nreconstruct a sequence of random integers in reverse order.\n\n3.2 Model Architecture\n\nThe research in this study was based on single-layer gated recurrent units (GRUs), long short-term memory (LSTMs), and a neural\nstate-space model, S4D (S4 with a diagonal state matrix). Each integer in the input sequences was first embedded, then concatenated\n\n2\n\n\fwith the positional encoding, and subsequently fed into the RNN/S4D. After processing the entire input sequence, the network\nreceived a command to produce the output. This command was represented by a time-invariant learnable vector and was fed to the\nRNN in place of the input embedding. The outputs from the RNN/S4D module were linearly projected into classification logits. The\ncross-entropy loss between these logits and the target sequence was used to optimize the entire network. Model predictions during\nthe testing phase were determined by the argmax of these logits for each time step.\n\nThis study used the standard sinusoidal positional encoding designed for Transformers. Specifically, each time step t was encoded by\nthe Dpos-dimensional vector, defined as follows:\n\nP Et, 2i := sin\n\nP Et, 2i + 1 := cos\n\n(cid:19)\n\n(cid:19)\n\n(cid:18)\n\n(cid:18)\n\nt \u2212 1\n\n10000\n\n2(i\u22121)\nDpos\n\nt \u2212 1\n\n10000\n\n2(i\u22121)\nDpos\n\n(1)\n\n(2)\n\nFor learning stability, the positional encoding was divided by the square root of Dpos/2, ensuring that the encoding vectors had a\nunit L2-norm. The time step t incremented throughout both the input and output phases (i.e., t = 1, ..., L, L+1, ..., 2L, where L is the\ninput length), without any hard-coded association between the input and output positions.\n\n3.3\n\nImplementation Details\n\nAcross the experiments, the dimensionality of the hidden layer of the RNNs was set to 512. The embedding of the input integers and\nthe memory cell of the LSTM also had the same dimensionality of 512. Similarly, the hidden dimensionality of S4D was set to 512,\nwhile its state size (or the order of the Legendre polynomials) was maintained at the default value of 64.\nThe models were trained for 300,000 iterations using the Adam optimizer with parameters (\u02d803b21, \u02d803b22) := (0.9, 0.999) and no\nweight decay. The learning rate was linearly warmed up from 0.0 to 0.001 for the first 1,000 iterations, and then annealed according\nto the cosine schedule. The batch size was 512.\n\nAll experiments were implemented in PyTorch (ver. 2.1.1) and each training-test trial was executed on a single NVIDIA A100 GPU\n(with 80GB VRAM).",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "4.1 Key Findings\n\nPositional encoding was found to enhance the ability of RNNs to manage a larger vocabulary in the reverse-ordering task. The\nposition-encoded GRU and LSTM successfully reversed input sequences of 64 integers drawn uniformly at random from vocabularies\nof sizes 32-256 and 256-16,384, respectively, achieving token-wise accuracy above 95%. In contrast, the performance of the standard\nmodels without positional encoding deteriorated as the vocabulary size increased. Similarly, positional encoding improved the\ncapacity of S4D to handle large vocabularies. These improvements are also evident in the reduced sequence-wise reconstruction\nerrors, as measured by the Damerau-Levenshtein distance. Neither additional training iterations nor larger batch sizes improved the\nperformance of the standard models.\n\n4.2 Frequency Matters\n\nThe most noticeable effect of increasing the vocabulary size was the decreased probability of observing individual vocabulary\nitems. Therefore, additional experiments were conducted with non-uniformly distributed tokens to examine the relationship between\ntoken frequency and RNN performance. Specifically, the input vocabulary was evenly divided into Frequent and Rare groups, with\nFrequent tokens having three times the probability of Rare tokens. The probability of each Frequent token was 7/8 \u02d800d7 2/K (where\nK is the total vocabulary size, set to 64, 1024, and 2048 for GRU, LSTM, and S4D, respectively), while the probability of each Rare\ntoken was 1/8 \u02d800d7 2/K.\n\nThe training data consisted of 64 independent samples from this dual-frequency vocabulary. The test data were systematically\nconstructed so that each sequence included a single \"target\" token (Frequent/Rare) whose retrieval accuracy was assessed, along\nwith 63 \"disturbants\" that were either all Frequent or all Rare. The experiment revealed that the frequency of the disturbant tokens\nsignificantly affected the performance of the standard RNNs and S4D. Rare targets were successfully retrieved as long as they were\nsurrounded by Frequent disturbants. However, the standard GRU struggled to recover Frequent targets when the other input tokens\nwere filled with Rare disturbants. LSTM performance also degraded, especially when targets were positioned in the first quarter of\nthe input sequence (1 \u02d82264 t \u02d82264 16). Similarly, Rare disturbants were detrimental to S4D; unlike the RNNs, the accuracy was\nlowest when targets were located in the middle of the input sequences (17 \u02d82264 t \u02d82264 32).\n\n3\n\n\fIn contrast, the position-encoded RNNs showed robustness to the frequency of both target and disturbant tokens. They achieved\nnearly perfect accuracies in most cases, except when the GRU processed fully Rare data with the target in the first half of the\nsequence (1 \u02d82264 t \u02d82264 32). Likewise, positional encoding enhanced the resilience of S4D against the influence of Rare disturbants.\n\n4.3 Analysis of Gradient Stability\n\nTo further investigate the influence of token frequency on RNN performance, the gradients of the RNN latent states were analyzed.\nPairs of input sequences were processed by RNNs trained on the dual-frequency vocabulary. Each pair shared the same initial token\n(t = 1; \"target\") but varied in subsequent tokens (2 \u02d82264 t \u02d82264 L; \"disturbants\"). Gradients were then computed for the distant\nmapping between the first and last updated states (at t = 1 and 2L) of the RNNs using backpropagation through time. The stability\nof RNN learning was assessed by measuring the dot-product similarity of the gradients between the paired input sequences (after\nnormalization over output dimensions).\n\nFormally, the paired input sequences, denoted as A and B, established two distinct, but ideally similar mappings, f(A) and f(B), from\nthe first to the last latent state of the RNNs. The gradient stability of the RNNs was defined by the dot-product similarities between\nthe normalized gradients of these paired mappings:\n\nStability(A, B) :=\n\nD\n(cid:88)\n\n\u27e8\u03b1(A)\n\ni \u2207f (A)\n\ni\n\ni=1\n\n(\u20d7z1), \u03b1(B)\n\ni \u2207f (B)\n\ni\n\n(\u20d7z1)\u27e9 =\n\nD\n(cid:88)\n\ni=1\n\ni \u03b1(B)\n\u03b1(A)\n\ni\n\n\uf8eb\n\n\uf8ed\n\n2D\n(cid:88)\n\nj=1\n\n\u2202h(A)\n2L,i\n\u2202z1,j\n\n\u2202h(B)\n2L,i\n\u2202z1,j\n\n\u00b7\n\n\uf8f6\n\n\uf8f8\n\nwhere the coefficients \u02d803b1(s) i normalized the raw gradients \u02d82207f (s) i ( z1) over the output dimensions i := 1, . . . , D:\n\n(cid:118)\n(cid:117)\n(cid:117)\n(cid:117)\n(cid:116)\n\n\u03b1(s)\ni\n\n:=\n\n\uf8eb\n\n\uf8ed\n\n2D\n(cid:88)\n\nj=1\n\n(cid:32) \u2202h(s)\n2L,i\n\u2202z1,j\n\n(cid:44)\n\n(cid:33)2\uf8f6\n\uf8f8\n\n(cid:118)\n(cid:117)\n(cid:117)\n(cid:117)\n(cid:116)\n\n\uf8eb\n\n\uf8ed\n\nD\n(cid:88)\n\n\uf8eb\n\n\uf8ed\n\n2D\n(cid:88)\n\nk=1\n\nj=1\n\n(cid:32) \u2202h(s)\n2L,k\n\u2202z1,j\n\n(cid:33)2\uf8f6\n\uf8f8\n\n\uf8f6\n\n\uf8f8\n\n(3)\n\n(4)\n\nConsequently, the stability metric emphasizes the consistency of the paired gradients that both have a greater L2-norm across the\noutput dimensions.\n\nIt is important to note that the mapping from the first to the last RNN state was conditioned on the disturbant tokens occurring at 2\n\u02d82264 t \u02d82264 L. Nevertheless, the reverse-ordering task trained the networks to retrieve the initial token as their final output regardless\nof the intervening tokens. Thus, a well-trained RNN would maintain invariance in its final state over the disturbants. Conversely,\nconsistent gradient directions across varied disturbants would lead to successful learning, which is the premise of the proposed\nanalysis.\n\nUnlike the RNN models, both the standard and position-encoded S4Ds achieved high accuracy over 96% for the initial target token\n(t = 1), regardless of the frequency of the target and disturbants. Therefore, for the analysis of S4D, the target token was positioned\nin the middle at t = 23, where the standard model exhibited its poorest accuracy with Rare disturbants. The disturbants were prefixed\nand suffixed to this target to construct input sequences. The prefix disturbants were shared between the paired sequences, ensuring\nthat the latent dynamics of the model remained identical up to the target token.\n\nIt should also be noted that the latent states of S4D are complex-valued (while its outputs are real-valued), and consequently, the\ngradients and their dot-product similarities are also complex-valued. For this analysis, the complex-valued gradients were treated as\ndouble-sized real arrays, and a real-valued similarity was defined by Eq. 3. This is equivalent to taking the real component of the\ncomplex-valued similarity and is intuitively natural given that a perfect alignment between complex gradient directions yields a\nreal-valued score of 1.0. Additionally, the extra dimension in the latent states representing the order of the Legendre polynomials\nwas merged with the channel dimension, and the entire state was treated as a flattened vector.\n\nMonitoring the gradients at training checkpoints revealed that Rare disturbants destabilize the learning of standard RNNs. The\nsimilarity of the paired gradients decreased gradually (GRU) or rapidly (LSTM) when the networks were exposed to Rare disturbants.\n\nMost notably, positional encoding endowed the RNNs with robustness to these Rare disturbants. Both the GRU and LSTM\nmaintained high similarity of the paired gradients across different target/disturbant conditions. In contrast, the impact of positional\nencoding on the gradient stability of S4D was marginal; unlike the RNNs, the standard S4D was highly stable by itself against Rare\ndisturbants throughout training, although there was a visible relative destabilization due to Rare disturbants compared to Frequent\ndisturbants in the early stages of training, as well as an observable improvement by positional encoding. It is also noteworthy that\nthe difference between Frequent and Rare disturbants diminished after 10,000 training iterations. Consequently, gradient stability\ndoes not fully account for the decline in S4D accuracy in the presence of Rare disturbants, nor does it explain the enhancement\nbrought about by positional encoding.\n\n4\n\n\f5 Discussion\n\n5.1 Difficulties in Handling a Large Vocabulary\n\nThis study introduced a novel challenge in training standard RNNs: large vocabularies. While investigating the manageable\nvocabulary size of RNNs appears to be a relevant research area, crucial for practical applications like natural language processing,\nprevious studies have primarily focused on evaluating and improving the memory duration of RNNs, typically setting the vocabulary\nsize to a small value (= 8).\n\nThis research examined RNN gradients and identified their destabilization when processing low-frequency tokens, which are\nnecessarily included in a large vocabulary. Specifically, inputs that do not contribute to gradient-based optimization at a target time\nstep (e.g., tokens at 2 \u02d82264 t \u02d82264 L upon the retrieval of the initial token at t = 2L in the reverse-ordering task) were found to be\ndetrimental.\n\nIn general time series processing, data points carrying crucial information for specific time steps become irrelevant otherwise.\nConsequently, each token exhibits a dual nature\u2014both crucial and noisy\u2014throughout the task. Processing rare tokens is particularly\nchallenging, presumably because they are irrelevant most of the time while making a large impact on learning through the greater\nloss to compensate for their fewer learning opportunities. Dealing with such \"unignorable noise\" presents a pervasive challenge for\nRNNs.\n\n5.2 Functionality of Positional Encoding beyond the Timekeeper for Transformers\n\nAlthough low-frequency tokens destabilize the gradient-based learning of RNNs, this study also discovered that this issue can be\nalleviated by positional encoding. This enhancement of RNNs via positional encoding is noteworthy because RNNs were specifically\ndesigned to process time series data on their own; hence, unlike Transformers, they are presumed to function without relying on an\n\"external clock\". Consequently, position-encoded RNNs have remained largely unexplored, with only two exceptions to the best of\nthe author\u2019s knowledge. The findings of this study\u2014namely, the improvement in the manageable vocabulary size due to enhanced\ngradient stability\u2014broaden the currently limited understanding of the impact of positional encoding on RNNs.\n\nAdditionally, the results of this study shed new light on the utility of positional encoding. While positional encoding has been viewed\nas nothing more than input timestamps for Transformers, this study demonstrated its effectiveness in stabilizing the gradients of\nRNNs against disruption by low-frequency tokens. This novel functionality of positional encoding would not have been visible in\nTransformer studies, as the model can dynamically adjust the relevance of input tokens through their attention mechanism and thus\ninherently mitigate the impact of disturbant tokens.\n\n5.3 Limitations and Future Directions\n\nA primary unresolved question in this study pertains to the mechanism behind the gradient stabilization by positional encoding. All\nfindings here are based on experimental investigations, lacking rigorous mathematical explanations for how and why the gradients of\nRNNs are destabilized by infrequent tokens and stabilized by positional encoding. Moreover, this study primarily focused on the\ncanonical implementation of sinusoidal positional encoding designed for Transformers (Eqs. 1, 2), leaving it open which parameters\nof the sinusoidal waves (i.e., frequencies and phases) are critical for gradient stabilization. Future research may broaden its scope to\nencompass more general forms of positional encoding, such as wavelets and non-periodic signals.\n\nMoreover, the analysis of gradient stability did not fully address the enhanced performance of the position-encoded state-space\nmodel (S4D). In terms of accuracy, the positioned-encoded S4D exhibited greater robustness to infrequent tokens compared to the\nstandard model, resembling the behavior observed in RNNs. However, the gradients of the standard S4D were too stable to account\nfor this decline in performance. This leaves open the question of how positional encoding influences gradient-based learning of\nstate-space models. Additionally, future studies may investigate a broader range of state-space models\u2014including the state-of-the-art\narchitecture of Mamba\u2014to achieve a comprehensive understanding of the interplay between positional encoding and these models.\n\nIn addition to these scientifically oriented questions, future studies could also address practical applications of position-encoded\nRNNs and neural state-space models. Although positional encoding enhanced model performance across different synthetic tasks,\nthe extent of this enhancement is task-dependent. Indeed, while a previous study reported the effectiveness of positional encoding\nfor an LSTM text summarizer, the present study found no empirical advantage for the language modeling task, aside from a slightly\nmore rapid decline in training loss. Thus, positional encoding is not a panacea for arbitrary tasks, and further investigations are\nnecessary to determine when it is effective.\n\n6 Appendix\n\n6.1 A Other Tasks\n\nThis section demonstrates the effectiveness of positional encoding on RNNs across different tasks, besides the reverse ordering task\ndiscussed in the main text.\n\n5\n\n\f6.1.1 A.1 Reverse-Ordering + Delayed-Addition\n\nThis section reports the performance of position-encoded RNNs on a more complicated, combinatorial task than the reverse ordering\nof input sequences. Extending the reverse-ordering task, the models received additional random input integers during the output\nphase, and added each of them to the corresponding token in the reverse-ordered input sequence (modulo the vocabulary size, so that\nthe output range was bounded).\n\nThis task was too challenging to GRUs\u2014even after reducing the input length to L = 16\u2014so only the results from LSTMs are reported\nbelow. Also, the network was trained for 600,000 iterations (i.e., twice longer than the other tasks) for ensuring the convergence.\nThe other conditions/hyperparameters were the same as reported in the main text.\n\nConsequently, positional encoding improved the model performance as the vocabulary size grew from 896 to 1088.\n\n6.1.2 A.2 Sorting\n\nIn the reverse ordering task, the order of input integers was important information for accomplishing the task. Thus, positional\nencoding may play its originally intended role in encoding the temporal information.\n\nThis section reports the effectiveness of positional encoding for a task in which the order of input observations was completely\nirrelevant; the learning objective was to simply sort the input integers in their inherent ascending order (e.g. 8, 29, 2, 11 \u02d82192 2, 8,\n11, 29). The input integers were uniformly randomly sampled with replacement, allowing for ties in the sorting process.\n\nAs a result, positional encoding also proved effective for RNNs to handle a larger vocabulary in the sorting task, though the\nimprovement remained marginal compared to the reverse-ordering task.\n\n6.1.3 A.3 Predecessor Query\n\nFinally, this section presents benchmark results for the predecessor-query task. The network first received a sequence of non-repeating\nrandom integers, x1, . . . , xL. Subsequently, one of the non-initial input integers, xtquery (2 \u02d82264 tquery \u02d82264 L), was randomly\nselected and reintroduced to the network at time t = L + 1. The learning objective is to return the predecessor of the reviewed integer\n(= xtquery\u02d822121). The predecessor-query task evaluates the capacity of RNNs to integrate information regarding both the order and\ncontent of input sequences.\n\nAs in the reverse-ordering + delayed-addition task, the input sequence was reduced to L = 16 due to the complexity of the task, and\nthe experiment focused on the LSTM. The number of training iterations was maintained at 300,000. Similar to the other benchmarks,\npositional encoding improved the LSTM\u2019s capacity to manage the larger vocabularies.\n\n6.2 B Robustness to Variations in Input Length\n\nSo far, all the tasks were experimented using fixed-length inputs (L = 64). One might wonder if positional encoding is exceptionally\neffective under this setting, informing RNNs with the exact timing when each input token should be returned as the output. Thus, it\nremains unclear whether or not position-encoded RNNs can also handle a larger vocabulary even when the input length is variable\nand, thus, the exact timing of the output emission is not identifiable from the positional encoding attached to the inputs.\n\nTo assess the robustness to variations in the input length, an additional experiment was conducted on the LSTM, with the input\nlength varied between 32 and 64. In this setup, the maximum input length (= 64) covers the entirety of the shortest input sequence\nplus its reversed reconstruction (= 32 + 32). Consequently, the positional encoding per se cannot even distinguish the input vs.\noutput phases at t = 33, . . . , 64. The vocabulary size was set to 16,384.\n\nAs a result, the positional encoding still improved the LSTM\u2019s performance on the reverse-ordering task against the perturbations in\nthe input length. This result suggests that the effectiveness of the positional encoding for RNNs is not limited to strictly scheduled\ntasks.\n\n6.3 C Effects of Additional Parameters in Position-Encoded RNNs\n\nThe concatenation of positional encoding with input embeddings inflates the number of learnable parameters in the input-to-hidden\nprojection weights. This additional parameterization per se does not influence the learning of the input embeddings, and therefore\ndoes not elucidate the enhanced performance of position-encoded RNNs. This section substantiates this argument by equalizing the\nnumber of learnable parameters between the standard and position-encoded models.\n\nSpecifically, the equalization was achieved by concatenating two identical copies of the input embeddings and feeding them to the\nLSTM. This configuration\u2014henceforth termed \"double standard\"\u2014effectively doubled the size of the input- to-hidden weight for\neach gate in the LSTM, aligning it with that of the position-encoded LSTM, while maintaining all other parameters, including the\ndimensionality of the (non-repeated) input embeddings.\n\nThe double standard LSTM did not yield any improvements in the reverse-ordering or sort- ing tasks. These results affirm that the\nreported enhancement of RNNs is not merely attributable to the additional parameterization associated with the positional encoding.\n\n6\n\n\f6.4 D Alternative Implementations of Positional Encoding\n\nWhile this study implemented positional encoding by sinusoidal waves, there are alternative implementations proposed in the\nprevious studies. For instance, the BERT-based models typically encode each token position by a learnable embedding. Moreover, it\nhas been pointed out that even random vectors can function as positional encoding.\n\nAccordingly, these two alternative forms of positional encoding were tested on the LSTM performing the reverse- ordering task.\nThe random position-encoding vectors were uniformly and independently sampled from the (512 - 1)- dimensional hypersphere.\nThe learnable embeddings were implemented using the canonical embedding module of PyTorch (torch.nn.Embedding). The input\nlength and vocabulary size were set to 64 and 16,384 respectively. Both the random vectors and learnable embeddings improved the\nperformance of LSTM.\n\nAmong the different implementations of positional encoding, the sinusoidal encoding outperformed the two alterna- tives. The\nadvantage of the sinusoidal encoding became more apparent when the input length was variable between 32 and 64; the sinusoidal\nencoding was more robust to the variations in the input length than the others.\n\n6.5 E Language Modeling\n\nThis section reports benchmark results for the language modeling task. Single-layer LSTMs with and without sinusoidal positional\nencoding were trained and tested on the WikiText-103 dataset. Due to constraints in computational resources, the vocabulary was\nreduced from the original size of 267,735 to 32,768 by retokenizing the raw data using SentencePiece. The headings were removed,\nand the main text was segmented by paragraphs (separated by the line break). Additionally, only the first 1024 tokens of each\nparagraph were utilized for training and testing, ensuring that the absolute positional encoding always aligned with the beginning of\neach paragraph. The hyperparameters were configured as specified in Section 3.3.\n\nPositional encoding proved effective only for marginally faster learning during the initial phase of training. The difference diminished\naround 10,000/30,000 iterations, and the test perplexities of the position-encoded model were inferior to those of the standard model.\n\nTable 1: Test perplexities on the WikiText-103 dataset. The minimum, mean, and maximum are obtained from five trials with\ndifferent random seeds.\n\nModel\n\nMin\n\nMean\n\nMax\n\nVanilla LSTM\n36.8257\nPosition-Encoded LSTM 38.0685\n\n37.7731\n38.5384\n\n38.916589\n38.893656\n\n7",
  "conclusion": "",
  "is_publishable": 1,
  "venue": NaN
}
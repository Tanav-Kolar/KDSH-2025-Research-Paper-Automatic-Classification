{
  "title": "Detecting and Summarizing Video Highlights with\nLag-Calibration",
  "abstract": "The increasing popularity of video sharing has led to a growing need for automatic\nvideo analysis, including highlight detection. Emerging platforms that feature\ncrowdsourced, time-synchronized video comments offer a valuable resource for\nidentifying video highlights. However, this task presents several challenges: (1)\ntime-synchronized comments often lag behind their corresponding shots; (2) these\ncomments are frequently sparse and contain noise semantically; and (3) determining\nwhich shots constitute highlights is inherently subjective. This paper introduces\na novel framework designed to address these challenges. The proposed method\nuses concept-mapped lexical chains to calibrate the lag in comments, models\nvideo highlights based on comment intensity and the combined concentration\nof emotion and concept within each shot, and summarizes detected highlights\nusing an enhanced SumBasic algorithm that incorporates emotion and concept\nmapping. Experiments conducted on extensive real-world datasets demonstrate\nthat our highlight detection and summarization methods substantially outperform\nexisting benchmark techniques.",
  "introduction": "Billions of hours of video content are viewed daily on platforms like YouTube, with mobile devices\naccounting for half of these views. This surge in video sharing has intensified the demand for efficient\nvideo analysis. Consider a scenario where a user wishes to quickly grasp the essence of a lengthy\nvideo without manually navigating through it. Automatically generated highlights would enable\nusers to digest the video\u2019s key moments in a matter of minutes, aiding their decision on whether to\nwatch the full video later. Furthermore, automated video highlight detection and summarization can\nsignificantly enhance video indexing, search, and recommendation systems.\n\nHowever, extracting highlights from a video is a complex task. Firstly, the perception of a \"highlight\"\ncan vary significantly among individuals. Secondly, analyzing low-level features such as image, audio,\nand motion may not always capture the essence of a highlight. The absence of high-level semantic\ninformation poses a significant limitation to highlight detection in conventional video processing.\n\nThe recent emergence of crowdsourced, time-synchronized video comments, also known as \"bullet-\nscreen comments,\" presents a new avenue for highlight detection. These real-time comments, which\nappear overlaid on the video screen, are synchronized with the video frames. This phenomenon has\ngained widespread popularity on platforms like niconico in Japan, Bilibili and Acfun in China, and\nYouTube Live and Twitch Live in the USA. The prevalence of time-synchronized comments offers a\nunique opportunity for leveraging natural language processing in video highlight detection.\n\nNevertheless, using time-synchronized comments for highlight detection and labeling still poses\nsignificant challenges. Primarily, there is an almost unavoidable delay between comments and their\ncorresponding shots. As illustrated in Figure 1, discussions about a particular shot may continue\ninto subsequent shots. Highlight detection and labeling without accounting for this lag may yield\ninaccurate outcomes. Secondly, time-synchronized comments are often semantically sparse, both\nin terms of the number of comments per shot and the number of words per comment. This sparsity\n\n\fcan hinder the performance of traditional bag-of-words statistical models. Thirdly, determining\nhighlights in an unsupervised manner, without prior knowledge, involves considerable uncertainty.\nThe defining characteristics of highlights must be clearly defined, captured, and modeled to ensure\naccurate detection.\n\nTo our knowledge, limited research has focused on unsupervised highlight detection and labeling\nusing time-synchronized comments. The most relevant work in this area proposes detecting highlights\nbased on the topic concentration derived from semantic vectors of bullet-comments, and labeling each\nhighlight using a pre-trained classifier based on predefined tags. However, we contend that emotion\nconcentration holds greater significance than general topic concentration in highlight detection.\nAnother study suggests extracting highlights based on the frame-by-frame similarity of emotion\ndistributions. However, neither of these approaches addresses the combined challenges of lag\ncalibration, balancing emotion-topic concentration, and unsupervised highlight labeling.\n\nTo overcome these challenges, this study proposes the following solutions: (1) employ word-to-\nconcept and word-to-emotion mapping based on global word embedding, enabling the construction of\nlexical chains for calibrating the lag in bullet-comments; (2) detect highlights based on the emotional\nand conceptual concentration and intensity of the lag-calibrated bullet-comments; and (3) summarize\nhighlights using a modified Basic Sum algorithm that considers emotions and concepts as fundamental\nunits within a bullet-comment.\n\nThe main contributions of this research are as follows: (1) We introduce a completely unsupervised\nframework for detecting and summarizing video highlights using time-synchronized comments;\n(2) We introduce a lag-calibration method that uses concept-mapped lexical chains; (3) We have\ncreated extensive datasets for bullet-comment word embedding, an emotion lexicon tailored for\nbullet-comments, and ground-truth data for evaluating highlight detection and labeling based on\nbullet-comments.",
  "related_work": "2.1 Highlight detection by video processing\n\nFollowing the definition from previous research, we define highlights as the most memorable shots in\na video characterized by high emotional intensity. It\u2019s important to note that highlight detection differs\nfrom video summarization. While video summarization aims to provide a condensed representation\nof a video\u2019s storyline, highlight detection focuses on extracting its emotionally impactful content.\n\nIn the realm of highlight detection, some researchers have proposed representing video emotions as a\ncurve on the arousal-valence plane, utilizing low-level features such as motion, vocal effects, shot\nlength, and audio pitch, or color, along with mid-level features like laughter and subtitles. However,\ndue to the semantic gap between low-level features and high-level semantics, the accuracy of highlight\ndetection based solely on video processing is limited.\n\n2.2 Temporal text summarization\n\nResearch on temporal text summarization shares similarities with the present study but also exhibits\nkey distinctions. Several works have approached temporal text summarization as a constrained\nmulti-objective optimization problem, a graph optimization problem, a supervised learning-to-rank\nproblem, and as an online clustering problem.\n\nThis study models highlight detection as a simpler two-objective optimization problem with specific\nconstraints. However, the features employed to assess the \"highlightness\" of a shot diverge from\nthose used in the aforementioned studies. Given that highlight shots are observed to correlate with\nhigh emotional intensity and topic concentration, coverage and non-redundancy are not primary\noptimization goals, as they are in temporal text summarization. Instead, our focus is on modeling\nemotional and topic concentration within the context of this study.\n\n2.3 Crowdsourced time-sync comment mining\n\nSeveral studies have explored the use of crowdsourced time-synchronized comments for tagging\nvideos on a shot-by-shot basis. These approaches involve manual labeling and supervised training,\n\n2\n\n\ftemporal and personalized topic modeling, or tagging the video as a whole. One work proposes\ngenerating a summarization for each shot through data reconstruction that jointly considers textual\nand topic levels.\n\nOne work proposed a centroid-diffusion algorithm to identify highlights. Shots are represented by\nlatent topics found through Latent Dirichlet Allocation (LDA). Another method suggests using pre-\ntrained semantic vectors of comments to cluster them into topics and subsequently identify highlights\nbased on topic concentration. Additionally, they utilize predefined labels to train a classifier for\nhighlight labeling. The current study differs from these two studies in several ways. First, before\nperforming highlight detection, we apply a lag-calibration step to mitigate inaccuracies caused\nby comment delays. Second, we represent each scene using a combination of topic and emotion\nconcentration. Third, we perform both highlight detection and labeling in an unsupervised manner.\n\n2.4 Lexical chain\n\nLexical chains represent sequences of words that exhibit a cohesive relationship spanning multiple\nsentences. Early work on lexical chains used syntactic relationships of words from Roget\u2019s Thesaurus,\nwithout considering word sense disambiguation. Subsequent research expanded lexical chains by\nincorporating WordNet relations and word sense disambiguation. Lexical chains are also built\nutilizing word-embedded relations for disambiguating multi-word expressions. This study constructs\nlexical chains for accurate lag calibration, leveraging global word embedding.\n\n3 Problem Formulation\n\nThe problem addressed in this paper can be formulated as follows: The input consists of a set of\ntime-synchronized comments, denoted as C = {c1, c2, c3, . . . , cn}, along with their correspond-\ning timestamps T = {t1, t2, t3, . . . , tn} for a given video v. We are also given a compression\nratio \u03c1highlight that determines the number of highlights to be generated, and a compression ratio\n\u03c1summary that specifies the number of comments to be included in each highlight summary. Our\nobjective is twofold: (1) to generate a set of highlight shots S(v) = {s1, s2, s3, . . . , sm}, and (2)\nto produce highlight summaries \u03a3(v) = {C1, C2, C3, . . . , Cm} that closely align with the ground\ntruth. Each highlight summary Ci comprises a subset of the comments associated with that shot:\nCi = {c1, c2, c3, . . . , ck}. The number of highlight shots m and the number of comments in each\nsummary k are determined by \u03c1highlight and \u03c1summary, respectively.\n\n4 Video Highlight Detection\n\nThis section introduces our proposed framework for detecting video highlights. We also describe\ntwo preliminary tasks: constructing a global word embedding for time-synchronized comments and\nbuilding an emotion lexicon.\n\n4.1 Preliminaries\n\nWord-Embedding of Time-Sync Comments\n\nAs previously highlighted, a key challenge in analyzing time-synchronized comments is their semantic\nsparsity, stemming from the limited number of comments and their brevity. Two semantically related\nwords might not appear related if they don\u2019t co-occur frequently within a single video. To address this,\nwe construct a global word embedding based on a large collection of time-synchronized comments.\n\nThis word-embedding dictionary can be represented as: D = {(w1 : v1), (w2 : v2), . . . , (wn : vn)},\nwhere wi is a word, vi is its corresponding word vector, and n is the vocabulary size of the corpus.\n\nEmotion Lexicon Construction\n\nExtracting emotions from time-synchronized comments is crucial for highlight detection, as em-\nphasized earlier. However, traditional emotion lexicons are not directly applicable in this context\ndue to the prevalence of internet slang specific to these platforms. For example, \"23333\" signifies\nlaughter (\"ha ha ha\"), and \"6666\" expresses admiration (\"really awesome\"). Therefore, we construct\nan emotion lexicon tailored for time-synchronized comments, derived from the word-embedding\n\n3\n\n\fdictionary generated in the previous step. We begin by manually labeling words corresponding to\nthe five basic emotion categories (happiness, sadness, fear, anger, and surprise) as seeds, selecting\nfrom the most frequent words in the corpus. The sixth emotion category, \"disgust,\" is omitted due\nto its rarity in the dataset but can be easily incorporated for other datasets. We then expand this\nemotion lexicon by identifying the top N neighbors of each seed word in the word-embedding space.\nA neighbor is added to the seeds if it meets a minimum percentage of overlap \u03b8overlap with all seeds,\nwith a minimum similarity score of simmin. Neighbors are determined based on cosine similarity\nwithin the word-embedding space.\n\n4.2 Lag-Calibration\n\nThis section details our method for lag calibration, which involves concept mapping, constructing\nword-embedded lexical chains, and performing the actual calibration.\n\nConcept Mapping\n\nTo tackle semantic sparsity in time-synchronized comments and build lexical chains of semantically\nrelated words, we first map words with similar meanings to the same concept. Given a set of\ncomments C for a video v, we define a mapping F from the vocabulary VC of comments C to a set of\nconcepts KC:\n\nF : VC \u2192 KC\n\n(|VC| \u2265 |KC|)\n\nSpecifically, the mapping F assigns each word wi to a concept k = F(wi) as follows:\nF(wi) = F(w1) = F(w2) = . . . = F(wtop_n) = k,\ns.t. {w|w \u2208 top_n(wi) \u2227 F(w) = k}/|top_n(wi)| \u2265 \u03b8overlap\n\n\u2203k \u2208 KC\n\ntop_n(wi) returns the n nearest neighbors of word wi based on cosine similarity. For each word wi\nin the comments C, we examine the percentage of its neighbors that have already been mapped to\na concept k. If this percentage exceeds the threshold \u03b8overlap, then word wi and its neighbors are\nmapped to concept k. Otherwise, they are assigned to a new concept, represented by wi itself.\n\nLexical Chain Construction\n\nThe next step involves constructing all lexical chains present in the time-synchronized comments for\nvideo v. This enables the calibration of lagged comments based on these chains. A lexical chain lik\nconsists of a set of triples lik = {(w, t, c)}, where w is the actual word mentioned for concept k in\ncomment c, and t is the timestamp of comment c. We create a lexical chain dictionary LC for the\ntime-synchronized comments C of video v:\n\nLC = {k1 : (l11, l12, l13, . . .), k2 : (l21, l22, l23, . . .), . . . , kn : (ln1, ln2, ln3, . . .)}\n\nwhere ki \u2208 KC represents a concept, and lik is the i-th lexical chain associated with concept k. The\nprocedure for constructing these lexical chains is detailed in Algorithm 1.\n\nSpecifically, each comment in C can either be appended to an existing lexical chain or added to a\nnew, empty chain. This decision is based on the comment\u2019s temporal distance from existing chains,\ncontrolled by the maximum silence parameter tsilence.\n\nIt\u2019s important to note that word senses within the constructed lexical chains are not disambiguated,\nunlike in most traditional algorithms. However, we argue that these lexical chains remain useful\nbecause our concept mapping is built from time-synchronized comments in their natural order.\nThis progressive semantic continuity naturally reinforces similar word senses for temporally close\ncomments. This continuity, combined with global word embedding, ensures the validity of our\nconcept mapping in most scenarios.\n\nComment Lag-Calibration\n\nWith the lexical chain dictionary LC constructed, we can now calibrate the comments in C based on\ntheir respective lexical chains. Our observations indicate that the initial comment pertaining to a\nshot typically occurs within that shot, while subsequent comments may not. Therefore, we adjust\nthe timestamp of each comment to match the timestamp of the first element within its corresponding\nlexical chain. If a comment belongs to multiple lexical chains (concepts), we select the chain with the\nhighest score scorechain. The scorechain is calculated as the sum of the frequencies of each word\n\n4\n\n\fin the chain, weighted by the logarithm of their global frequencies, denoted as log(D(w).count).\nConsequently, each comment will be assigned to its most semantically significant lexical chain\n(concept) for calibration. The calibration algorithm is presented in Algorithm 2.\n\nIt\u2019s worth noting that if multiple consecutive shots, {s1, s2, . . . , sn}, contain comments with similar\ncontent, our lag-calibration method might shift many comments from shots s2, s3, . . . , sn to the\ntimestamp of the first shot, s1, if these comments are connected through lexical chains originating\nfrom s1. This is not necessarily a drawback, as it helps us avoid selecting redundant consecutive\nhighlight shots and allows for the inclusion of other potential highlights, given a fixed compression\nratio.\n\n4.3 Shot Importance Scoring\n\nIn this section, we first segment comments into shots of equal temporal length, denoted as tshot. We\nthen model the importance of each shot, enabling highlight detection based on these importance\nscores.\n\nA shot\u2019s importance is modeled as a function of two factors: comment concentration and commenting\nintensity. Regarding comment concentration, as mentioned earlier, both concept and emotional\nconcentration contribute to highlight detection. For instance, a cluster of concept-concentrated\ncomments like \"the background music/bgm/soundtrack of this shot is classic/inspiring/the best\" could\nindicate a highlight related to memorable background music. Similarly, comments such as \"this plot\nis so funny/hilarious/lmao/lol/2333\" might suggest a highlight characterized by a single concentrated\nemotion. Therefore, our model combines these two types of concentration. We define the emotional\nconcentration Cemotion(Cs) of shot s based on time-synchronized comments Cs and the emotion\nlexicon E as follows:\nCemotion(Cs) = 1 \u2212 (cid:80)|E|\npe = |{w|w\u2208Cs\u2227w\u2208E(e)}|\n\ne=1 pe log(pe)\n\n|Cs|\n\nHere, we calculate the inverse of the entropy of probabilities for the five emotions within a shot to\nrepresent emotion concentration. Next, we define topical concentration Ctopic as:\n\n(cid:80)J\n\nj=1 pj log(pj)\n\nCtopic(Cs) = 1\nJ\n(cid:80)\n\npj =\n\nw\u2208Cs \u2229F (kj )\n(cid:80)\n\nw\u2208Cs\n\n1\nlog(D(w))\n1\nlog(D(w))\n\nwhere we calculate the inverse of the entropy of all concepts within a shot to represent topic\nconcentration. The probability of each concept k is determined by the sum of the frequencies of\nits mentioned words, weighted by their global frequencies, and then divided by the sum of these\nweighted frequencies for all words in the shot.\n\nNow, the comment importance Icomment(Cs, s) of shot s can be defined as:\n\nIcomment(Cs, s) = \u03bb \u00b7 Cemotion(Cs, s) + (1 \u2212 \u03bb) \u00b7 Ctopic(Cs, s)\n\nwhere \u03bb is a hyperparameter that controls the balance between emotion and concept concentration.\n\nFinally, the overall importance of a shot is defined as:\n\nI(Cs, s) = Icomment(Cs, s) \u00b7 log(|Cs|)\n\nwhere |Cs| represents the total length of all time-synchronized comments within shot s, serving as a\nstraightforward yet effective indicator of comment intensity per shot.\n\nThe problem of highlight detection can now be formulated as a maximization problem:\nMaximize (cid:80)\n\ns\u2208S I(Cs, s)\n\nSubject to |S| \u2264 \u03c1highlight \u00b7 N\n\n5\n\n\f5 Video Highlight Summarization\n\nGiven a set of detected highlight shots S(v) = {s1, s2, s3, . . . , sm} for video v, each associated with\nits lag-calibrated comments Cs, our goal is to generate summaries \u03a3(v) = {C1, C2, C3, . . . , Cm}\nsuch that Ci \u2282 Csi, with a compression ratio of \u03c1summary, and Ci closely resembles the ground\ntruth.\n\nWe propose a simple yet highly effective summarization model, building upon SumBasic with\nenhancements that incorporate emotion and concept mapping, along with a two-level updating\nmechanism.\n\nIn our modified SumBasic, instead of solely down-weighting the probabilities of words in a selected\nsentence to mitigate redundancy, we down-weight the probabilities of both words and their mapped\nconcepts to re-weight each comment. This two-level updating approach achieves two key objectives:\n(1) it penalizes the selection of sentences containing semantically similar words, and (2) it allows for\nthe selection of a sentence with a word already present in the summary if that word occurs significantly\nmore frequently. Additionally, we introduce an emotion bias parameter, bemotion, to weight words\nand concepts during probability calculations. This increases the frequencies of emotional words and\nconcepts by a factor of bemotion compared to non-emotional ones.\n\n6 Experiment\n\nThis section presents the experiments conducted on large-scale real-world datasets to evaluate\nhighlight detection and summarization. We describe the data collection process, evaluation metrics,\nbenchmark methods, and experimental results.\n\n6.1 Data\n\nThis section describes the datasets collected and constructed for our experiments. All datasets and\ncode will be made publicly available on Github.\n\nCrowdsourced Time-sync Comment Corpus\n\nTo train the word embedding described earlier, we collected a large corpus of time-synchronized\ncomments from Bilibili, a content-sharing website in China that features such comments. The corpus\ncomprises 2,108,746 comments, 15,179,132 tokens, and 91,745 unique tokens, extracted from 6,368\nlong videos. On average, each comment contains 7.20 tokens.\n\nBefore training, each comment undergoes tokenization using the Chinese word tokenization package\nJieba. Repeated characters within words, such as \"233333,\" \"66666,\" and \"\u02d854c8\u02d854c8\u02d854c8\u02d854c8,\" are\nreplaced with two instances of the same character.\n\nThe word embedding is trained using word2vec with the skip-gram model. We set the number of\nembedding dimensions to 300, the window size to 7, and the down-sampling rate to 1e-3. Words with\na frequency lower than 3 are discarded.\n\nEmotion Lexicon Construction\n\nAfter training the word embedding, we manually select emotional words belonging to the five basic\nemotion categories from the 500 most frequent words in the embedding. We then iteratively expand\nthese emotion seeds using Algorithm 1. After each expansion iteration, we manually review the\nexpanded lexicon, removing any inaccurate words to prevent concept drift. The filtered expanded\nseeds are then used for further expansion in the next round. The minimum overlap \u03b8overlap is set to\n0.05, and the minimum similarity simmin is set to 0.6. These values are determined through a grid\nsearch within the range of [0, 1]. The number of words for each emotion, both initially and after the\nfinal expansion, is presented in Table 3.\n\nVideo Highlights Data\n\nTo evaluate our highlight detection algorithm, we constructed a ground-truth dataset. This dataset\nleverages user-uploaded mixed-clips related to a specific video on Bilibili. Mixed-clips represent a\ncollection of video highlights chosen according to the user\u2019s preferences. We then consider the most\nfrequently selected highlights as the ground truth for a given video.\n\n6\n\n\fTable 1: Number of Initial and Expanded Emotion Words\n\nHappy\n\nSeeds\nAll\n\n17\n157\n\nSad\n\n13\n235\n\nFear Anger\n\nSurprise\n\n19\n258\n\n21\n284\n\n14\n226\n\nThe dataset consists of 11 videos totaling 1333 minutes in length, with 75,653 time-synchronized\ncomments. For each video, 3-4 video mix-clips are collected from Bilibili. Shots that appear in at\nleast two of these mix-clips are considered ground-truth highlights. These highlights are mapped to\nthe original video timeline, and their start and end times are recorded as ground truth. Mix-clips are\nselected based on the following criteria: (1) they are found on Bilibili using the search query \"video\ntitle + mixed clips\"; (2) they are sorted by play count in descending order; (3) they primarily focus on\nvideo highlights rather than a plot-by-plot summary or gist; (4) they are under 10 minutes in length;\nand (5) they contain a mix of several highlight shots instead of just one.\n\nOn average, each video contains 24.3 highlight shots. The mean duration of these highlight shots is\n27.79 seconds, while the mode is 8 and 10 seconds (with a frequency of 19).\n\nHighlights Summarization Data\n\nWe also created a highlight summarization (labeling) dataset for the 11 videos. For each highlight\nshot and its associated comments, we asked annotators to create a summary by selecting as many\ncomments as they deemed necessary. The guiding principles were: (1) comments with identical\nmeanings should not be selected more than once; (2) the most representative comment among similar\ncomments should be chosen; and (3) comments that stand out and are irrelevant to the current\ndiscussion should be discarded.\n\nAcross the 11 videos and 267 highlights, each highlight has an average of 3.83 comments in its\nsummary.\n\n6.2 Evaluation Metrics\n\nThis section introduces the evaluation metrics employed for both highlight detection and summariza-\ntion.\n\nVideo Highlight Detection Evaluation\n\nTo evaluate video highlight detection, we need to define a \"hit\" between a candidate highlight and a\nreference highlight. A strict definition would require a perfect match between the start and end times\nof the candidate and reference highlights. However, this criterion is overly stringent for any model.\nA more lenient definition would consider an overlap between a candidate and a reference highlight.\nHowever, this can still underestimate model performance, as users\u2019 choices of highlight start and end\ntimes can sometimes be arbitrary. Instead, we define a \"hit\" with a relaxation parameter \u03b4 between a\ncandidate h and the reference set R as follows:\n\nhit(h, R) = { 1 \u2203r \u2208 R : (sh, eh) \u2229 (sr \u2212 \u03b4, er + \u03b4) \u0338= \u2205\n0otherwise\n\nwhere sh, eh represent the start and end times of highlight h, and \u03b4 is the relaxation length applied to\nthe reference set R. We can then define precision, recall, and F1-score as:\n\nP recision(H, R) =\n\n(cid:80)\n\nh\u2208H hit(h,R)\n|H|\n\nRecall(H, R) =\n\n(cid:80)\n\nr\u2208R hit(r,H)\n|R|\n\nF 1(H, R) = 2\u00b7P recision(H,R)\u00b7Recall(H,R)\nP recision(H,R)+Recall(H,R)\n\nIn this study, we set the relaxation length \u03b4 to 5 seconds. The candidate highlight length is set to 15\nseconds.\n\nVideo Highlight Summarization Evaluation\n\nWe utilize ROUGE-1 and ROUGE-2 as recall metrics for evaluating candidate summaries:\n\n7\n\n\fROU GE \u2212 n(C, R) =\n\n(cid:80)\n\n(cid:80)\n\nr\u2208R\n(cid:80)\n\nr\u2208R\n\nn\u2212gram\u2208r Countmatch(n\u2212gram)\n(cid:80)\nn\u2212gram\u2208r Count(n\u2212gram)\n\nWe employ BLEU-1 and BLEU-2 as precision metrics. BLEU is chosen for two reasons. First, a\nnaive precision metric would be biased towards shorter comments, and BLEU mitigates this with the\nBP (Brevity Penalty) factor:\n\nBLEU \u2212 n(C, R) = BP \u00b7\n\nBP = { 1 if |C| > |R|\ne(1\u2212|R|/|C|)if |C| \u2264 |R|\n\n(cid:80)\n\n(cid:80)\n\nc\u2208C\n(cid:80)\n\nc\u2208C\n\nn\u2212gram\u2208c Countclip(n\u2212gram)\n(cid:80)\nn\u2212gram\u2208c Count(n\u2212gram)\n\nwhere C is the candidate summary and R is the reference summary. Second, while the reference\nsummary contains no redundancy, the candidate summary might incorrectly select multiple similar\ncomments that match the same keywords in the reference.\nIn such cases, precision would be\nsignificantly overestimated. BLEU addresses this by counting matches one-by-one; the number of\nmatches for a word will be the minimum of its frequencies in the candidate and reference summaries.\n\nFinally, the F1-score is defined as:\nF 1 \u2212 n(C, R) = 2\u00b7BLEU \u2212n(C,R)\u00b7ROU GE\u2212n(C,R)\nBLEU \u2212n(C,R)+ROU GE\u2212n(C,R)\n\n6.3 Benchmark methods\n\nBenchmarks for Video Highlight Detection\n\nFor highlight detection, we compare different combinations of our model against three benchmark\nmethods:\n\n* **Random-Selection:** Highlight shots are randomly selected from all shots in a video. *\n**Uniform-Selection:** Highlight shots are selected at equal intervals. * **Spike-Selection:** High-\nlight shots are chosen based on the highest number of comments within the shot. * **Spike+E+T:**\nThis is our method, incorporating emotion and topic concentration but without lag calibration. *\n**Spike+L:** This is our method, including only the lag-calibration step and not considering content\nconcentration. * **Spike+L+E+T:** This represents our full model.\n\nBenchmarks for Video Highlight Summarization\n\nFor highlight summarization, we compare our method against five benchmark methods:\n\n* **SumBasic:** Summarization that relies solely on frequency for summary construction. * **Latent\nSemantic Analysis (LSA):** Text summarization based on singular value decomposition (SVD)\nfor latent topic discovery. * **LexRank:** Graph-based summarization that calculates sentence\nimportance using the concept of eigenvector centrality in a sentence graph. * **KL-Divergence:**\nSummarization based on minimizing KL-divergence between the summary and the source corpus,\nemploying a greedy search approach. * **Luhn method:** A heuristic summarization method that\nconsiders both word frequency and sentence position within an article.\n\n6.4 Experiment Results\n\nThis section presents the experimental results for both highlight detection and highlight summariza-\ntion.\n\nResults of Highlight Detection\n\nIn our highlight detection model, the maximum silence threshold for lexical chains, tsilence, is set\nto 11 seconds. The threshold for concept mapping, \u03b8overlap, is set to 0.5. The number of neighbors\nconsidered for concept mapping, top_n, is set to 15. The parameter \u03bb, which controls the balance\nbetween emotion and concept concentration, is set to 0.9. A detailed parameter analysis is provided\nin Section 7.\n\nTable 4 presents the precision, recall, and F1-scores for different combinations of our method and the\nbenchmark methods. Our full model (Spike+L+E+T) outperforms all other benchmarks across all\nmetrics. Random and uniform selection exhibit low precision and recall, as they don\u2019t incorporate\nstructural or content information. Spike-selection shows significant improvement by leveraging\n\n8\n\n\fcomment intensity. However, not all comment-intensive shots are highlights. For example, comments\nat the beginning and end of a video are often high-volume greetings or goodbyes, which may not be\nindicative of highlights. Additionally, spike-selection tends to cluster highlights within consecutive\nshots with high comment volumes. In contrast, our method can identify less intensive but emotionally\nor conceptually concentrated shots that might be missed by spike-selection. This is evident in the\nperformance of Spike+E+T.\n\nWe also observe that lag calibration alone (Spike+L) considerably enhances the performance of\nSpike-selection, partially supporting our hypothesis that lag calibration is crucial for tasks involving\ntime-synchronized comments.\n\nTable 2: Comparison of Highlight Detection Methods\n\nMethod\n\nPrecision Recall\n\nF1-score\n\nRandom-Selection\nUniform-Selection\nSpike-Selection\nSpike+E+T\nSpike+L\nSpike+L+E+T\n\n0.1578\n0.1797\n0.2594\n0.2796\n0.3125\n0.3099\n\n0.1567\n0.1830\n0.2167\n0.2357\n0.2690\n0.3071\n\n0.1587\n0.1775\n0.2321\n0.2500\n0.2829\n0.3066\n\nResults of Highlight Summarization\n\nIn our highlight summarization model, the emotion bias bemotion is set to 0.3.\n\nTable 5 compares the 1-gram BLEU, ROUGE, and F1-scores of our method and the benchmark\nmethods. Our method outperforms all others, particularly in terms of ROUGE-1. LSA exhibits the\nlowest BLEU score, primarily because it statistically favors longer, multi-word sentences, which are\nnot representative in time-synchronized comments. The SumBasic method also performs relatively\npoorly, as it treats semantically related words separately, unlike our method, which uses concepts\ninstead of individual words.\n\nTable 3: Comparison of Highlight Summarization Methods (1-Gram)\n\nMethod\n\nBLEU-1 ROUGE-1\n\nF1-1\n\nLSA\nSumBasic\nKL-divergence\nLuhn\nLexRank\nOur method\n\n0.2382\n0.2854\n0.3162\n0.2770\n0.3045\n0.3333\n\n0.4855\n0.3898\n0.3848\n0.4970\n0.4325\n0.6006\n\n0.3196\n0.3295\n0.3471\n0.3557\n0.3574\n0.4287",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "This work presents a novel unsupervised framework for video highlight detection and summarization,\nbased on crowdsourced time-synchronized comments. We introduce a lag-calibration technique\nthat re-aligns delayed comments to their corresponding video scenes by using concept-mapped\nlexical chains. Video highlights are identified based on comment intensity and the concentration\nof concepts and emotions within each shot. For summarization, a two-level SumBasic is proposed\nwhich updates word and concept probabilities iteratively when selecting sentences. Future work\nincludes integrating additional data sources such as video meta-data, audience profiles, and low-level\nmulti-modal features.\n\n9",
  "is_publishable": 1,
  "venue": NaN
}
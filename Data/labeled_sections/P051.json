{
  "title": "Real-Time Adaptation of Lexical Embeddings for\nEnhanced Part-of-Speech Tagging",
  "abstract": "This research introduces a method for real-time unsupervised domain adaptation\n(DA) that can be applied incrementally as new information arrives. This method is\nespecially useful when conventional batch DA is unfeasible. Through evaluations\nfocused on part-of-speech (POS) tagging, we observe that real-time unsupervised\nDA achieves accuracy levels on par with those of batch DA.",
  "introduction": "Unsupervised domain adaptation is a frequently encountered challenge for developers aiming to\ncreate robust natural language processing (NLP) systems. This situation typically arises when labeled\ndata is available for a source domain, but there is a need to enhance performance in a target domain\nusing only unlabeled data. A majority of the current NLP research on unsupervised domain adaptation\nemploys batch learning, which presumes the availability of a substantial corpus of unlabeled data\nfrom the target domain before the testing phase. However, batch learning is impractical in numerous\nreal-world situations where data from a new target domain must be processed without delay. Further,\nin many practical scenarios, data may not be neatly categorized by domain, making it difficult to\nimmediately discern when an input stream begins providing data from a new domain.\n\nFor instance, consider an NLP system within a company that is tasked with analyzing a continuous\nstream of emails. This stream evolves over time without any explicit signals indicating that the\ncurrent models should be adjusted to the new data distribution. Given that the system is expected to\noperate in real-time, it would be beneficial for any system adaptation to be done in an online manner,\nas opposed to the batch method, which involves halting the system, modifying it, and then restarting\nit.\n\nThis paper introduces real-time unsupervised domain adaptation as an enhancement to conventional\nunsupervised DA. In this approach, domain adaptation is carried out incrementally as data is received.\nSpecifically, our implementation involves a type of representation learning, where the focus is on\nupdating word representations in our experiments. Every instance a word appears in the data stream\nduring testing, its representation is refined.\n\nTo our understanding, the research presented here is the first to examine real-time unsupervised\nDA. In particular, we assess this method for POS tagging tasks. We analyze POS tagging outcomes\nusing three different methods: a static baseline, batch learning, and real-time unsupervised DA. Our\nfindings indicate that real-time unsupervised DA performs comparably to batch learning, yet it does\nnot require retraining or pre-existing data from the target domain.\n\n2 Experimental setup\n\nTagger. We have adapted the FLORS tagger, which is recognized for its speed and simplicity,\nand is particularly effective in DA scenarios. This tagger approaches POS tagging as a multi-label\nclassification problem within a window-based framework, rather than a sequence classification\none. FLORS is well-suited for real-time unsupervised DA because its word representations include\n\n\fdistributional vectors, which can be updated during both batch learning and real-time unsupervised\nDA. Each word\u2019s representation in FLORS consists of four feature vectors: one for its suffix, one for\nits shape, and one each for its left and right distributional neighbors. Suffix and shape features are\nstandard in the literature, and we utilize them as described previously.\n\nDistributional features. The ith element xi of the left distributional vector for a word w is the\nweighted count of times the indicator word ci appears immediately to the left of w:\n\nxi = tf (f req(bigram(ci, w)))\n\n(1)\n\nwhere ci is the word with frequency rank i in the corpus, freq(bigram(ci, w)) is the occurrence count\nof the bigram \"ci w\", and non-zero frequencies are weighted logarithmically: tf(x) = 1 + log(x). The\nright distributional vector is defined similarly. We limit the set of indicator words to the 500 most\nfrequent. To avoid zero vectors, an additional element xn+1 is added to each vector to account for\nomitted contexts:\n\nxn + 1 = tf (\n\n(cid:88)\n\n.5f req(bigram(ci, w)))\n\n(2)\n\nLet f(w) be the concatenation of the two distributional, suffix, and shape vectors of word w. Then\nFLORS represents token vi as follows:\n\nf (vi\u03a622122)\u03a62295f (vi\u03a622121)\u03a62295f (vi)\u03a62295f (vi + 1)\u03a62295f (vi + 2)\n\n(3)\n\nwhere \u02d82295 is vector concatenation. FLORS then tags token vi based on this representation.\n\nFLORS operates under the assumption that the fundamental relationship between distributional\nfeatures and labels remains consistent when transitioning from the source to the target domain. This\ncontrasts with other studies that select \"stable\" distributional features and discard \"unstable\" ones.\nThe central hypothesis of FLORS is that fundamental distributional POS characteristics are relatively\nstable across different domains, unlike semantic or more intricate tasks. The effectiveness of FLORS\nsuggests the validity of this hypothesis.\n\nData. Test set. Our evaluation utilizes the development sets from six different target domains (TDs):\nfive SANCL domains (newsgroups, weblogs, reviews, answers, emails) and sections 22-23 of the\nWall Street Journal (WSJ) for in-domain testing.\n\nTwo training sets of varying sizes are employed. In the l:big condition (large labeled data set), FLORS\nis trained on sections 2-21 of the WSJ. The l:small condition uses 10% of the l:big data set.\n\nData for word representations. We also adjust the size of the datasets used for computing word\nrepresentations before training the FLORS model. In the u:big condition, distributional vectors are\ncomputed on the combined corpus of all labeled and unlabeled text from both source and target\ndomains (excluding test sets), along with 100,000 WSJ sentences from 1988 and 500,000 sentences\nfrom a large external corpus. In the u:0 condition, only labeled training data is utilized.\n\nMethods. We implemented a modification from the original setup: distributional vectors are stored\nin memory as count vectors, enabling count increases during online tagging.\n\nExperiments are conducted with three versions of FLORS: STATIC, BATCH, and ONLINE. All three\nmethods compute word representations on \"data for word representations\" before model training on\none of the two \"training sets\".\n\nSTATIC. Word representations remain unchanged during testing.\n\nBATCH. Before testing, count vectors are updated by freq(bigram(ci, w)) += freq*(bigram(ci, w)),\nwhere freq*(\u02d800b7) denotes the bigram \"ci w\" occurrences in the entire test set.\n\nONLINE. Before tagging a test sentence, both left and right distributional vectors are updated via\nfreq(bigram(ci, w)) += 1 for each \"ci w\" bigram appearance in the sentence. The sentence is then\ntagged using the updated word representations. As tagging progresses, distributional representations\nbecome increasingly specific to the target domain (TD), converging to the representations that BATCH\nuses at the end of the tagging process.\n\n2\n\n\fIn all three modes, suffix and shape features are always fully specified, for both known and unknown\nwords.\n\n3 Experimental results\n\nTable 1 shows that the performance levels of BATCH and ONLINE are on par with each other and\nrepresent the current state-of-the-art. The highest accuracy in each column is highlighted in bold.\n\nTable 1: BATCH and ONLINE accuracies are comparable and state-of-the-art. Best number in each\ncolumn is bold.\n\nnewsgroups\n\nreviews\n\nweblogs\n\nanswers\n\nemails\n\nwsj\n\nOOV\n\nTnT\n88.30\nStanford\n90.25\nSVMTool\n87.96\nC&P\n88.65\nS&S\n90.37\nS&S (reimpl.)\n89.70\nBATCH\n91.86\nONLINE\n91.69\n\nALL OOV ALL OOV ALL OOV ALL OOV ALL OOV ALL\n\n88.66\n\n54.73\n\n90.40\n\n56.75\n\n93.33\n\n74.17\n\n88.55\n\n48.32\n\n88.14\n\n58.09\n\n95.75\n\n89.11\n\n56.02\n\n91.43\n\n58.66\n\n94.15\n\n77.13\n\n88.92\n\n49.30\n\n88.68\n\n58.42\n\n96.83\n\n89.14\n\n53.82\n\n91.30\n\n54.20\n\n94.21\n\n76.44\n\n88.96\n\n47.25\n\n88.64\n\n56.37\n\n96.63\n\n89.51\n\n57.23\n\n91.58\n\n59.67\n\n94.41\n\n78.46\n\n89.08\n\n48.46\n\n88.74\n\n58.62\n\n96.78\n\n90.86\n\n66.42\n\n92.95\n\n75.29\n\n94.71\n\n83.64\n\n90.30\n\n62.16\n\n89.44\n\n62.61\n\n96.59\n\n90.68\n\n65.52\n\n93.00\n\n75.50\n\n94.64\n\n82.91\n\n90.18\n\n61.98\n\n89.53\n\n62.46\n\n96.60\n\n90.87\n\n71.18\n\n93.07\n\n79.03\n\n94.86\n\n86.53\n\n90.70\n\n65.29\n\n89.84\n\n65.44\n\n96.63\n\n90.85\n\n71.00\n\n93.07\n\n79.03\n\n94.86\n\n86.53\n\n90.68\n\n65.16\n\n89.85\n\n65.48\n\n96.62\n\nTable 2 shows that the accuracy rates for ONLINE and BATCH methods are generally superior\nto those of the STATIC method, as indicated by the numbers in bold. It also demonstrates that\nperformance improves with an increase in both training data and unlabeled data.\n\nThe performance of ONLINE is similar to that of BATCH. It is slightly lower than BATCH in the\nu:0 condition, with the most significant difference in accuracy being 0.29, and it is at most 0.02\ndifferent from BATCH in terms of overall accuracy in the u:big condition. The reasons for ONLINE\noccasionally outperforming BATCH, particularly in certain conditions, are discussed subsequently.\n\n3.1 Time course of tagging accuracy\n\nThe ONLINE model introduced here has a unique characteristic not commonly found in other\nstatistical NLP research: its predictive accuracy evolves as it processes text due to the modification of\nits representations.\n\nTo analyze the progression of these changes over time, a substantial application domain is necessary\nbecause subtle changes might be too inconsistent in the smaller test sets of the SANCL TDs. The\nWSJ corpus is the only labeled domain that is sufficiently large for this purpose. Consequently, we\ninvert the usual setup by training the model on the development sets of the five SANCL domains\n(l:big) or on the initial 5000 labeled words of reviews (l:small). In this reversed setup, u:big utilizes\nthe five unlabeled SANCL datasets along with a large external corpus as before. Given the importance\nof performance variability, we conduct 100 trials on randomly selected 50% samples of WSJ and\nreport both the average and standard deviation of tagging errors across these trials.\n\nThe results presented in Table 3 indicate that ONLINE\u2019s error rates are only marginally higher than,\nor comparable to, those of BATCH. Specifically, in the l:small/u:0 condition, the error rate for known\nwords is lower for ONLINE (0.1186) than for BATCH, similar to observations in Table 2.\n\n3\n\n\fTable 2: ONLINE / BATCH accuracies are generally better than STATIC (see bold numbers) and\nimprove with both more training data and more unlabeled data.\n\nl:small\n\nnewsgroups\n\nl:big\n\nl:small\n\nreviews\n\nl:big\n\nl:small\n\nweblogs\n\nl:big\n\n!\n\nl:small\n\nanswers\n\nl:big\n\nl:small\n\nemails\n\nl:big\n\nl:small\n\nl:big\n\nwsj\n\nu:0\n\nu:big\n\nALL\n\nKN\n\nSHFT OOV ALL\n\nKN\n\nSHFT OOV\n\n87.02\nSTATIC\nONLINE 87.99\nBATCH 88.28\nSTATIC\n89.69\nONLINE 90.51\nBATCH 90.69\n\nSTATIC\n89.08\nONLINE 89.67\nBATCH 89.79\nSTATIC\n91.96\nONLINE 92.33\nBATCH 92.42\n\n91.58\nSTATIC\nONLINE 92.51\nBATCH 92.68\nSTATIC\n93.45\nONLINE 94.18\nBATCH 94.34\n\nSTATIC\n86.93\nONLINE 87.48\nBATCH 87.56\n89.54\nSTATIC\nONLINE 89.98\nBATCH 90.14\n\nSTATIC\n85.43\nONLINE 86.30\nBATCH 86.42\nSTATIC\n88.31\nONLINE 88.86\nBATCH 88.96\n\nSTATIC\n94.64\nONLINE 94.86\nBATCH 94.80\nSTATIC\n96.44\nONLINE 96.50\nBATCH 96.57\n\n90.87\n90.87\n91.08\n93.00\n93.13\n93.12\n\n91.96\n92.14\n92.23\n93.94\n94.03\n94.09\n\n94.29\n94.52\n94.60\n95.64\n95.82\n95.85\n\n90.89\n91.18\n91.11\n92.76\n92.97\n93.10\n\n90.85\n91.26\n91.31\n92.98\n93.08\n93.11\n\n95.44\n95.53\n95.46\n96.85\n96.85\n96.82\n\n71.12\n76.10\n77.01\n82.65\n82.51\n83.24\n\n66.55\n70.14\n69.86\n82.30\n83.59\n83.53\n\n79.95\n81.76\n82.34\n90.15\n89.80\n90.03\n\n66.51\n68.07\n68.25\n78.65\n79.07\n79.01\n\n57.85\n60.56\n61.03\n71.38\n72.38\n72.28\n\n83.38\n85.37\n85.51\n92.75\n93.55\n93.48\n\n57.16\n65.64\n66.37\n57.82\n67.57\n69.43\n\n65.90\n69.67\n71.27\n67.97\n72.50\n73.35\n\n72.74\n80.46\n81.20\n72.68\n80.35\n81.84\n\n53.43\n56.47\n58.44\n56.22\n59.77\n60.72\n\n51.65\n55.83\n56.32\n52.71\n57.78\n58.85\n\n82.72\n85.22\n85.38\n85.38\n86.38\n86.54\n\n89.02\n89.84\n89.82\n89.93\n90.85\n90.87\n\n91.45\n92.11\n92.10\n92.42\n93.07\n93.07\n\n93.42\n94.21\n94.20\n94.09\n94.86\n94.86\n\n88.98\n89.71\n89.71\n90.06\n90.68\n90.70\n\n87.76\n88.45\n88.46\n89.21\n89.85\n89.84\n\n95.73\n95.80\n95.80\n96.56\n96.62\n96.63\n\n91.48\n92.38\n92.37\n92.41\n93.04\n93.03\n\n92.47\n93.62\n93.60\n93.53\n94.36\n94.36\n\n94.77\n95.40\n95.42\n95.54\n95.81\n95.82\n\n91.09\n92.42\n92.43\n92.18\n93.21\n93.22\n\n90.35\n92.31\n92.32\n91.74\n93.30\n93.30\n\n95.88\n96.21\n96.22\n96.72\n96.89\n96.89\n\n81.53\n82.58\n82.65\n84.94\n84.94\n85.20\n\n80.11\n81.46\n81.51\n84.65\n85.71\n85.71\n\n89.80\n91.08\n91.03\n91.90\n92.60\n92.60\n\n77.63\n78.11\n78.23\n80.70\n81.48\n81.54\n\n70.86\n71.67\n71.71\n73.80\n75.32\n75.27\n\n90.36\n89.89\n89.89\n93.35\n93.35\n93.42\n\n58.30\n67.09\n67.03\n58.97\n71.00\n71.18\n\n70.81\n78.42\n78.42\n69.97\n79.03\n79.03\n\n77.42\n84.03\n83.87\n76.94\n86.53\n86.53\n\n57.36\n64.21\n64.09\n58.25\n65.16\n65.29\n\n56.76\n61.57\n61.65\n58.99\n65.48\n65.44\n\n87.87\n89.70\n89.70\n88.04\n91.69\n91.86\n\nTable 3 also includes data on \"unseens\" along with unknowns, as prior research indicates that unseens\nlead to at least as many errors as unknowns. Unseens are defined as words with tags not present in\nthe training data, and error rates for unseens are calculated across all their occurrences, including\nthose with both seen and unseen tags. As shown in Table 3, the error rate for unknowns is higher than\nthat for unseens, which in turn is higher than the error rate for known words.\n\nWhen examining individual conditions, ONLINE generally outperforms STATIC, showing better\nresults in 10 out of 12 cases and only slightly underperforming in the l:small/u:big condition for\nunseens and known words (0.1086 vs. 0.1084, 0.0802 vs. 0.0801). In four conditions, ONLINE is\nsignificantly better, with improvements ranging from 0.005 to over 0.06. The differences between\nONLINE and STATIC in the remaining eight conditions are minimal. For the six u:big conditions,\nthis is expected as the large unlabeled dataset is from the news domain, similar to WSJ. Therefore, if\nlarge unlabeled datasets similar to the target domain are available, using STATIC tagging may suffice\nsince the additional effort for ONLINE/BATCH may not be justified.\n\n4\n\n\fTable 3: Error rates (err) and standard deviations (std) for tagging. \u02d82020 (resp. \u02d82217): significantly\ndifferent from ONLINE error rate above&below (resp. from \u201cu:0\u201d error rate to the left).\n\nunknowns\n\nu:0\n\nu:big\n\nl:small\n\nl:big\n\nerr\n.3670\u02d82020\nSTATIC\nONLINE .3050\u02d82020\nBATCH\n\n.3094\n.1451\u02d82020\n.1404\n.1382\u02d82020\n\nSTATIC\nONLINE\nBATCH\n\nstd\n\nerr\n\nstd\n\n.00085\n.00143\n.00160\n\n.00114\n.00125\n.00140\n\n.3094\n.2104\n.2102\u02d82217\n\n.1042\n.1037\u02d82217\n.1033\n\n.00160\n.00081\n.00093\n\n.00100\n.00098\n.00112\n\nu:0\n\nerr\n.1659\u02d82020\n.1646\u02d82020\n.1404\n\n.0732\n.0727\n.0723\n\nunseens\n\nu:big\n\nstd\n\nerr\n\nstd\n\n.00076\n.00145\n.00125\n\n.00052\n.00051\n.00065\n\n.1467\n.1084\n.1037\u02d82217\n\n.0690\n.0689\u02d82217\n.0680\n\n.00120\n.00056\n.00098\n\n.00042\n.00051\n.00062\n\nu:0\n\nerr\n.1309\u02d82020\n.1251\u02d82020\n.1186\n\n.0534\n.0529\n.0528\n\nknown words\n\nu:big\n\nstd\n\n.00056\n\n.00103\n\n.00095\n\n.00027\n\n.00031\n\n.00033\n\nerr\n\n.1186\n\n.0801\n\n.0802\u02d82217\n\n.0503\n\n.0502\u02d82217\n\n.0502\n\nstd\n\n.00095\n\n.00042\n\n.00048\n\n.00025\n\n.00031\n\n.00031\n\nIncreasing the amount of labeled data consistently reduces error rates, as does increasing unlabeled\ndata. The differences are significant for ONLINE tagging in all six cases, marked by \u02d82217 in the\ntable.\n\nThere is no significant difference in variability between ONLINE and BATCH, suggesting that\nONLINE is preferable due to its equal variability and higher performance, without requiring a dataset\navailable before tagging begins.\n\nThe progression of tagging accuracy over time is illustrated in Figure 1. BATCH and STATIC\nmaintain constant error rates as they do not adjust representations during tagging. ONLINE\u2019s error\nrate for unknown words decreases, approaching BATCH\u2019s error rate, as more is learned with each\noccurrence of an unknown word.",
  "related_work": "Online learning typically refers to supervised learning algorithms that update the model after process-\ning a few training examples. Many supervised learning algorithms are online or have online versions.\nActive learning is another supervised learning framework that processes training examples \u02d82014\nusually obtained interactively \u02d82014 in small batches. All of this work on supervised online learning is\nnot directly relevant to this paper since we address the problem of unsupervised domain adaptation.\nUnlike online supervised learners, we keep the statistical model unchanged during domain adaptation\nand adopt a representation learning approach: each unlabeled context of a word is used to update its\nrepresentation.\n\nThere is much work on unsupervised domain adaptation for part-of-speech tagging, including work\nusing constraint-based methods, instance weighting, self-training, and co-training. All of this work\nuses batch learning. For space reasons, we do not discuss supervised domain adaptation.",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "This study introduces a method for real-time updating of word representations, a new form of domain\nadaptation designed for scenarios where target domain data are processed in a stream, making\nBATCH processing unfeasible. We demonstrate that real-time unsupervised domain adaptation\nachieves performance levels comparable to batch learning. Moreover, it significantly reduces error\nrates compared to STATIC methods, which do not employ domain adaptation.\n\nAcknowledgments. This research was supported by a scholarship from Baidu awarded to Wenpeng\nYin and by the Deutsche Forschungsgemeinschaft (grant DFG SCHU 2246/10-1 FADeBaC).\n\n5",
  "is_publishable": 1,
  "venue": NaN
}
{
  "title": "A Toolkit for Scrutinizing Neural Network Activations",
  "abstract": "This document introduces diagNNose, an open-source toolkit designed for the\nexamination of activations within deep neural networks. diagNNose offers a diverse\ncollection of interpretability methods, enabling a deeper understanding of the\noperational dynamics of neural networks. The utility of diagNNose is showcased\nthrough an investigation into subject-verb agreement in language models.",
  "introduction": "We present diagNNose, a publicly available library for analyzing the behavior of\ndeep neural networks. The diagNNose library equips researchers with tools to gain\nenhanced understanding of the internal representations formed by these networks,\nproviding a comprehensive suite of established analysis methods. It accommodates\na variety of model types, with a particular focus on NLP architectures, such as\nLSTMs and Transformers.\nThe availability of open-source libraries has been instrumental in the advancement\nand wider adoption of NLP technologies. We enhance the open-source ecosystem\nby integrating several interpretability techniques.\nRecent years have witnessed significant interest in enhancing our understanding of\nthe mechanisms by which deep neural networks function. The high-dimensional\narchitecture of these models makes deciphering their internal dynamics a complex\nendeavor. This complexity has spurred the emergence of a specialized subfield\nwithin AI, dedicated to interpretability. diagNNose seeks to consolidate a range of\nthese interpretability techniques into a unified library.\nThe primary objective of diagNNose is to facilitate the discovery of linguistic\nknowledge encoded within a model\u2019s representations. The library offers abstrac-\ntions that enable the investigation of recurrent models in a manner similar to\nTransformer models, using a modular design. It includes a module for extracting\nmodel activations. The analysis methods currently implemented in the library\ninclude targeted syntactic evaluation tasks, probing with diagnostic classifiers, and\nfeature attributions.\nThis paper provides a comprehensive overview of the library and illustrates its\napplication in a case study centered on subject-verb agreement within language\nmodels. Subsequently, we provide a survey of diagNNose and elaborate on its\nspecific modules. We conclude with the case study.\n\n2 Background\n\nThe increasing capabilities of language models have resulted in a vibrant area of\nresearch focused on understanding their functionality. Approaches in this field\nare frequently interdisciplinary. diagNNose facilitates several influential analysis\nmethods.\n\n2.1 Targeted Syntactic Evaluations\n\nLanguage models have been central to numerous achievements in NLP. These\nmodels are trained to predict the probability of upcoming or masked tokens. To\n\n\fachieve success in this task, models must grasp various linguistic aspects, including\nsyntax, semantics, and general domain knowledge. One notable area of research\ninvestigating a model\u2019s linguistic competence employs targeted syntactic evalu-\nations. This analysis method contrasts a model\u2019s outputs on minimally different\npairs of grammatical and ungrammatical constructions. If a model assigns a higher\nprobability to the grammatical construction, it suggests an understanding of the\nrelevant linguistic principles.\ndiagNNose supports a diverse set of syntactic tasks and offers an interface for\nincorporating new tasks seamlessly.\n\n2.2 Diagnostic Classifiers\n\nAnother line of research evaluates a model\u2019s comprehension of linguistic properties\nby training diagnostic classifiers on its representations. This technique, also known\nas probing, has yielded valuable insights into the internal mechanisms of language\nmodels. The activations used for training these classifiers are not limited to the\nhidden states of a language model at its top layer.\nThere have been recent discussions regarding the extent to which high accuracy in a\ndiagnostic classifier truly signifies that a property is actively encoded by the model.\nSeveral methods have been put forward to address this, such as using control tasks\nor assessing classifiers based on minimum description length. diagNNose currently\nsupports the training of diagnostic classifiers and control tasks.\n\n2.3 Feature Attributions\n\nWhile probing helps us identify specific properties embedded in model repre-\nsentations, it does not clarify how a model converts input features into accurate\npredictions. This can be addressed by calculating the contributions of input fea-\ntures to subsequent outputs. This is a complex task due to the high-dimensional,\nnon-linear nature of deep learning models.\nFeature attributions can be calculated in various manners. One common method\ninvolves a concept from cooperative game theory, referred to as the Shapley value.\nComputing Shapley values is computationally intensive, leading to the develop-\nment of several approximation algorithms. diagNNose currently supports feature\nattribution computation using Contextual Decomposition and its generalization.\n\n3 Library Overview\n\n3.1 Modules\n\nThe library is organized into multiple modules that can be utilized as components\nfor constructing an experimental pipeline.\n\n3.1.1 Core Modules\n\nThe foundational modules underpinning the various pipelines that can be built\nusing diagNNose are detailed below.\n**models:** We offer a generalized framework for language models, enabling\nboth recurrent and Transformer models to be accessed through a unified interface.\nImporting pre-trained Transformer models is accomplished using the transform-\ners library. For recurrent models, we provide an interface that allows access to\nintermediate activations, including gate activations.\n**corpus:** Corpora are imported as Datasets from the torchtext package. A\nCorpus can be converted into an iterator for processing. Tokenization can be\nperformed traditionally, token-by-token, or based on subword units, such as byte\npair encodings.\n**extract:** The extraction of activations is fundamental to most analysis modules.\nWe provide an Extractor class capable of extracting a model\u2019s activations given a\ncorpus. This process is not restricted to the top layer; intermediate (gate) activations\ncan also be extracted. Activations can be dynamically saved to disk to facilitate the\nextraction from large corpora with limited computational resources.\n\n2\n\n\f**activations:** Extracted activations can be readily accessed using an Activation-\nReader, which provides access to activations corresponding to specific subsets of\ncorpus sentences. We also offer functionality for extracting only particular subsets\nof activations, based on sentence and token information.\n**config:** The pipeline of diagNNose is driven by configuration defined in JSON\nformat. Individual attributes can also be directly set from the command line.\n\n3.1.2 Analysis Modules\n\nWe presently offer three primary types of experimental modules.\n**syntax:** The library offers capabilities for a broad range of targeted syntactic\nevaluation tasks.\n**probe:** We furnish convenient tools for training diagnostic classifiers on ex-\ntracted activations to probe for linguistic information that may be embedded within\nthem. Our extraction module also enables training diagnostic classifiers on in-\ntermediate activations, including gate activations. To address concerns that high\nprobing accuracy does not necessarily indicate that linguistic information is actively\nencoded, we have incorporated functionality for Control Tasks.\n**attribute:** We offer capabilities for model-agnostic feature attributions, en-\nabling the decomposition of a model\u2019s output into a sum of contributions. This\nis accomplished by implementing a wrapper over PyTorch operations, allowing\nintermediate feature contributions to be propagated during a forward pass. Our\nimplementation supports various Shapley-based attribution methods and facili-\ntates approximation procedures such as (Generalized) Contextual Decomposition\nand Shapley sampling values, in addition to the exact computation of propagated\nShapley values.\n\n3.2 Requirements\n\ndiagNNose can be installed using pip (pip install diagnnose) or cloned directly\nfrom the GitHub repository. The library is compatible with Python 3.6 or later,\nand its primary dependencies are PyTorch (v1.5+), torchtext, and HuggingFace\u2019s\ntransformers. diagNNose is released under the MIT License. It operates on both\nCPUs and GPUs and has been optimized for smaller consumer setups.\nThe diagNNose codebase is fully typed using Python type hints and formatted\nusing Black. All methods and classes are documented, with an overview available\nonline.\n\n4 Case Study: Subject-Verb Agreement\n\nTo exemplify the functionality of diagNNose, we examine subject-verb agreement\ncorpora on a selection of language models. For our experiments, we analyze the\nfollowing models: BERT, RoBERTa, DistilRoBERTa, and an LSTM language\nmodel.\n\n4.1 Corpora\n\nThe corpora consist of seven tasks based on template-based syntactic constructions.\nThese constructions feature an \"agreement attractor\" between the subject and the\nverb, which may mislead a language model into predicting the incorrect number of\nthe verb. Consequently, a model must possess a robust understanding of sentence\nstructure.\nThe seven tasks are defined by the following templates:\n* SIMPLE: The athletes approve * ADV: The uncle probably avoids * 2ADV: The\nathlete most probably understands * COADV: The farmer overtly and deliberately\nknows * NAMEPP: The women near John remember * NOUNPP: The athlete\nbeside the tables approves * NOUNPPADV: The aunt behind the bikes certainly\nknows\n\n3\n\n\fEach task encompasses 600 to 900 distinct sentences. Sentences are categorized\ninto multiple conditions based on the number of the subject and the intervening\nnoun phrase.\nTo assess these corpora on a recurrent model, we initially compute the model\u2019s\nhidden state at the verb\u2019s position by feeding it the sub-sentence up to that point.\nBased on this hidden state, we compute and compare the output probabilities of the\nverb with the correct number (vc) and the incorrect number (vx):\nP(vc | he) > P(vx | he)\nFor bi-directional masked language models, such as BERT, we cannot compute\nan intermediate hidden state by passing a sub-sentence because these models also\nincorporate input from future tokens. To address this, we substitute the verb in\neach sentence with a <mask> token and evaluate the model\u2019s probabilities at this\ntoken\u2019s position.\nMany contemporary language models employ BPE tokenization, which may seg-\nment a word into multiple subwords. Therefore, in our experiments, we only\ncompare verb forms where both the plural and singular forms are split into a single\ntoken.\n\n4.2 Targeted Syntactic Evaluations\n\nWe execute the targeted syntactic evaluation suite on all seven templates. The\nresults of this experiment are presented in Table 1.\n\nTable 1: Results of the targeted syntactic evaluation tasks.\n\nCorpus\n\nCondition BERT RoBERTa DistilRoBERTa LSTM\n\nADV\n\n2ADV\n\nCOADV\n\nSIMPLE\n\nNAMEPP\n\nS\nP\nS\nP\nS\nP\nS\nP\nSS\nPS\nSS\nSP\nPS\nPP\nNOUNPPADV SS\nSP\nPS\nPP\n\nNOUNPP\n\n100\n100\n100\n100\n100\n100\n100\n100\n93.0\n88.4\n95.7\n93.3\n96.7\n100\n99.6\n99.2\n100\n100\n\n100\n100\n100\n100\n100\n100\n100\n100\n75.7\n65.9\n88.9\n84.7\n90.6\n100\n100\n99.8\n100\n100\n\n100\n100\n100\n100\n100\n100\n100\n100\n81.5\n32.4\n98.1\n91.1\n85.3\n100\n100\n100\n100\n100\n\n100\n100\n100\n99.6\n99.2\n99.3\n98.7\n99.3\n99.3\n68.9\n99.2\n87.2\n92.0\n99.0\n99.5\n91.2\n99.2\n99.8\n\nIt is evident that Transformer language models generally attain higher scores\ncompared to the LSTM model. Notably, the NAMEPP task presents a challenge for\nall models, with both RoBERTa and DistilRoBERTa scoring lower on this task than\nthe LSTM. Another intriguing observation is the disparity in performance between\nRoBERTa and DistilRoBERTa on the NAMEPP and NOUNPP tasks. Despite\nDistilRoBERTa being trained to mimic RoBERTa\u2019s behavior, its performance on\na downstream task like this differs considerably. These findings can serve as a\nfoundation for more detailed analysis.\n\n4.3 Feature Attributions\n\nTo gain a deeper understanding of why language models exhibit particularly poor\nperformance on the NAMEPP corpus, we employ the feature attribution module on\nthese constructions. The results for this experiment are presented below, illustrating\n\n4\n\n\fthe attributions for DistilRoBERTa on an example sentence from the corpus. This\nhighlights the differential impact of the intervening attractor on the verb\u2019s number.\nThe score at the top of the attribution represents the model\u2019s full logit for that\nclass; these logits are transformed into probabilities using SoftMax. This logit is\ndecomposed into a sum of contributions, indicated at the bottom of each token.\nIt can be verified that the contributions sum to the logit, which is an important\ncharacteristic of feature attribution methods, ensuring a degree of faithfulness to\nthe model. A negative value signifies a negative feature contribution to an output\nclass: the influence of that feature diminished the preference for the class. Feature\nattributions also incorporate the influence of model biases.\nIn the provided example sentence, DistilRoBERTa produces an incorrect prediction:\nthe logit of the incorrect singular form \u2019approves\u2019 is greater than that of the plural\n\u2019approve\u2019. The model\u2019s error in predicting the correct verb form arises from the\nsubject \u2019athletes\u2019 not providing sufficient contribution to outweigh the negative\ncontributions from other input features. A model with a comprehensive grasp of\nsubject-verb agreement should assign a larger contribution to the subject when\npredicting the main verb.\nThe attribute module is under active development. The exponential complexity of\ncomputing Shapley values makes generating these explanations a challenging task.",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "diagNNose offers crucial tools for interpretability research, providing advanced\nanalysis techniques such as diagnostic classifiers and feature attributions. The\nlibrary\u2019s modular architecture enables rapid testing of complex hypotheses and es-\ntablishes a robust groundwork for the development of new interpretability methods.\nThe library\u2019s code is open-source, and contributions are encouraged.\n\n5",
  "is_publishable": 1,
  "venue": NaN
}
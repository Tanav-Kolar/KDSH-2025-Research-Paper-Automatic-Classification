{
  "title": "Addressing Min-Max Challenges in Nonconvex-Nonconcave Problems\nwith Solutions Exhibiting Weak Minty Properties",
  "abstract": "strate a characteristic known as weak Minty solutions. This concept, which has only recently been defined, has\nalready demonstrated its effectiveness by encompassing various generalizations of monotonicity at the same time.\nWe establish new convergence findings for an enhanced variant of the optimistic gradient method (OGDA) within\nthis framework, achieving a convergence rate of 1/k for the most effective iteration, measured by the squared\noperator norm, a result that aligns with the extragradient method (EG). Furthermore, we introduce a modified\nversion of EG that incorporates an adaptive step size, eliminating the need for prior knowledge of the problem\u2019s\nspecific parameters.",
  "introduction": "have generated significant interest in saddle point problems. Examples of these models include generative adversarial networks,\nadversarial learning frameworks, adversarial example games, and actor-critic methods. While practical methods have been developed\nthat generally perform well, the theoretical understanding of scenarios where the objective function is nonconvex in the minimization\ncomponent and nonconcave in the maximization component remains limited, with some research even suggesting intractability in\ncertain cases.\n\nA specific subset of nonconvex-nonconcave min-max problems was analyzed, and it was found that the extragradient method (EG)\nexhibited favorable convergence behavior in experimental settings. Surprisingly, these problems did not appear to possess any of\nthe recognized favorable characteristics, such as monotonicity or Minty solutions. Subsequently, a suitable concept was identified\n(see Assumption 1), which is less restrictive than the presence of a Minty solution (a condition frequently employed in the existing\nliterature) and also extends the idea of negative comonotonicity. Because of these properties that unify and generalize, the concept of\nweak Minty solutions was quickly investigated.\nAssumption 1 (Weak Minty solution). For a given operator F : Rd \u2192 Rd, there is a point u\u2217 \u2208 Rd and a parameter \u03c1 > 0 such that:\n\u03c1\n2\n\n\u2225F (u)\u22252 \u2200u \u2208 Rd.\n\n\u27e8F (u), u \u2212 u\u2217\u27e9 \u2265 \u2212\n\n(1)\n\nMoreover, it has been demonstrated that a modified version of EG is capable of addressing problems with such solutions, achieving\na complexity of O(\u03f5\u22121) for the squared operator norm. This adaptation, referred to as EG+, is based on a bold extrapolation step\nfollowed by a cautious update step. A similar step size approach has been previously examined in the context of a stochastic variant\nof EG.\n\nIn a similar vein, we explore a variation of the optimistic gradient descent ascent (OGDA), also known as Forward-Reflected-\nBackward (FoRB). We address the following question with an affirmative answer:\n\nCan OGDA achieve convergence guarantees comparable to those of EG when dealing with weak Minty solutions?\n\nSpecifically, we demonstrate that a modified version of the OGDA method, defined for a step size a > 0 and a parameter 0 < \u03b3 \u2264 1\nas follows:\n\nuk = \u00afuk \u2212 aF (\u00afuk),\n\u00afuk+1 = \u00afuk \u2212 \u03b3aF (uk),\n\n\u2200k \u2265 0,\n\ncan achieve the same convergence bounds as EG+ by requiring only a single gradient oracle call in each iteration.\n\nIt is worth noting that OGDA is most frequently expressed in a form where \u03b3 = 1. However, two recent studies have examined\na more generalized coefficient. While these earlier studies focused on the monotone setting, the true significance of \u03b3 becomes\n\n\fapparent only when dealing with weak Minty solutions. In this context, we find that \u03b3 must be greater than 1 to ensure convergence,\na phenomenon that is not observed in monotone problems.\n\nWhen examining a general smooth min-max problem:\n\nmin\nx\n\nmax\ny\n\nf (x, y)\n\nthe operator F mentioned in Assumption 1 naturally emerges as F (u) := [\u2207xf (x, y), \u2212\u2207yf (x, y)] with u = (x, y). However,\nby examining saddle point problems from the broader viewpoint of variational inequalities (VIs) through the operator F , we can\nconcurrently address more scenarios, such as certain equilibrium problems.\n\nThe parameter \u03c1 in the definition of weak Minty solutions (1) is crucial for both the analysis and the experiments. Specifically, it\nis essential that the step size exceeds a value proportional to \u03c1. Simultaneously, as is typical, the step size is limited from above\nby the inverse of the Lipschitz constant of F . For instance, since some researchers require the step size to be less than 1\n4L , their\nconvergence claim is valid only if \u03c1 < 1\n4L . This condition was later improved to \u03c1 < 1\nL for\neven smaller values of \u03b3. As in the monotone setting, OGDA requires a smaller step size than EG. Nevertheless, through a different\nanalysis, we are able to match the most general condition on the weak Minty parameter \u03c1 < 1\n\n2L for the choice \u03b3 = 1 and to \u03c1 < 1\n\nL for appropriate \u03b3 and a.\n\n1.1 Contribution\n\nOur contributions are summarized as follows:\n\n1. We establish a new convergence rate of O(1/k), measured by the squared operator norm, for a modified version of OGDA,\nwhich we call OGDA+. This rate matches that of EG and builds upon the recently introduced concept of weak solutions to\nthe Minty variational inequality.\n\n2. Even when a stronger condition is imposed, specifically that the operator is also monotone, we enhance the range of feasible\n\nstep sizes for OGDA+ and obtain the most favorable result known for the standard method (\u03b3 = 1).\n\n3. We demonstrate a complexity bound of O(\u03f5\u22122) for a stochastic variant of the OGDA+ method.\n4. We also introduce an adaptive step size version of EG+. This version achieves the same convergence guarantees without\nrequiring any knowledge of the Lipschitz constant of the operator F . Consequently, it can potentially take larger steps in\nareas with low curvature, enabling convergence where a fixed step size strategy might fail.\n\n1.2 Related literature\n\nWe will concentrate on the nonconvex-nonconcave setting, as there is a substantial body of work on convergence rates in terms of a gap\nfunction or distance to a solution for monotone problems, as well as generalizations such as nonconvex-concave, convex-nonconcave,\nor under the Polyak-\u0141ojasiewicz assumption.\n\nWeak Minty. It was observed that a specific parameterization of the von Neumann ratio game exhibits a novel type of solution,\ntermed \"weak Minty,\" without having any of the previously known characteristics like (negative) comonotonicity or Minty solutions.\nConvergence in the presence of such solutions was demonstrated for EG, provided that the extrapolation step size is twice as large as\nthe update step. Subsequently, it was shown that the condition on the weak Minty parameter can be relaxed by further reducing the\nlength of the update step, and this is done adaptively. To avoid the need for additional hyperparameters, a backtracking line search is\nalso proposed, which may incur extra gradient computations or require second-order information (in contrast to the adaptive step\nsize we propose in Algorithm 3). A different approach is taken by focusing on the min-max setting and using multiple ascent steps\nper descent step, achieving the same O(1/k) rate as EG.\n\nMinty solutions. Numerous studies have presented various methods for scenarios where the problem at hand has a Minty solution.\nIt was shown that weakly monotone VIs can be solved by iteratively adding a quadratic proximity term and repeatedly optimizing\nthe resulting strongly monotone VI using any convergent method. The convergence of the OGDA method was proven, but without a\nspecific rate. It was noted that the convergence proof for the golden ratio algorithm (GRAAL) is valid without any changes. While\nthe assumption that a Minty solution exists is a generalization of the monotone setting, it is challenging to find non-monotone\nproblems that possess such solutions. In our setting, as per Assumption 1, the Minty inequality (MVI) can be violated at any point\nby a factor proportional to the squared operator norm.\n\nNegative comonotonicity. Although previously studied under the term \"cohypomonotonicity,\" the concept of negative comono-\ntonicity has recently been explored. It offers a generalization of monotonicity, but in a direction distinct from the concept of Minty\nsolutions, and only a limited number of studies have examined methods in this context. An anchored version of EG was studied, and\nan improved convergence rate of O(1/k2) (in terms of the squared operator norm) was shown. Similarly, an accelerated version of\nthe reflected gradient method was investigated. Whether such acceleration is possible in the more general setting of weak Minty\nsolutions remains an open question (any Stampacchia solution to the VI given by a negatively comonotone operator is a weak Minty\nsolution). Another intriguing observation was made, where for cohypomonotone problems, a monotonically decreasing gradient\nnorm was demonstrated when using EG. However, we did not observe this in our experiments, emphasizing the need to differentiate\nthis class from problems with weak Minty solutions.\n\n2\n\n\fInteraction dominance. The concept of \u03b1-interaction dominance for nonconvex-nonconcave min-max problems was investigated,\nand it was shown that the proximal-point method converges sublinearly if this condition is met in y and linearly if it is met in both\ncomponents. Furthermore, it was demonstrated that if a problem is interaction dominant in both components, it is also negatively\ncomonotone.\n\nOptimism. The positive effects of introducing the simple modification commonly known as optimism have recently attracted the\nattention of the machine learning community. Its name comes from online optimization. The idea dates back even further and has\nalso been studied in the mathematical programming community.\n\n2 Preliminaries\n\n2.1 Notions of solution\n\nWe outline the most frequently used solution concepts in the context of variational inequalities (VIs) and related areas. These\nconcepts are typically defined with respect to a constraint set C \u2286 Rd. A Stampacchia solution of the VI given by F : Rd \u2192 Rd is a\npoint u\u2217 such that:\n\n\u27e8F (u\u2217), u \u2212 u\u2217\u27e9 \u2265 0 \u2200u \u2208 C.\n(SVI)\nIn this work, we only consider the unconstrained case where C = Rd, and the above condition simplifies to F (u\u2217) = 0. Closely\nrelated is the following concept: A Minty solution is a point u\u2217 \u2208 C such that:\n\n\u27e8F (u), u \u2212 u\u2217\u27e9 \u2265 0 \u2200u \u2208 C.\n(MVI)\nFor a continuous operator F , a Minty solution of the VI is always a Stampacchia solution. The converse is generally not true but\nholds, for example, if the operator F is monotone. Specifically, there are nonmonotone problems with Stampacchia solutions but\nwithout any Minty solutions.\n\n2.2 Notions of monotonicity\n\nThis section aims to revisit some fundamental and more contemporary concepts of monotonicity and the relationships between them.\nAn operator F is considered monotone if:\n\n\u27e8F (u) \u2212 F (v), u \u2212 v\u27e9 \u2265 0.\nSuch operators naturally arise as the gradients of convex functions, from convex-concave min-max problems, or from equilibrium\nproblems.\n\nTwo frequently studied notions that fall into this category are strongly monotone operators, which satisfy:\n\n\u27e8F (u) \u2212 F (v), u \u2212 v\u27e9 \u2265 \u00b5\u2225u \u2212 v\u22252,\n\nand cocoercive operators, which fulfill:\n\n\u27e8F (u) \u2212 F (v), u \u2212 v\u27e9 \u2265 \u03b2\u2225F (u) \u2212 F (v)\u22252.\n(2)\nStrongly monotone operators emerge as gradients of strongly convex functions or in strongly-convex-strongly-concave min-max\nproblems. Cocoercive operators appear, for instance, as gradients of smooth convex functions, in which case (2) holds with \u03b2 equal\nto the inverse of the gradient\u2019s Lipschitz constant.\n\nDeparting from monotonicity. Both of the aforementioned subclasses of monotonicity can serve as starting points for exploring\nthe non-monotone domain. Given that general non-monotone operators may display erratic behavior, such as periodic cycles and\nspurious attractors, it is reasonable to seek settings that extend the monotone framework while remaining manageable. First and\nforemost is the extensively studied setting of \u03bd-weak monotonicity:\n\n\u27e8F (u) \u2212 F (v), u \u2212 v\u27e9 \u2265 \u2212\u03bd\u2225u \u2212 v\u22252.\nSuch operators arise as the gradients of the well-studied class of weakly convex functions, a rather general class of functions as it\nincludes all functions without upward cusps. In particular, every smooth function with a Lipschitz gradient turns out to fulfill this\nproperty. On the other hand, extending the notion of cocoercivity to allow for negative coefficients, referred to as cohypomonotonicity,\nhas received much less attention and is given by:\n\nClearly, if a Stampacchia solution exists for such an operator, then it also fulfills Assumption 1.\n\n\u27e8F (u) \u2212 F (v), u \u2212 v\u27e9 \u2265 \u2212\u03b3\u2225F (u) \u2212 F (v)\u22252.\n\nBehavior with respect to the solution. While the above properties are standard assumptions in the literature, it is usually sufficient\nto require the corresponding condition to hold when one of the arguments is a (Stampacchia) solution. This means that instead of\nmonotonicity, it is enough to ask for the operator F to be star-monotone, i.e.,\n\n\u27e8F (u), u \u2212 u\u2217\u27e9 \u2265 0,\n\nor star-cocoercive,\n\n\u27e8F (u), u \u2212 u\u2217\u27e9 \u2265 \u03b3\u2225F (u)\u22252.\nIn this spirit, we can provide a new interpretation to the assumption of the existence of a weak Minty solution as asking for the\noperator F to be negatively star-cocoercive (with respect to at least one solution). Furthermore, we want to point out that while the\nabove star notions are sometimes required to hold for all solutions u\u2217, in the following we only require it to hold for a single solution.\n\n3\n\n\f3 OGDA for problems with weak Minty solutions\n\nThe generalized version of OGDA, which we denote with a \"+\" to emphasize the presence of the additional parameter \u03b3, is given by:\n\nAlgorithm 1 OGDA+\nRequire: Starting point u0 = u\u22121 \u2208 Rd, step size a > 0 and parameter 0 < \u03b3 < 1.\nfor k = 0, 1, ... do\n\nuk+1 = uk \u2212 a((1 + \u03b3)F (uk) \u2212 F (uk\u22121))\n\nend for\n\nTheorem 3.1. Let F : Rd \u2192 Rd be L-Lipschitz continuous satisfying Assumption 1 with 1\ngenerated by Algorithm 1 with step size a satisfying a > \u03c1 and\n\nL > \u03c1, and let (uk)k\u22650 be the iterates\n\naL \u2264\n\n1 \u2212 \u03b3\n1 + \u03b3\n\n.\n\n(3)\n\nThen, for all k \u2265 0,\n\nmin\ni=0,...,k\u22121\n\n\u2225F (ui)\u22252 \u2264\n\n1\nka\u03b3(a \u2212 \u03c1)\n\n\u2225u0 + aF (u0) \u2212 u\u2217\u22252.\n\nIn particular, as long as \u03c1 < 1\n\nL , we can find a \u03b3 small enough such that the above bound holds.\n\nThe first observation is that we would like to choose a as large as possible, as this allows us to treat the largest class of problems\nwith \u03c1 < a. To be able to choose a large step size a, we must decrease \u03b3, as evident from (3). However, this degrades the algorithm\u2019s\nspeed by making the update steps smaller. The same effect can be observed for EG+ and is therefore not surprising. One could\nderive an optimal \u03b3 (i.e., minimizing the right-hand side) from Theorem 3.1, but this results in a non-intuitive cubic dependence on\n\u03c1. In practice, the strategy of decreasing \u03b3 until convergence is achieved, but not further, yields reasonable results.\nFurthermore, we want to point out that the condition \u03c1 < 1\n\nL is precisely the best possible bound for EG+.\n\n3.1\n\nImproved bounds under monotonicity\n\nWhile the above theorem also holds if the operator F is monotone, we can modify the proof slightly to obtain a better dependence on\nthe parameters:\nTheorem 3.2. Let F : Rd \u2192 Rd be monotone and L-Lipschitz. If aL = 2\u2212\u03b3\nfulfill\n\n2+\u03b3 \u2212 \u03f5 for \u03f5 > 0, then the iterates generated by OGDA+\n\nmin\ni=0,...,k\u22121\n\n\u2225F (ui)\u22252 \u2264\n\n2\nka2\u03b32\u03f5\n\n\u2225u0 + aF (u0) \u2212 u\u2217\u22252.\n\nIn particular, we can choose \u03b3 = 1 and a < 1\n\n2L .\n\nThere are different works discussing the convergence of OGDA in terms of the iterates or a gap function with a < 1\n2L . However, we\nwant to compare the above bound to more similar results on rates for the best iterate in terms of the operator norm. The same rate as\nours for OGDA is shown, but requires the conservative step size bound a \u2264 1\n3L . All of these\nonly deal with the case \u03b3 = 1. The only other reference that deals with a generalized (i.e., not necessarily \u03b3 = 1) version of OGDA\nis another work, where the resulting step size condition is a \u2264 2\u2212\u03b3\n4L , which is strictly worse than ours for any \u03b3. To summarize, not\nonly do we show for the first time that the step size of a generalization of OGDA can go above 1\n2L , but we also provide the least\nrestrictive bound for any value of \u03b3.\n\n16L . This was later improved to a \u2264 1\n\n3.2 OGDA+ stochastic\n\nIn this section, we discuss the setting where, instead of the exact operator F , we only have access to a collection of independent\nestimators F (\u00b7, \u03bei) at every iteration. We assume here that the estimator F is unbiased, i.e., E[F (uk, \u03be)|uk\u22121] = F (uk), and has\nbounded variance E[\u2225F (uk, \u03be) \u2212 F (uk)\u22252] \u2264 \u03c32. We show that we can still guarantee convergence by using batch sizes B of order\nO(\u03f5\u22121).\n\nAlgorithm 2 stochastic OGDA+\nRequire: Starting point u0 = u\u22121 \u2208 Rd, step size a > 0, parameter 0 < \u03b3 \u2264 1 and batch size B.\nfor k = 0, 1, ... do\n\nSample i.i.d. (\u03bei)B\n\ni=1 and compute estimator \u02dcgk = 1\nB\nuk+1 = uk \u2212 a((1 + \u03b3)\u02dcgk \u2212 \u02dcgk\u22121)\n\nend for\n\n4\n\n(cid:80)B\n\ni=1 F (uk, \u03bek\ni )\n\n\fTheorem 3.3. Let F : Rd \u2192 Rd be L-Lipschitz satisfying Assumption 1 with 1\niterates generated by stochastic OGDA+, with a and \u03b3 satisfying \u03c1 < a < 1\u2212\u03b3\n1+\u03b3\nmini=0,...,k\u22121 E[\u2225F (ui)\u22252] < \u03f5, we require\n\nL > \u03c1, and let (uk)k\u22650 be the sequence of\n1\nL . Then, to visit an \u03f5-stationary point such that\n\n1\nka\u03b3(a \u2212 \u03c1)\n\n\u2225u0 + a\u02dcg0 \u2212 u\u2217\u22252 max\n\n(cid:26)\n\n1,\n\n(cid:27)\n\n4\u03c32\naL\u03f5\n\ncalls to the stochastic oracle \u02dcF , with large batch sizes of order O(\u03f5\u22121).\nIn practice, large batch sizes of order O(\u03f5\u22121) are typically not desirable; instead, a small or decreasing step size is preferred. In the\nweak Minty setting, this causes additional trouble due to the necessity of large step sizes to guarantee convergence. Unfortunately,\nthe current analysis does not allow for variable \u03b3.\n\n4 EG+ with adaptive step sizes\n\nIn this section, we present Algorithm 3, which is able to solve the previously mentioned problems without any knowledge of the\nLipschitz constant L, as it is typically difficult to compute in practice. Additionally, it is well known that rough estimates will lead to\nsmall step sizes and slow convergence behavior. However, in the presence of weak Minty solutions, there is additional interest in\nchoosing large step sizes. We observed in Theorem 3.1 and related works the fact that a crucial ingredient in the analysis is that the\nstep size is chosen larger than a multiple of the weak Minty parameter \u03c1 to guarantee convergence at all. For these reasons, we want\nto outline a method using adaptive step sizes, meaning that no step size needs to be supplied by the user and no line-search is carried\nout.\n\nSince the analysis of OGDA+ is already quite involved in the constant step size regime, we choose to equip EG+ with an adaptive\nstep size which estimates the inverse of the (local) Lipschitz constant, see (4). Due to the fact that the literature on adaptive methods,\nespecially in the context of VIs, is so vast, we do not aim to give a comprehensive review but highlight only a few with especially\ninteresting properties. In particular, we do not want to touch on methods with a linesearch procedure, which typically result in\nmultiple gradient computations per iteration.\n\nWe use a simple and therefore widely used step size choice that naively estimates the local Lipschitz constant and forces a monotone\ndecreasing behavior. Such step sizes have been used extensively for monotone VIs and similarly in the context of the mirror-prox\nmethod, which corresponds to EG in the setting of (non-Euclidean) Bregman distances.\n\nA version of EG with a different adaptive step size choice has been investigated, with the unique feature that it is able to achieve the\noptimal rates for both smooth and nonsmooth problems without modification. However, these rates are only for monotone VIs and\nare in terms of the gap function.\n\nOne of the drawbacks of adaptive methods resides in the fact that the step sizes are typically required to be nonincreasing, which",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "knowledge, the only method that is allowed to use nonmonotone step sizes to treat VIs and does not use a possibly costly linesearch\nis the golden ratio algorithm. It comes with the additional benefit of not requiring a global bound on the Lipschitz constant of F at\nall. While it is known that this method converges under the stronger assumption of the existence of Minty solutions, a quantitative\nconvergence result is still open.\n\nAlgorithm 3 EG+ with adaptive step size\nRequire: Starting points u0, \u00afu0 \u2208 Rd, initial step size a0 and parameters \u03c4 \u2208 (0, 1) and 0 < \u03b3 \u2264 1.\nfor k = 0, 1, ... do\n\nFind the step size:\n\n(cid:26)\n\nak = min\n\nak\u22121,\n\n\u03c4 \u2225\u00afuk \u2212 \u00afuk\u22121\u2225\n\u2225F (\u00afuk) \u2212 F (\u00afuk\u22121)\u2225\n\n(cid:27)\n\nCompute next iterate:\n\nuk = \u00afuk \u2212 akF (\u00afuk)\n\u00afuk+1 = \u00afuk \u2212 ak\u03b3F (uk).\n\nend for\n\n(4)\n\nClearly, ak is monotonically decreasing by construction. Moreover, it is bounded away from zero by the simple observation that\nak \u2265 min{a0, \u03c4 /L} > 0. The sequence therefore converges to a positive number, which we denote by a\u221e := limk ak.\nTheorem 4.1. Let F : Rd \u2192 Rd be L-Lipschitz that satisfies Assumption 1, where u\u2217 denotes any weak Minty solution, with\n2 and \u03c4 \u2208 (0, 1). Then, there exists a k0 \u2208 N such that\na\u221e > 2\u03c1, and let (uk)k\u22650 be the iterates generated by Algorithm 3 with \u03b3 = 1\n\nmin\ni=k0,...,k\n\n\u2225F (uk)\u22252 \u2264\n\n1\nk \u2212 k0\n\nL\n\u03c4 (a\u221e/2 \u2212 \u03c1)\n\n\u2225\u00afuk0 \u2212 u\u2217\u22252.\n\n5\n\n\fAlgorithm 3 presented above provides several benefits but also some drawbacks. The main advantage resides in the fact that the\nLipschitz constant of the operator F does not need to be known. Moreover, the step size choice presented in (4) might allow us\nto take steps much larger than what would be suggested by a global Lipschitz constant if the iterates never, or only during later\niterations, visit the region of high curvature (large local L). In such cases, these larger step sizes come with the additional advantage\nthat they allow us to solve a richer class of problems, as we are able to relax the condition \u03c1 < 1\n4L in the case of EG+ to \u03c1 < a\u221e/2,\nwhere a\u221e = limk ak \u2265 \u03c4 /L.\n\nOn the other hand, we face the problem that the bounds in Theorem 4.1 only hold after an unknown number of initial iterations when\nak/ak+1 \u2264 1\n\u03c4 is finally satisfied. In theory, this might take a long time if the curvature around the solution is much higher than in\nthe starting area, as this will force the need to decrease the step size very late into the solution process, resulting in the quotient\nak/ak+1 being too large. This drawback could be mitigated by choosing \u03c4 smaller. However, this will result in poor performance\ndue to small step sizes. Even for monotone problems where this type of step size has been proposed, this problem could not be\ncircumvented, and authors instead focused on the convergence of the iterates without any rate.\n\n5 Numerical experiments\n\nIn the following, we compare the EG+ method with the two methods we propose: OGDA+ and EG+ with adaptive step size (see\nAlgorithm 1 and Algorithm 3, respectively). Last but not least, we also include the CurvatureEG+ method, which is a modification\nof EG+ that adaptively chooses the ratio of extrapolation and update steps. In addition, a backtracking linesearch is performed with\nan initial guess made by second-order information, whose extra cost we ignore in the experiments.\n\n5.1 Von Neumann\u2019s ratio game\n\nWe consider von Neumann\u2019s ratio game, which is given by:\n\nmin\nx\u2208\u2206m\n\nmax\ny\u2208\u2206n\n\nV (x, y) =\n\n\u27e8x, Ry\u27e9\n\u27e8x, Sy\u27e9\n\n,\n\n(5)\n\nwhere R \u2208 Rm\u00d7n and S \u2208 Rm\u00d7n with \u27e8x, Sy\u27e9 > 0 for all x \u2208 \u2206m, y \u2208 \u2206n, with \u2206 := {z \u2208 Rd : zi > 0, (cid:80)d\ni=1 zi = 1} denoting\nthe unit simplex. Expression (5) can be interpreted as the value V (x, y) for a stochastic game with a single state and mixed strategies.\n\nWe see an illustration of a particularly difficult instance of (5). Interestingly, we still observe good convergence behavior, although\nan estimated \u03c1 is more than ten times larger than the estimated Lipschitz constant.\n\n5.2 Forsaken\n\nA particularly difficult min-max toy example with a \"Forsaken\" solution was proposed and is given by:\n\nmin\nx\u2208R\n\nmax\ny\u2208R\n\nx(y \u2212 0.45) + \u03d5(x) \u2212 \u03d5(y),\n\n(6)\n\n4 z2 \u2212 1\n\n6 z6 \u2212 2\n\n4 z4 + 1\n\nwhere \u03d5(z) = 1\n2 z. This problem exhibits a Stampacchia solution at (x\u2217, y\u2217) \u2248 (0.08, 0.4), but also two limit\ncycles not containing any critical point of the objective function. In addition, it was also observed that the limit cycle closer to\nthe solution repels possible trajectories of iterates, thus \"shielding\" the solution. Later, it was noticed that, restricted to the box\n\u2225(x, y)\u2225\u221e < 3, the above-mentioned solution is weak Minty with \u03c1 \u2265 2 \u00b7 0.477761, which is much larger than 1\n2L \u2248 0.08. In line\nwith these observations, we can see that none of the fixed step size methods with a step size bounded by 1\nL converge. In light of this\nobservation, a backtracking linesearch was proposed, which potentially allows for larger steps than predicted by the global Lipschitz\nconstant. Similarly, our proposed adaptive step size version of EG+ (see Algorithm 3) is also able to break through the repelling\nlimit cycle and converge to the solution. On top of this, it does so at a faster rate and without the need for additional computations in\nthe backtracking procedure.\n\n5.3 Lower bound example\n\nThe following min-max problem was introduced as a lower bound on the dependence between \u03c1 and L for EG+:\n\nmin\nx\u2208R\n\nmax\ny\u2208R\n\n\u00b5xy +\n\n\u03b6\n2\n\n(x2 \u2212 y2).\n\n(7)\n\nIn particular, it was stated that EG+ (with any \u03b3) and constant step size a = 1\nweak Minty solution with \u03c1 < 1\u2212\u03b3\n\nL , where \u03c1 and L can be computed explicitly in the above example and are given by:\n\nL converges for this problem if and only if (0, 0) is a\n\nL =\n\n(cid:112)\n\n\u00b52 + \u03b6 2\n\nand \u03c1 =\n\n\u00b52 \u2212 \u03b6 2\n2\u00b5\n\n.\n\nBy choosing \u00b5 = 3 and \u03b6 = \u22121, we get exactly \u03c1 = 1\nempirically observed. Although the general upper bound proved in Theorem 3.1 only states convergence in the case \u03c1 < 1\nobserve rapid convergence of OGDA+ for this example, showcasing that it can drastically outperform EG+ in some scenarios.\n\nL , therefore predicting divergence of EG+ for any \u03b3, which is exactly what is\nL , we\n\n6",
  "conclusion": "framework. Very recently, it was demonstrated that the O(1/k) bounds on the squared operator norm for EG and OGDA for the\nlast iterate (and not just the best one) are valid even in the negatively comonotone setting. Deriving a comparable statement in the\npresence of merely weak Minty solutions remains an open question.\n\nIn general, our analysis and experiments seem to suggest that there is minimal benefit in employing OGDA+ over EG+ for the\nmajority of problems, as the reduced iteration cost is counterbalanced by the smaller step size. An exception is presented by problem\n(7), which is not covered by theory, and OGDA+ is the only method capable of converging.\n\nFinally, we note that the previous paradigm in pure minimization of \"smaller step size ensures convergence\" but \"larger step size\ngets there faster,\" where the latter is typically constrained by the reciprocal of the gradient\u2019s Lipschitz constant, does not appear\nto hold true for min-max problems anymore. The analysis of various methods in the presence of weak Minty solutions indicates\nthat convergence can be lost if the step size is excessively small and sometimes needs to be larger than 1\nL , which one can typically\nonly hope for in adaptive methods. Our EG+ method with adaptive step size accomplishes this even without the added expense of a\nbacktracking linesearch.article graphicx\n\n7",
  "is_publishable": 1,
  "venue": "TMLR"
}
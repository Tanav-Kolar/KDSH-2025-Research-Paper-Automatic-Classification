{
  "title": "Advancements in 3D Food Modeling: A Review of the\nMetaFood Challenge Techniques and Outcomes",
  "abstract": "tion tracking has spurred the creation of sophisticated 3D reconstruction methods\nfor food. The lack of comprehensive, high-fidelity data, coupled with limited\ncollaborative efforts between academic and industrial sectors, has significantly\nhindered advancements in this domain. This study addresses these obstacles by\nintroducing the MetaFood Challenge, aimed at generating precise, volumetrically\naccurate 3D food models from 2D images, utilizing a checkerboard for size cal-\nibration. The challenge was structured around 20 food items across three levels\nof complexity: easy (200 images), medium (30 images), and hard (1 image). A\ntotal of 16 teams participated in the final assessment phase. The methodologies\ndeveloped during this challenge have yielded highly encouraging outcomes in\n3D food reconstruction, showing great promise for refining portion estimation in\ndietary evaluations and nutritional tracking. Further information on this workshop\nchallenge and the dataset is accessible via the provided URL.",
  "introduction": "approaches to dietary monitoring and nutritional assessment. The MetaFood Workshop Challenge\nrepresents a landmark initiative in this emerging field, responding to the pressing demand for precise\nand scalable techniques for estimating food portions and monitoring nutritional consumption. Such\ntechnologies are vital for fostering healthier eating behaviors and addressing health issues linked to\ndiet.\n\nBy concentrating on the development of accurate 3D models of food derived from various visual\ninputs, including multiple views and single perspectives, this challenge endeavors to bridge the\ndisparity between current methodologies and practical needs. It promotes the creation of unique\nsolutions capable of managing the intricacies of food morphology, texture, and illumination, while also\nmeeting the real-world demands of dietary evaluation. This initiative gathers experts from computer\nvision, machine learning, and nutrition science to propel 3D food reconstruction technologies forward.\nThese advancements have the potential to substantially enhance the precision and utility of food\nportion estimation across diverse applications, from individual health tracking to extensive nutritional\ninvestigations.\n\nConventional methods for assessing diet, like 24-Hour Recall or Food Frequency Questionnaires\n(FFQs), are frequently reliant on manual data entry, which is prone to inaccuracies and can be\nburdensome. The lack of 3D data in 2D RGB food images further complicates the use of regression-\nbased methods for estimating food portions directly from images of eating occasions. By enhancing\n3D reconstruction for food, the aim is to provide more accurate and intuitive nutritional assessment\ntools. This technology could revolutionize the sharing of culinary experiences and significantly\nimpact nutrition science and public health.\n\nParticipants were tasked with creating 3D models of 20 distinct food items from 2D images, mim-\nicking scenarios where mobile devices equipped with depth-sensing cameras are used for dietary\n\n.\n\n\frecording and nutritional tracking. The challenge was segmented into three tiers of difficulty based\non the number of images provided: approximately 200 images for easy, 30 for medium, and a single\ntop-view image for hard. This design aimed to rigorously test the adaptability and resilience of\nproposed solutions under various realistic conditions. A notable feature of this challenge was the use\nof a visible checkerboard for physical referencing and the provision of depth images for each frame,\nensuring the 3D models maintained accurate real-world measurements for portion size estimation.\n\nThis initiative not only expands the frontiers of 3D reconstruction technology but also sets the stage\nfor more reliable and user-friendly real-world applications, including image-based dietary assessment.\nThe resulting solutions hold the potential to profoundly influence nutritional intake monitoring and\ncomprehension, supporting broader health and wellness objectives. As progress continues, innovative\napplications are anticipated to transform personal health management, nutritional research, and the\nwider food industry. The remainder of this report is structured as follows: Section 2 delves into the\nexisting literature on food portion size estimation, Section 3 describes the dataset and evaluation\nframework used in the challenge, and Sections 4, 5, and 6 discuss the methodologies and findings of\nthe top three teams (VolETA, ININ-VIAUN, and FoodRiddle), respectively.",
  "related_work": "volume, energy content, or macronutrients directly from images of meals. Unlike the well-studied\ntask of food recognition, estimating food portions is particularly challenging due to the lack of 3D\ninformation and physical size references necessary for accurately judging the actual size of food\nportions. Accurate portion size estimation requires understanding the volume and density of food,\nelements that are hard to deduce from a 2D image, underscoring the need for sophisticated techniques\nto tackle this problem. Current methods for estimating food portions are grouped into four categories.\n\nStereo-Based Approaches use multiple images to reconstruct the 3D structure of food. Some methods\nestimate food volume using multi-view stereo reconstruction based on epipolar geometry, while\nothers perform two-view dense reconstruction. Simultaneous Localization and Mapping (SLAM) has\nalso been used for continuous, real-time food volume estimation. However, these methods are limited\nby their need for multiple images, which is not always practical.\n\nModel-Based Approaches use predefined shapes and templates to estimate volume. For instance,\ncertain templates are assigned to foods from a library and transformed based on physical references to\nestimate the size and location of the food. Template matching approaches estimate food volume from\na single image, but they struggle with variations in food shapes that differ from predefined templates.\nRecent work has used 3D food meshes as templates to align camera and object poses for portion size\nestimation.\n\nDepth Camera-Based Approaches use depth cameras to create depth maps, capturing the distance from\nthe camera to the food. These depth maps form a voxel representation used for volume estimation.\nThe main drawback is the need for high-quality depth maps and the extra processing required for\nconsumer-grade depth sensors.\n\nDeep Learning Approaches utilize neural networks trained on large image datasets for portion\nestimation. Regression networks estimate the energy value of food from single images or from an\n\"Energy Distribution Map\" that maps input images to energy distributions. Some networks use both\nimages and depth maps to estimate energy, mass, and macronutrient content. However, deep learning\nmethods require extensive data for training and are not always interpretable, with performance\ndegrading when test images significantly differ from training data.\n\nWhile these methods have advanced food portion estimation, they face limitations that hinder their\nwidespread use and accuracy. Stereo-based methods are impractical for single images, model-based\napproaches struggle with diverse food shapes, depth camera methods need specialized hardware,\nand deep learning approaches lack interpretability and struggle with out-of-distribution samples. 3D\nreconstruction offers a promising solution by providing comprehensive spatial information, adapting\nto various shapes, potentially working with single images, offering visually interpretable results,\nand enabling a standardized approach to food portion estimation. These benefits motivated the\norganization of the 3D Food Reconstruction challenge, aiming to overcome existing limitations and\n\n2\n\n\fdevelop more accurate, user-friendly, and widely applicable food portion estimation techniques,\nimpacting nutritional assessment and dietary monitoring.\n\n3 Datasets and Evaluation Pipeline\n\n3.1 Dataset Description\n\nThe dataset for the MetaFood Challenge features 20 carefully chosen food items from the MetaFood3D\ndataset, each scanned in 3D and accompanied by video recordings. To ensure precise size accuracy\nin the reconstructed 3D models, each food item was captured alongside a checkerboard and pattern\nmat, serving as physical scaling references. The challenge is divided into three levels of difficulty,\ndetermined by the quantity of 2D images provided for reconstruction:\n\n\u2022 Easy: Around 200 images taken from video.\n\n\u2022 Medium: 30 images.\n\n\u2022 Hard: A single image from a top-down perspective.\n\nTable 1 details the food items included in the dataset.\n\nTable 1: MetaFood Challenge Data Details\n\nObject Index\n\nFood Item\n\nDifficulty Level Number of Frames\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\nEasy\nEasy\nEasy\nEasy\nEasy\nEasy\nEasy\nEasy\n\nStrawberry\nCinnamon bun\nPork rib\nCorn\nFrench toast\nSandwich\nBurger\nCake\nBlueberry muffin Medium\nMedium\nBanana\nMedium\nSalmon\nMedium\nSteak\nMedium\nBurrito\nMedium\nHotdog\nChicken nugget\nMedium\nEverything bagel Hard\nHard\nCroissant\nHard\nShrimp\nHard\nWaffle\nHard\nPizza\n\n199\n200\n200\n200\n200\n200\n200\n200\n30\n30\n30\n30\n30\n30\n30\n1\n1\n1\n1\n1\n\n3.2 Evaluation Pipeline\n\nThe evaluation process is split into two phases, focusing on the accuracy of the reconstructed 3D\nmodels in terms of shape (3D structure) and portion size (volume).\n\n3.2.1 Phase-I: Volume Accuracy\n\nIn the first phase, the Mean Absolute Percentage Error (MAPE) is used to evaluate portion size\naccuracy, calculated as follows:\n\nMAPE =\n\n1\nn\n\nn\n(cid:88)\n\ni=1\n\n(cid:12)\n(cid:12)\n(cid:12)\n(cid:12)\n\nAi \u2212 Fi\nAi\n\n(cid:12)\n(cid:12)\n(cid:12)\n(cid:12)\n\n\u00d7 100%\n\n3\n\n(1)\n\n\fwhere Ai is the actual volume (in ml) of the i-th food item obtained from the scanned 3D food mesh,\nand Fi is the volume calculated from the reconstructed 3D mesh.\n\n3.2.2 Phase-II: Shape Accuracy\n\nTeams that perform well in Phase-I are asked to submit complete 3D mesh files for each food item.\nThis phase involves several steps to ensure precision and fairness:\n\n\u2022 Model Verification: Submitted models are checked against the final Phase-I submissions for\n\nconsistency, and visual inspections are conducted to prevent rule violations.\n\n\u2022 Model Alignment: Participants receive ground truth 3D models and a script to compute the\nfinal Chamfer distance. They must align their models with the ground truth and prepare a\ntransformation matrix for each submitted object. The final Chamfer distance is calculated\nusing these models and matrices.\n\n\u2022 Chamfer Distance Calculation: Shape accuracy is assessed using the Chamfer distance\n\nmetric. Given two point sets X and Y , the Chamfer distance is defined as:\n\ndCD(X, Y ) =\n\n1\n|X|\n\n(cid:88)\n\nx\u2208X\n\nmin\ny\u2208Y\n\n\u2225x \u2212 y\u22252\n\n2 +\n\n1\n|Y |\n\n(cid:88)\n\ny\u2208Y\n\nmin\nx\u2208X\n\n\u2225x \u2212 y\u22252\n2\n\n(2)\n\nThis metric offers a comprehensive measure of similarity between the reconstructed 3D models and\nthe ground truth. The final ranking is determined by combining scores from both Phase-I (volume\naccuracy) and Phase-II (shape accuracy). Note that after the Phase-I evaluation, quality issues were\nfound with the data for object 12 (steak) and object 15 (chicken nugget), so these items were excluded\nfrom the final overall evaluation.\n\n4 First Place Team - VolETA\n\n4.1 Methodology\n\nThe team\u2019s research employs multi-view reconstruction to generate detailed food meshes and calculate\nprecise food volumes.\n\n4.1.1 Overview\n\nThe team\u2019s method integrates computer vision and deep learning to accurately estimate food volume\nfrom RGBD images and masks. Keyframe selection ensures data quality, supported by perceptual\nhashing and blur detection. Camera pose estimation and object segmentation pave the way for neural\nsurface reconstruction, creating detailed meshes for volume estimation. Refinement steps, including\nisolated piece removal and scaling factor adjustments, enhance accuracy. This approach provides a\nthorough solution for accurate food volume assessment, with potential uses in nutrition analysis.\n\n4.1.2 The Team\u2019s Proposal: VolETA\n\ni }n\n\nThe team starts by acquiring input data, specifically RGBD images and corresponding food object\nmasks. The RGBD images, denoted as ID = {IDi}n\ni=1, where n is the total number of frames,\nprovide depth information alongside RGB images. The food object masks, {M f\ni=1, help identify\nregions of interest within these images.\nNext, the team selects keyframes. From the set {IDi}n\ni=1 are\nchosen. A method is implemented to detect and remove duplicate and blurry images, ensuring\nhigh-quality frames. This involves applying a Gaussian blurring kernel followed by the fast Fourier\ntransform method. Near-Image Similarity uses perceptual hashing and Hamming distance threshold-\ning to detect similar images and retain overlapping ones. Duplicates and blurry images are excluded\nto maintain data integrity and accuracy.\nUsing the selected keyframes {I K\nj=1, the team estimates camera poses through a method called\nPixSfM, which involves extracting features using SuperPoint, matching them with SuperGlue, and\nrefining them. The outputs are the camera poses {Cj}k\nj=1, crucial for understanding the scene\u2019s\nspatial layout.\n\ni=1, keyframes {I K\n\nj=1 \u2286 {IDi}n\n\nj }k\n\nj }k\n\n4\n\n\fIn parallel, the team uses a tool called SAM for reference object segmentation. SAM segments\nthe reference object with a user-provided prompt, producing a reference object mask M R for each\nkeyframe. This mask helps track the reference object across all frames. The XMem++ method\nextends the reference object mask M R to all frames, creating a comprehensive set of reference object\nmasks {M R\n\ni=1. This ensures consistent reference object identification throughout the dataset.\n\ni }n\n\nTo create RGBA images, the team combines RGB images, reference object masks {M R\nfood object masks {M F\nunified format for further processing.\ni }n\nThe team converts the RGBA images {I R\nand modeled data Dm. This transformation facilitates accurate scene reconstruction.\n\ni=1. This step, denoted as {I R\n\ni=1, and\ni=1, integrates various data sources into a\n\ni=1 and camera poses {Cj}k\n\nj=1 into meaningful metadata\n\ni }n\n\ni }n\n\ni }n\n\nThe modeled data Dm is input into NeuS2 for mesh reconstruction. NeuS2 generates colorful meshes\n{Rf , Rr} for the reference and food objects, providing detailed 3D representations. The team uses the\n\"Remove Isolated Pieces\" technique to refine the meshes. Given that the scenes contain only one food\nitem, the diameter threshold is set to 5% of the mesh size. This method deletes isolated connected\ncomponents with diameters less than or equal to 5%, resulting in a cleaned mesh {RC f , RC r}. This\nstep ensures that only significant parts of the mesh are retained.\n\nThe team manually identifies an initial scaling factor S using the reference mesh via MeshLab. This\nfactor is fine-tuned to Sf using depth information and food and reference masks, ensuring accurate\nscaling relative to real-world dimensions. Finally, the fine-tuned scaling factor Sf is applied to the\ncleaned food mesh RC f , producing the final scaled food mesh RF f . This step culminates in an\naccurately scaled 3D representation of the food object, enabling precise volume estimation.\n\n4.1.3 Detecting the scaling factor\n\nGenerally, 3D reconstruction methods produce unitless meshes by default. To address this, the team\nmanually determines the scaling factor by measuring the distance for each block of the reference\nobject mesh. The average of all block lengths lavg is calculated, while the actual real-world length is\nconstant at lreal = 0.012 meters. The scaling factor S = lreal/lavg is applied to the clean food mesh\nRC f , resulting in the final scaled food mesh RF f in meters.\n\nThe team uses depth information along with food and reference object masks to validate the scaling\nfactors. The method for assessing food size involves using overhead RGB images for each scene.\nInitially, the pixel-per-unit (PPU) ratio (in meters) is determined using the reference object. Subse-\nquently, the food width (fw) and length (fl) are extracted using a food object mask. To determine the\nfood height (fh), a two-step process is followed. First, binary image segmentation is performed using\nthe overhead depth and reference images, yielding a segmented depth image for the reference object.\nThe average depth is then calculated using the segmented reference object depth (dr). Similarly,\nemploying binary image segmentation with an overhead food object mask and depth image, the\naverage depth for the segmented food depth image (df ) is computed. The estimated food height fh is\nthe absolute difference between dr and df . To assess the accuracy of the scaling factor S, the food\nbounding box volume (fw \u00d7 fl \u00d7 fh) \u00d7 P P U is computed. The team evaluates if the scaling factor\nS generates a food volume close to this potential volume, resulting in Sf ine. Table 2 lists the scaling\nfactors, PPU, 2D reference object dimensions, 3D food object dimensions, and potential volume.\n\nFor one-shot 3D reconstruction, the team uses One-2-3-45 to reconstruct a 3D model from a single\nRGBA view input after applying binary image segmentation to both food RGB and mask images.\nIsolated pieces are removed from the generated mesh, and the scaling factor S, which is closer to the\npotential volume of the clean mesh, is reused.\n\n4.2 Experimental Results\n\n4.2.1 Implementation settings",
  "methodology": "",
  "experiments": "Hamming distance for near image similarity was set to 12. For Gaussian kernel radius, even numbers\nin the range [0...30] were used for detecting blurry images. The diameter for removing isolated pieces\nwas set to 5%. NeuS2 was run for 15,000 iterations with a mesh resolution of 512x512, a unit cube\n\"aabb scale\" of 1, \"scale\" of 0.15, and \"offset\" of [0.5, 0.5, 0.5] for each food scene.\n\n5\n\n\f4.2.2 VolETA Results\n\nThe team extensively validated their approach on the challenge dataset and compared their results\nwith ground truth meshes using MAPE and Chamfer distance metrics. The team\u2019s approach was\napplied separately to each food scene. A one-shot food volume estimation approach was used if\nthe number of keyframes k equaled 1; otherwise, a few-shot food volume estimation was applied.\nNotably, the keyframe selection process chose 34.8% of the total frames for the rest of the pipeline,\nshowing the minimum frames with the highest information.\n\nTable 2: List of Extracted Information Using RGBD and Masks\n\nLevel\n\nId\n\nLabel\n\nSf\n\nEasy\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nMedium 11\n13\n14\n16\n17\n18\n19\n20\n\nHard\n\nStrawberry\nCinnamon bun\nPork rib\nCorn\nFrench toast\nSandwich\nBurger\nCake\nBlueberry muffin\nBanana\nSalmon\nBurrito\nFrankfurt sandwich\nEverything bagel\nCroissant\nShrimp\nWaffle\nPizza\n\n0.08955223881\n0.1043478261\n0.1043478261\n0.08823529412\n0.1034482759\n0.1276595745\n0.1043478261\n0.1276595745\n0.08759124088\n0.08759124088\n0.1043478261\n0.1034482759\n0.1034482759\n0.08759124088\n0.1276595745\n0.08759124088\n0.01034482759\n0.01034482759\n\nPPU\n\n0.01786\n0.02347\n0.02381\n0.01897\n0.02202\n0.02426\n0.02435\n0.02143\n0.01801\n0.01705\n0.02390\n0.02372\n0.02115\n0.01747\n0.01751\n0.02021\n0.01902\n0.01913\n\nRw \u00d7 Rl\n320 \u00d7 360\n236 \u00d7 274\n246 \u00d7 270\n291 \u00d7 339\n266 \u00d7 292\n230 \u00d7 265\n208 \u00d7 264\n256 \u00d7 300\n291 \u00d7 357\n315 \u00d7 377\n242 \u00d7 269\n244 \u00d7 271\n266 \u00d7 304\n306 \u00d7 368\n319 \u00d7 367\n249 \u00d7 318\n294 \u00d7 338\n292 \u00d7 336\n\n(fw \u00d7 fl \u00d7 fh)\n(238 \u00d7 257 \u00d7 2.353)\n(363 \u00d7 419 \u00d7 2.353)\n(435 \u00d7 778 \u00d7 1.176)\n(262 \u00d7 976 \u00d7 2.353)\n(530 \u00d7 581 \u00d7 2.53)\n(294 \u00d7 431 \u00d7 2.353)\n(378 \u00d7 400 \u00d7 2.353)\n(298 \u00d7 310 \u00d7 4.706)\n(441 \u00d7 443 \u00d7 2.353)\n(446 \u00d7 857 \u00d7 1.176)\n(201 \u00d7 303 \u00d7 1.176)\n(251 \u00d7 917 \u00d7 2.353)\n(400 \u00d7 1022 \u00d7 2.353)\n(458 \u00d7 134 \u00d7 1.176)\n(395 \u00d7 695 \u00d7 2.176)\n(186 \u00d7 95 \u00d7 0.987)\n(465 \u00d7 537 \u00d7 0.8)\n(442 \u00d7 651 \u00d7 1.176)\n\nAfter finding keyframes, PixSfM estimated the poses and point cloud. After generating scaled meshes,\nthe team calculated volumes and Chamfer distance with and without transformation metrics. Meshes\nwere registered with ground truth meshes using ICP to obtain transformation metrics.\n\nTable 3 presents quantitative comparisons of the team\u2019s volumes and Chamfer distance with and\nwithout estimated transformation metrics from ICP. For overall method performance, Table 4 shows\nthe MAPE and Chamfer distance with and without transformation metrics.\n\nAdditionally, qualitative results on one- and few-shot 3D reconstruction from the challenge dataset\nare shown. The model excels in texture details, artifact correction, missing data handling, and color\nadjustment across different scene parts.\n\nLimitations: Despite promising results, several limitations need to be addressed in future work:\n\n\u2022 Manual processes: The current pipeline includes manual steps like providing segmentation\nprompts and identifying scaling factors, which should be automated to enhance efficiency.\n\n\u2022 Input requirements: The method requires extensive input information, including food\nmasks and depth data. Streamlining these inputs would simplify the process and increase\napplicability.\n\n\u2022 Complex backgrounds and objects: The method has not been tested in environments with\n\ncomplex backgrounds or highly intricate food objects.\n\n\u2022 Capturing complexities: The method has not been evaluated under different capturing\n\ncomplexities, such as varying distances and camera speeds.\n\n\u2022 Pipeline complexity: For one-shot neural rendering, the team currently uses One-2-3-45.\nThey aim to use only the 2D diffusion model, Zero123, to reduce complexity and improve\nefficiency.\n\n6\n\n\fTable 3: Quantitative Comparison with Ground Truth Using Chamfer Distance\n\nL\n\nId\n\nTeam\u2019s Vol. GT Vol. Ch. w/ t.m Ch. w/o t.m\n\nE\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nM 11\n13\n14\n16\nH 17\n18\n19\n20\n\n40.06\n216.9\n278.86\n279.02\n395.76\n205.17\n372.93\n186.62\n224.08\n153.76\n80.4\n363.99\n535.44\n163.13\n224.08\n25.4\n110.05\n130.96\n\n38.53\n280.36\n249.67\n295.13\n392.58\n218.44\n368.77\n173.13\n232.74\n163.09\n85.18\n308.28\n589.83\n262.15\n181.36\n20.58\n108.35\n119.83\n\n1.63\n7.12\n13.69\n2.03\n13.67\n6.68\n4.70\n2.98\n3.91\n2.67\n3.37\n5.18\n4.31\n18.06\n9.44\n4.28\n11.34\n15.59\n\n85.40\n111.47\n172.88\n61.30\n102.14\n150.78\n66.91\n152.34\n160.07\n138.45\n151.14\n147.53\n89.66\n28.33\n28.94\n12.84\n23.98\n31.05\n\nTable 4: Quantitative Comparison with Ground Truth Using MAPE and Chamfer Distance\n\nMAPE Ch. w/ t.m\n\n(%)\n\n10.973\n\nsum\n\n0.130\n\nmean\n\n0.007\n\nCh. w/o t.m\nsum\n\n1.715\n\nmean\n\n0.095\n\n5 Second Place Team - ININ-VIAUN\n\n5.1 Methodology\n\nThis section details the team\u2019s proposed network, illustrating the step-by-step process from original\nimages to final mesh models.\n\n5.1.1 Scale factor estimation\n\nThe procedure for estimating the scale factor at the coordinate level is illustrated in Figure 9. The\nteam adheres to a method involving corner projection matching. Specifically, utilizing the COLMAP\ndense model, the team acquires the pose of each image along with dense point cloud data. For any\ngiven image imgk and its extrinsic parameters [R|t]k, the team initially performs threshold-based\ncorner detection, setting the threshold at 240. This step allows them to obtain the pixel coordinates\nof all detected corners. Subsequently, using the intrinsic parameters k and the extrinsic parameters\n[R|t]k, the point cloud is projected onto the image plane. Based on the pixel coordinates of the\ncorners, the team can identify the closest point coordinates P k\ni for each corner, where i represents the\nindex of the corner. Thus, they can calculate the distance between any two corners as follows:\n\nDk\n\nij = (P k\n\ni \u2212 P k\n\nj )2 \u2200i \u0338= j\n\n(3)\n\nTo determine the final computed length of each checkerboard square in image k, the team takes the\nminimum value of each row of the matrix Dk (excluding the diagonal) to form the vector dk. The\nmedian of this vector is then used. The final scale calculation formula is given by Equation 4, where\n0.012 represents the known length of each square (1.2 cm):\n\nscale =\n\n0.012\ni=1 med(dk)\n\n(cid:80)n\n\n7\n\n(4)\n\n\f5.1.2 3D Reconstruction\n\nThe 3D reconstruction process, depicted in Figure 10, involves two different pipelines to accommodate\nvariations in input viewpoints. The first fifteen objects are processed using one pipeline, while the\nlast five single-view objects are processed using another.\n\nFor the initial fifteen objects, the team uses COLMAP to estimate poses and segment the food using\nthe provided segment masks. Advanced multi-view 3D reconstruction methods are then applied to\nreconstruct the segmented food. The team employs three different reconstruction methods: COLMAP,\nDiffusioNeRF, and NeRF2Mesh. They select the best reconstruction results from these methods and\nextract the mesh. The extracted mesh is scaled using the estimated scale factor, and optimization\ntechniques are applied to obtain a refined mesh.\n\nFor the last five single-view objects, the team experiments with several single-view reconstruction\nmethods, including Zero123, Zero123++, One2345, ZeroNVS, and DreamGaussian. They choose\nZeroNVS to obtain a 3D food model consistent with the distribution of the input image. The\nintrinsic camera parameters from the fifteenth object are used, and an optimization method based\non reprojection error refines the extrinsic parameters of the single camera. Due to limitations in\nsingle-view reconstruction, depth information from the dataset and the checkerboard in the monocular\nimage are used to determine the size of the extracted mesh. Finally, optimization techniques are\napplied to obtain a refined mesh.\n\n5.1.3 Mesh refinement\n\nDuring the 3D Reconstruction phase, it was observed that the model\u2019s results often suffered from low\nquality due to holes on the object\u2019s surface and substantial noise, as shown in Figure 11.\n\nTo address the holes, MeshFix, an optimization method based on computational geometry, is em-\nployed. For surface noise, Laplacian Smoothing is used for mesh smoothing operations. The\nLaplacian Smoothing method adjusts the position of each vertex to the average of its neighboring\nvertices:\n\n\uf8eb\n\nV (new)\ni\n\n= V (old)\ni\n\n+ \u03bb\n\n\uf8ed\n\n\uf8f6\n\nj \u2212 V (old)\nV (old)\n\ni\n\n\uf8f8\n\n1\n|N (i)|\n\n(cid:88)\n\nj\u2208N (i)\n\n(5)\n\nIn their implementation, the smoothing factor \u03bb is set to 0.2, and 10 iterations are performed.\n\n5.2 Experimental Results\n\n5.2.1 Estimated scale factor\n\nThe scale factors estimated using the described method are shown in Table 5. Each image and the\ncorresponding reconstructed 3D model yield a scale factor, and the table presents the average scale\nfactor for each object.\n\n5.2.2 Reconstructed meshes\n\nThe refined meshes obtained using the described methods are shown in Figure 12. The predicted\nmodel volumes, ground truth model volumes, and the percentage errors between them are presented\nin Table 6.\n\n5.2.3 Alignment\n\nThe team designs a multi-stage alignment method for evaluating reconstruction quality. Figure 13\nillustrates the alignment process for Object 14. First, the central points of both the predicted and\nground truth models are calculated, and the predicted model is moved to align with the central point\nof the ground truth model. Next, ICP registration is performed for further alignment, significantly\nreducing the Chamfer distance. Finally, gradient descent is used for additional fine-tuning to obtain\nthe final transformation matrix.\n\nThe total Chamfer distance between all 18 predicted models and the ground truths is 0.069441169.\n\n8\n\n\fTable 5: Estimated Scale Factors\n\nObject Index\n\nFood Item\n\nScale Factor\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n13\n14\n\nStrawberry\nCinnamon bun\nPork rib\nCorn\nFrench toast\nSandwich\nBurger\nCake\nBlueberry muffin\nBanana\nSalmon\nBurrito\nHotdog\n\n0.060058\n0.081829\n0.073861\n0.083594\n0.078632\n0.088368\n0.103124\n0.068496\n0.059292\n0.058236\n0.083821\n0.069663\n0.073766\n\nTable 6: Metric of Volume\n\nObject Index\n\nPredicted Volume Ground Truth Error Percentage\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n13\n14\n16\n17\n18\n19\n20\n\n44.51\n321.26\n336.11\n347.54\n389.28\n197.82\n412.52\n181.21\n233.79\n160.06\n86.0\n334.7\n517.75\n176.24\n180.68\n13.58\n117.72\n117.43\n\n38.53\n280.36\n249.67\n295.13\n392.58\n218.44\n368.77\n173.13\n232.74\n163.09\n85.18\n308.28\n589.83\n262.15\n181.36\n20.58\n108.35\n119.83\n\n15.52\n14.59\n34.62\n17.76\n0.84\n9.44\n11.86\n4.67\n0.45\n1.86\n0.96\n8.57\n12.22\n32.77\n0.37\n34.01\n8.64\n20.03\n\n6 Best 3D Mesh Reconstruction Team - FoodRiddle\n\n6.1 Methodology\n\nTo achieve high-fidelity food mesh reconstruction, the team developed two procedural pipelines as\ndepicted in Figure 14. For simple and medium complexity cases, they employed a structure-from-\nmotion strategy to ascertain the pose of each image, followed by mesh reconstruction. Subsequently,\na sequence of post-processing steps was implemented to recalibrate the scale and improve mesh\nquality. For cases involving only a single image, the team utilized image generation techniques to\nfacilitate model generation.\n\n6.1.1 Multi-View Reconstruction\n\nFor Structure from Motion (SfM), the team enhanced the advanced COLMAP method by integrating\nSuperPoint and SuperGlue techniques. This integration significantly addressed the issue of limited\nkeypoints in scenes with minimal texture, as illustrated in Figure 15.\n\nIn the mesh reconstruction phase, the team\u2019s approach builds upon 2D Gaussian Splatting, which\nemploys a differentiable 2D Gaussian renderer and includes regularization terms for depth distortion\n\n9\n\n\fand normal consistency. The Truncated Signed Distance Function (TSDF) results are utilized to\nproduce a dense point cloud.\n\nDuring post-processing, the team applied filtering and outlier removal methods, identified the outline\nof the supporting surface, and projected the lower mesh vertices onto this surface. They utilized\nthe reconstructed checkerboard to correct the model\u2019s scale and employed Poisson reconstruction to\ncreate a complete, watertight mesh of the subject.\n\n6.1.2 Single-View Reconstruction\n\nFor 3D reconstruction from a single image, the team utilized advanced methods such as LGM, Instant\nMesh, and One-2-3-45 to generate an initial mesh. This initial mesh was then refined in conjunction\nwith depth structure information.\n\nTo adjust the scale, the team estimated the object\u2019s length using the checkerboard as a reference,\nassuming that the object and the checkerboard are on the same plane. They then projected the 3D\nobject back onto the original 2D image to obtain a more precise scale for the object.\n\n6.2 Experimental Results\n\nThrough a process of nonlinear optimization, the team sought to identify a transformation that\nminimizes the Chamfer distance between their mesh and the ground truth mesh. This optimization\naimed to align the two meshes as closely as possible in three-dimensional space. Upon completion\nof this process, the average Chamfer dis- tance across the final reconstructions of the 20 objects\namounted to 0.0032175 meters. As shown in Table 7, Team FoodRiddle achieved the best scores for\nboth multi- view and single-view reconstructions, outperforming other teams in the competition.\n\nTable 7: Total Errors for Different Teams on Multi-view and Single-view Data\n\nTeam\n\nMulti-view (1-14)\n\nSingle-view (16-20)\n\nFoodRiddle\nININ-VIAUN\nVolETA\n\n0.036362\n0.041552\n0.071921\n\n0.019232\n0.027889\n0.058726",
  "results": "",
  "conclusion": "challenge on 3D Food Reconstruction. The challenge sought to enhance 3D reconstruction methods\nby concentrating on food items, tackling the distinct difficulties presented by varied textures, reflective\nsurfaces, and intricate geometries common in culinary subjects.\n\nThe competition involved 20 diverse food items, captured under various conditions and with differing\nnumbers of input images, specifically designed to challenge participants in creating robust reconstruc-\ntion models. The evaluation was based on a two-phase process, assessing both portion size accuracy\nthrough Mean Absolute Percentage Error (MAPE) and shape accuracy using the Chamfer distance\nmetric.\n\nOf all participating teams, three reached the final submission stage, presenting a range of innovative\nsolutions. Team VolETA secured first place with the best overall performance in both Phase-I and\nPhase-II, followed by team ININ-VIAUN in second place. Additionally, the FoodRiddle team\nexhibited superior performance in Phase-II, highlighting a competitive and high-caliber field of\nentries for 3D mesh reconstruction. The challenge has successfully advanced the field of 3D food\nreconstruction, demonstrating the potential for accurate volume estimation and shape reconstruction\nin nutritional analysis and food presentation applications. The novel methods developed by the\nparticipating teams establish a strong foundation for future research in this area, potentially leading\nto more precise and user-friendly approaches for dietary assessment and monitoring.\n\n10",
  "is_publishable": 1,
  "venue": "CVPR"
}
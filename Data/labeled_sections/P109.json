{
  "title": "Multimodal Deep Ensemble for Hateful Meme\nIdentification",
  "abstract": "This paper delves into the utilization of machine learning techniques for identify-\ning hate speech, while addressing the persisting technical challenges to enhance\ntheir performance to match human-level accuracy. We explore several current\nvisual-linguistic Transformer models and suggest enhancements to boost their ef-\nfectiveness for this task. The model we propose demonstrates superior performance\ncompared to the established benchmarks, achieving a 5th place ranking out of over\n3,100 participants.",
  "introduction": "This paper addresses the critical influence of the internet on our daily lives, where our online presence\nshowcases our personalities and beliefs, as well as our biases. Daily, billions of individuals engage\nwith various forms of online content, and despite some of this content being valuable and informative,\nan increasing portion is harmful, including hate speech and misinformation. There is a growing need\nto quickly detect this content, improve the review process and automate decisions to rapidly remove\nharmful material, thereby reducing any harm to viewers.\n\nSocial media platforms are frequently used for interactions, sharing messages and images with private\ngroups and the public. Facebook AI launched a competition to tag hateful memes that include both\nimages and text. For this, a dataset of 10,000+ labeled multimodal memes was provided. The aim of\nthe challenge is to develop an algorithm that identifies multimodal hate speech in memes, while also\nbeing robust to their benign alterations. A meme\u2019s hateful nature could stem from its image, text, or\nboth. Benign alteration is a technique used by organizers to switch a meme\u2019s label from hateful to\nnon-hateful, requiring modifications to either the text or the image.\n\nThe core assessment metric for this binary classification task is the area under the receiver operating\ncharacteristic curve (AUROC), representing the area under the ROC curve. This curve plots the True\nPositive Rate (TPR) against the False Positive Rate (FPR) at various classification thresholds. The\nprimary objective is to maximize the AUROC.\n\nAU ROC =\n\n(cid:90) 1\n\n0\n\nT P R(T )dF P R(T )\n\n(1)\n\nAccuracy is the secondary metric, calculating the proportion of instances where the predicted class\nmatches the actual class in the test set.\n\nAccuracy =\n\n1\nN\n\nN\n(cid:88)\n\ni=1\n\nI(yi = \u02c6yi)\n\n(2)\n\nThe aim is to maximize both metrics.\n\nIn brief, this paper makes three contributions:\n\n.\n\n\f\u2022 We conduct experiments using single-stream and dual-stream architectures such as VL-\nBERT, VLP, UNITER and LXMERT and compare their performance with the established\nbaselines. These models were chosen because of their pre-training on diverse datasets.\n\n\u2022 We put forward a novel bidirectional cross-attention mechanism that connects caption\ninformation with meme caption text, which increases performance in detecting hateful\nmemes. This is similar to the cross-attention between images in other research.\n\n\u2022 We demonstrate that deep ensembles greatly improve single model predictions.",
  "related_work": "Transformer models pre-trained on extensive datasets have shown state-of-the-art results in numerous\nlanguage processing tasks. BERT is one of the most popular due to its ease of use and strong\nperformance. Recently, training these large models on combined visual-linguistic embeddings\nhas shown very promising outcomes for visual-linguistic tasks such as visual question answering,\nreasoning, and image captioning. LXMERT uses dual networks to process text and images, learning\ncross-modality encoder representations by using a Transformer to combine the two streams of\ninformation. The images\u2019 features are derived using a Faster R-CNN feature extractor. This is also\nused in single-stream architectures, VL-BERT and UNITER, which employ a single Transformer\non top of the combined image-text embeddings. A unified model for visual understanding and\nvision-language tasks has also been proposed.\n\nTable 1: Pre-training datasets for each model\n\nBooks Corpus CC COCO VG SBU GQA VQA 2.0 VG-QA\n\nVL-BERT\nVLP\nUNITER\nLXMERT\n\nX\nX\nX\nX\n\nX\nX\n\nX\nX\n\nX\nX\nX\n\nX\n\nX\n\nA dataset for multimodal hate speech detection was created by gathering data from Twitter, using\nparticular hateful keywords. However, studies found that multimodal models did not do better than\ntext-only models.",
  "methodology": "One goal of this research is to leverage the fact that single and dual stream Transformer models have\nbeen pre-trained on a variety of datasets across various fields. Transformer attention models excel at\nNLP tasks, and the masked language modeling pre-training method in BERT is both powerful and\nversatile. Studies show that the pre-training process can better align visual-linguistic embeddings\nand help downstream tasks like visual question answering and reasoning. Given that pre-training a\nvisual-linguistic Transformer architecture is helpful for downstream tasks, might ensembling different\nmodels pre-trained on different datasets yield better results?\n\nTable 1 shows the pre-training datasets used for each model.\n\n3.1 UNITER with Meme Text and Inferred Caption Cross-Attention\n\nThe Natural Language for Visual Reasoning for Real (NLVR2) is an academic dataset of human\nwritten sentences connected to pairs of photos. The dataset includes pairs of visually intricate images\ncoupled with a statement and a binary label. UNITER was among the top models in this challenge\nby adding a cross-attention module between text-image pairs, dividing each sample in two and\nrepeating the text. They then apply attention pooling to each sequence, concatenate them and add the\nclassification head, a multi-layer perceptron. Similar to this, we propose to repeat the meme image in\neach half-sequence and add an inferred meme caption as the second text. We generate captions using\nthe Show and Tell model. This way, the model could learn from both the original meme text and the\nnew captions generated by a model trained on a different dataset.\n\n2",
  "experiments": "We carry out several experiments using LXMERT, VLP, VL-BERT, and UNITER. We apply bidirec-\ntional cross-attention using inferred captions for UNITER, VL-BERT, and VLP, but not for LXMERT\ndue to its low performance on the dataset.\n\nWe also experiment with a dataset from previous research. We filter and balance it down to 16K\nsamples by excluding cartoon memes and memes with little text. We fine-tune VL-BERTLARGE\nusing the reduced dataset for four rounds, then fine-tune it using the hateful memes dataset for another\nfour rounds. The results were lower than the majority of the other models.\n\nThe baselines for models trained on the Hateful Memes dataset are in Table 2.",
  "results": "Our best performing solutions are derived from averaging probabilities using a single VL-\nBERTLARGE and one UNITERLARGE+PA (UNITERLARGE with extra attention). We used\nthe default training parameters of the vanilla pre-trained UNITERLARGE model, but changed the\ntraining steps according to the dataset size. A deep ensemble of UNITERLARGE+PA models got\nthe best performance. For this ensemble, we simply rerun training using various random seeds and\naverage the predictions from each model. Table 2 displays the top results for the final competition\nphase as well as the improvements cross-attention brings to the UNITER model in the first phase.\nThe final results are significantly better than the baselines.\n\nThe most important findings are as follows:\n\n\u2022 Single-stream Transformer models pre-trained on the Conceptual Captions (CC) dataset give\nthe best results, and deep ensembles improve the overall performance further. The choice of\npre-training datasets matters in terms of domain similarity to the fine-tuning dataset.\n\n\u2022 We believe that UNITER gets better results due to being pre-trained on the COCO dataset\nwhich has less noise. Similarly to the Hateful Memes dataset this is also high quality. Further\nwork should investigate if pre-training VL-BERT on COCO would improve its results.\n\u2022 Interestingly, the paired attention technique only works for UNITER and not for the other\n\nmodels.\n\n\u2022 Training large models from scratch did poorly, which is expected due to the small dataset\n\nsize.\n\n\u2022 The dataset of multimodal hate speech is heavily skewed towards hateful text and the\nkeywords used to collect it. The memes are less subtle compared to the ones in the Hateful\nMemes dataset, although they are perhaps more typical of what is seen online.",
  "conclusion": "We present effective techniques to detect hate speech in a distinct dataset of multimodal memes from\nFacebook AI. The aim is to identify hate speech using a multimodal model, and to be robust to the\n\u201cbenign confounders\u201d that cause the binary label of a meme to change.\n\nWe have performed tests on various large pre-trained Transformer models and fine-tuned state-of-the-\nart single-stream models like VL-BERT, VLP, and UNITER, and dual-stream models like LXMERT.\nWe compare their performance against the baselines, showing that the single-stream models perform\nsignificantly better. Our choice for these models stems from their pre-training on a wide variety of\ndatasets from different fields. We also adapt a novel bidirectional cross-attention mechanism that\nlinks caption information with meme text. This leads to increased accuracy in identifying hateful\nmemes. Furthermore, deep ensembles can improve single model predictions. Training the models\nfrom scratch performed poorly due to the small dataset size. We also observed that the pre-training\ndataset influences results.\n\nWe conclude that despite the improvements in multimodal models, there is still a gap when comparing\nto human performance. This suggests considerable scope for the development of better algorithms\nfor multimodal understanding.\n\n3\n\n\fTable 2: Baselines from previous research. For our final models, we report the top performance\nscores, specifying both Accuracy and AUROC results.\n\nAcc. Validation AUROC Acc.\n\nTest AUROC\n\n\u2013\n\n58.79\n57.98\n64.65\n\n65.97\n\n65.25\n\n68.57\n71.03\n71.13\n70.60\n\n70.07\n\n73.97\n\n\u2013\n\u2013\n\u2013\n\n75.94\n79.39\n\n84.70\n\n52.00\n52.13\n59.20\n\n59.66\n\n59.13\n\n60.06\n60.23\n62.30\n63.20\n\n61.10\n\n64.73\n\n68.70\n68.30\n66.60\n\n73.90\n74.30\n\n82.65\n\n52.63\n55.92\n65.08\n\n64.75\n\n65.79\n\n67.92\n70.73\n70.45\n71.33\n\n70.03\n\n71.41\n\n74.14\n75.29\n76.81\n\n79.21\n79.43\n\nType\n\n3*Unimodal\n\n5*\n\nMultimodal\n(Unimodal\nPretraining)\n\n2*\n\nMultimodal\n(Multimodal\nPretraining)\n\n3*(Phase 1)\n\nModel\n\nHuman\n\nImage-Grid\nImage-Region\nText BERT\n\nLate Fusion\n\nConcat BERT\n\nMMBT-Grid\nMMBT-Region\nViLBERT\nVisual BERT\n\n\u2013\n\n52.73\n52.66\n58.26\n\n61.53\n\n58.60\n\n58.20\n58.73\n62.20\n62.10\n\nViLBERT CC\n\n61.40\n\nVisual BERT COCO\n\n65.06\n\nUNITER\nUNITERPA\nUNITERPA Ensemble\n\n\u2013\n\u2013\n\u2013\n\n2*(Phase 2)\n\nVL-BERT + UNITERPA 74.53\n72.50\n\nUNITERPA Ensemble\n\n4",
  "is_publishable": 1,
  "venue": NaN
}
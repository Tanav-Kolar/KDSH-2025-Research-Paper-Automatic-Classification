{
  "title": "Enhancing Visual Representation Learning Through\nOriginal Image Utilization in Contrastive Learning",
  "abstract": "Contrastive instance discrimination techniques exhibit superior performance in\ndownstream tasks, including image classification and object detection, compared to\nsupervised learning. However, a strong reliance on data augmentation during repre-\nsentation learning is a hallmark of these methods, potentially causing suboptimal\noutcomes if not meticulously executed. A prevalent data augmentation approach in\ncontrastive learning involves random cropping followed by resizing. This practice\nmight diminish the quality of representation learning when two random crops\nencompass disparate semantic information. To counter this, we propose an inno-\nvative framework termed LeOCLR (Leveraging Original Images for Contrastive\nLearning of Visual Representations). This framework integrates a novel instance\ndiscrimination strategy and a refined loss function, effectively mitigating the loss\nof crucial semantic features that may arise from mapping different object segments\nduring representation learning. Our empirical evaluations reveal that LeOCLR con-\nsistently enhances representation learning across a spectrum of datasets, surpassing\nbaseline models. Notably, LeOCLR exhibits a 5.1% improvement over MoCo-v2\non ImageNet-1K in linear evaluation and demonstrates superior performance in\ntransfer learning and object detection tasks compared to several other techniques.",
  "introduction": "Self-supervised learning (SSL) methods based on instance discrimination are heavily dependent on\ndata augmentations, like random cropping, rotation, and color jitter, to construct invariant repre-\nsentations for all instances within a dataset. These augmentations are used to generate two altered\nviews (positive pairs) of the same instance, which are subsequently drawn closer in the latent space.\nSimultaneously, strategies are employed to prevent a collapse to a trivial solution, commonly referred\nto as representation collapse. The efficacy of these methods in acquiring meaningful representations\nhas been demonstrated through various downstream tasks, such as image classification and object\ndetection, serving as proxies for evaluating representation learning. However, these techniques\noften overlook the crucial aspect that augmented views may diverge in semantic content because\nof random cropping, potentially degrading the quality of visual representation learning. Creating\npositive pairs via random cropping and subsequently prompting the model to align them based on\nshared information in both views poses an increased challenge to the SSL task, ultimately leading to\nan enhancement in representation quality. Moreover, random cropping followed by resizing guides\nthe model\u2019s representation to encompass object-related information across diverse aspect ratios,\nthereby promoting invariance to occlusions. Conversely, minimizing the feature distance in the latent\nspace, which equates to maximizing similarity, between views that encompass distinct semantic\nconcepts may inadvertently discard valuable image information.\n\nInstances of incorrect semantic positive pairs, which are pairs containing mismatched semantic\ninformation about the same object, might arise from random cropping. When the model is compelled\nto align the representations of different parts of an object closer in the latent space, it may discard\ncrucial semantic features. This occurs because the model\u2019s representations are based on the shared\narea between the two views. If this shared region lacks semantically consistent information, the\n\n.\n\n\frepresentations become trivial. For random cropping to be effective and achieve occlusion invariance,\nthe shared area must convey the same semantic meaning in both views. Nevertheless, contrasting\npairs that might include diverse semantic information about the same object can be valuable, as it can\nfacilitate learning global features.\n\nThe creation of random crops for a one-centric object does not ensure the acquisition of accurate\nsemantic pairs. This observation holds significant importance for the enhancement of representation\nlearning. Instance discrimination SSL techniques encourage the model to approximate positive pairs,\ni.e., two views of the same instance, in the latent space, irrespective of their semantic content. This\nlimitation might hinder the model\u2019s ability to learn representations of different object components\nand could potentially impair its capability to learn semantic feature representations (see Figure 2\n(left) in the original paper).\n\nUndesirable views containing different semantic content may be unavoidable when employing random\ncropping. Therefore, a method is needed to train the model on different parts of an object, developing\nrobust representations against natural transformations like scale and occlusion, rather than merely\npulling augmented views together indiscriminately. Addressing this issue is vital, as downstream task\nperformance relies on high-quality visual representations learned through self-supervised learning.\n\nOur work presents a new instance discrimination SSL approach designed to avoid compelling the\nmodel to create similar representations for two positive views, irrespective of their semantic content.\nAs shown in Figure 2 (right) of the original paper, we incorporate the original image X into the\ntraining process, since it contains all the semantic features of the views X1 and X2. In our method, the\npositive pairs (i.e., X1 and X2) are drawn towards the original image X in the latent space, in contrast\nto contrastive state-of-the-art (SOTA) approaches like SimCLR and MoCo-v2, which draw the two\nviews towards each other. This training method guarantees that the information in the shared region\nbetween the attracted views (X, X1) and (X, X2) is semantically accurate. Consequently, the model\nacquires enhanced semantic features by aligning with the appropriate semantic content, rather than\nmatching random views that might contain disparate semantic information. In essence, the model\nlearns representations of various object parts because the shared region encompasses correct semantic\ncomponents of the object. This contrasts with other methods that may discard vital semantic features\nby incorrectly mapping object parts in positive pairs. Our contributions are outlined as follows:\n\n\u2022 We present a new contrastive instance discrimination SSL method, LeOCLR, created to\nminimize the loss of semantic features caused by mapping two semantically inconsistent\nrandom views.\n\n\u2022 We establish that our method enhances visual representation learning in contrastive instance\ndiscrimination SSL, surpassing state-of-the-art techniques across a variety of downstream\ntasks.\n\n\u2022 We show that our method consistently improves visual representation learning for contrastive\n\ninstance discrimination across multiple datasets and contrastive mechanisms.",
  "related_work": "Self-supervised learning (SSL) techniques are categorized into two primary groups: contrastive and\nnon-contrastive learning. While all these techniques endeavor to approximate positive pairs in the\nlatent space, they employ distinct strategies to circumvent representation collapse.\n\n**Contrastive Learning:** Instance discrimination techniques, such as SimCLR, MoCo, and PIRL,\nemploy a similar concept. These methods bring the positive pairs closer while driving the negative\npairs apart in the embedding space, albeit through different mechanisms. SimCLR employs an\nend-to-end strategy where a large batch size is utilized for negative examples, and the parameters of\nboth encoders in the Siamese network are updated simultaneously. PIRL uses a memory bank for\nnegative examples, and both encoders\u2019 parameters are updated together. MoCo adopts a momentum\ncontrastive approach where the query encoder is updated during backpropagation, which subsequently\nupdates the key encoder. Negative examples are maintained in a separate dictionary, facilitating the\nuse of large batch sizes.\n\n**Non-Contrastive Learning:** Non-contrastive techniques utilize solely positive pairs to learn\nvisual representations, employing a variety of strategies to prevent representation collapse. The\n\n2\n\n\finitial category encompasses clustering-based techniques, where samples exhibiting similar features\nare assigned to the same cluster. DeepCluster employs pseudo-labels from the previous iteration,\nrendering it computationally demanding and challenging to scale. SWAV addresses this challenge by\nimplementing online clustering, though it necessitates determining the correct number of prototypes.\nThe second category involves knowledge distillation. Techniques like BYOL and SimSiam utilize\nknowledge distillation methods, where a Siamese network comprises an online encoder and a target\nencoder. The target network\u2019s parameters are not updated during backpropagation. Instead, solely\nthe online network\u2019s parameters are updated while being encouraged to predict the representation of\nthe target network. Despite the encouraging results, the mechanism by which these methods prevent\ncollapse remains not fully understood. Inspired by BYOL, Self-distillation with no labels (DINO)\nemploys centering and sharpening, along with a distinct backbone (ViT), enabling it to surpass other\nself-supervised techniques while maintaining computational efficiency. Another method, Bag of\nvisual words (BoW), employs a teacher-student framework inspired by natural language processing\n(NLP) to avert representation collapse. The student network predicts a histogram of the features for\naugmented images, analogous to the teacher network\u2019s histogram. The final category is information\nmaximization. Methods like Barlow twins and VICReg eschew negative examples, stop gradient,\nor clustering. Instead, they utilize regularization to avoid representation collapse. The objective\nfunction of these techniques seeks to eliminate redundant information in the embeddings by aligning\nthe correlation of the embedding vectors closer to the identity matrix. While these techniques exhibit\nencouraging results, they possess limitations, including the sensitivity of representation learning to\nregularization and reduced effectiveness if certain statistical properties are absent in the data.\n\n**Instance Discrimination With Multi-Crops:** Various SSL techniques introduce multi-crop strate-\ngies to enable models to learn visual representations of objects from diverse perspectives. However,\nwhen generating multiple cropped views from the same object instance, these views might contain\ndisparate semantic information. To tackle this issue, LoGo generates two random global crops and\nN local views. They posit that global and local views of an object share similar semantic content,\nenhancing similarity between these views. Simultaneously, they contend that different local views\npossess distinct semantic content, thus diminishing similarity among them. SCFS proposes a different\napproach for managing unmatched semantic views by searching for semantically consistent features\nbetween the contrasted views. CLSA generates multiple crops and applies both strong and weak\naugmentations, using distance divergence loss to enhance instance discrimination in representation\nlearning. Prior methods assume that global views contain similar semantic content and treat them\nindiscriminately as positive pairs. However, our technique suggests that global views might contain\nincorrect semantic pairs due to random cropping, as illustrated in Figure 1 in the original paper.\nTherefore, we aim to attract the two global views to the original (intact and uncropped) image, which\nfully encapsulates the semantic features of the crops.",
  "methodology": "The mapping of incorrect semantic positive pairs, specifically those containing different semantic\nviews, results in the loss of semantic features, which in turn degrades the model\u2019s representation\nlearning. To address this, we propose a novel contrastive instance discrimination SSL strategy called\nLeOCLR. Our approach is designed to capture meaningful features from two random positive pairs,\neven when they encompass different semantic content, thereby improving representation learning.\nAchieving this necessitates ensuring the semantic correctness of the information within the shared\nregion between the attracted views. This is crucial because the selection of views dictates the\ninformation captured by the representations learned in contrastive learning. Given that we cannot\nguarantee the inclusion of correct semantic parts of the object within the shared region between the\ntwo views, we propose the inclusion of the original image in the training process. The original image\nX, which is not subjected to random cropping, encompasses all the semantic features of the two\ncropped views, X1 and X2.\n\nOur method, illustrated in Figure 3 (left) in the original paper, generates three views (X, X1, and\nX2). The original image (X) is resized without cropping, while the other views (X1 and X2) undergo\nrandom cropping and resizing. All views are then randomly augmented to prevent the model from\nlearning trivial features. We employ data augmentations akin to those used in MoCo-v2. The original\nimage (X) is encoded by the encoder fq, while the two views (X1, X2) are encoded by a momentum\nencoder fk. The parameters of fk are updated using the formula:\n\n3\n\n\f\u03b8k \u2190 m\u03b8k + (1 \u2212 m)\u03b8q (1)\n\nwhere m is a coefficient set to 0.999, \u03b8q represents the encoder parameters of fq updated through\nbackpropagation, and \u03b8k denotes the momentum encoder parameters of fk updated by \u03b8q. Ultimately,\nthe objective function compels the model to draw both views (X1, X2) closer to the original image\n(X) in the embedding space while simultaneously pushing apart all other instances, as depicted in\nFigure 3 (right) in the original paper.\n\n3.1 Loss function\n\nInitially, we briefly outline the loss function of MoCo-v2, given our utilization of momentum\ncontrastive learning. Subsequently, we will detail our modification to the loss function.\n\n\u2113(u, v+) = \u2212 log\n\nexp(u\u00b7v+/\u03c4 )\nn=0 exp(u\u00b7vn/\u03c4 ) (2)\n\n(cid:80)P N\n\ni=1 \u2113(ui, sg(v1\n\ni )) + \u2113(ui, sg(v2\n\nwhere similarity is quantified by the dot product. The objective function amplifies the similarity\nbetween the positive pairs (u . v+) by drawing them closer in the embedding space, while simultane-\nously driving apart all the negative samples (vn) in the dictionary to prevent representation collapse.\n\u03c4 denotes the temperature hyperparameter of the softmax function. In our method, we augment the\nsimilarity between the original image\u2019s feature representation, u = fq(x), and the positive pair\u2019s feature\nrepresentation, v+ = fk(xi) (i = 1, 2), while driving apart all the negative examples (vn). Consequently,\nthe total loss for the mini-batch is:\nlt = (cid:80)N\nwhere sg(.) denotes the stop-gradient operation, which is vital for averting representation collapse.\nAs depicted in Equation 3, the total loss lt attracts the two views (v1\ni ) to their original instance\nui. This enables the model to capture semantic features from the two random views, even if they\ncontain different semantic information. Our technique captures improved semantic features compared\nto prior contrastive methods, as we ensure that the shared region between the attracted views contains\naccurate semantic information. Since the original image contains all segments of the object, any part\ncontained in the random crop is also present in the original image. Thus, when we draw the original\nimage and the two random views closer in the embedding space, the model learns representations\nof the different parts, creating an occlusion-invariant representation of the object across various\nscales and angles. This contrasts with earlier techniques, which draw the two views together in the\nembedding space regardless of their semantic content, leading to the loss of semantic features.\n\ni and v2\n\ni )) (3)\n\nEquation 3 and Algorithm 1 in the original paper highlight the primary distinctions between our\nmethod and prior multi-crop techniques, such as CLSA, SCFC, and DINO. The key differences are\nas follows:\n\n\u2022 Previous methods assume that two global views contain identical semantic information,\nencouraging the model to concentrate on similarities and generate similar representations\nIn contrast, our method utilizes the original images instead of global\nfor both views.\nviews, as we contend that global views may contain incorrect semantic information for the\nsame object. While they may aid in capturing certain global features, this could restrict\nthe model\u2019s capacity to learn more universally applicable semantic features, ultimately\nimpacting performance.\n\n\u2022 Prior methods employ several local random crops, which might be time- and memory-\n\nintensive, while our method utilizes only two random crops.\n\n\u2022 Our objective function employs different strategies to enhance the model\u2019s visual represen-\ntation learning. We encourage the model to align the two random crops with the original\nimage, which encompasses the semantic information for all random crops while avoiding\ncompelling the two crops to have similar representations if they do not share similar semantic\ninformation. This approach differs from prior methods, which encourage all crops (global\nand local) to have similar representations, regardless of their semantic content. Conse-\nquently, although useful for learning certain global features, those methods may discard\npertinent semantic information, potentially hindering the transferability of the resulting\nrepresentations to downstream tasks.\n\n4",
  "experiments": "We executed multiple experiments on three datasets: STL-10 \"unlabeled\", comprising 100,000\ntraining images, CIFAR-10, containing 50,000 training images, and ImageNet-1K, with 1.28 million\ntraining images.\n\n**Training Setup:** We employed ResNet50 as the backbone architecture. The model was trained\nusing the SGD optimizer, with a weight decay set to 0.0001, momentum at 0.9, and an initial learning\nrate of 0.03. The mini-batch size was configured to 256, and the model underwent training for up to\n800 epochs on the ImageNet-1K dataset.\n\n**Evaluation:** We employed diverse downstream tasks to assess LeOCLR\u2019s representation learning\nagainst leading SOTA approaches on ImageNet-1K: linear evaluation, semi-supervised learning,\ntransfer learning, and object detection. For linear evaluation, we adhered to the standard evaluation\nprotocol, where a linear classifier was trained for 100 epochs on top of a frozen backbone pre-trained\nwith LeOCLR. The ImageNet-1K training set was used to train the linear classifier from scratch, with\nrandom cropping and left-to-right flipping augmentations. Results are presented on the ImageNet-\n1K validation set using a center crop (224 x 224). In the semi-supervised setting, we fine-tuned\nthe network for 60 epochs using 1% of labeled data and 30 epochs using 10% of labeled data.\nAdditionally, we evaluated the learned features on smaller datasets, such as CIFAR, and fine-grained\ndatasets, using transfer learning. Lastly, we utilized the PASCAL VOC dataset for object detection.\n\n**Comparing with SOTA Approaches:** We employed vanilla MoCo-v2 as a baseline for comparison\nwith our method across various benchmark datasets, considering our use of a momentum contrastive\nlearning framework. Furthermore, we benchmarked our method against other SOTA techniques on\nthe ImageNet-1K dataset.\n\nTable 1: Comparisons between our approach LeOCLR and SOTA approaches on ImageNet-1K.\n\nApproach\n\nEpochs Batch Accuracy\n\nMoCo-v2\nBYOL\nSWAV\nSimCLR\nHEXA\nSimSiam\nVICReg\nMixSiam\nOBoW\nDINO\nBarlow Twins\nCLSA\nRegionCL-M\nUnMix\nHCSC\nUniVIP\nHAIEV\nSCFS\nLeOCLR (ours)\n\n800\n1000\n800\n1000\n800\n800\n1000\n800\n200\n800\n1000\n800\n800\n800\n200\n300\n200\n800\n800\n\n256\n4096\n4096\n4096\n256\n512\n2048\n128\n256\n1024\n2048\n256\n256\n256\n256\n4096\n256\n1024\n256\n\n71.1%\n74.4%\n75.3%\n69.3%\n71.7%\n71.3%\n73.2%\n72.3%\n73.8%\n75.3%\n73.2%\n76.2%\n73.9%\n71.8%\n73.3%\n74.2%\n70.1%\n75.7%\n76.2%\n\nTable 1 presents the linear evaluation of our method in comparison to other SOTA techniques. As\nshown, our method surpasses all others, outperforming the baseline (i.e., vanilla MoCo-v2) by 5.1%.\nThis lends credence to our hypothesis that while two global views can capture certain global features,\nthey may also encompass distinct semantic information for the same object (e.g., a dog\u2019s head\nversus its leg), which should be taken into account to enhance representation learning. The observed\nperformance gap (i.e., the difference between vanilla MoCo-v2 and LeOCLR) demonstrates that\nmapping pairs with divergent semantic content impedes representation learning and impacts the\nmodel\u2019s performance in downstream tasks.\n\n**Semi-Supervised Learning on ImageNet-1K:** In this section, we assess the performance of\nLeOCLR under a semi-supervised setting. Specifically, we utilize 1% and 10% of the labeled training\n\n5\n\n\fdata from ImageNet-1K for fine-tuning, adhering to the semi-supervised protocol introduced in\nSimCLR. The top-1 accuracy, presented in Table 2 after fine-tuning with 1% and 10% of the training\ndata, demonstrates LeOCLR\u2019s superiority over all compared techniques. This can be attributed to\nLeOCLR\u2019s enhanced representation learning capabilities, particularly in comparison to other SOTA\nmethods.\n\nTable 2: Semi-supervised training results on ImageNet-1K: Top-1 performances are reported for\nfine-tuning a pre-trained ResNet-50 with the ImageNet-1K 1% and 10% datasets. * denotes the\nresults are reproduced in this study.\n\nApproach\n\nImageNet-1K 1% ImageNet-1K 10%\n\nMoCo-v2 *\nSimCLR\nBYOL\nSWAV\nDINO\nRegionCL-M\nSCFS\nLeOCLR (ours)\n\n47.6%\n48.3%\n53.2%\n53.9%\n50.2%\n46.1%\n54.3%\n62.8%\n\n64.8%\n65.6%\n68.8%\n70.2%\n69.3%\n60.4%\n70.5%\n71.5%\n\n**Transfer Learning on Downstream Tasks:** We evaluate our self-supervised pretrained model\nusing transfer learning by fine-tuning it on small datasets such as CIFAR, Stanford Cars, Oxford-IIIT\nPets, and Birdsnap. We adhere to the transfer learning procedures to identify optimal hyperparameters\nfor each downstream task. As shown in Table 3, our method, LeOCLR, surpasses all compared\napproaches on a variety of downstream tasks. This demonstrates that our model acquires valuable\nsemantic features, enabling it to generalize more effectively to unseen data in different downstream\ntasks compared to other techniques. Our method preserves the semantic features of the given objects,\nthereby enhancing the model\u2019s representation learning capabilities. Consequently, it is more effective\nat extracting crucial features and predicting correct classes on transferred tasks.\n\nTable 3: Transfer learning results from ImageNet-1K with the standard ResNet-50 architecture. *\ndenotes the results are reproduced in this study.\n\nApproach\n\nCIFAR-10 CIFAR-100\n\nCar\n\nBirdsnap\n\nPets\n\nMoCo-v2 *\nSimCLR\nBYOL\nDINO\nSCFS\nLeOCLR (ours)\n\n97.2%\n97.7%\n97.8%\n97.7%\n97.8%\n98.1%\n\n85.6%\n85.9%\n86.1%\n86.6%\n86.7%\n86.9%\n\n91.2%\n91.3%\n91.6%\n91.1%\n91.6%\n91.6%\n\n75.6%\n75.9%\n76.3%\n-\n-\n76.8%\n\n90.3%\n89.2%\n91.7%\n91.5%\n91.9%\n92.1%\n\n**Object Detection Task:** To further assess the transferability of the learned representation, we\ncompare our method with other SOTA techniques using object detection on the PASCAL VOC. We\nfollow the same settings as MoCo-v2, fine-tuning on the VOC07+12 trainval dataset using Faster\nR-CNN with an R50-C4 backbone, and evaluating on the VOC07 test dataset. The model is fine-\ntuned for 24k iterations (\u02d82248 23 epochs). As shown in Table 4, our method surpasses all compared\ntechniques. This superior performance can be attributed to our model\u2019s ability to capture richer\nsemantic features compared to the baseline (MoCo-v2) and other techniques, leading to improved\nresults in object detection and related tasks.\n\n5 Ablation Studies\n\nIn the subsequent subsections, we further analyze our approach using a different contrastive instance\ndiscrimination technique (i.e., an end-to-end mechanism) to investigate how our method performs\nwithin this framework. Moreover, we conduct studies on the benchmark datasets STL-10 and\nCIFAR-10 using a distinct backbone (ResNet-18) to assess the consistency of our approach across\nvarious datasets and backbones. Additionally, we employ a random crop test to simulate natural\n\n6\n\n\fTable 4: Results (Average Precision) for PASCAL VOC object detection using Faster R-CNN with\nResNet-50-C4.\n\nApproach\n\nAP50\n\nAP\n\nAP75\n\nMoCo-v2\nCLSA\nSCFS\nLeOCLR (ours)\n\n82.5% 57.4% 64%\n-\n83.2%\n83% 57.4% 63.6%\n83.2% 57.5% 64.2%\n\n-\n\ntransformations, such as variations in scale or occlusion of objects in the image, to analyze the\nrobustness of the features learned by our approach, LeOCLR. We also compare our approach with\nvanilla MoCo-v2 by manipulating their data augmentation techniques to determine which model\u2019s\nperformance is more significantly affected by the removal of certain augmentations. In addition,\nwe experiment with different fine-tuning settings to evaluate which model learns better and faster.\nFurthermore, we adapt the attraction strategy and cropping method of the original image, as well as\ncompute the running time of our approach. Lastly, we examine our approach on a non-centric object\ndataset where the probability of mapping two views containing distinct information is higher.\n\n5.1 Different Contrastive Instance Discrimination Framework\n\nWe utilize an end-to-end framework in which the two encoders fq and fk are updated through\nbackpropagation to train a model with our approach for 200 epochs with a batch size of 256.\nSubsequently, we conduct a linear evaluation of our model against SimCLR, which also employs\nan end-to-end mechanism. As presented in Table 5, our approach outperforms vanilla SimCLR by\na substantial margin of 3.5%, demonstrating its suitability for integration with various contrastive\nlearning frameworks.\n\nTable 5: Comparing vanilla SimCLR with LeOCLR after training our approach 200 epochs on\nImageNet-1K.\n\nApproach\n\nImageNet-1K\n\nSimCLR\nLeOCLR (ours)\n\n62%\n65.5%\n\n5.2 Scalability\n\nIn Table 6, we evaluate our approach on different datasets (STL-10 and CIFAR-10) using a ResNet-18\nbackbone to ensure its consistency across various backbones and datasets (i.e., scalability). We\npre-trained all the approaches for 800 epochs with a batch size of 256 on both datasets and then\nconducted a linear evaluation. Our approach demonstrates superior performance on both datasets\ncompared to all approaches. For instance, our approach outperforms vanilla MoCo-v2, achieving\naccuracies of 5.12% and 5.71% on STL-10 and CIFAR-10, respectively.\n\nTable 6: SOTA approaches versus LeOCLR on CIFAR-10 and STL-10 with ResNet-18.\n\nApproach\n\nSTL-10 CIFAR-10\n\nMoCo-v2\nDINO\nCLSA\nBYOL\nLeOCLR (ours)\n\n80.08%\n84.30%\n82.62%\n79.90%\n85.20%\n\n73.88%\n78.50%\n77.20%\n73.00%\n79.59%\n\n5.3 Center and Random Crop Test\n\nIn Table 7, we report the top-1 accuracy for vanilla MoCo-v2 and our approach after 200 epochs\non ImageNet-1K, concentrating on two tasks: a) center crop test, where images are resized to 256\n\n7\n\n\fpixels along the shorter side using bicubic resampling, followed by a 224 x 224 center crop; and\nb) random crop, where images are resized to 256 x 256 and then randomly cropped and resized to\n224 x 224. According to the results, the performance of MoCo-v2 dropped by 4.3% with random\ncropping, whereas our approach experienced a smaller drop of 2.8%. This suggests that our approach\nlearns improved semantic features, demonstrating greater invariance to natural transformations like\nocclusion and variations in object scales. Additionally, we compare the performance of CLSA with\nour approach, given that both perform similarly after 800 epochs (see Table 1). Note that the CLSA\napproach uses multi-crop (i.e., five strong and two weak augmentations), while our approach employs\nonly two random crops and the original image. As shown in Table 7, LeOCLR outperforms the\nCLSA approach by 2.3% after 200 epochs on ImageNet-1K. To address concerns about the increased\ncomputational cost associated with training LeOCLR compared to MoCo V2, we include the training\ntime for both approaches in Table 7. We trained both models on three A100 GPUs with 80GB for\n200 epochs. Our approach took an additional 13 hours to train over the same number of epochs, but it\ndelivers significantly better performance than the baseline.\n\nTable 7: Comparing LeOCLR with vanilla MoCo-v2 and CLSA after training 200 epochs on\nImageNet-1K.\n\nApproach\n\nCenter Crop Random Crop Time\n\nMoCo-v2\nCLSA\nLeOCLR (ours)\n\n67.5%\n69.4%\n71.7%\n\n63.2%\n-\n68.9%\n\n68h\n-\n81h\n\ngraph1.pdf\n\ngraph2.pdf\n\nFigure 1: *\n(a) Top-1 accuracy\n\nFigure 2: *\n(b) Top-5 accuracy\n\nFigure 3: Semi-supervised training with a fraction of ImageNet-1K labels on a ResNet-50.\n\n5.4 Augmentation and Fine-tuning\n\nContrastive instance discrimination techniques are sensitive to the choice of image augmentations.\nThis sensitivity necessitates further analysis comparing our approach to Moco-v2. These experiments\naim to explore which model learns better semantic features and produces more robust representations\nunder different data augmentations. As shown in Figure 4, both models are affected by the removal\nof certain data augmentations. However, our approach shows a more invariant representation and\nexhibits less performance degradation due to transformation manipulation compared to vanilla MoCo-\nv2. For instance, when we apply only random cropping augmentation, the performance of vanilla\nMoCo-v2 drops by 28 percentage points (from a baseline of 67.5% to 39.5% with only random\ncropping). In contrast, our approach experiences a decrease of only 25 percentage points (from a\nbaseline of 71.7% to 46.6% with only random cropping). This indicates that our approach learns\n\n8\n\n\fimproved semantic features and produces more effective representations for the given objects than\nvanilla MoCo-v2.\n\ngraph3.pdf\n\nFigure 4: Decrease in top-1 accuracy (in % points) of LeOCLR and our\ntion of vanilla MoCo-v2 after 200 epochs, under\nRGrayscaleref erstoresultswithoutgrayscaleaugmentations, whileRcolorref erstoresultswithoutcolorjitterbutwithgrayscaleaugmentations.\n\nreproduc-\nlinear evaluation on ImageNet-1K.\n\nIn Table 2, presented in Section 4, we fine-tune the representations over the 1% and 10% ImageNet-1K\nsplits using the ResNet-50 architecture. In the ablation study, we compare the fine-tuned representa-\ntions of our approach with the reproduced vanilla MoCo-v2 across 1%, 2%, 5%, 10%, 20%, 50%, and\n100% of the ImageNet-1K dataset. In this setting, we observe that tuning a LeOCLR representation\nconsistently outperforms vanilla MoCo-v2. For instance, Figure 3 (a) demonstrates that LeOCLR\nfine-tuned with 10% of ImageNet-1K labeled data outperforms vanilla Moco-v2 fine-tuned with\n20% of labeled data. This indicates that our approach is advantageous when the labeled data for\ndownstream tasks is limited.\n\n5.5 Attraction Strategy\n\nIn this subsection, we apply a random crop to the original image (x) and attract the two views (x1,\nx2) toward it to evaluate its impact on our approach\u2019s performance. We also conducted an experiment\nwhere all views were attracted to each other. However, in our method, we avoid attracting the two\nviews to each other, enforcing the model to draw the two views toward the original image only\n(i.e., the uncropped image containing semantic features for all crops). For these experiments, we\npre-trained the model on ImageNet-1K for 200 epochs using the same hyperparameters employed\nin the main experiment. The experiments in Table 8 underscore the significance of the information\nshared between the two views. They also highlight the importance of leveraging the original image\nand avoiding the attraction of views containing varied semantic information to preserve the semantic\nfeatures of the objects. When we create a random crop of the original image (x) and force the model\nto make the two views similar to the original image (i.e., LeOCLR(Random original image)), the\nmodel performance decreases by 2.4%.\n\nThis performance reduction occurs because cropping the original image and compelling the model to\nattract the two views towards it increases the probability of having two views with differing semantic\ninformation, resulting in a loss of semantic features of the objects. The situation deteriorates when\nwe attract all views (x, x1, x2) to each other in LeOCLR (attract all crops), causing performance to\ndrop closer to that of vanilla MoCo-v2 (67.5%). This decline is attributed to the high likelihood of\nattracting two views containing distinct semantic information.\n\n9\n\n\fTable 8: Comparisons of augmentation strategies using our proposed approach after 200 epochs.\n\nApproach\n\nAccuracy\n\nLeOCLR (Random original image)\nLeOCLR (attract all crops)\nLeOCLR (ours)\n\n69.3%\n67.7%\n71.7%\n\n5.6 Non-Object-Centric Tasks\n\nNon-object-centric datasets, like COCO, depict real-world scenes where the objects of interest are\nnot centered or prominently positioned, unlike object-centric datasets such as ImageNet-1K. In this\nscenario, the chance of generating two views containing distinct semantic information for the object\nis elevated, thus exacerbating the issue of losing semantic features. Therefore, we train both our\napproach and the MoCo-v2 baseline from scratch on the COCO dataset to evaluate how our method\nmanages the discarding of semantic features in such datasets. We utilized identical hyperparameters\nas for ImageNet-1K, training the models with a batch size of 256 over 500 epochs. Subsequently, we\nfine-tuned these pre-trained models on the COCO dataset for object detection.\n\nTable 9: Results for pre-training followed by fine-tuning on COCO for object detection using Faster\nR-CNN with ResNet-50-C4.\n\nApproach\n\nAP50\n\nAP\n\nAP75\n\nMoCo-v2\nLeOCLR (ours)\n\n57.2% 37.6% 41.5%\n59.3% 39.1% 43.0%\n\nTable 9 reveals that our approach captured enhanced semantic features for the given object compared\nto the baseline. This emphasizes that our method of avoiding the attraction of two distinct views is\nmore effective at preserving semantic features, even in a non-object-centric dataset.",
  "results": "",
  "conclusion": "This paper presents a new contrastive instance discrimination approach for SSL to improve represen-\ntation learning. Our method reduces the loss of semantic features by including the original image\nduring training, even when the two views contain different semantic content. We show that our\napproach consistently enhances the representation learning of contrastive instance discrimination\nacross various benchmark datasets, backbones, and mechanisms, including momentum contrast\nand end-to-end methods. In linear evaluation, we achieved an accuracy of 76.2% on ImageNet-1K\nafter 800 epochs, surpassing several SOTA instance discrimination SSL methods. Furthermore, we\ndemonstrated the invariance and robustness of our approach across different downstream tasks, such\nas transfer learning and semi-supervised fine-tuning.\n\n10",
  "is_publishable": 1,
  "venue": NaN
}
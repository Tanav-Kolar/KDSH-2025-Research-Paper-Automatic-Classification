{
  "title": "Evaluating the Resilience of White-Box Defenses\nAgainst Adversarial Examples",
  "abstract": "It is well-established that neural networks exhibit susceptibility to adversarial ex-\namples. This paper assesses two defenses designed to counter white-box attacks\nand demonstrates their lack of effectiveness. Through the implementation of es-\ntablished methodologies, we successfully diminish the accuracy of these protected\nmodels to zero percent.",
  "introduction": "A significant hurdle in the field is the development of neural networks that are resistant to adversarial\nexamples. This paper shows that defenses created to address this issue are inadequate when faced\nwith a white box scenario. Adversarial examples are generated that diminish classifier accuracy to\nzero percent on a well known dataset, while adhering to a minimal perturbation constraint of 4/255, a\nmore stringent limit than what was taken into account in the initial studies. The proposed attacks\neffectively generate targeted adversarial examples, achieving a success rate exceeding 97\n\n2 Background\n\nThis paper assumes prior knowledge of neural networks and the methods for creating potent attacks\nagainst adversarial examples, alongside calculating such examples for neural networks possessing\nnon-differentiable layers. A concise review of essential details and notation will be provided.\n\nAdversarial examples are defined as inputs that closely resemble a given input with regard to a certain\ndistance metric (\u02d800a3, in this instance), yet their classification differs from that of the original input.\nTargeted adversarial examples are instances engineered to be classified as a predetermined target\nlabel.\n\nTwo defenses are scrutinized: Pixel Deflection and High-level Representation Guided Denoiser. The\nauthors of these defenses are thanked for making their source code and pre-trained models accessible.\n\nPixel Deflection introduces a non-differentiable preprocessing step for inputs. A subset of pixels,\ndetermined by an adjustable parameter, is substituted with adjacent pixels. The resultant image often\nexhibits noise. To mitigate this, a denoising procedure is employed.\n\nHigh-level Representation Guided Denoiser (HGR) employs a trained neural network to denoise\ninputs prior to their classification by a standard classifier. This denoiser is a differentiable, non-\nrandomized neural network.",
  "related_work": "",
  "methodology": "The defenses are evaluated under the white-box threat model, generating adversarial examples using\nProjected Gradient Descent (PGD) to maximize cross-entropy loss, with the \u02d800a3, distortion limited\nto 4/255.\n\n.\n\n\fMany studies assert that white-box security is only applicable against attackers who are entirely\nignorant of the defense mechanism in use. HGD, for example, states that the white-box attacks\ndescribed in their research should be classified as oblivious attacks, according to previous research\nwork\u2019s definition.\n\nProtection against oblivious attacks proves to be ineffective. The concept of the oblivious threat\nmodel was introduced in prior work to examine the scenario involving an exceptionally weak attacker,\nhighlighting that certain defenses fail to provide robustness even under such lenient conditions.\nMoreover, numerous previously disclosed systems already demonstrate security against oblivious\nattacks. A determined attacker would undoubtedly explore the potential presence of a defense and\ndevise strategies to bypass it, should a viable method exist.\n\nConsequently, security against oblivious attacks falls considerably short of being either intriguing or\npractical in real-world scenarios. Even the black-box threat model permits an attacker to recognize\nthe implementation of a defense, while keeping the precise parameters of the defense confidential.\nFurthermore, it has been observed that systems vulnerable to white-box attacks are frequently\nsusceptible to black-box attacks as well. Hence, this paper concentrates on evaluating systems against\nwhite-box attacks.\n\n3.1 Pixel Deflection\n\nIt is demonstrated that Pixel Deflection lacks robustness. The defense, as implemented by the original\nauthors, is analyzed and the code used for this evaluation is accessible to the public.\n\nBPDA is applied to Pixel Deflection to address its non-differentiable replacement operation. This\nattack successfully diminishes the defended classifier\u2019s accuracy to 0\n\n3.2 High-Level Representation Guided Denoiser\n\nIt is shown that employing a High-level representation Guided Denoiser is not resilient in the white-\nbox threat model. The defense, as implemented by its developers, has been analyzed, and the code\nfor this evaluation is openly accessible.\n\nPGD is utilized in an end-to-end fashion without any alterations. This method reduces the accuracy\nof the defended classifier to 0",
  "experiments": "",
  "results": "",
  "conclusion": "This paper shows that Pixel Deflection and High-level representation Guided Denoiser (HGD) are\nvulnerable to adversarial examples.\n\n2",
  "is_publishable": 1,
  "venue": NaN
}
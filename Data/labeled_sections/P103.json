{
  "title": "Equivariant Adaptation of Large Pretrained Models:\nA Study on the NLC2CMD Competition",
  "abstract": "This paper presents an investigation into the challenges of adapting pretrained\nmodels, specifically in the context of the NLC2CMD competition.",
  "introduction": "This paper addresses the critical need for effective methods to translate natural language descriptions\ninto executable command-line instructions. The command line interface (CLI) is an important tool\nfor software development due to its expressiveness and efficiency. While GUIs have difficulties\nkeeping up with the rapid pace of new features in software development, CLIs provide a text-based\ninterface to a wide range of software functionalities. The use of natural language for CLI interaction\ncould transform how people interact with various operating systems and cloud platforms. This paper\nexplores the possibilities of leveraging natural language to interact with CLIs making computational\nresources more accessible to a wider range of users.\n\n2 Task Description\n\nThe primary objective of the NLC2CMD task is to transform a natural language (NL) description\nof a command-line action into its corresponding Bash command. An algorithm is expected to\nmodel the top-k Bash translations given the natural language description. This can be represented\nmathematically as:\n\nAnlc \u2208 {p | p = (c, \u03b4)}; |A(nlc)| < k\n\nEach prediction from the model includes a set of Bash commands along with a confidence score,\n\u03b4, ranging from 0.0 to 1.0. This confidence score can be utilized to filter out uncertain predictions\nand is incorporated into the evaluation process. The default confidence is set to 1.0, indicating full\nconfidence in the model\u2019s prediction.\n\n3 Competition Overview\n\nThe competition occurred between July and November of 2020, encompassing training, validation,\nand testing phases. A total of 20 teams registered for the competition, and among these, 9 teams\nparticipated through the end of the testing phase. The teams were allowed 100 submissions in the\nfirst two phases, and a maximum of 10 submissions for the final phase, with daily submission limits.\nThe EvalAI platform was used for hosting the competition.\n\n4 Data\n\n4.1 NL2Bash\n\nThe NL2Bash dataset was utilized, consisting of around 10,000 pairs of natural language descriptions\npaired with corresponding command line syntax.\n\n.\n\n\f4.2 Tellina Query Log\n\nAround 1000 natural language utterances recorded from user interactions with the Tellina system was\ncollected. Three programmers with Bash experience annotated these, resulting in multiple ground\ntruth labels for many examples in the dataset.\n\n4.3 NLC2CMD Data Collection Track\n\nA parallel data-collection track was included in the competition, collecting natural language to bash\ncommand pairs through a web interface on the competition website. 21 participants from industry\nand academia submitted over 120 examples, which after being filtered, were part of the final phase of\nthe challenge.\n\n4.4 Data partitions and pipeline\n\nThe data was filtered for each data sample through a Bash parser to ensure that only valid Bash\ncommands were included. Any text that was not a valid Bash command or used utilities not in the\nUbuntu 18.04 LTS command set was removed. For training, participants were provided with a filtered\nversion of the NL2Bash dataset, as well as man pages for Ubuntu 18.04 LTS. In addition, participants\nwere allowed to use any other publicly available data for training. The data set was split into training,\nvalidation and test sets with different sizes for each. In addition to the original utilities of the first\nphase of the competition, 27 additional utilities were added in subsequent phases.\n\n5 Metrics\n\nThe submissions to the NLC2CMD competition were assessed based on two primary metrics:\naccuracy and energy consumption. This approach was utilized to better evaluate submitted solutions.\n\n5.1 Accuracy\n\nThis section discusses the metrics used to evaluate the task of translating natural language to Bash\ncode. Existing metrics such as Full Command Accuracy, BLEU score, and Template Accuracy, are\nreviewed and it is found that they all have shortcomings. The paper presents a metric, verification\nby execution, which is able to solve these problems. Finally, the metric that was proposed for the\ncompetition is discussed in depth.\n\n5.1.1 Existing Metrics\n\nFull Command Accuracy is a metric that measures the exact match between a generated code and a\nreference code. BLEU scores computes the n-grams of candidate translations with the n-grams of the\nreference translation. Template Accuracy measures if the command templates match but not exact\narguments of the command.\n\n5.1.2 Verification by Execution\n\nBecause Bash is a Turing complete language, the equivalence of two commands is undecidable. To\nhandle this issue, the execution of predicted and reference commands is compared to determine\naccuracy.\n\n5.1.3 NLC2CMD Metric\n\nThis paper presents a metric that ignores the arguments in the predicted commands, considers the\norder of utilities in piped commands and penalizes excess flags.\ni (Cpred, Cref ) = 2 \u2217 |F (U (Cpred)i)\u2229F (U (Cref )i)|\nSF\n|F (U (Cpred)i)\u222aF (U (Cref )i)|\n\nS(p) = maxCref\nThe overall score is then computed as follows:\n\ni=1 I[U (Cpred)i == U (Cref )i] \u2217 SF\n\n1\nT\n\n(cid:80)T\n\ni (Cpred, Cref )\n\n2\n\n\fScore(A(nlc)) = { m axp\u2208A(nlc)S(p), if \u2203p \u2208 A(nlc)suchthatS(p) > 0\navgp\u2208A(nlc)S(p), otherwise\nThis metric encourages the correct utilities and their flags, weighted by the algorithm\u2019s reported\nconfidence. This metric was chosen for the competition due to the constraints of a conference setting\nand the need to focus on the core aspects of command synthesis.\n\n5.2 Energy Efficiency\n\nThis section discusses the metric of energy efficiency of models, and its relevance in the current\nresearch environment. The energy consumption of machine learning models is an area of focus, with\nthe deployment of these models, their inference phase energy consumption can outweigh their training\ncost over time. The experiment-impact-tracker library was used to measure the energy consumption\nof submitted solutions.\n\n6 Competing Solutions\n\nThe final leaderboard of the NLC2CMD competition consisted of 6 teams/entries, along with 2\nbaselines. The leaderboard included the accuracy score, energy consumption and latency of the\nmodels.\n\nTable 1: Final leaderboard for the NLC2CMD competition, showing the accuracy score for the final\n(test) phase, along with the energy consumed and latency for every invocation.\n\nTeam Name\n\nAccuracy Score Energy (Joules) Latency (sec)\n\nMagnum\nHubris\nJb\nAICore\nAINixCLAISimple\ncoinse-team\nTellina\n\n0.532\n0.513\n0.499\n0.489\n0.429\n0.163\n0.138\n\n0.484\n12.037\n2.604\n0.252\nN.A.\n343.577\n2.969\n\n0.709\n14.870\n3.142\n0.423\n0.010\n0.452\n3.242\n\n6.1 TF/IDF and Proposed New Baselines\n\nThe team AINixCLAISimple developed several simple baselines for the task. The approach that\nwas most successful used an information retrieval (IR) method based on Tf-IDF rankings. Several\nvariations of this method were tested, with the addition of the AInix Archie data, pruning duplicates,\nnormalizing NL tokens and adjusting the confidence.\n\nTable 2: Results from simple IR baselines. Additions to the raw predictor are retained cumulatively\ntop- to-bottom.\n\nIR-Baseline Variation Accuracy Score\n\nTf-IDF Raw\n+ AInix Data\n+ Prune Duplicates\n+ Normalize NL\n+ Adjust Conf.\n\n0.361\n0.404\n0.413\n0.429\n0.472\n\n6.2 Transformer with Beam Search\n\nTeam Magnum reached an accuracy score of 0.532 using an ensemble of 5 separately-trained\ntransformer models. Key strategies used in their approach include: Replacing command parameters\nwith generic tokenizations, producing scores using an approximation for confidence, and testing\ndifferent combinations of encoders and decoders.\n\n3\n\n\f6.3 Fine-tuned GPT-2 in Ensemble\n\nThe team Hubris fine-tuned pre-trained transformer models, specifically, the GPT-2 architecture. The\nNL2Bash dataset was also augmented with heuristically mined data from stack-overflow questions.\nTwo models of different sizes and pre-training were used, and the final commands were selected by a\nheuristic algorithm that maximized the minimal word distance between the commands.\n\n6.4 Multi-Step Pipelines\n\nThe multi-step approach involves combining two different models for two separate steps. The first\nstep involves predicting the best utility, and the second step involves predicting the correct flags to\nuse. This can be seen in the models of team jb and team coinse.\n\n7 Discussion\n\nThis section summarizes lessons learned and discussions with participants during the competition.\n\n7.1 Metrics Revision\n\nThis section discusses suggested alternatives for accuracy and energy measurements.\n\n7.1.1 Suggested Alternatives for Accuracy Measurement\n\nSome suggestions for future metrics include: a metric that measures semantic match instead of\nexact command matching; restricting the range of commands covered; a metric that measures mean\nreciprocal rank; a metric that measures session scores over multiple interactions instead of one; using\nadaptability of algorithms; making fast retraining available; and calibration of penalties. The issues\nof statefulness of commands, command injection, full text match and underdetermined invocations\nare also reviewed.\n\n7.1.2 Suggested Alternatives for Energy Measurement\n\nThe issues with power measurement, such as reducing computation to lower peak consumption are\ndiscussed. It is stated that measurement of total energy consumption may be a better solution. It is\nargued whether there is even any point to measuring energy at all due to how small the amount of\nenergy is consumed.\n\n7.2 Other Enhancements\n\nOther enhancements include communication of explanations to users by converting commands back\nto natural language, and conversational interfaces to allow for more context for the system.",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "In this paper, the NLC2CMD competition is discussed, including the methodology, data used and\nthe metrics of the competition. Going forward, the feedback received will be incorporated in future\niterations of the competition.\n\n4",
  "is_publishable": 1,
  "venue": NaN
}
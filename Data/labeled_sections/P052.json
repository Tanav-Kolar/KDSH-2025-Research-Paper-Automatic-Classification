{
  "title": "Specialized Neural Network for Extracting Financial Trading Signals:\nThe Alpha Discovery Neural Network",
  "abstract": "Genetic programming (GP) is currently the leading method for automated feature generation in financial applica-\ntions. It utilizes reverse Polish notation to denote features and subsequently performs an evolutionary procedure.\nNevertheless, with the advancements in deep learning, more effective feature extraction instruments have become\naccessible. This research introduces the Alpha Discovery Neural Network (ADNN), a customized neural network\narchitecture designed to autonomously generate a variety of financial technical indicators using established\nknowledge. Our primary contributions are threefold. Firstly, we employ domain-specific expertise in quantitative\ntrading to formulate sampling guidelines and the objective function. Secondly, we substitute genetic programming\nwith pre-training and model pruning techniques to enable a more streamlined evolutionary process. Thirdly, the\nfeature extraction components within ADNN can be interchanged with various other feature extractors, resulting\nin the creation of diverse functions. Empirical findings demonstrate that ADNN can produce more distinct and\ninformative features in comparison to GP, thereby effectively augmenting the existing pool of factors. Fully\nconnected and recurrent networks demonstrate superior performance in extracting information from financial\ntime series compared to convolutional neural networks. In practical scenarios, the features generated by ADNN\nconsistently enhance the revenue, Sharpe ratio, and maximum drawdown of multi-factor strategies when contrasted\nwith investment strategies that do not incorporate these factors.",
  "introduction": "Predicting the future returns of stocks is a paramount and demanding endeavor in the field of quantitative trading. Numerous\nfactors, including historical price, volume, and a company\u2019s financial information, can be employed to forecast the future returns of\nstocks. Typically, researchers categorize features derived from price and volume as technical indicators, while those derived from\na company\u2019s financial data are classified as fundamental data. Various well-known multi-factor models have been introduced to\naddress this task, and numerous established technical and fundamental factors have been developed. For instance, the Fama-French\nThree-Factor Model utilizes three crucial factors that furnish the majority of the information required to elucidate stock returns.\nSubsequently, the Fama-French Five-Factor Model and numerous other factors have been formulated by domain experts. Nonetheless,\ntwo limitations exist. Firstly, recruiting human specialists is quite costly. Secondly, humans are unable to create certain nonlinear\nfeatures from data with high dimensionality. Consequently, both academic scholars and institutional investors have increasingly\nfocused on the task of automated financial feature engineering.\n\nFeature engineering is a procedure that uncovers the connections between features and expands the feature space by deducing or\ngenerating novel features. During this operation, new features can be created by combining pre-existing features. A more explicit\nexplanation is that algorithms employ operators, hyper-parameters, and existing features to construct a new feature. Occasionally,\nfeature construction and feature selection can be integrated into a single process. These methodologies encompass wrapper, filtering,\nand embedded techniques. Filtering is straightforward but yields suboptimal results; it merely employs certain criteria to select a\nfeature and can sometimes aid in overseeing the feature construction process. The wrapper method exhibits strong performance\nby directly utilizing the model\u2019s outcomes as an objective function. Consequently, it can treat an independently trained model as\na newly generated feature. Nevertheless, a substantial quantity of computational resources and time are necessary. Embedded is\nan approach that employs generalized factors and a pruning method to choose or amalgamate features, serving as an intermediate\noption between filtering and wrapper techniques.",
  "related_work": "With the progression of deep learning, an increasing number of researchers are utilizing neural networks to derive features from raw\ndata and subsequently incorporating a fully connected layer to modify the feature\u2019s output. Similarly, a trained model signifies a\nnewly developed feature. Researchers have leveraged it on pattern recognition tasks, employing a CNN model to construct facial\ndescriptors, and this method generates features that possess considerably more information than the previous method. Experiments\n\n\fhave been conducted on this task, employing a deeper and wider convolutional neural network. Recurrent neural networks have\nbeen used to pre-locate feature-rich regions and successfully construct more refined features. In a text classification task, recurrent\nneural networks have been utilized to build a rule-based classifier among text data, wherein each classifier represents a portion\nof the text. A network structure that uses both a recurrent neural network and a convolutional neural network to extract text\ninformation has been proposed. Utilizing a neural network\u2019s robust fitting capability, we can generate highly informative features by\ncustomizing the network architecture for diverse industries. In financial feature engineering tasks, researchers have commenced\nemploying neural networks to provide an embedding representation of financial time series. More specifically, LSTM has been\nutilized to embed various stock time series, followed by adversarial training to perform binary classification on a stock\u2019s future\nreturn. Well-designed LSTM has been adopted to extract features from unstructured news data, subsequently forming a continuous\nembedding. The experimental outcomes indicate that these unstructured data can furnish substantial information and are highly\nbeneficial for event-driven trading. A Skip-gram architecture has been employed to learn stock embedding, inspired by a valuable\nknowledge repository formed by fund managers\u2019 collective investment behaviors. This embedding can more effectively represent\nthe varying affinities across technical indicators. Adopting a similar concept, we employ a neural network to provide a concise\nembedding of extended financial time series.",
  "methodology": "The ADNN\u2019s network architecture is structured in a specific way. The primary contributions of this innovative network structure are:\n1) ADNN employs Spearman Correlation as its loss function, mirroring the practices of human quantitative investment. Furthermore,\nthe sampling guidelines adhere to economic principles. 2) A significant, derivable kennel function is introduced as a substitute for\nthe non-derivable operator. 3) We utilize pre-training and pruning in place of the GP\u2019s evolutionary process, resulting in enhanced\nefficiency.\n\nIn each back-propagation cycle, ADNN randomly selects data from a certain number of trading days and subsequently computes the\nSpearman Coefficient between the factor value and factor return for each of those days. The number of days should be greater than 3,\nand incorporating information from multiple trading days enables the neural network to achieve a more consistent convergence.\nQuantitative investors prioritize the relative strength of each stock on a given trading day over its absolute strength. Therefore,\nperforming calculations for each trading day and employing the Spearman Coefficient as the loss function is justifiable.\n\nWe posit that there are a certain number of stocks pertaining to a given trading day in each batch. The input tensor has a specific\nshape because there are a certain number of samples, and five categories of time series: the opening price, high price, low price,\nclosing price, and volume. Each time series has an input length. We also designate the output tensor as the factor value, possessing a\nparticular shape. The factor return tensor has a specific shape, denoting the profit we can obtain from this asset over an extended\nduration. The holding period\u2019s length is defined. Here, we presume that all feature extractors are Multi-layer Perceptrons (MLPs),\nsimplifying the provision of a general mathematical description. In the experimental section, we will present the experimental\noutcomes based on more intricate and varied feature extractors.",
  "experiments": "We utilize daily trading data from the Chinese A-share stock market, encompassing the daily opening, high, low, closing prices, and\ntrading volume over the preceding 30 trading days. The raw data is standardized using its time-series mean and standard deviation\nderived from the training set. Both the mean and standard deviation are computed from the training set. We endeavor to employ\nthese inputs to forecast the stock return for the subsequent 5 trading days (utilizing 3-15 trading days is advisable). Furthermore, we\nmust adhere to market regulations when devising a trading strategy.\n\nExtensive experiments have been performed to identify appropriate hyper-parameters. For each experiment, 250 trading days\nconstitute the training set, the ensuing 30 trading days serve as the validation set, and the subsequent 90 trading days function as the\ntesting set. The generated factors maintain a high Information Coefficient (IC) throughout the subsequent 90 trading days. Most\nsignificantly, we emphasize a counter-intuitive configuration: the training period should not surpass 250 trading days due to the\nnon-stationary nature of financial features. If we mandate a feature to function effectively over an extended duration, we will only\nencounter this feature in an over-fitting scenario. Consequently, we devise a rolling forecast framework wherein we automatically\nidentify potent features for each trading day. Each autonomously generated feature will have its own period of prominence on that\nparticular trading day. Moreover, these factors not only perform effectively on this single day but also maintain their efficacy for\nseveral trading days, exhibiting a gradual decline.\n\nTo ensure an equitable comparison, the identical configuration is implemented for the GP algorithm. The logic of this algorithm\nreferences related work. Moreover, the input data\u2019s period and type must be consistent. In this paper, we scrutinize the performance\nof the constructed features from diverse angles. Typically, institutional investors employ the Information Coefficient (IC), to\nquantify the amount of information conveyed by a feature. For diversity, cross-entropy is utilized to gauge the distance between the\ndistributions of two distinct features on the same trading day.\n\n2",
  "results": "The network structure can equip ADNN with different deep neural networks. In order to show the general situation, we equip ADNN\nwith 4 fully-connected layers. Each layer has 128 neural, tanh activate function, L2 Regularization, and dropout technic. This\ngeneral and simple setting is enough to beat the GP. We put forward three schemes help to show how ADNN beat the GP. Only GP\nmeans only using genetic programming, Only ADNN means only use ADNN to construct factors, GP&ADNN means use GP\u2019s\nvalue to initialize ADNN and then construct factors. All the experiments are conducted out of the sample.\n\nTable 1 shows that Only ADNN is better than Only GP, which means ADNN outperforms GP on this task. And we also find that\nGP&ADNN is the best, it means that our method can even improve the performance of GP.\n\nTable 1: The performance of different schemes.\n\nObject\n\nInformation Coefficient Diversity\n\nOnly GP\nGP&ADNN\nOnly ADNN\n\n0.094\n0.122\n0.107\n\n17.21\n25.44\n21.65\n\nIn real practice, we should leverage the constructed factors to form a multi-factor strategy and compare its performance with GP. The\nspecific strategy setting is same as section 3.4, and we have repeated this experiment on different periods of time. The long-term\nbacktest result is shown in Table 2, Only ADNN always has better performance than the Only GP. It shows that ADNN has also\nbeaten the SOTA in real practice. Similar to the conculsions made above, if we combine these two methods together, the combined\nfactors\u2019 strategy has the best performance in backtesting.\n\nTable 2: Strategy\u2019s absolute return for each scheme.\n\nTime\n\nOnly GP GP&ADNN Only ADNN\n\nZZ500\n\nTrain:2015.01-2015.12 Test: 2016.02-2016.03\nTrain:2016.01-2016.12 Test: 2017.02-2017.03\nTrain:2017.01-2017.12 Test: 2018.02-2018.03\nTrain:2018.01-2018.12 Test: 2019.02-2019.03\n\n+2.59%\n+5.40%\n-5.27%\n+13.00%\n\n+5.74%\n+10.26%\n-4.95%\n+15.62%\n\n+4.52%\n+8.33%\n-4.16%\n+15.41%\n\n+1.67%\n+2.53%\n-6.98%\n+13.75%\n\nAll the results shown above is based on the most basic feature extractors. So will there be more powerful feature extractors to\ndiscover knowledge from financial time series? And what is the suitable input data structure for financial time series?\n\nTable 3 shows that, basically, all neural networks can produce more diversified features than using GP. But temporal extractors are\nespecially better at producing diversified features, such as LSTM and Transformer. As for TCN, the author who put forward this\nnetwork structure proves its ability to capture the temporal rules buried in data. However, there is a huge difference. TCN relies\non a convolution neural network, but LSTM and Transformer still contain recurrent neural networks (Normally, the transformer\nuses a recurrent neural network to embedded the input data). The existence of a recurrent neural network structure may contribute\nto the difference in diversity. For Le-net and Resnet, they don\u2019t provide us with more informative features. It looks like that the\nconvolution network structure is not suitable to extract information from the financial time series.\n\nTable 3: The higher are the information coefficient (IC) and diversity, the better is their performance. Normally, a good feature\u2019s\nlong-term IC should be higher than 0.05, but it cannot be higher than 0.2 in an A-share market.\n\nType\n\nNetwork\n\nIC\n\nDiversity\n\nTime\n\nBaseline\nVanilla\n\nSpatial\n\nGP\nFCN\nLe-net\nResnet-50\nLSTM\n\nTemporal TCN\n\nTransformer\n\n0.072\n0.124\n0.123\n0.108\n0.170\n0.105\n0.111\n\n17.532\n22.151\n20.194\n21.403\n24.469\n21.139\n25.257\n\n0.215 hours\n0.785 hours\n1.365 hours\n3.450 hours\n1.300 hours\n2.725 hours\n4.151 hours\n\nIn practical applications, we integrate conventional factors with those generated by ADNN to formulate a quantitative investment\nstrategy. Our objective is to ascertain whether ADNN can enhance the factor pool and improve upon the traditional multi-factor\nstrategy.\n\nWe establish a commonly employed multi-factor strategy to assess its performance in a real-world context. Within the training set,\nsamples whose returns rank in the top 30% for each trading day are designated as 1, while those ranking in the bottom 30% are\nlabeled as 0. The remaining samples in the training set are discarded. Following the training of these features using XGBoost in\n\n3\n\n\fbinary logistics mode, the prediction outcome reflects the probability of a stock exhibiting exceptional performance in the subsequent\n5 trading days. It designates the 50 features constructed by human experts as PK 50, the features constructed by ADNN as New 50,\nand the features constructed by both GP and PK as GP-PK 50. In separate experiments, we use XGBoost to pre-train both PK 50\nand New 50 in the training set and then using the weight score from XGBoost to choose the 50 most important features as Combined\n50. This feature selection process only happens once, and only be conducted in training set.\n\nTable 4 shows the results of the backtesting.\n\nTable 4: Back testing starts from Jan 2019 to June 2019. The investment target is all A-share, except for the stock can\u2019t be traded\nduring this period of time. Strategy\u2019s commission fee is 0.5%. SR refers to Sharpe Ratio, MD represents Max- Drawdown.\n\nType\nSR\n\n1.982\nBaseline\n1.606\n\n2.314\n\n1.435\nGP\n2.672\n\n2.189\nVanilla\n3.167\n\n1.800\n\n!\n\n2.921\nSpatial\n1.962\n\n2.787\n\n2.205\n\n3.289\nTemporal\n2.440\n\n2.729\n\n2.203\n\n2.806\n\nTarget\n\nZZ500\n\nHS300\n\nPK\n\nGroup\n\nRevenue\n\nMD\n\nStock Index\n\n19.60% 13,50%\n\nStock Index\n\n18.60% 20.30%\n\nPK 50\n\n24.70% 18.90%\n\nGP 50\n\n17.60% 25.30%\n\nGP-PK 50\n\n25.40% 14.80%\n\nNew 50\n\n20.60% 15.80%\n\nFCN\n\nCombined 50\n\n29.60% 15.70%\n\nNew 50\n\n18.00% 16.90%\n\nLe-net\n\nCombined 50\n\n27.50% 16.40%\n\nNew 50\n\n19.90% 15.40%\n\nResnet-50\n\nCombined 50\n\n29.30% 17.20%\n\nNew 50\n\n19.50% 13.00%\n\nLSTM\n\nCombined 50\n\n29.90% 15.00%\n\nNew 50\n\n22.40% 14.70%\n\nTCN\n\nCombined 50\n\n26.90% 16.80%\n\nNew 50\n\n21.10% 15.90%\n\nTransformer Combined 50\n\n27.20% 15.10%\n\nAs shown in Table 4, HS300 and ZZ500 are important stock indices in the A-share stock market. Revenue represents the annualized\nexcess return, by longing portfolio and shorting the index. The max drawdown is the worst loss of the excess return from its peak.\nThe Sharpe ratio is the annually adjusted excess return divided by a certain level of risk. These indicators can show the strategy\u2019s\nperformance from the perspective of both return and risk.\n\nFor the New 50, although they have higher IC than the PK 50, their overall performance is not always better than PK 50. Because the\noverall performance of a multi-factor strategy is determined by both diversity and information volume (IC), we guess the diversity of\nPK 50 is remarkably higher than the diversity of New 50. We also did experiment to verify this guess. Thus, although every single\nnew factor is better than the old factor, their overall performance not always be better. ADNN\u2019s diversity is larger than the GP, but\nfor further research, making ADNN\u2019s diversity even larger is still badly needed. In the real world use case, all investors have their\nown reliable and secret factor pool, what they want is that the new constructed factors can bring in margin benefits. Thus, they will\nuse both new and old factors to do trading. That\u2019s the reason why Combined 50 can represent ADNN\u2019s contribution in the real\nsituation. In all cases, Combined 50 is better than PK 50 and GP-PK 50, which means that the ADNN not only perform better than\nGP, but also can enrich investors\u2019 factor pool.\n\n4",
  "conclusion": "In this research, we introduce the Alpha Discovery Neural Network (ADNN), a system capable of autonomously generating financial\nfeatures from raw data. We have meticulously crafted its network architecture in accordance with economic principles and furnished\nit with a variety of sophisticated feature extractors. Empirical results indicate that ADNN can generate features that are more\ninformative and diverse than those produced by the benchmark method in this specific application. In practical scenarios, ADNN also\ndemonstrates superior revenue, Sharpe ratio, and maximum drawdown compared to genetic programming. Furthermore, different\nfeature extractors assume distinct roles. We have conducted numerous experiments to validate this observation and endeavor to\ncomprehend its functionality. For future research, we intend to employ this framework to automatically generate valuable features\nbased on companies\u2019 fundamental data and sentiment data.\n\n5",
  "is_publishable": 1,
  "venue": NaN
}
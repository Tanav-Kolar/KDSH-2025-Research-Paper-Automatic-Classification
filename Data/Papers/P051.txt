Real-Time Adaptation of Lexical Embeddings for
Enhanced Part-of-Speech Tagging

Abstract

This research introduces a method for real-time unsupervised domain adaptation
(DA) that can be applied incrementally as new information arrives. This method is
especially useful when conventional batch DA is unfeasible. Through evaluations
focused on part-of-speech (POS) tagging, we observe that real-time unsupervised
DA achieves accuracy levels on par with those of batch DA.

1

Introduction

Unsupervised domain adaptation is a frequently encountered challenge for developers aiming to
create robust natural language processing (NLP) systems. This situation typically arises when labeled
data is available for a source domain, but there is a need to enhance performance in a target domain
using only unlabeled data. A majority of the current NLP research on unsupervised domain adaptation
employs batch learning, which presumes the availability of a substantial corpus of unlabeled data
from the target domain before the testing phase. However, batch learning is impractical in numerous
real-world situations where data from a new target domain must be processed without delay. Further,
in many practical scenarios, data may not be neatly categorized by domain, making it difficult to
immediately discern when an input stream begins providing data from a new domain.

For instance, consider an NLP system within a company that is tasked with analyzing a continuous
stream of emails. This stream evolves over time without any explicit signals indicating that the
current models should be adjusted to the new data distribution. Given that the system is expected to
operate in real-time, it would be beneficial for any system adaptation to be done in an online manner,
as opposed to the batch method, which involves halting the system, modifying it, and then restarting
it.

This paper introduces real-time unsupervised domain adaptation as an enhancement to conventional
unsupervised DA. In this approach, domain adaptation is carried out incrementally as data is received.
Specifically, our implementation involves a type of representation learning, where the focus is on
updating word representations in our experiments. Every instance a word appears in the data stream
during testing, its representation is refined.

To our understanding, the research presented here is the first to examine real-time unsupervised
DA. In particular, we assess this method for POS tagging tasks. We analyze POS tagging outcomes
using three different methods: a static baseline, batch learning, and real-time unsupervised DA. Our
findings indicate that real-time unsupervised DA performs comparably to batch learning, yet it does
not require retraining or pre-existing data from the target domain.

2 Experimental setup

Tagger. We have adapted the FLORS tagger, which is recognized for its speed and simplicity,
and is particularly effective in DA scenarios. This tagger approaches POS tagging as a multi-label
classification problem within a window-based framework, rather than a sequence classification
one. FLORS is well-suited for real-time unsupervised DA because its word representations include

distributional vectors, which can be updated during both batch learning and real-time unsupervised
DA. Each word’s representation in FLORS consists of four feature vectors: one for its suffix, one for
its shape, and one each for its left and right distributional neighbors. Suffix and shape features are
standard in the literature, and we utilize them as described previously.

Distributional features. The ith element xi of the left distributional vector for a word w is the
weighted count of times the indicator word ci appears immediately to the left of w:

xi = tf (f req(bigram(ci, w)))

(1)

where ci is the word with frequency rank i in the corpus, freq(bigram(ci, w)) is the occurrence count
of the bigram "ci w", and non-zero frequencies are weighted logarithmically: tf(x) = 1 + log(x). The
right distributional vector is defined similarly. We limit the set of indicator words to the 500 most
frequent. To avoid zero vectors, an additional element xn+1 is added to each vector to account for
omitted contexts:

xn + 1 = tf (

(cid:88)

.5f req(bigram(ci, w)))

(2)

Let f(w) be the concatenation of the two distributional, suffix, and shape vectors of word w. Then
FLORS represents token vi as follows:

f (viΦ22122)Φ2295f (viΦ22121)Φ2295f (vi)Φ2295f (vi + 1)Φ2295f (vi + 2)

(3)

where ˘2295 is vector concatenation. FLORS then tags token vi based on this representation.

FLORS operates under the assumption that the fundamental relationship between distributional
features and labels remains consistent when transitioning from the source to the target domain. This
contrasts with other studies that select "stable" distributional features and discard "unstable" ones.
The central hypothesis of FLORS is that fundamental distributional POS characteristics are relatively
stable across different domains, unlike semantic or more intricate tasks. The effectiveness of FLORS
suggests the validity of this hypothesis.

Data. Test set. Our evaluation utilizes the development sets from six different target domains (TDs):
five SANCL domains (newsgroups, weblogs, reviews, answers, emails) and sections 22-23 of the
Wall Street Journal (WSJ) for in-domain testing.

Two training sets of varying sizes are employed. In the l:big condition (large labeled data set), FLORS
is trained on sections 2-21 of the WSJ. The l:small condition uses 10% of the l:big data set.

Data for word representations. We also adjust the size of the datasets used for computing word
representations before training the FLORS model. In the u:big condition, distributional vectors are
computed on the combined corpus of all labeled and unlabeled text from both source and target
domains (excluding test sets), along with 100,000 WSJ sentences from 1988 and 500,000 sentences
from a large external corpus. In the u:0 condition, only labeled training data is utilized.

Methods. We implemented a modification from the original setup: distributional vectors are stored
in memory as count vectors, enabling count increases during online tagging.

Experiments are conducted with three versions of FLORS: STATIC, BATCH, and ONLINE. All three
methods compute word representations on "data for word representations" before model training on
one of the two "training sets".

STATIC. Word representations remain unchanged during testing.

BATCH. Before testing, count vectors are updated by freq(bigram(ci, w)) += freq*(bigram(ci, w)),
where freq*(˘00b7) denotes the bigram "ci w" occurrences in the entire test set.

ONLINE. Before tagging a test sentence, both left and right distributional vectors are updated via
freq(bigram(ci, w)) += 1 for each "ci w" bigram appearance in the sentence. The sentence is then
tagged using the updated word representations. As tagging progresses, distributional representations
become increasingly specific to the target domain (TD), converging to the representations that BATCH
uses at the end of the tagging process.

2

In all three modes, suffix and shape features are always fully specified, for both known and unknown
words.

3 Experimental results

Table 1 shows that the performance levels of BATCH and ONLINE are on par with each other and
represent the current state-of-the-art. The highest accuracy in each column is highlighted in bold.

Table 1: BATCH and ONLINE accuracies are comparable and state-of-the-art. Best number in each
column is bold.

newsgroups

reviews

weblogs

answers

emails

wsj

OOV

TnT
88.30
Stanford
90.25
SVMTool
87.96
C&P
88.65
S&S
90.37
S&S (reimpl.)
89.70
BATCH
91.86
ONLINE
91.69

ALL OOV ALL OOV ALL OOV ALL OOV ALL OOV ALL

88.66

54.73

90.40

56.75

93.33

74.17

88.55

48.32

88.14

58.09

95.75

89.11

56.02

91.43

58.66

94.15

77.13

88.92

49.30

88.68

58.42

96.83

89.14

53.82

91.30

54.20

94.21

76.44

88.96

47.25

88.64

56.37

96.63

89.51

57.23

91.58

59.67

94.41

78.46

89.08

48.46

88.74

58.62

96.78

90.86

66.42

92.95

75.29

94.71

83.64

90.30

62.16

89.44

62.61

96.59

90.68

65.52

93.00

75.50

94.64

82.91

90.18

61.98

89.53

62.46

96.60

90.87

71.18

93.07

79.03

94.86

86.53

90.70

65.29

89.84

65.44

96.63

90.85

71.00

93.07

79.03

94.86

86.53

90.68

65.16

89.85

65.48

96.62

Table 2 shows that the accuracy rates for ONLINE and BATCH methods are generally superior
to those of the STATIC method, as indicated by the numbers in bold. It also demonstrates that
performance improves with an increase in both training data and unlabeled data.

The performance of ONLINE is similar to that of BATCH. It is slightly lower than BATCH in the
u:0 condition, with the most significant difference in accuracy being 0.29, and it is at most 0.02
different from BATCH in terms of overall accuracy in the u:big condition. The reasons for ONLINE
occasionally outperforming BATCH, particularly in certain conditions, are discussed subsequently.

3.1 Time course of tagging accuracy

The ONLINE model introduced here has a unique characteristic not commonly found in other
statistical NLP research: its predictive accuracy evolves as it processes text due to the modification of
its representations.

To analyze the progression of these changes over time, a substantial application domain is necessary
because subtle changes might be too inconsistent in the smaller test sets of the SANCL TDs. The
WSJ corpus is the only labeled domain that is sufficiently large for this purpose. Consequently, we
invert the usual setup by training the model on the development sets of the five SANCL domains
(l:big) or on the initial 5000 labeled words of reviews (l:small). In this reversed setup, u:big utilizes
the five unlabeled SANCL datasets along with a large external corpus as before. Given the importance
of performance variability, we conduct 100 trials on randomly selected 50% samples of WSJ and
report both the average and standard deviation of tagging errors across these trials.

The results presented in Table 3 indicate that ONLINE’s error rates are only marginally higher than,
or comparable to, those of BATCH. Specifically, in the l:small/u:0 condition, the error rate for known
words is lower for ONLINE (0.1186) than for BATCH, similar to observations in Table 2.

3

Table 2: ONLINE / BATCH accuracies are generally better than STATIC (see bold numbers) and
improve with both more training data and more unlabeled data.

l:small

newsgroups

l:big

l:small

reviews

l:big

l:small

weblogs

l:big

!

l:small

answers

l:big

l:small

emails

l:big

l:small

l:big

wsj

u:0

u:big

ALL

KN

SHFT OOV ALL

KN

SHFT OOV

87.02
STATIC
ONLINE 87.99
BATCH 88.28
STATIC
89.69
ONLINE 90.51
BATCH 90.69

STATIC
89.08
ONLINE 89.67
BATCH 89.79
STATIC
91.96
ONLINE 92.33
BATCH 92.42

91.58
STATIC
ONLINE 92.51
BATCH 92.68
STATIC
93.45
ONLINE 94.18
BATCH 94.34

STATIC
86.93
ONLINE 87.48
BATCH 87.56
89.54
STATIC
ONLINE 89.98
BATCH 90.14

STATIC
85.43
ONLINE 86.30
BATCH 86.42
STATIC
88.31
ONLINE 88.86
BATCH 88.96

STATIC
94.64
ONLINE 94.86
BATCH 94.80
STATIC
96.44
ONLINE 96.50
BATCH 96.57

90.87
90.87
91.08
93.00
93.13
93.12

91.96
92.14
92.23
93.94
94.03
94.09

94.29
94.52
94.60
95.64
95.82
95.85

90.89
91.18
91.11
92.76
92.97
93.10

90.85
91.26
91.31
92.98
93.08
93.11

95.44
95.53
95.46
96.85
96.85
96.82

71.12
76.10
77.01
82.65
82.51
83.24

66.55
70.14
69.86
82.30
83.59
83.53

79.95
81.76
82.34
90.15
89.80
90.03

66.51
68.07
68.25
78.65
79.07
79.01

57.85
60.56
61.03
71.38
72.38
72.28

83.38
85.37
85.51
92.75
93.55
93.48

57.16
65.64
66.37
57.82
67.57
69.43

65.90
69.67
71.27
67.97
72.50
73.35

72.74
80.46
81.20
72.68
80.35
81.84

53.43
56.47
58.44
56.22
59.77
60.72

51.65
55.83
56.32
52.71
57.78
58.85

82.72
85.22
85.38
85.38
86.38
86.54

89.02
89.84
89.82
89.93
90.85
90.87

91.45
92.11
92.10
92.42
93.07
93.07

93.42
94.21
94.20
94.09
94.86
94.86

88.98
89.71
89.71
90.06
90.68
90.70

87.76
88.45
88.46
89.21
89.85
89.84

95.73
95.80
95.80
96.56
96.62
96.63

91.48
92.38
92.37
92.41
93.04
93.03

92.47
93.62
93.60
93.53
94.36
94.36

94.77
95.40
95.42
95.54
95.81
95.82

91.09
92.42
92.43
92.18
93.21
93.22

90.35
92.31
92.32
91.74
93.30
93.30

95.88
96.21
96.22
96.72
96.89
96.89

81.53
82.58
82.65
84.94
84.94
85.20

80.11
81.46
81.51
84.65
85.71
85.71

89.80
91.08
91.03
91.90
92.60
92.60

77.63
78.11
78.23
80.70
81.48
81.54

70.86
71.67
71.71
73.80
75.32
75.27

90.36
89.89
89.89
93.35
93.35
93.42

58.30
67.09
67.03
58.97
71.00
71.18

70.81
78.42
78.42
69.97
79.03
79.03

77.42
84.03
83.87
76.94
86.53
86.53

57.36
64.21
64.09
58.25
65.16
65.29

56.76
61.57
61.65
58.99
65.48
65.44

87.87
89.70
89.70
88.04
91.69
91.86

Table 3 also includes data on "unseens" along with unknowns, as prior research indicates that unseens
lead to at least as many errors as unknowns. Unseens are defined as words with tags not present in
the training data, and error rates for unseens are calculated across all their occurrences, including
those with both seen and unseen tags. As shown in Table 3, the error rate for unknowns is higher than
that for unseens, which in turn is higher than the error rate for known words.

When examining individual conditions, ONLINE generally outperforms STATIC, showing better
results in 10 out of 12 cases and only slightly underperforming in the l:small/u:big condition for
unseens and known words (0.1086 vs. 0.1084, 0.0802 vs. 0.0801). In four conditions, ONLINE is
significantly better, with improvements ranging from 0.005 to over 0.06. The differences between
ONLINE and STATIC in the remaining eight conditions are minimal. For the six u:big conditions,
this is expected as the large unlabeled dataset is from the news domain, similar to WSJ. Therefore, if
large unlabeled datasets similar to the target domain are available, using STATIC tagging may suffice
since the additional effort for ONLINE/BATCH may not be justified.

4

Table 3: Error rates (err) and standard deviations (std) for tagging. ˘2020 (resp. ˘2217): significantly
different from ONLINE error rate above&below (resp. from “u:0” error rate to the left).

unknowns

u:0

u:big

l:small

l:big

err
.3670˘2020
STATIC
ONLINE .3050˘2020
BATCH

.3094
.1451˘2020
.1404
.1382˘2020

STATIC
ONLINE
BATCH

std

err

std

.00085
.00143
.00160

.00114
.00125
.00140

.3094
.2104
.2102˘2217

.1042
.1037˘2217
.1033

.00160
.00081
.00093

.00100
.00098
.00112

u:0

err
.1659˘2020
.1646˘2020
.1404

.0732
.0727
.0723

unseens

u:big

std

err

std

.00076
.00145
.00125

.00052
.00051
.00065

.1467
.1084
.1037˘2217

.0690
.0689˘2217
.0680

.00120
.00056
.00098

.00042
.00051
.00062

u:0

err
.1309˘2020
.1251˘2020
.1186

.0534
.0529
.0528

known words

u:big

std

.00056

.00103

.00095

.00027

.00031

.00033

err

.1186

.0801

.0802˘2217

.0503

.0502˘2217

.0502

std

.00095

.00042

.00048

.00025

.00031

.00031

Increasing the amount of labeled data consistently reduces error rates, as does increasing unlabeled
data. The differences are significant for ONLINE tagging in all six cases, marked by ˘2217 in the
table.

There is no significant difference in variability between ONLINE and BATCH, suggesting that
ONLINE is preferable due to its equal variability and higher performance, without requiring a dataset
available before tagging begins.

The progression of tagging accuracy over time is illustrated in Figure 1. BATCH and STATIC
maintain constant error rates as they do not adjust representations during tagging. ONLINE’s error
rate for unknown words decreases, approaching BATCH’s error rate, as more is learned with each
occurrence of an unknown word.

4 Related Work

Online learning typically refers to supervised learning algorithms that update the model after process-
ing a few training examples. Many supervised learning algorithms are online or have online versions.
Active learning is another supervised learning framework that processes training examples ˘2014
usually obtained interactively ˘2014 in small batches. All of this work on supervised online learning is
not directly relevant to this paper since we address the problem of unsupervised domain adaptation.
Unlike online supervised learners, we keep the statistical model unchanged during domain adaptation
and adopt a representation learning approach: each unlabeled context of a word is used to update its
representation.

There is much work on unsupervised domain adaptation for part-of-speech tagging, including work
using constraint-based methods, instance weighting, self-training, and co-training. All of this work
uses batch learning. For space reasons, we do not discuss supervised domain adaptation.

5 Conclusion

This study introduces a method for real-time updating of word representations, a new form of domain
adaptation designed for scenarios where target domain data are processed in a stream, making
BATCH processing unfeasible. We demonstrate that real-time unsupervised domain adaptation
achieves performance levels comparable to batch learning. Moreover, it significantly reduces error
rates compared to STATIC methods, which do not employ domain adaptation.

Acknowledgments. This research was supported by a scholarship from Baidu awarded to Wenpeng
Yin and by the Deutsche Forschungsgemeinschaft (grant DFG SCHU 2246/10-1 FADeBaC).

5


{
  "title": "BladeDISC++: Enhancing Memory Usage Through\nSymbolic Shape Analysis",
  "abstract": "The increasing prevalence of dynamic characteristics in modern deep learning tasks\nhas led to the growing importance of dynamic shape compilers. These compilers\nare designed to create effective kernels for dynamic shape graphs, which have a\nstable structure but uncertain tensor shapes. However, memory optimization, which\nis vital in the era of large models, has not been thoroughly investigated for dynamic\nshape graphs. The core issue lies in the absence of specific tensor shapes, which are\ngenerally required by existing methods like operation scheduling and rematerializa-\ntion. To overcome this issue, we present operation scheduling and rematerialization\nstrategies that utilize symbolic shapes, implemented in BladeDISC++. Furthermore,\ngiven that rematerialization decisions cannot be determined at compile time alone\ndue to unknown tensor shapes, BladeDISC++ uses a hybrid approach combining\ncompilation and runtime to address shape changes effectively. Our findings demon-\nstrate that BladeDISC++ significantly reduces memory consumption for dynamic\nshape graphs, achieving levels similar to those of optimizations with precise shapes.\nThis advancement facilitates the broader use of dynamic shape compilers.",
  "introduction": "Dynamic shape compilers are becoming more and more necessary due to their ability to optimize\ndeep learning tasks that have dynamic attributes. While advancements in kernel generation have been\nmade by systems like TorchInductor and Modular, memory optimization remains a less-explored area.\nTraditional methods like operation scheduling and rematerialization, which encompass recomputation\nand offloading, depend on precise tensor shapes to evaluate the memory impact of operations or\nsubgraphs, and consequently make optimization choices during compilation. However, these methods\nbecome impractical when shape values are not available.\n\nBladeDISC++, which is based on the dynamic shape compiler BladeDISC, uses symbolic shapes\nto address these challenges. With symbolic shapes, BladeDISC++ is capable of comparing the\nmemory effects of different operation sequences, and identifying the ideal scheduling order. For\nrematerialization, symbolic shapes are used to identify the optimal recomputation subgraph at compile\ntime, and assist in making final rematerialization decisions during runtime.\n\nOur experiments reveal that BladeDISC++ can efficiently reduce memory usage during training\nwith dynamic shape graphs when compared to BladeDISC. Furthermore, BladeDISC++ achieves\nmemory consumption similar to static shape training while eliminating the overhead associated with\nrecompilation and tensor padding.\n\n2 Memory optimizations based on symbolic shapes\n\nAs shown in Figure 1, BladeDISC++ starts with a dynamic shape computation graph, and proceeds by\nconducting a symbolic shape analysis to construct a global symbolic shape graph. This graph details\nthe mathematical connections between the shape symbols, which will be discussed in section 2.1.\nFollowing this, the symbolic shape graph, along with the computation graph, is optimized through\n\n.\n\n\fsteps that include operation fusion, operation scheduling, and rematerialization. These steps are\naimed at memory usage reduction.\n\nAs previous work on BladeDISC has addressed operation fusion, this paper focuses on operation\nscheduling, which will be discussed in section 2.2, and rematerialization, which will be discussed\nin section 2.3. Using the symbolic shape graph instead of exact tensor shapes, BladeDISC++ can\nstill compare the memory usage of different operation sequences and determine the benefit of\nrecomputation subgraphs. Moreover, because the memory needs of a dynamic shape graph can\nfluctuate between different runs, it is not practical to base rematerialization decisions, such as how\nmuch memory to free, solely on compile time. Consequently, BladeDISC++ investigates all possible\nrematerialization options, searches for the corresponding regeneration subgraphs, and makes final\nrematerialization decisions during runtime.\n\n[width=0.8]placeholder.png figureMemory optimizations based on symbolic shapes in BladeDISC++\n\n2.1 Symbolic shape graph analysis\n\nBladeDISC++ systematically analyzes and obtains shape information from the semantics of each\noperation within the dynamic shape computation graph. Following this, it establishes a global\nsymbolic shape graph. This graph is designed to show the mathematical relationships between shape\ndimensions through shape value extraction and input-output shape inference.\n\nfunc . func @main (% arg0 : tensor <? ,[ @S0 ] > , % arg1 : tensor <12 x11008 >) {\n\n%1 = broadcast (% arg1 ) -> tensor <4096 x ? , [ @C4096 , @S0 ] >\n%2 = d yna mi c_r eshape (% arg0 , % new_shape ) -> tensor <? x12 ,[ @S1 , @C12 ] >\n// The last consumer of %2\n%3 = dot (%2 , % arg1 ) -> tensor <? x11008 , [ @S1 , @C11008 ] >\n// The last consumer of %3\n%4 = reduce (%3) -> tensor <? , [ @S1 ] >\n%1084 = broadcast (%4) -> tensor <11008 x ? , [ @C11008 , @S1 ] >\n%1085 = broadcast (% arg0 ) -> tensor <1024 x ? , [ @C1024 , @S0 ] >\n\n}\n\nfunc . func @ s y m b o l i c _ s h a p e _ g r a p h () {\n\nSymbolicDim @S0\nSymbolicDim @S1\n@S0 = Mul @C12 , @S1\n\n}\n\nListing 1: Example of a dynamic shape graph and its symbolic shape graph\n\nAs shown in Listing 1, BladeDISC++ uses a SymbolicDim operation to represent a symbolic value.\nThis value is linked to a dimension of a tensor shape in the dynamic shape graph as an attribute, for\nexample, tensor<?x?, [@S0, @S1]>. The equation @S0 = 12 * @S1, for instance, is derived from a\nDynamicReshapeOp. It means the input and output tensors have an equivalent number of elements.\n\nThe comparison of tensor memory sizes is vital for both operation scheduling and rematerialization.\nBladeDISC++ uses SymbolicExpr to show mathematical expressions of symbolic dimensions. This\nallows for comparisons using a best-effort approach. For example, the element count of tensors\n\n2.2 Operation scheduling\n\nOperation scheduling aims to discover a memory-efficient sequence of operations from the initial\ncomputation graph. Existing scheduling algorithms typically traverse the graph and select an operation\nfrom a ReadySet, which includes operations whose predecessors have been scheduled, at each step.\nThe selection is mainly based on a comparison of the memory impact of the different operations,\nwhich is determined by calculating the difference between the memory freed and the memory allocated\nafter scheduling a particular operation. BladeDISC++ employs a similar strategy, emphasizing the\ncalculation and comparison of memory impact among different operations when exact tensor shapes\nare unavailable in dynamic shape graphs. In BladeDISC++, the memory impact of each operation\n\n2\n\n\fis calculated using symbolic shapes, resulting in a SymbolicExpr. These SymbolicExprs are then\ncompared using the symbolic shape graph.\n\nIn Listing 1, the DynamicReshapeOp and DotOp are present in the ReadySet at a particular step.\nDotOp, being the last consumer of\n\nWhen comparing memory impact SymbolicExprs is not possible, we use a standard approach:\nselecting the operation that results in shorter overall tensor lifespans based on the graph\u2019s structure.\n\n2.3 Rematerialization\n\nTraditional rematerialization methods use algorithms to decide which tensors to release early to reduce\nmemory pressure, and how to conduct the following regeneration via reloading or recomputation.\nThese methods also search for optimal recomputation subgraphs, evaluating their memory effects.\nTensor rematerialization can negatively impact end-to-end performance, so it should only be used\nwhen the graph\u2019s execution could exceed memory limits. However, dynamic shape graphs, with\nuncertain tensor shapes, may show varied peak memory use between different runs. Some runs may\nnot need rematerialization as they remain within memory limits, whereas others may. Therefore, it is\nimpractical to make decisions solely at compilation time. Also, the absence of exact shapes presents\nchallenges in evaluating the memory effects of potential recomputation subgraphs.\n\nTo address these challenges, BladeDISC++ uses a combined compilation-runtime approach based on\nsymbolic shapes to better manage shape variations during graph runs. At compile time, it explores all\npossible rematerialization candidates and identifies the regeneration subgraphs associated with them.\nThese subgraphs are incorporated into the original computation graph as separate execution paths.\nFinal choices regarding which tensor to release and the related regeneration method are made during\nruntime.\n\nDuring compilation, as shown in Figure 1, BladeDISC++ adds a Remat::EvictOp after each operation.\nThis checks if active tensors at that point need to be released to lower memory pressure. Regeneration\nsubgraphs, including reload and recomputation, are created for each potential tensor. While reloading\nonly involves a host-to-device instruction and has no impact on memory, finding recomputation\nsubgraphs needs thorough evaluation as poor choices can increase peak memory consumption.\nBladeDISC++ uses a standard search approach, but assesses the memory impact of subgraphs using\nSymbolicExpr.\n\nTaking the recomputation subgraph searching for\n\nFollowing this, BladeDISC++ inserts Remat::RegenerateOps, with corresponding regeneration sub-\ngraphs for both reload and recompute. These are inserted before each potential tensor\u2019s subsequent\nconsumers. The Remat::RegenerateOp checks if a tensor has been released, and which regeneration\nmethod is being used.\n\nDuring runtime, BladeDISC++ monitors memory usage throughout kernel execution. Whenever an\nEvictOp is triggered, BladeDISC++ checks the present memory usage. When the memory limit is\nabout to be exceeded, it performs a real-time analysis of all potential tensors offered by the EvictOp.\nFinal decisions about which tensor needs to be released, and the regeneration method, are determined\nby taking memory savings and end-to-end performance into account, following a similar approach as\ndetailed in. Subsequent Remat::RegenerateOps then check these choices to decide which regeneration\nsubgraphs to trigger.\n\n3 Evaluation\n\nFor our evaluation, we performed experiments on the supervised fine-tuning of Llama-2-1b, which is\na customized model from the official Llama-2-7b with only the number of hidden layers decreased\nfrom 32 to 4. This was done on an Alibaba Cloud instance, with 40GB of GPU RAM. We used\nthe CodeAlpaca-20K dataset, which contains text samples with lengths from about 100 to 3000\ncharacters. During each training cycle, a fixed amount of randomly selected samples are put into a\nbatch. This leads to variations in batch shapes between cycles.\n\nTo evaluate the effectiveness of BladeDISC++, we compared memory usage and end-to-end per-\nformance of dynamic shape training with BladeDISC++ against both dynamic and static shape\n\n3\n\n\ftraining with BladeDISC. For static shape training, following common methods, input sequences are\npadded to the closest power of 2 in length. This balances redundant computation and compilation\noverhead. Additionally, we set the largest bucket size to be equal to the longest sequence length in\nthe dataset. This was done to investigate whether comparable memory optimization can be achieved\nusing symbolic shapes instead of exact shapes.\n\nThe experimental results show that BladeDISC++ is able to reduce peak memory consumption\nduring dynamic shape training. BladeDISC++ also demonstrated memory consumption similar\nto static shape training, while improving end-to-end performance by eliminating the overheads of\nrecompilation and input bucketing.\n\nTable 1: Training throughput of Llama-2-1b on CodeAlpaca-20K(tokens/second)\n\nBatchsize\n\n14\n\n16\n\n18\n\nBladeDISC(dynamic shape training)\nBladeDISC(static shape training)\nBladeDISC++\n\n5662.34(38.20 GiB)\n5242.02(35.75 GiB)\n5749.20(35.76 GiB)\n\nOOM\n5429.38(37.71 GiB)\n6078.71(37.89 GiB)\n\nOOM\n5103.31(38.92 GiB)\n5738.79(39.18 GiB)",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "This study presents our practical experience in optimizing memory for dynamic shape graphs. We\nhave introduced operation scheduling and rematerialization strategies that use symbolic shapes,\nimplemented in BladeDISC++. Evaluations demonstrate that BladeDISC++ effectively decreases\nmemory usage for dynamic shape training and can match the memory optimization results of static\nshape training. To the best of our knowledge, this work is the first attempt in this area. We hope\nit will support the compiler community in handling dynamic shape tasks, and increase the use of\ndynamic shape compilers.\n\n4"
}
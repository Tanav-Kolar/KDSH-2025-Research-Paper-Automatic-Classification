{
  "title": "3D Food Modeling from Images: Advancements in\nPhysically-Aware Reconstruction",
  "abstract": "The growing focus on computer vision for applications in nutritional monitoring\nand dietary tracking has spurred the creation of sophisticated 3D reconstruction\nmethods for various food items. A lack of high-quality data, combined with\ninsufficient collaboration between academic research and industry applications,\nhas hindered advancements in this area. This paper outlines a comprehensive\nworkshop and challenge centered on physically informed 3D food reconstruction,\nleveraging recent progress in 3D reconstruction technologies. The central objective\nof this challenge is to create volume-accurate 3D models of food using 2D images,\nwith a visible checkerboard serving as a critical size reference. Participants were\nassigned the task of building 3D models for 20 distinct food items, each presenting\nvarying degrees of difficulty: easy, medium, and hard. The easy category offers\n200 images, the medium provides 30, and the hard level includes only a single\nimage to facilitate the reconstruction process. During the final evaluation stage, 16\nteams presented their results. The methodologies developed during this challenge\nhave yielded encouraging outcomes in 3D food reconstruction, demonstrating\nconsiderable potential for enhancing portion estimation in dietary evaluations and\nnutritional tracking.",
  "introduction": "The merging of computer vision with the culinary domain has unveiled new possibilities in dietary\noversight and nutritional evaluation. The 3D Food Modeling Workshop Challenge signifies a notable\nadvancement in this domain, responding to the escalating demand for precise and adaptable techniques\nfor estimating food portions and monitoring nutritional consumption. These technological solutions\nare essential for encouraging beneficial eating patterns and addressing health issues related to diet.\n\nThis initiative aims to close the divide between current methodologies and practical needs by\nconcentrating on the development of accurate 3D models of food items from multi-view and single-\nview image data. The challenge promotes the creation of novel methods capable of managing the\nintricacies of food forms, textures, and variations in lighting, all while adhering to the practical\nlimitations inherent in real-world dietary assessment situations.\n\nConventional methods for diet assessment, like 24-Hour Recall or Food Frequency Questionnaires\n(FFQ), frequently depend on manual data entry, which can be imprecise and difficult to manage.\nAdditionally, the lack of 3D data in 2D RGB food images poses significant hurdles for methods\nthat rely on regression to estimate food portions directly from images of eating occasions. By\nmaking progress in 3D reconstruction techniques for food, the aim is to provide tools for nutritional\nassessment that are more accurate and easier to use. This technology holds the potential to enhance\nthe way food experiences are shared and could significantly influence areas such as nutritional science\nand public health initiatives.\n\nParticipants were tasked with creating 3D models of 20 different food items from 2D images,\nsimulating a scenario where a smartphone equipped with a depth-sensing camera is employed for\ndietary recording and nutritional oversight. The challenge was divided into three levels of complexity:\n\n.\n\n\fThe easy level provided approximately 200 frames uniformly sampled from a video, the medium level\noffered about 30 images, and the hard level presented participants with just one monocular top-view\nimage. This arrangement was intended to assess the resilience and adaptability of the suggested\nsolutions under various real-world conditions. One of the main aspects of the challenge involves the\nuse of a visible checkerboard as a tangible benchmark, coupled with the inclusion of depth images\nfor each frame of the video, thereby ensuring the generated 3D models retain precise real-world\nmeasurements for estimating portion sizes.",
  "related_work": "Estimating food portions is a crucial part of image-based dietary assessment, with the objective of\ndetermining the volume, energy content, or macronutrient breakdown directly from images of meals.\nUnlike the extensively researched area of food recognition, determining food portions presents a\ndistinct difficulty because of the lack of 3D data and physical benchmarks, which are necessary\nfor precisely deducing the actual sizes of food portions. Specifically, accurately estimating portion\nsizes requires an understanding of the volume and density of the food, aspects that cannot be easily\ndetermined from a two-dimensional image, which highlights the need for advanced methodologies\nand technologies to address this issue. Current methods for estimating food portions are classified\ninto four primary categories.\n\nStereo-Based Approaches. These techniques depend on multiple frames to deduce the 3D con-\nfiguration of food items. For instance, some methods calculate food volume through multi-view\nstereo reconstruction based on epipolar geometry, while others use a two-view dense reconstruction\napproach. Another technique, Simultaneous Localization and Mapping (SLAM), is employed for\ncontinuous, real-time estimation of food volume. However, the need for multiple images limits the\npracticality of these methods in real-world situations.\n\nModel-Based Approach. This approach uses predefined shapes and templates to estimate the target\nvolume. Some methods assign specific templates to foods from a reference set and make adjustments\nbased on physical cues to gauge the size and position of the food. A similar approach that matches\ntemplates is employed to estimate food volume from just one image. However, these methods struggle\nto accommodate foods with shapes that do not conform to the established templates.\n\nDepth Camera-Based Approach. This method utilizes depth cameras to create maps that indicate\nthe distance from the camera to the food in the picture. The depth map is then used to create a voxel\nrepresentation of the image, which aids in estimating the food\u2019s volume. The primary drawbacks are\nthe need for high-quality depth maps and the additional processing steps required for depth sensors\nused by consumers.\n\nDeep Learning Approach. Techniques based on neural networks use the vast amount of image data\navailable to train sophisticated networks for estimating food portions. Some use regression networks\nto estimate the caloric value of food from a single image or from an \"Energy Distribution Map\" that\ncorrelates the input image with the energy distribution of the foods shown. Others use regression\nnetworks trained on images and depth maps to deduce the energy, mass, and macronutrients of the\nfood in the image. These methods require extensive data for training and are generally not transparent.\nTheir performance can significantly decline if the input test image deviates substantially from the\ntraining data.\n\nDespite the progress these methods have made in estimating food portions, they each have limitations\nthat restrict their broad use and precision in practical scenarios. Methods based on stereo are not\nsuitable for single-image inputs, those based on models have difficulty with a variety of food shapes,\napproaches using depth cameras necessitate specialized equipment, and deep learning methods are not\neasily interpretable and have difficulty with samples that are different from those they were trained on.\nTo tackle these issues, 3D reconstruction provides a viable solution by offering thorough spatial data,\naccommodating different food shapes, possibly functioning with just one image, presenting results\nthat are visually understandable, and facilitating a uniform method for estimating food portions. These\nbenefits were the driving force behind the organization of the 3D Food Reconstruction challenge,\nwhich seeks to surmount the current limitations and create techniques for food portion estimation\nthat are more accurate, user-friendly, and broadly applicable, thereby making a significant impact on\nnutritional assessment and dietary monitoring.\n\n2\n\n\f3 Datasets and Evaluation Pipeline\n\n3.1 Dataset Description\n\nThe dataset for the 3D Food Modeling Challenge includes 20 carefully chosen food items, each\nhaving been scanned with a 3D scanner and also captured on video. To ensure the reconstructed 3D\nmodels accurately represent size, each food item was captured alongside a checkerboard and pattern\nmat, which provide a physical reference for scaling. The challenge is segmented into three levels of\ndifficulty, based on the number of 2D images provided for reconstruction:\n\n\u2022 Easy: Roughly 200 images taken from video.\n\n\u2022 Medium: 30 images.\n\n\u2022 Hard: A single top-down image.\n\nTable 1: 3D Food Modeling Challenge Data Details\n\nObject Index\n\nFood Item\n\nDifficulty Level Number of Frames\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\nEasy\nEasy\nEasy\nEasy\nEasy\nEasy\nEasy\nEasy\n\nStrawberry\nCinnamon bun\nPork rib\nCorn\nFrench toast\nSandwich\nBurger\nCake\nBlueberry muffin Medium\nMedium\nBanana\nMedium\nSalmon\nMedium\nSteak\nMedium\nBurrito\nMedium\nHotdog\nChicken nugget\nMedium\nEverything bagel Hard\nHard\nCroissant\nHard\nShrimp\nHard\nWaffle\nHard\nPizza\n\n199\n200\n200\n200\n200\n200\n200\n200\n30\n30\n30\n30\n30\n30\n30\n1\n1\n1\n1\n1\n\n3.2 Evaluation Pipeline\n\nThe evaluation is divided into two stages, focusing on the accuracy of the reconstructed 3D models in\nterms of their form (3D structure) and portion size (volume).\n\n3.2.1 Phase-I: Volume Accuracy\n\nIn the first phase, the Mean Absolute Percentage Error (MAPE) is used as the metric to evaluate the\naccuracy of portion size. The calculation for MAPE is as follows:\n\nMAPE =\n\n1\nn\n\nn\n(cid:88)\n\ni=1\n\n(cid:12)\n(cid:12)\n(cid:12)\n(cid:12)\n\nAi \u2212 Fi\nAi\n\n(cid:12)\n(cid:12)\n(cid:12)\n(cid:12)\n\n\u00d7 100%\n\nwhere Ai represents the actual volume (in milliliters) of the i-th food item, as determined from the\nscanned 3D mesh, and Fi is the volume calculated from the reconstructed 3D mesh.\n\n3\n\n\f3.2.2 Phase-II: Shape Accuracy\n\nTeams that perform well in Phase-I are asked to provide full 3D mesh files for each food item. This\nphase includes multiple steps to guarantee both accuracy and fairness:\n\n1. Model Verification: Submitted models are checked against the final submissions from\nPhase-I to ensure they are consistent. Visual inspections are also conducted to prevent any\nviolations of the rules, such as submitting basic shapes (like spheres) rather than detailed\nreconstructions.\n\n2. Model Alignment: Participants are given the true 3D models and the script used for\ncalculating the final Chamfer distance. They must align their models with these true models\nand create a transformation matrix for each item submitted. The ultimate Chamfer distance\nscore is then calculated using the submitted models and their corresponding transformation\nmatrices.\n\n3. Chamfer Distance Calculation: The accuracy of the shape is assessed using the Chamfer\ndistance. For two sets of points, X and Y , the Chamfer distance is computed as follows:\n\ndCD(X, Y ) =\n\n1\n|X|\n\n(cid:88)\n\nx\u2208X\n\nmin\ny\u2208Y\n\n\u2225x \u2212 y\u22252\n\n2 +\n\n1\n|Y |\n\n(cid:88)\n\ny\u2208Y\n\nmin\nx\u2208X\n\n\u2225x \u2212 y\u22252\n2\n\nThis metric offers a thorough assessment of how closely the reconstructed 3D models match the\nactual models. The ultimate ranking is determined by merging the scores from both Phase-I (accuracy\nof volume) and Phase-II (accuracy of shape). It should be noted that after evaluating Phase-I, some\nissues with the data quality for object 12 (steak) and object 15 (chicken nugget) were found. To\nmaintain the competition\u2019s quality and fairness, these two items have been removed from the final\noverall evaluation.\n\n4 First Place Team - VolETA\n\n4.1 Methodology\n\nThe team\u2019s research employs multi-view reconstruction to generate detailed food meshes and accu-\nrately determine food volumes.\n\n4.1.1 Overview\n\nThe team\u2019s method integrates computer vision and deep learning to accurately estimate food volume\nfrom RGBD images and masks. Keyframe selection, supported by perceptual hashing and blur\ndetection, ensures data quality. The estimation of camera poses and object segmentation establishes\nthe basis for neural surface reconstruction, resulting in detailed meshes for volume estimation.\nRefinement processes, such as removing isolated parts and adjusting the scaling factor, improve\naccuracy.\n\n4.1.2 The Team\u2019s Proposal: VolETA\n\ni }n\n\ni }n\n\ni }n\n\nThe team starts their process by obtaining input data, specifically RGBD images and their correspond-\ning food object masks. These RGBD images are denoted as I D = {I D\ni=1, where n is the total\nnumber of frames, providing the necessary depth information alongside the RGB images. The food\nobject masks, denoted as {M F\n\ni=1, help identify the regions of interest within these images.\nj }k\ni=1, keyframes {I K\n\nNext, the team proceeds with keyframe selection. From the set {I D\nj=1 \u2286\n{I D\ni=1 are selected. The team implements a method to detect and remove duplicates and blurry\nimages to ensure high-quality frames. This involves applying the Gaussian blurring kernel followed\nby the fast Fourier transform method. Near-Image Similarity employs a perceptual hashing and\nhamming distance thresholding to detect similar images and keep overlapping. The duplicates and\nblurry images are excluded from the selection process to maintain data integrity and accuracy.\nUsing the selected keyframes {I K\nj=1, the team estimates the camera poses through a Structure\nfrom Motion approach (i.e., extracting features using a feature detection method, matching them\n\ni }n\n\nj }k\n\n4\n\n\fusing a matching algorithm, and refining them). The outputs are the set of camera poses {Cj}k\nwhich are crucial for spatial understanding of the scene.\n\nj=1,\n\nIn parallel, the team utilizes a segmentation algorithm for reference object segmentation. This\nalgorithm segments the reference object with a user-provided segmentation prompt (i.e., user click),\nproducing a reference object mask M R for each keyframe. This mask is a foundation for tracking the\nreference object across all frames. The team then applies a memory tracking method, which extends\nthe reference object mask M R to all frames, resulting in a comprehensive set of reference object\nmasks {M R\ni=1. This ensures consistency in reference object identification throughout the dataset.\ni=1, and\ni=1, integrates the various data sources into\n\nTo create RGBA images, the team combines the RGB images, reference object masks {M R\nfood object masks {M F\ni }n\na unified format suitable for further processing.\nThe team converts the RGBA images {I R\nand modeled data Dm. This transformation facilitates the accurate reconstruction of the scene.\n\ni=1. This step, denoted as {I R\n\ni=1 and camera poses {Cj}k\n\nj=1 into meaningful metadata\n\ni }n\n\ni }n\n\ni }n\n\ni }n\n\nThe modeled data Dm is then input into a neural surface reconstruction algorithm for mesh recon-\nstruction. This algorithm generates colorful meshes {Rf , Rr} for the reference and food objects,\nproviding detailed 3D representations of the scene components. The team applies the \"Remove\nIsolated Pieces\" technique to refine the reconstructed meshes. Given that the scenes contain only\none food item, the team sets the diameter threshold to 5% of the mesh size. This method deletes\nisolated connected components whose diameter is less than or equal to this 5% threshold, resulting in\na cleaned mesh {RCf , RCr}. This step ensures that only significant and relevant parts of the mesh\nare retained.\n\nThe team manually identifies an initial scaling factor S using the reference mesh via a mesh processing\ntool for scaling factor identification. This factor is then fine-tuned Sf using depth information and\nfood and reference masks, ensuring accurate scaling relative to real-world dimensions. Finally, the\nfine-tuned scaling factor Sf is applied to the cleaned food mesh RCf , producing the final scaled\nfood mesh RFf . This step culminates in an accurately scaled 3D representation of the food object,\nenabling precise volume estimation.\n\n4.1.3 Detecting the scaling factor\n\nGenerally, 3D reconstruction methods generate unitless meshes (i.e., no physical scale) by default.\nTo overcome this limitation, the team manually identifies the scaling factor by measuring the distance\nfor each block for the reference object mesh. Next, the team takes the average of all blocks lengths\nlavg, while the actual real-world length is constant lreal = 0.012 in meter. Furthermore, the team\napplies the scaling factor S = lreal/lavg on the clean food mesh RCf , producing the final scaled\nfood mesh RFf in meter.\n\nThe team leverages depth information alongside food and reference object masks to validate the\nscaling factors. The team\u2019s method for assessing food size entails utilizing overhead RGB images\nfor each scene. Initially, the team determines the pixel-per-unit (PPU) ratio (in meters) using the\nreference object. Subsequently, the team extracts the food width (fw) and length (fl) employing a\nfood object mask. To ascertain the food height (fh), the team follows a two-step process. Firstly, the\nteam conducts binary image segmentation using the overhead depth and reference images, yielding a\nsegmented depth image for the reference object. The team then calculates the average depth utilizing\nthe segmented reference object depth (dr). Similarly, employing binary image segmentation with an\noverhead food object mask and depth image, the team computes the average depth for the segmented\nfood depth image (df). Finally, the estimated food height fh is computed as the absolute difference\nbetween dr and df. Furthermore, to assess the accuracy of the scaling factor S, the team computes\nthe food bounding box volume ((fw \u00d7 fl \u00d7 fh) \u00d7 PPU). The team evaluates if the scaling factor S\ngenerates a food volume close to this potential volume, resulting in Sf ine.\n\nFor one-shot 3D reconstruction, the team leverages a single view reconstruction method for recon-\nstructing a 3D from a single RGBA view input after applying binary image segmentation on both\nfood RGB and mask. Next, the team removes isolated pieces from the generated mesh. After that, the\nteam reuses the scaling factor S, which is closer to the potential volume of the clean mesh.\n\n5\n\n\f4.2 Experimental Results\n\n4.2.1 Implementation settings\n\nThe experiments were conducted using two GPUs: a GeForce GTX 1080 Ti with 12GB of memory\nand an RTX 3060 with 6GB of memory. For near-image similarity detection, the Hamming distance\nwas set to 12. To identify blurry images, even numbers within the range of [0...30] were used as the\nGaussian kernel radius. In the process of removing isolated pieces, a diameter threshold of 5% was\napplied. Neural surface reconstruction involved 15,000 iterations, with a mesh resolution of 512x512.\nThe unit cube parameters were set with an \"aabb scale\" of 1, \"scale\" at 0.15, and \"offset\" at [0.5, 0.5,\n0.5] for each food scene.\n\n4.2.2 VolETA Results\n\nThe team extensively validated their approach on the challenge dataset and compared their results with\nground truth meshes using MAPE and Chamfer distance metrics. More Briefly, the team leverages\ntheir approach for each food scene separately. A one-shot food volume estimation approach is applied\nif the number of keyframes k equals 1. Otherwise, a few-shot food volume estimation is applied. The\nteam\u2019s keyframe selection process chooses 34.8% of total frames for the rest of the pipeline, where it\nshows the minimum frames with the highest information.\n\nTable 2: List of Extracted Information Using RGBD and Masks\n\nLevel\n\nId\n\nLabel\n\nSf\n\nPPU\n\nRw \u00d7 Rl\n\nfw \u00d7 fl \u00d7 fh\n\nVolume (cm3)\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n9\n10\n11\n13\n14\n\n16\n17\n18\n19\n20\n\nstrawberry\ncinnamon bun\npork rib\ncorn\nfrench toast\nsandwich\nburger\ncake\n\nblueberry muffin\nbanana\nsalmon\nburrito\nfrankfurt sandwich\n\neverything bagel\ncroissant\nshrimp\nwaffle\npizza\n\n0.08955\n0.10435\n0.10435\n0.08824\n0.10345\n0.12766\n0.10435\n0.12766\n\n0.08759\n0.08759\n0.10435\n0.10345\n0.10345\n\n0.08759\n0.12766\n0.08759\n0.01034\n0.01034\n\n0.01786\n0.02347\n0.02381\n0.01897\n0.02202\n0.02426\n0.02435\n0.02143\n\n0.01801\n0.01705\n0.02390\n0.02372\n0.02115\n\n0.01747\n0.01751\n0.02021\n0.01902\n0.01913\n\n320 \u00d7 360\n236 \u00d7 274\n246 \u00d7 270\n291 \u00d7 339\n266 \u00d7 292\n230 \u00d7 265\n208 \u00d7 264\n256 \u00d7 300\n\n291 \u00d7 357\n315 \u00d7 377\n242 \u00d7 269\n244 \u00d7 271\n266 \u00d7 304\n\n306 \u00d7 368\n319 \u00d7 367\n249 \u00d7 318\n294 \u00d7 338\n292 \u00d7 336\n\n238 \u00d7 257 \u00d7 2.353\n363 \u00d7 419 \u00d7 2.353\n435 \u00d7 778 \u00d7 1.176\n262 \u00d7 976 \u00d7 2.353\n530 \u00d7 581 \u00d7 2.53\n294 \u00d7 431 \u00d7 2.353\n378 \u00d7 400 \u00d7 2.353\n298 \u00d7 310 \u00d7 4.706\n\n441 \u00d7 443 \u00d7 2.353\n446 \u00d7 857 \u00d7 1.176\n201 \u00d7 303 \u00d7 1.176\n251 \u00d7 917 \u00d7 2.353\n400 \u00d7 1022 \u00d7 2.353\n\n458 \u00d7 484 \u00d7 1.176\n395 \u00d7 695 \u00d7 2.176\n186 \u00d7 195 \u00d7 0.987\n465 \u00d7 537 \u00d7 0.8\n442 \u00d7 651 \u00d7 1.176\n\nEasy\n\nMedium\n\nHard\n\n45.91\n197.07\n225.79\n216.45\n377.66\n175.52\n211.03\n199.69\n\n149.12\n130.80\n40.94\n304.87\n430.29\n\n79.61\n183.39\n14.64\n72.29\n123.97\n\nAfter generating the scaled meshes, the team calculates the volumes and Chamfer distance with and\nwithout transformation metrics. The team registered their meshes and ground truth meshes to obtain\nthe transformation metrics using ICP.\n\n5 Second Place Team - ININ-VIAUN\n\n5.1 Methodology\n\nThis section provides a detailed explanation of the proposed network, demonstrating how to progress\nfrom the original images to the final mesh models step by step.\n\n5.1.1 Scale factor estimation\n\nThe pipeline for coordinate-level scale factor estimation is described as follows. The team follows\na corner projection matching method. Specifically, using a dense reconstruction model, the team\n\n6\n\n\fTable 3: Quantitative Comparison of Team\u2019s Approach with Ground Truth\n\nL\n\nId\n\nTeam\u2019s Vol. GT Vol. Ch. w/ t.m Ch. w/o t.m\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n9\n10\n11\n13\n14\n\n16\n17\n18\n19\n20\n\nE\n\nM\n\nH\n\n40.06\n216.9\n278.86\n279.02\n395.76\n205.17\n372.93\n186.62\n\n224.08\n153.76\n80.4\n363.99\n535.44\n\n163.13\n224.08\n25.4\n110.05\n130.96\n\n38.53\n280.36\n249.67\n295.13\n392.58\n218.44\n368.77\n173.13\n\n232.74\n163.09\n85.18\n308.28\n589.83\n\n262.15\n181.36\n20.58\n108.35\n119.83\n\n1.63\n7.12\n13.69\n2.03\n13.67\n6.68\n4.70\n2.98\n\n3.91\n2.67\n3.37\n5.18\n4.31\n\n18.06\n9.44\n4.28\n11.34\n15.59\n\n85.40\n111.47\n172.88\n61.30\n102.14\n150.78\n66.91\n152.34\n\n160.07\n138.45\n151.14\n147.53\n89.66\n\n28.33\n28.94\n12.84\n23.98\n31.05\n\nTable 4: Overall Method Performance\nMAPE Ch. sum w/tm mean Ch. w/o tm mean\n\n10.973\n\n0.130\n\n0.007\n\n1.715\n\n0.095\n\nobtains the pose of each image as well as dense point cloud information. For any image imgk\nand its extrinsic parameters [R|t]k, the team first performs a threshold-based corner detection with\nthe threshold set to 240. This allows them to obtain the pixel coordinates of all detected corners.\nSubsequently, using the intrinsic parameters k and the extrinsic parameters [R|t]k, the point cloud is\nprojected onto the image plane. Based on the pixel coordinates of the corners, the team can identify\nthe closest point coordinates P k\ni for each corner, where i represents the index of the corner. Thus,\nthey can calculate the distance between any two corners as follows:\n\nDij = (P k\n\ni \u2212 P k\n\nj )2 \u2200i \u0338= j\n\nTo determine the final computed length of each checkerboard square in image k, the team takes the\nminimum value of each row of the matrix Dk (excluding the diagonal) to form the vector dk. The\nmedian of this vector is then used. The final scale calculation formula is given by the following\nequation, where 0.012 represents the known length of each square (1.2 cm):\n\nscale =\n\n0.012\nmed(dk)\n\n5.1.2 3D Reconstruction\n\nConsidering the differences in input viewpoints, the team utilizes two pipelines to process the first\nfifteen objects and the last five single view objects.\n\nFor the first fifteen objects, the team uses a Structure from Motion algorithm to estimate the poses\nand segment the food using the provided segment masks in the dataset. Then, they apply advanced\nmulti-view 3D reconstruction methods to reconstruct the segmented food. In practice, the team\nemploys three different reconstruction methods. They select the best reconstruction results from these\nmethods and extract the mesh from the reconstructed model. Next, they scale the extracted mesh\nusing the estimated scale factor. Finally, they apply some optimization techniques to obtain a refined\nmesh.\n\n7\n\n\fFor the last five single-view objects, the team experiments with several single-view reconstruction\nmethods. They choose a specific method to obtain a 3D food model consistent with the distribution\nof the input image. In practice, they use the intrinsic camera parameters from the fifteenth object\nand employ an optimization method based on reprojection error to refine the extrinsic parameters\nof the single camera. However, due to the limitations of single-view reconstruction, the team needs\nto incorporate depth information from the dataset and the checkerboard in the monocular image to\ndetermine the size of the extracted mesh. Finally, they apply optimization techniques to obtain a\nrefined mesh.\n\n5.1.3 Mesh refinement\n\nIn the 3D Reconstruction phase, the team observes that the model\u2019s results often suffer from low\nquality due to the presence of holes on the object surface and substantial noise.\n\nTo address the holes, the team employs an optimization method based on computational geometry.\nFor surface noise, they utilize Laplacian Smoothing for mesh smoothing operations. The Laplacian\nSmoothing method works by adjusting the position of each vertex to the average of its neighboring\nvertices:\n\n\uf8eb\n\ni = V old\nV new\n\ni + \u03bb\n\n\uf8ed\n\n\uf8f6\n\nj \u2212 V old\nV old\n\ni\n\n\uf8f8\n\n1\n|N (i)|\n\n(cid:88)\n\nj\u2208N (i)\n\nIn their implementation, the team sets the smoothing factor \u03bb to 0.2 and performs 10 iterations.\n\n5.2 Experimental Results\n\n5.2.1 Estimated scale factor\n\nThe scale factors estimated using the method described earlier are shown in Table 5. Each image and\nthe corresponding reconstructed 3D model yield a scale factor, and the table presents the average\nscale factor for each object.\n\nTable 5: Estimated Scale Factors\n\nObject Index\n\nFood Item\n\nScale Factor\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n13\n14\n\nStrawberry\nCinnamon bun\nPork rib\nCorn\nFrench toast\nSandwich\nBurger\nCake\nBlueberry muffin\nBanana\nSalmon\nBurrito\nHotdog\n\n0.060058\n0.081829\n0.073861\n0.083594\n0.078632\n0.088368\n0.103124\n0.068496\n0.059292\n0.058236\n0.083821\n0.069663\n0.073766\n\n5.2.2 Reconstructed meshes\n\nThe refined meshes obtained using the methods described earlier are shown in Figure 12. The\npredicted model vol- umes, ground truth model volumes, and the percentage errors between them are\nshown in Table 6. The unit is cubic millimeters.\n\n8\n\n\fTable 6: Metric of Volume\n\nObject Index\n\nPredicted Volume Ground Truth Error Percentage\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n13\n14\n16\n17\n18\n19\n20\n\n44.51\n321.26\n336.11\n347.54\n389.28\n197.82\n412.52\n181.21\n233.79\n160.06\n86.0\n334.7\n517.75\n176.24\n180.68\n13.58\n117.72\n117.43\n\n38.53\n280.36\n249.67\n295.13\n392.58\n218.44\n368.77\n173.13\n232.74\n163.09\n85.18\n308.28\n589.83\n262.15\n181.36\n20.58\n108.35\n119.83\n\n15.52\n14.59\n34.62\n17.76\n0.84\n9.44\n11.86\n4.67\n0.45\n1.86\n0.96\n8.57\n12.22\n32.77\n0.37\n34.01\n8.64\n20.03\n\n5.2.3 Alignment\n\nThe team designs a multi-stage alignment method for evaluating reconstruction quality. Figure 13\nillustrates the alignment process for Object 14. First, the team calculates the central points of both the\npredicted model and the ground truth model, and moves the predicted model to align the central point\nof the ground truth model. Next, they perform ICP registration for further alignment, significantly\nreducing the Chamfer distance. Finally, they use gradient descent for additional fine-tuning, and\nobtain the final transformation matrix. The total Chamfer distance between all 18 predicted models\nand the ground truths is 0.069441169.\n\n6 Best 3D Mesh Reconstruction Team - FoodRiddle\n\n6.1 Methodology\n\nTo achieve high-quality food mesh reconstruction, the team designed two pipeline processes. For\nsimple and medium cases, they employed a structure-from-motion approach to determine the pose of\neach image, followed by mesh reconstruction. Subsequently, a series of post-processing steps were\nimplemented to recalibrate scale and enhance mesh quality. For cases with only a single image, the\nteam utilized image generation methods to aid in model generation.\n\n6.1.1 Multi-View Reconstruction\n\nFor Structure from Motion (SfM), the team extended the state-of-the-art method by incorporating\nmethodologies. This significantly mitigated the issue of sparse keypoints in weakly textured scenes.\nFor mesh reconstruction, the team\u2019s method is based on a differentiable renderer and incorporates\nregularization terms for depth distortion and normal consistency. The Truncated Signed Distance\nFunction (TSDF) results are used to generate a dense point cloud. In the post-processing stage, the\nteam applied filtering and outlier removal techniques, identified the contour of the supporting surface,\nand projected the lower mesh vertices onto the supporting surface. They used the reconstructed\ncheckerboard to rectify the scale of the model and used Poisson reconstruction to generate a watertight,\ncomplete mesh of the subject.\n\n6.1.2 Single-View Reconstruction\n\nFor 3D reconstruction from a single image, the team employed state-of-the-art methods to generate\nan initial prior mesh. This prior mesh was then jointly corrected with depth structure information.\n\n9\n\n\fTo adjust the scale, the team estimated the object\u2019s length using the checkerboard as a reference,\nassuming the object and the checkerboard are on the same plane. They then projected the 3D object\nback onto the original 2D image to recover a more accurate scale of the object.\n\n6.2 Experimental Results\n\nThrough a process of nonlinear optimization, the team sought to identify a transformation that\nminimizes the Chamfer distance between their mesh and the ground truth mesh. This optimization\naimed to align the two meshes as closely as possible in three-dimensional space. Upon completion of\nthis process, the average Chamfer distance across the final reconstructions of the 20 objects amounted\nto 0.0032175 meters. As shown in Table 7, Team FoodRiddle achieved the best scores for both\nmulti-view and single-view reconstructions, outperforming other teams in the competition.\n\nTable 7: Total Errors for Different Teams on Multi-view and Single-view Data\n\nTeam\n\nMulti-view (1-14)\n\nSingle-view (16-20)\n\nFoodRiddle\nININ-VIAUN\nVolETA\n\n0.036362\n0.041552\n0.071921\n\n0.019232\n0.027889\n0.058726",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "In this report, we provide a summary and analysis of the methodologies and findings from the\n3D Food Reconstruction challenge. The primary goal of this challenge was to push the envelope\nin 3D reconstruction technologies, with an emphasis on the unique challenges presented by food\nitems, such as their varied textures, reflective surfaces, and complex geometries. The competition\nfeatured 20 diverse food items, captured under various conditions and with varying numbers of input\nimages, specifically designed to challenge participants in developing robust reconstruction models.\nThe evaluation was based on a two-phase process, assessing both portion size accuracy through\nMean Absolute Percentage Error (MAPE) and shape accuracy using the Chamfer distance metric.\nOf all participating teams, three made it to the final submission, showcasing a range of innovative\nsolutions. Team VolETA won first place with the overall best performance on both Phase-I and\nPhase-II, followed by team ININ-VIAUN who won second place. In addition, FoodRiddle team\ndemonstrated superior performance in Phase-II, indicating a competitive and high-caliber field of\nentries for 3D mesh reconstruction. The challenge has successfully pushed the boundaries of 3D food\nreconstruction, demonstrating the potential for accurate volume estimation and shape reconstruction\nin nutritional analysis and food presentation applications. The innovative approaches developed by\nthe participating teams provide a solid foundation for future research in this field, potentially leading\nto more accurate and user-friendly methods for dietary assessment and monitoring.\n\n10"
}
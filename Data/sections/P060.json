{
  "title": "Background Modeling Using Adaptive Pixelwise\nKernel Variances in a Hybrid Feature Space",
  "abstract": "Recent work on background subtraction has shown developments on two major\nfronts. In one, there has been increasing sophistication of probabilistic models,\nfrom mixtures of Gaussians at each pixel, to kernel density estimates at each\npixel, and more recently to joint domain-range density estimates that incorporate\nspatial information. Another line of work has shown the benefits of increasingly\ncomplex feature representations, including the use of texture information, local\nbinary patterns, and recently scale-invariant local ternary patterns. In this work, we\nuse joint domain-range based estimates for background and foreground scores and\nshow that dynamically choosing kernel variances in our kernel estimates at each\nindividual pixel can significantly improve results. We give a heuristic method for\nselectively applying the adaptive kernel calculations which is nearly as accurate as\nthe full procedure but runs much faster. We combine these modeling improvements\nwith recently developed complex features and show significant improvements on a\nstandard backgrounding benchmark.",
  "introduction": "Background modeling is often an important step in detecting moving objects in video sequences. A\ncommon approach to background modeling is to define and learn a background distribution over\nfeature values at each pixel location and then classify each image pixel as belonging to the background\nprocess or not. The distributions at each pixel may be modeled in a parametric manner using a mixture\nof Gaussians or using non-parametric kernel density estimation. More recently, models that allow\na pixel\u2019s spatial neighbors to influence its distribution have been developed by joint domain-range\ndensity estimation. These models that allow spatial influence from neighboring pixels have been\nshown to perform better than earlier neighbor-independent models.\n\nAlso, the use of an explicit foreground model along with a background model can be useful. In a\nmanner similar to theirs, we use a kernel estimate to obtain the background and foreground scores\nat each pixel location using data samples from a spatial neighborhood around that location from\nprevious frames. The background score is computed as a kernel estimate depending on the distance\nin the joint domain-range space between the estimation point and the samples in the background\nmodel. A similar estimate is obtained for the foreground score. Each pixel is then assigned a (soft)\nlabel based on the ratio of the background and foreground scores.\n\nThe variance used in the estimation kernel reflects the spatial and appearance uncertainties in the\nscene. On applying our method to a data set with wide variations across the videos, we found that\nchoosing suitable kernel variances during the estimation process is very important. With various\nexperiments, we establish that the best kernel variance could vary for different videos and more\nimportantly, even within a single video, different regions in the image should be treated with different\nvariance values. For example, in a scene with a steady tree trunk and leaves that are waving in the\nwind, the trunk region can be explained with a small amount of spatial variance. The leaf regions\nmay be better explained by a process with a large variance. Interestingly, when there is no wind, the\nleaf regions may also be explained with a low variance. The optimal variance hence changes for\n\n.\n\n\feach region in the video and also across time. This phenomenon is captured reasonably in MoG by\nuse of different parameters for each pixel which adapt dynamically to the scene statistics, but the\npixel-wise model does not allow a pixel\u2019s neighbors to affect its distribution. address the phenomenon\nby updating the model with data samples from the most recent frame. We show that using location-\nspecific variances in addition to updating the model greatly improves background modeling. Our\napproach with pixel-wise variances, which we call the variable kernel score (VKS) method results in\nsignificant improvement over uniform variance models and state of the art backgrounding systems.\n\nThe idea of using a pixel-wise variance for background modeling is not new. Although use a uniform\nvariance, they discuss the use of variances that change as a function of the data samples or as a\nfunction of the point at which the estimation is made. Variance selection for KDE is a well studied\nproblem with common solutions including mean integrated square error (MISE), asymptotic MISE\n(AMISE), and the leave-one-out-estimator based solutions. In the background subtraction context,\nthere has been work on using a different covariance at each pixel. While require that the uncertainties\nin the feature values can be calculated in closed form, learn the covariances for each pixel from a\ntraining set of frames and keep the learned covariances fixed for the entire classification phase. We\nuse a maximum-likelihood approach to select the best variance at each pixel location. For every\nframe of the video, at each pixel location, the best variance is picked from a set of variance values\nby maximizing the likelihood of the pixel\u2019s observation under different variances. This makes our\nmethod a balloon estimator. By explicitly selecting the best variance from a range of variance values,\nwe do not require the covariances to be calculable in closed-form and also allow for more flexibility\nat the classification stage.\n\nSelecting the best of many kernel variances for each pixel means increased computation. One possible\ntrade-off between accuracy and speed can be achieved by a caching scheme where the best kernel\nvariances from the previous frame are used to calculate the scores for the current frame pixels. If the\nresulting classification is overwhelmingly in favor of either label, there is no need to perform a search\nfor the best kernel variance for that pixel. The expensive variance selection procedure can be applied\nonly to pixels where there is some contention between the two labels. We present a heuristic that\nachieves significant reduction in computation compared to our full implementation while maintaining\nthe benefits of adaptive variance.\n\nDevelopment and improvement of the probabilistic models is one of the two main themes in back-\nground modeling research in recent years. The other theme is the development of complex features\nlike local binary and ternary patterns that are more robust than color features for the task of back-\nground modeling. Scale-invariant local ternary patterns (SILTP) are recently developed features that\nhave been shown to be very robust to lighting changes and shadows in the scene. By combining color\nfeatures with SILTP features in our adaptive variance kernel model, we bring together the best ideas\nfrom both themes in the field and achieve state of the art results on a benchmark data set.\n\nThe main contributions of this paper are:\n\n1. A practical scheme for pixel-wise variance selection for background modeling.\n\n2. A heuristic for selectively updating variances to improve speed further.\n\n3. Incorporation of complex SILTP features into the joint domain-range kernel framework to\n\nachieve state of the art results.\n\nThe paper is organized as follows. Section 2 discusses our background and foreground models.\nDynamic adaptation of kernel variances is discussed in Section 3. Results and comparisons are in\nSection 4. An efficient algorithm is discussed in Section 5. We end with a discussion in Section 6.\n\n2 Background and foreground models\n\nIn a video captured by a static camera, the pixel values are influenced by the background phenomenon,\nand new or existing foreground objects. We refer to any phenomenon that can affect image pixel\nvalues as a process. Like , we model the background and foreground processes using data samples\nfrom previous frames. The scores for the background and foreground processes at each pixel location\nare calculated using contributions from the data samples in each model. One major difference between\nand our model is that we allow \u201csoft labeling\u201d, i.e. the data samples contribute probabilistically to the\nbackground score depending on the samples\u2019 probability of belonging to the background.\n\n2\n\n\f(1)\n\n(2)\n\n(3)\n\nLet a pixel sample a = [ax, ay, ar, ag, ab], where (ax, ay) are the location of the pixel and (ar, ag, ab)\nare the red, green, and blue values of the pixel. In each frame of the video, we compute background\nand foreground scores using pixel samples from the previous frames. The background model consists\nof the samples B = bi : i [1 : nB] and foreground samples are F = fi : i [1 : nF ], with nB and nF being\nthe number of background and foreground samples respectively, and bi and fi being pixel samples\nobtained from previous frames in the video. Under a KDE model, the likelihood of the sample under\nthe background model is\n\nP (a|bg; \u03c3) =\n\n1\nnB\n\nnB(cid:88)\n\ni=1\n\nG(a \u2212 bi; \u03c3B)\n\nwhere G(x; ) is a multivariate Gaussian with zero mean and covariance B.\n\nG(x; \u03c3) = (2\u03c0)\u2212 D\n\n2 |\u03c3|\u2212 1\n\n2 exp(\u2212\n\n1\n2\n\nxT \u03c3\u22121x),\n\nwhere D is the dimensionality of the vector x.\n\nIn our model, we approximate the background score at sample a as\n\nSB(a; \u03c3d\n\nB, \u03c3rgb\n\nB ) =\n\n1\nNB\n\nNB(cid:88)\n\ni=1\n\nG(argb \u2212 birgb; \u03c3rgb\n\nB ) \u00d7 G(axy \u2212 bixy ; \u03c3d\n\nB) \u00d7 P (bg|bi)\n\nNB is the number of frames from which the background samples have been collected, B d and B\nrgb are two and three dimensional background covariance matrices in spatial and color dimensions\nrespectively. A large spatial covariance allows neighboring pixels to contribute more to the score at a\ngiven pixel location. Color covariance allows for some color appearance changes at a given pixel\nlocation. Use of NB in the denominator compensates for the different lengths of the background and\nforeground models.\n\nThe above equation basically sums the contribution from each background sample based on its\ndistance in color space, weighted by its distance in spatial dimensions and the probability of the\nsample belonging to the background.\n\nThe use of P (bg|bi) in Equation 3 and normalization by the number of frames as opposed to the\nnumber of samples means that the score does not sum to 1 over all possible values of a. Thus, the\nscore, although similar to the likelihood in Equation 1, is not a probability distribution.\n\nA similar equation holds for the foreground score:\n\nSF (a; \u03c3d\n\nF , \u03c3rgb\n\nF ) =\n\n1\nNF\n\nNF(cid:88)\n\ni=1\n\nG(argb \u2212 firgb; \u03c3rgb\n\nF ) \u00d7 G(axy \u2212 fixy ; \u03c3d\n\nF ) \u00d7 P (f g|fi)\n\n(4)\n\nNF is the number of frames from which the foreground samples have been collected, F d and F rgb\nare the covariances associated with the foreground process.\n\nHowever, for the foreground process, to account for emergence of new colors in the scene, we mix\nin a constant contribution independent of the estimation point\u2019s and data samples\u2019 color values. We\nassume that each data sample in a pixel\u2019s spatial neighborhood contributes a constant value u to the\nforeground score. The constant contribution UF (a) is given by\n\nUF (a; \u03c3d\n\nF ) =\n\nNF(cid:88)\n\ni=1\n\nu \u00d7 G(axy \u2212 fixy ; \u03c3d\nF )\n\nWe get a modified foreground score by including the constant contribution:\n\n\u02c6SF (a; \u03c3d\n\nF , \u03c3rgb\n\nF ) = \u03b1F \u00d7 UF (a; \u03c3d\n\nF ) + (1 \u2212 \u03b1F ) \u00d7 SF (a; \u03c3d\n\nF , \u03c3rgb\n\nF ).\n\n(5)\n\n(6)\n\nF is a parameter that represents the amount of mixing between the constant contribution and the color\ndependent foreground score. u is set to 106 and is set to 0.5 for our experiments.\n\nTo classify a particular sample as background or foreground, we can use a Bayes-like formula:\n\nP (bg|a) =\n\nSB(a; \u03c3d\nB, \u03c3rgb\n\nB, \u03c3rgb\nB )\nB ) + \u02c6SF (a; \u03c3d\n\nF , \u03c3rgb\nF )\n\nSB(a; \u03c3d\n\n(7)\n\n3\n\n\fP (f g|a) = 1 \u2212 P (bg|a).\n\n(8)\n\nAdding the constant factor U to the foreground score (and hence to the denominator of the Bayes-like\nequation) has the interesting property that when either one of the foreground or background scores is\nsignificantly larger than U , U has little effect on the classification. However, if both the background\nand foreground scores are less than U , then Equation 7 will return a low value as P (bg|a). Hence,\nan observation that has very low background and foreground scores will be classified as foreground.\nThis is desirable because if a pixel observation is not well explained by either model, it is natural to\nassume that the pixel is a result of a new object in the scene and is hence foreground. In terms of\nlikelihoods, adding the constant factor to the foreground likelihood is akin to mixing it with a uniform\ndistribution.\n\n2.1 Model initialization and update\n\nTo initialize the models, it is assumed that the first few frames (typically 50) are all background pixels.\nThe background model is populated using pixel samples from these frames. In order to improve\nefficiency, we sample 5 frames at equal time intervals from these 50 frames. The foreground model is\ninitialized to have no samples. The modified foreground score (Equation 6) enables colors that are\nnot well explained by the background model to be classified as foreground, thus bootstrapping the\nforeground model. Once the pixel at location (ax, ay) from a new frame is classified using Equation\n7, the background and foreground models at the location (ax, ay) can then be updated with the new\nsample a. Background and foreground samples at location (ax, ay) from the oldest frame in the\nmodels are replaced by a. Samples from the previous 5 frames are maintained in memory as the\nforeground model samples. The label probabilities of the background/foreground from Equation 7\nare also saved along with the sample values for subsequent use in the Equations 3 and 4.\n\nOne consequence of the update procedure described above is that when a large foreground object\noccludes a background pixel at (ax, ay) for more than 50 frames, all the background samples in the\nspatial neighborhood of (ax, ay) are replaced by these foreground samples that have very low P (bg|bi)\nvalues. This causes the pixel at (ax, ay) to be misclassified as foreground even when the occluding\nforeground object has moved away (because the background score will be extremely low due to the\ninfluence of P (bg|bi) in Equation 3). To avoid this problem, we replace the background sample from\nlocation (ax, ay) in the oldest frame in the background model with the new sample a from the current\nframe only if P (bg|a) estimated from Equation 7 is greater than 0.5.\n\nIn our chosen evaluation data set, there are several videos with moving objects in the first 50 frames.\nThe assumption that all these pixels are background is not severely limiting even in these videos.\nThe model update procedure allows us to recover from any errors that are caused by the presence of\nforeground objects in the initialization frames.\n\n2.2 Using MRF to clean the classification\n\nSimilar to , we use a Markov random field (MRF) defined over the posterior label probabilities of\nthe 4-neighbors of each pixel and perform the min-cut procedure to post-process the labels. The\ninteraction factor between the nodes was set to 1 for all our experiments.\n\n3 Pixel-wise adaptive kernel variance selection\n\nBackground and foreground kernels. use the same kernel parameters for background and foreground\nmodels. Given the different nature of the two processes, it is reasonable to use different kernel\nparameters. For instance, foreground objects typically move between 5 and 10 pixels per frame in the\ndata set, whereas background pixels are either stationary or move very little. Hence, it is useful to\nhave a larger spatial variance for the foreground model than for the background model.\n\nOptimal kernel variance for all videos. In the results section, we show that for a data set with\nlarge variations like , a single value for kernel variance for all videos is not sufficient to capture the\nvariability in all the videos.\n\nVariable kernel variance for a single video. As explained in the introduction, different parts of the\nscene may have different statistics and hence need different kernel variance values. For example, in\nFigure 1a to 1d, having a high spatial dimension kernel variance helps in accurate classification of\n\n4\n\n\fthe water surface pixels, but doing so causes some pixels on the person\u2019s leg to become part of the\nbackground. Ideally, we would have different kernel variances for the water surface pixels and the rest\nof the pixels. Similarly in the second video (Figure 1e to 1h), having a high kernel variance allows\naccurate classification of some of the fountain pixels as background at the cost of misclassifying\nmany foreground pixels. The figure also shows that while the medium kernel variance may be the\nbest choice for the first video, the low kernel variance may be best for the second video.\n\nOptimal kernel variance for classification. Having different variances for the background and\nforeground models reflects the differences between the expected uncertainty in the two processes.\nHowever, having different variances for the two processes could cause erroneous classification of\npixels. Figure 2 shows a 1-dimensional example where using a very wide kernel (high variance)\nor very narrow kernel for the background process causes misclassification. Assuming that the red\npoint (square) is a background sample and the blue point (triangle) is a foreground sample, having a\nvery low variance kernel (dashed red line) or a very high variance (solid red line) for the background\nprocess makes the background likelihood of the center point \u2018x\u2019 lower than the foreground likelihood.\nThus, it is important to pick the optimal kernel variance for each process during classification.\n\nIn order to address all four issues discussed above, we propose the use of location-specific variances.\nFor each location in the image, a range of kernel variances is tried and the variance which results in\nthe highest score is chosen for the background and the foreground models separately.\n\nThe background score with location-dependent variances is\n\nSB(a; \u03c3Bd,x,y , \u03c3Brgb,x,y ) =\n\n1\nNB\n\ni=1\n\nNB(cid:88)\n\nG(argb \u2212 birgb; \u03c3Brgb,x,y ) \u00d7 G(axy \u2212 bixy ; \u03c3Bd,x,y ) \u00d7 P (bg|bi)\n\n(9)\nwhere B d,x,y and B rgb,x,y represent the location-specific spatial and color dimension variances at\nlocation (x, y).\n\nFor each pixel location (ax, ay), the optimal variance for the background process is selected by\nmaximizing the score of the background label at sample a under different variance values:\n} = argmax\u03c3Bd,ax,ay ,\u03c3Brgb,ax,ay\nSB(a; \u03c3Bd,ax,ay , \u03c3Brgb,ax,ay ).\n\nBrgb,ax,ay\n\nBd,ax,ay\n\n{\u03c3\u2217\n\n, \u03c3\u2217\n\n(10)\n\nHere, B RB d and B rgb. RB d and RB rgb,ax,ay d,ax,ay rgb represent the set of spatial and color\ndimension variances from which to choose the optimal variance.\n\nA similar procedure may be followed for the foreground score. However, in practice, it was found\nthat the variance selection procedure yielded large improvements when applied to the background\nmodel and little improvement in the foreground model. Hence, our final implementation uses an\nadaptive kernel variance procedure for the background model and a fixed kernel variance for the\nforeground model.",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "For comparisons, we use the data set which consists of 9 videos taken using a static camera in\nvarious environments. The data set offers various challenges including dynamic background like trees\nand waves, gradual and sudden illumination changes, and the presence of multiple moving objects.\nGround truth for 20 frames in each video is provided with the data set. The F-measure is used to\nmeasure accuracy.\n\nThe effect of choosing various kernel widths for the background and foreground models is shown in\nTable 1. The table shows the F-measure for each of the videos in the data set for various choices of\nthe kernel variances. The first 5 columns correspond to using a constant variance for each process at\nall pixel locations in the video. Having identical kernel variances for the background and foreground\nmodels (columns 1, 2) is not as effective as having different variances (all other columns). Comparing\ncolumns 2 and 3 shows that using a larger spatial variance for the foreground model than for the\nbackground model is beneficial. Changing the spatial variance from 3 (column 3) to 1 (column 4)\nhelps the overall accuracy in one video (Fountain). Using a selection procedure where the best kernel\nvariance is chosen from a set of values gives the best results for most videos (column 6) and frames.\n\nComparison of our selection procedure to a baseline method of using a standard algorithm for variance\nselection in KDE (AMISE criterion) shows that the standard algorithm is not as accurate as our\n\n5\n\n\fmethod (column 7). Our choice for the variance values for spatial dimension reflects no motion (B d\n= 1/4) and very little motion (B d = 3/4) for the background, and moderate amount of motion (F d\n= 12/4) for the foreground. For the color dimension, the choice is between little variation (B rgb=\n5/4), moderate variation (B rgb= 15/4), and high variation (B rgb= 45/4) for the background, and\nmoderate variation (F rgb= 15/4) for the foreground. These choices are based on our intuition about\nthe processes involved. For videos that differ significantly from the videos we use, it is possible that\nthe baseline AMISE method would perform better.\n\nWe would like to point out that ideally the variance value sets should be learned automatically from a\nseparate training data set. In absence of suitable training data for these videos in particular and for\nbackground subtraction research in general, we resort to manually choosing these values. This also\nappears to be the common practice among researchers in this area.\n\nBenchmark comparisons are provided for selected existing methods - MOG, the complex foreground\nmodel (ACMMM03), and SILTP. To evaluate our results, the posterior probability of the background\nlabel is thresholded at a value of 0.5 to get the foreground pixels. Following the same procedure as ,\nany foreground 4-connected components smaller than a size threshold of 15 pixels are ignored.\n\nFigure 3 shows qualitative results for the same frames that were reported by . We present results for\nour kernel method with uniform variances and adaptive variances with RGB features (Uniform-rgb\nand VKS-rgb respectively), and adaptive variances with a hybrid feature space of LAB color and\nSILTP features (VKS-lab+siltp). Except for the Lobby video, the VKS results are better than other\nmethods. The Lobby video is an instance where there is a sudden change in illumination in the\nscene (turning a light switch on and off). Due to use of an explicit foreground model, our kernel\nmethods misclassify most of the pixels as foreground and take a long time to recover from this error.\nA possible solution for this case is presented later. Compared to the uniform variance kernel estimates,\nwe see that VKS-rgb has fewer false positive foreground pixels.\n\nQuantitative results in Table 3 compare the F-measure scores for our method against MoG,\nACMMM03, and SILTP results as reported by . The table shows that methods that share spa-\ntial information (uniform kernel and VKS) with RGB features give significantly better results than\nmethods that use RGB features without spatial sharing. Comparing the variable kernel method to\na uniform kernel method in the same feature space (RGB), we see a significant improvement in\nperformance for most videos. Scale-invariant local ternary pattern (SILTP) is a recent texture feature\nthat is robust to soft shadows and lighting changes. We believe SILTP represents the state of the art\nin background modeling and hence compare our results to this method. Scale-invariant local states is\na slight variation in the representation of the SILTP feature. For comparison, we use SILTP results\nfrom because in human judgement was used to vary a size threshold parameter for each video. We\nbelieve results from the latter fall under a different category of human-assisted backgrounding and\nhence do not compare to our method where no video-specific hand-tuning of parameters was done.\nTable 3 shows that SILTP is very robust to lighting changes and works well across the entire data set.\nBlue entries in Table 3 correspond to videos where our method performs better than SILTP. VKS\nwith RGB features (VKS-rgb) performs well in videos that have few shadows and lighting changes.\nUse of color features that are more robust to illumination change, like LAB features in place of RGB\nhelps in successful classification of the shadow regions as background. Texture features are robust\nto lighting changes but not effective on large texture-less objects. Color features are effective on\nlarge objects, but not very robust to varying illumination. By combining texture features with LAB\ncolor features, we expect to benefit from the strengths of both feature spaces. Such a combination has\nproved useful in earlier work. Augmenting the LAB features with SILTP features (computed at 3\nresolutions) in the VKS framework (VKS-lab+siltp) results in an improvement in 7 out of 9 videos\n(last column). The variance values used in our implementation are given in Table 2.\n\nWe also compare our results (VKS-lab+siltp) to the 5 videos that were submitted as supplementary\nmaterial by . Figure 4 highlights some key frames that highlight the strengths and weaknesses of\nour system versus the SILTP results. The common problems with our algorithm are shadows being\nclassified as foreground (row e) and initialization errors (row e shows a scene where the desk was\noccluded by people when the background model was initialized. Due to the explicit foreground\nmodel, VKS takes some time to recover from the erroneous initialization). A common drawback with\nSILTP is that large texture-less objects have \u201choles\u201d in them (row a). Use of color features helps\navoid these errors. The SILTP system also loses objects that stop moving (rows b, c, d, f). Due to the\nexplicit modeling of the foreground, VKS is able to detect objects that stop moving.\n\n6\n\n\fThe two videos in the data set where our algorithm performs worse than SILTP are the Escalator\nvideo (rows g, h) and the Lobby video (rows i, j). In the Escalator video, our algorithm fails at the\nescalator steps due to large variation in color in the region.\n\nIn the Lobby video, at the time of sudden illumination change, many pixels in the image get classified\nas foreground. Due to the foreground model, these pixels continue to be misclassified for a long\nduration (row j). The problem is more serious for RGB features (Figure 3 column 2). One method to\naddress the situation is to observe the illumination change from one frame to the next. If more than\nhalf the pixels in the image change in illumination by a threshold value of TI or more, we throw away\nall the background samples at that instance and begin learning a new model from the subsequent 50\nframes. This method allows us to address the poor performance in the Lobby video with resulting\nF-measure values of 86.77 for uniform-rgb, 78.46 for VKS-rgb, and 77.76 for VKS-lab+siltp. TI of\n10 and 2.5 were used for RGB and LAB spaces respectively. The illumination change procedure does\nnot affect the performance of VKS on any other video in the data set.\n\n5 Caching optimal kernel variances from previous frame\n\nA major drawback with trying multiple variance values at each pixel to select the best variance is\nthat the amount of computation per pixel increases significantly. In order to reduce the complexity\nthe algorithm, we use a scheme where the current frame\u2019s optimal variance values for each pixel\nlocation for both the background and foreground processes is stored (Bcache x,y , Fcache x,y ) for\neach location (x, y) in the image. When classifying pixels in the next frame, these cached variance\nvalues are first tried. If the resulting scores are very far apart, then it is very likely that the pixel\nhas not changed its label from the previous frame. The expensive variance selection procedure is\nperformed only at pixels where the resulting scores are close to each other. Algorithm 1 for efficient\ncomputation results in a reduction in computation in about 80\n\n6 Discussion\n\nBy applying kernel estimate method to a large data set, we have established, as do , that the use\nof spatial information is extremely helpful. Some of the important issues pertaining to the choice\nof kernel parameters for data sets with wide variations have been addressed. Having a uniform\nkernel variance for the entire data set and for all pixels in the image results in a poor overall system.\nDynamically adapting the variance for each pixel results in a significant increase in accuracy.\n\nUsing color features in the joint domain-range kernel estimation approach can complement complex\nbackground model features in settings where the latter are known to be inaccurate. Combining robust\ncolor features like LAB with texture features like SILTP in a VKS framework yields a highly accurate\nbackground classification system.\n\nFor future work, we believe our method could be explained more elegantly in a probabilistic frame-\nwork where the scores are replaced by likelihoods and informative priors are used in the Bayes rule\nclassification.\n\n7\n\n\fColumn num\n4*B d \u2192\n4*B rgb\u2192\n4*F d \u2192\n4*F rgb\u2192\n\nAirportHall\nBootstrap\nCurtain\nEscalator\nFountain\nShoppingMall\nLobby\nTrees\nWaterSurface\n\n(1)\n\n3\n15\n3\n15\n\n40.72\n49.01\n66.26\n20.92\n41.87\n55.19\n22.18\n30.14\n85.82\n\n(2)\n\n3\n45\n3\n45\n\n59.53\n57.90\n83.33\n30.24\n51.89\n60.17\n23.81\n58.41\n94.04\n\n(3)\n\n3\n45\n12\n45\n\n67.07\n63.04\n91.91\n34.69\n73.24\n64.95\n25.79\n73.53\n94.93\n\n(4)\n\n1\n45\n12\n45\n\n63.53\n58.39\n89.52\n28.58\n74.58\n62.18\n25.69\n47.03\n92.91\n\n(5)\n\n3\n15\n12\n15\n\n47.21\n51.49\n81.54\n22.65\n67.60\n63.85\n25.06\n67.80\n94.64\n\nAverage\n\n45.79\n\n57.70\n\n65.46\n\n60.27\n\n52.98\n\n(6)\n\n(7)\n\n[1 3]\n\nAMISE\n[5 15 45] AMISE\n\n[12]\n[15]\n\n70.44\n71.25\n94.11\n48.61\n75.84\n76.48\n18.00\n82.09\n94.83\n\n70.18\n\n[12]\n[15]\n\n53.01\n63.38\n52.00\n32.02\n28.50\n70.14\n36.77\n64.30\n30.29\n\n47.82\n\nTable 1: F-measure for different kernel variances. Using our selection procedure ( Column 6) results\nin the highest accuracy.\n\n8",
  "conclusion": ""
}
{
  "title": "Enhanced Normalization in Vision Transformers: The Dual PatchNorm\nApproach",
  "abstract": "This study introduces Dual PatchNorm, a modification for Vision Transformers that incorporates two Layer\nNormalization layers (LayerNorms) positioned before and after the patch embedding layer. The effectiveness of\nDual PatchNorm is demonstrated through its superior performance compared to alternative LayerNorm placement\nstrategies within the Transformer block, as determined through extensive testing. Experimental results across\nvarious tasks, including image classification, contrastive learning, semantic segmentation, and transfer learning on\ndownstream classification datasets, consistently show that this simple adjustment leads to improved accuracy over\nwell-optimized standard Vision Transformers, without any negative impact.",
  "introduction": "Layer Normalization is essential for the successful and stable training of Transformer models, enabling high performance across\ndiverse tasks. This normalization technique is equally vital in Vision Transformers (ViTs), which largely adhere to the standard\narchitecture of the original Transformer model.\n\nThis research investigates whether a different arrangement of LayerNorms can enhance ViT models. Initially, we evaluate five\nViT architectures on ImageNet-1k and find that an exhaustive search for optimal LayerNorm placements within the Transformer\nblock\u2019s components does not yield improvements in classification accuracy. This suggests that the pre-LN approach in ViTs is nearly\noptimal. Further investigation reveals that alternative LayerNorm placements, such as NormFormer and Sub-LN, also do not surpass\nthe performance of robust ViT classification models when used independently.\n\nA significant finding of this study is the observation that the addition of LayerNorms before and after the standard ViT-projection\nlayer, termed Dual PatchNorm (DPN), can substantially improve performance over well-tuned baseline ViTs. Experiments conducted\non image classification across three datasets with varying sample sizes, as well as contrastive learning, confirm the effectiveness\nof DPN. Notably, qualitative analysis indicates that the LayerNorm scale parameters assign greater weight to pixels located at the\ncenter and corners of each patch.",
  "related_work": "Prior research has explored modifications to the patch-embedding layer in ViTs. For instance, one study demonstrated that adding a\nLayerNorm after patch-embedding enhances ViT\u2019s resilience to image corruptions on smaller datasets. Another study replaced the\nstandard Transformer stem with a series of stacked stride-two 3x3 convolutions with batch normalizations, resulting in improved\nsensitivity to optimization hyperparameters and increased final accuracy.\n\nFurther analysis of LayerNorm has shown that the derivatives of the mean and variance significantly contribute to performance, as\nopposed to forward normalization. Alternative strategies like Image-LN and Patch-LN have been considered for efficiently training\na single model across different patch sizes. Some researchers have added extra LayerNorms before the final dense projection in\nthe self-attention block and the non-linearity in the MLP block, employing a different initialization strategy. Others have proposed\nadding LayerNorms after the final dense projection in the self-attention block, along with a LayerNorm after the non-linearity in the\nMLP block.\n\nIn contrast to previous studies, our work demonstrates that applying LayerNorms both before and after the embedding layer\nconsistently enhances performance in classification and contrastive learning tasks. While other research has focused on incorporating\nconvolutional inductive biases into Vision Transformers, our study exclusively and thoroughly examines LayerNorm placements\nwithin the standard ViT architecture.",
  "methodology": "3.1 Patch Embedding Layer in Vision Transformer\n\nVision Transformers consist of a patch embedding layer (PE) followed by multiple Transformer blocks. The PE layer first transforms\nan image x \u2208 RH\u00d7W \u00d73 into a sequence of patches xp \u2208 RP 2\u00d7P 2HW , where P is the patch size. Each patch is then independently\nprojected using a dense layer, creating a sequence of \"visual tokens\" xt \u2208 RHW P 2\u00d7D. The patch size P determines the trade-off\nbetween the granularity of the visual tokens and the computational demands of subsequent Transformer layers.\n\n3.2 Layer Normalization\n\nWhen applied to a sequence of N patches x \u2208 RN \u00d7D, LayerNorm in ViTs involves two steps:\n\nx =\n\nx \u2212 \u00b5(x)\n\u03c3(x)\ny = \u03b3x + \u03b2\n\n(1)\n\n(2)\n\nwhere \u00b5(x) \u2208 RN , \u03c3(x) \u2208 RN , \u03b3 \u2208 RD, and \u03b2 \u2208 RD.\nFirst, Equation 3.1 normalizes each patch xi \u2208 RD in the sequence to have zero mean and unit standard deviation. Then, Equation\n3.2 applies learnable shifts and scales \u03b2 and \u03b3, which are shared across all patches.\n\n3.3 Alternate LayerNorm placements:\n\nFollowing established practices, ViTs typically place LayerNorms before each self-attention and MLP layer, known as the pre-LN\nstrategy. We assess three different strategies for each self-attention and MLP layer: placing LayerNorm before (pre-LN), after\n(post-LN), and both before and after (pre+post-LN). This results in nine distinct combinations.\n\n3.4 Dual PatchNorm\n\nInstead of adding LayerNorms within the Transformer block, we propose applying them to the stem alone, both before and after the\npatch embedding layer. Specifically, we replace:\n\nwith\n\nx = P E(x)\n\nx = LN (P E(LN (x)))\n\n(3)\n\n(4)\n\nwhile keeping the rest of the architecture unchanged. We refer to this approach as Dual PatchNorm (DPN).",
  "experiments": "4.1 Setup\n\nWe utilize the standard Vision Transformer formulation, which has demonstrated broad applicability across various vision tasks. We\ntrain ViT architectures, both with and without DPN, in a supervised manner on three datasets with varying numbers of examples:\nImageNet-1k (1M), ImageNet-21k (21M), and JFT (4B). In our experiments, we apply DPN directly to the baseline ViT recipes\nwithout any additional hyperparameter tuning. We divide the ImageNet training set into training and validation subsets and use the\nvalidation set to finalize the DPN recipe.\n\nFor ImageNet-1k, we train five architectures: Ti/16, S/16, S/32, B/16, and B/32 using a standard recipe for 93,000 steps with a batch\nsize of 4,096. We report the accuracy on the official ImageNet validation split. Additionally, we evaluate an S/16 baseline (S/16+)\nwith extensive hyperparameter tuning on ImageNet. We also apply DPN to the base and small DeiT variants.\n\nOn ImageNet-21k, we use a similar setup as ImageNet-1k and report ImageNet 25-shot accuracies in two training regimes: 93K and\n930K steps.\n\nFor JFT, we evaluate the ImageNet 25-shot accuracies of three variants (B/32, B/16, and L/16) in two training regimes (220K and\n1.1M steps) with a batch size of 4,096, without additional data augmentation or mixup regularization.\n\nWe report the 95% confidence interval across at least three independent runs on ImageNet-1k. Due to the computational cost of\ntraining on ImageNet-21k and JFT, we train each model once and report the mean 25-shot accuracy with a 95% confidence interval\nacross three random seeds.\n\n2\n\n\f4.2 DPN versus alternate LayerNorm placements\n\nEach Transformer block in ViT includes a self-attention (SA) and an MLP layer. Following the pre-LN strategy, LN is placed before\nboth the SA and MLP layers. We first demonstrate that the default pre-LN strategy in ViT models is nearly optimal by evaluating\nalternative LN placements on ImageNet-1k. We then compare this with the performance of NormFormer, Sub-LN, and DPN.\n\nFor each SA and MLP layer, we evaluate three LN placements: Pre, Post, and Pre+Post, resulting in nine total LN placement\nconfigurations. Additionally, we assess the LayerNorm placements in NormFormer and Sub LayerNorm, which add extra Layer-\nNorms within the self-attention and MLP layers in the transformer block. Figure 1 shows that none of these placements significantly\noutperform the default Pre-LN strategy, indicating that the default strategy is close to optimal. NormFormer provides some\nimprovements on ViT models with a patch size of 32. However, DPN consistently enhances performance across all five architectures.\n\nFigure 1: This plot illustrates the accuracy gains achieved by various LayerNorm placement strategies over the default pre-LN\nstrategy. Each blue point represents a different LN placement within the Transformer block. None of the alternative placements\nsurpass the default Pre-LN strategy on ImageNet-1k. The application of DPN (represented by the black cross) consistently improves\nperformance across all five architectures.\n\n4.3 Comparison to ViT\n\nTable 1 (left) shows that DPN improved the accuracy of B/16, the best ViT model, by 0.7, while S/32 achieved the maximum\naccuracy gain of 1.9. The average gain across all architectures is 1.4. On top of DeiT-S and DeiT-B, DPN provides improvements of\n0.3 and 0.2, respectively. Furthermore, we fine-tune B/16 and B/32 models with and without DPN on high-resolution ImageNet\n(384x384) for 5,000 steps with a batch size of 512. Applying DPN improves the high-resolution, fine-tuned B/16 and B/32 by 0.6\nand 1.0, respectively.\n\nDPN enhances all architectures trained on ImageNet-21k (Table 1, right) and JFT (Table 2) in shorter training regimes, with average\ngains of 1.7 and 0.8, respectively. In longer training regimes, DPN improves the accuracy of the best-performing architectures on\nJFT and ImageNet-21k by 0.5 and 0.4, respectively.\n\nIn three cases (Ti/16 and S/32 with ImageNet-21k, and B/16 with JFT), DPN matches or slightly underperforms compared to the\nbaseline. Nevertheless, across a large proportion of ViT models, simply applying DPN out-of-the-box on top of well-tuned ViT\nbaselines leads to significant improvements.\n\nTable 1: Left: ImageNet-1k validation accuracies of five ViT architectures with and without Dual PatchNorm after 93,000 steps.\nRight: Training ViT models on ImageNet-21k in two regimes (93k and 930k steps) with a batch size of 4,096, showing ImageNet\n25-shot accuracies with and without Dual PatchNorm.\n\nViT AugReg\n\nImageNet-21k\n\nBase\n\nDPN\n\nArch\n\nBase\n\nDPN\n\nArch\n\nS/32\nTi/16\nB/32\nS/16\nS/16+\nB/16\n\n72.1 \u00b1 0.07\n72.5 \u00b1 0.07\n74.8 \u00b1 0.06\n78.6 \u00b1 0.32\n79.7 \u00b1 0.09\n80.4 \u00b1 0.06\n\nDeiT\n\n74.0 \u00b1 0.09\n73.9 \u00b1 0.09\n76.2 \u00b1 0.07\n79.7 \u00b1 0.2\n80.2 \u00b1 0.03\n81.1 \u00b1 0.09\n\nS/16\nB/16\n\n80.1 \u00b1 0.03\n81.8 \u00b1 0.03\n\n80.4 \u00b1 0.06\n82.0 \u00b1 0.05\n\nAugReg + 384x384 Finetune\n\nB/32\nB/16\n\n79.0 \u00b1 0.00\n82.2 \u00b1 0.03\n\n80.0 \u00b1 0.03\n82.8 \u00b1 0.00\n\nTi/16\nS/32\nB/32\nS/16\nB/16\n\nTi/16\nS/32\n\nB/32\n\nS/16\nB/16\n\n93K Steps\n\n52.2 \u00b1 0.07\n54.1 \u00b1 0.03\n60.9 \u00b1 0.03\n64.3 \u00b1 0.15\n70.8 \u00b1 0.09\n\n53.6 \u00b1 0.07\n56.7 \u00b1 0.03\n63.7 \u00b1 0.03\n65.0 \u00b1 0.06\n72.0 \u00b1 0.03\n\n930K Steps\n\n61.0 \u00b1 0.03\n63.8 \u00b1 0.00\n\n61.2 \u00b1 0.03\n65.1 \u00b1 0.12\n\n72.8 \u00b1 0.03\n\n73.1 \u00b1 0.07\n\n72.5 \u00b1 0.1\n78.0 \u00b1 0.06\n\n72.5 \u00b1 0.1\n78.4 \u00b1 0.03\n\n4.4 Finetuning on ImageNet with DPN\n\nWe fine-tune four models trained on JFT-4B with two resolutions on ImageNet-1k: (B/32, B/16) \u00d7 (220K, 1.1M) steps at resolutions\n224x224 and 384x384. For B/32, we observe consistent improvement across all configurations. With L/16, DPN outperforms the\nbaseline in three out of four configurations.\n\n3\n\n\fTable 2: Left: Training three ViT models on JFT-4B in two regimes (200K and 1.1M steps) with a batch size of 4,096, showing\nImageNet 25-shot accuracies with and without DPN. Right: Corresponding full fine-tuning results on ImageNet-1k.\n\nJFT-4B\n\nImageNet-1k Finetuning\n\nArch\n\nBase\n\nDPN\n\nArch Resolution\n\nSteps\n\nBase\n\nDPN\n\n220K steps\n\n63.8 \u00b1 0.03\n72.1 \u00b1 0.09\n77.3 \u00b1 0.00\n\n65.2 \u00b1 0.03\n72.4 \u00b1 0.07\n77.9 \u00b1 0.06\n\n1.1M steps\n\n70.7 \u00b1 0.1\n76.9 \u00b1 0.03\n80.9 \u00b1 0.03\n\n71.1 \u00b1 0.09\n76.6 \u00b1 0.03\n81.4 \u00b1 0.06\n\nB/32\nB/16\nL/16\n\nB/32\nB/16\nL/16\n\nB/32\nB/32\nB/32\nB/32\n\nL/16\nL/16\nL/16\nL/16\n\n224\n384\n224\n384\n\n224\n384\n224\n384\n\n220K 77.6 \u00b1 0.06\n220K 81.3 \u00b1 0.09\n1.1M 80.8 \u00b1 0.1\n1.1M 83.8 \u00b1 0.03\n\n220K 84.9 \u00b1 0.06\n220K 86.7 \u00b1 0.03\n1.1M 86.7 \u00b1 0.03\n1.1M 88.2 \u00b1 0.00\n\n78.3 \u00b1 0.00\n81.6 \u00b1 0.00\n81.3 \u00b1 0.00\n84.1 \u00b1 0.00\n\n85.3 \u00b1 0.03\n87.0 \u00b1 0.00\n87.1 \u00b1 0.00\n88.3 \u00b1 0.06\n\n5 Experiments on Downstream Tasks\n\n5.1 Finetuning on VTAB\n\nWe fine-tune ImageNet-pretrained B/16 and B/32 models, both with and without DPN, on the Visual Task Adaptation Benchmark\n(VTAB), which consists of 19 datasets categorized as Natural, Specialized, and Structured. Natural datasets contain images\ncaptured with standard cameras, Specialized datasets have images from specialized equipment, and Structured datasets require scene\ncomprehension. We use the VTAB training protocol, which defines a standard training split of 800 examples and a validation split of\n200 examples per dataset. We perform a lightweight sweep across three learning rates for each dataset and select the best model\nbased on the mean validation accuracy across three seeds. The corresponding mean test scores across three seeds are reported in\nTable 3.\n\nOn Natural datasets, which are most similar to the source dataset ImageNet, B/32 and B/16 with DPN significantly outperform the\nbaseline in 7 out of 7 and 6 out of 7 datasets, respectively. The only exception is Sun397, where DPN performs worse. However,\nadditional experiments show that DPN is beneficial when B/16 is trained from scratch on Sun397. On Structured datasets, applying\nDPN improves accuracy in 4 out of 8 datasets and remains neutral in 2 for both B/16 and B/32. On Specialized datasets, DPN\nimproves performance in 1 out of 4 datasets and is neutral in 2. In conclusion, DPN offers the most significant improvements when\nfine-tuned on Natural datasets. For Structured and Specialized datasets, DPN serves as a lightweight alternative that can enhance or\nat least not harm performance in most cases.\n\nTable 3: Evaluation of DPN on VTAB. When fine-tuned on Natural datasets, B/32 and B/16 with DPN significantly outperform the\nbaseline in 7 out of 7 and 6 out of 7 datasets, respectively. On Structured datasets, DPN improves both B/16 and B/32 in 4 out of 8\ndatasets and remains neutral in 2. On Specialized datasets, DPN improves performance in 1 out of 4 datasets and is neutral in 2.\n\nNatural\n\nSpecialized\n\nCaltech101\n\nCIFAR-100\n\nDTD\n\nFlowers102\n\n87.1\n87.7\n86.1\n86.6\n\n53.7\n58.1\n35.5\n51.4\n\n56.0\n60.7\n60.1\n63.1\n\n83.9\n86.4\n90.8\n91.3\n\nStructured\n\nPets\n\n87.2\n88.0\n90.9\n92.1\n\nSun397\n\nSVHN\n\nCamelyon\n\nEuroSAT Resisc45 Retinopathy\n\n32.0\n35.4\n33.9\n32.5\n\n76.8\n80.3\n76.7\n78.3\n\n77.9\n78.5\n81.3\n80.6\n\n94.8\n95.0\n95.9\n95.8\n\n78.2\n81.6\n81.2\n83.5\n\n71.2\n\n70.3\n\n74.7\n\n73.3\n\nClevr-Count\n\nClevr-Dist\n\nDMLab\n\ndSpr-Loc\n\ndSpr-Ori KITTI-Dist\n\nsSNORB-Azim sNORB-Elev\n\n58.3\n62.5\n65.2\n73.7\n\n52.6\n55.5\n59.8\n48.3\n\n39.2\n40.7\n39.7\n41.0\n\n71.3\n60.8\n72.1\n72.4\n\n59.8\n61.6\n61.9\n63.0\n\n73.6\n73.4\n81.3\n80.6\n\n20.7\n20.9\n18.9\n21.6\n\n47.2\n34.4\n50.4\n36.2\n\nB/32\n+ DPN\nB/16\n+ DPN\n\nB/32\n+ DPN\nB/16\n+ DPN\n\n5.2 Contrastive Learning\n\nWe apply DPN to image-text contrastive learning. Each minibatch consists of image and text pairs. We train a text and image\nencoder to map an image to its correct text over all other texts in the minibatch. Specifically, we adopt a method where we initialize\nand freeze the image encoder from a pretrained checkpoint and train the text encoder from scratch. To evaluate zero-shot ImageNet\naccuracy, we represent each ImageNet class by its text label, which the text encoder maps into a class embedding. For a given image\nembedding, the prediction is the class corresponding to the nearest class embedding.\n\n4\n\n\fWe evaluate four frozen image encoders: two architectures (B/32 and L/16) trained with two schedules (220K and 1.1M steps). We\nreuse standard hyperparameters and train only the text encoder using a contrastive loss for 55,000 steps with a batch size of 16,384.\nTable 4 shows that on B/32, DPN improves over the baselines in both setups, while on L/16, DPN provides improvement when the\nimage encoder is trained with shorter training schedules.\n\nTable 4: Zero-Shot ImageNet accuracy in the contrastive learning setup.\n\nArch\n\nSteps\n\nBase\n\nDPN\n\nB/32\nB/32\nL/16\nL/16\n\n220K 61.9 \u00b1 0.12\n1.1M 67.4 \u00b1 0.07\n220K 75.0 \u00b1 0.11\n1.1M 78.7 \u00b1 0.05\n\n63.0 \u00b1 0.09\n68.0 \u00b1 0.09\n75.4 \u00b1 0.00\n78.7 \u00b1 0.1\n\n5.3 Semantic Segmentation\n\nWe fine-tune ImageNet-pretrained B/16 models, with and without DPN, on the ADE-20K 512x512 semantic segmentation task.\nFollowing established methods, a single dense layer maps the ViT features into per-patch output logits. A bilinear upsampling layer\nthen transforms the output distribution into the final high-resolution 512x512 semantic segmentation output. We fine-tune the entire\nViT backbone with a standard per-pixel cross-entropy loss. Table 5 reports the mean mIOU across 10 random seeds and different\nfractions of training data. The improvement in IoU is consistent across all setups.\n\nTable 5: Fine-tuning ImageNet pretrained B/16 models with and without DPN on the ADE20K Semantic Segmentation task, with\nvarying fractions of ADE20K training data. The table reports the mean IoU across ten random seeds. Applying DPN improves IoU\nacross all settings.\n\nFraction of Train Data\n\n1/16\n\n1/8\n\n1/4\n\n1/2\n\n1\n\nB/16\n+DPN\n\n27.3 \u00b1 0.09\n28.0 \u00b1 0.21\n\n32.6 \u00b1 0.09\n33.7 \u00b1 0.11\n\n36.9 \u00b1 0.13\n38.0 \u00b1 0.11\n\n40.8 \u00b1 0.1\n41.9 \u00b1 0.09\n\n45.6 \u00b1 0.08\n46.1 \u00b1 0.11\n\n6 Ablations\n\nIs normalizing both the inputs and outputs of the embedding layer optimal? In Eq 4, DPN applies LN to both the inputs and outputs\nof the embedding layer. We evaluate three alternative strategies: Pre, Post, and Post PosEmb. Pre applies LayerNorm only to\nthe inputs, Post applies it only to the outputs, and Post PosEmb applies it to the outputs after they are summed with positional\nembeddings.\n\nTable 6 shows the accuracy gains with these alternative strategies. Pre is unstable on B/32, leading to a significant drop in accuracy,\nand it also results in minor accuracy drops on S/32 and Ti/16. Post and Post PosEmb perform worse on smaller models (B/32, S/32,\nand Ti/16). Our experiments demonstrate that applying LayerNorm to both inputs and outputs of the embedding layer is necessary\nfor consistent accuracy improvements across all ViT variants.\n\nTable 6: Ablations of various components of DPN. Pre: LayerNorm only to the inputs of the embedding layer. Post: LayerNorm\nonly to the outputs of the embedding layer. No learnable: Per-patch normalization without learnable LayerNorm parameters. Only\nlearnable: Learnable scales and shifts without standardization.\n\nB/16\n\nS/16 B/32\n\nS/32 Ti/16\n\nPre\nPost\nPost PosEmb\nOnly learnable\nRMSNorm\nNo learnable\n\n-0.1\n0.0\n0.0\n-0.8\n0.0\n-0.5\n\n0.0\n-0.2\n-0.1\n-0.9\n-0.1\n0.0\n\n-2.6\n-0.5\n-0.4\n-1.2\n-0.4\n-0.2\n\n-0.2\n-0.7\n-0.9\n-1.6\n-0.5\n-0.1\n\n-0.3\n-1.1\n-1.1\n-1.6\n-1.7\n-0.1\n\nNormalization vs. Learnable Parameters: As seen in Sec. 3.2, LayerNorm involves a normalization operation followed by learnable\nscales and shifts. We also ablate the effect of each of these operations in DPN.\n\nApplying only learnable scales and shifts without normalization significantly decreases accuracy across all architectures (See: Only\nlearnable in Table 6). Additionally, removing the learnable parameters leads to unstable training on B/16 (No learnable in Table 6).\nFinally, removing the centering and bias parameters, as done in RMSNorm, reduces the accuracy of B/32, S/32, and Ti/16. We\nconclude that while both normalization and learnable parameters contribute to the success of DPN, normalization has a greater\nimpact.\n\n5\n\n\f7 Analysis\n\n7.1 Gradient Norm Scale\n\nWe present per-layer gradient norms for B/16, both with and without DPN. Figure 2 (Left) displays the mean gradient norm of the last\n1000 training steps as a function of depth. Notably, the gradient norm of the base ViT patch embedding (black) is disproportionately\nlarge compared to other layers. Applying DPN (red) scales down the gradient norm of the embedding layer. Figure 2 (Right) further\nshows that the gradient norm of the embedding layer is reduced not only before convergence but also throughout the training process.\nThis characteristic is consistent across ViT architectures of different sizes.\n\n7.2 Visualizing Scale Parameters\n\nThe first LayerNorm in Eq. 4 is applied directly to patches, i.e., raw pixels. Thus, the learnable parameters (biases and scales) of\nthe first LayerNorm can be visualized directly in pixel space. Figure 3 shows the scales of our smallest and largest models: Ti/16\ntrained on ImageNet for 90,000 steps and L/16 trained on JFT for 1.1M steps, respectively. Since the absolute magnitude of the scale\nparameters varies across the R, G, and B channels, we visualize the scale separately for each channel. Interestingly, for both models,\nthe scale parameter increases the weight of the pixels in the center of the patch and at the corners.",
  "results": "",
  "conclusion": "We propose a straightforward modification to standard ViT models\n\n6"
}
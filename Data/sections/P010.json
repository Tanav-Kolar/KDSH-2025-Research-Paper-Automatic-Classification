{
  "title": "Enhanced Reinforcement Learning for Recommender Systems:\nMaximizing Sample Efficiency and Minimizing Variance",
  "abstract": "Optimizing long-term user satisfaction in recommender systems, such as news feeds, is crucial during continuous\nuser-system interactions. Reinforcement learning has shown promise in addressing this challenge. However,\npractical hurdles like low sample efficiency, potential risks, and high variance hinder the implementation of deep\nreinforcement learning in online systems. We introduce a new reinforcement learning approach called model-based\ncounterfactual advantage learning (MBCAL) to tackle these challenges. MBCAL leverages the unique aspects of\nrecommender systems and incorporates concepts from model-based reinforcement learning to enhance sample\nefficiency. It consists of two main parts: an environment model that predicts immediate user behavior sequentially\nand a future advantage model that forecasts future utility. Counterfactual comparisons from the environment model\nare used to mitigate the excessive variance when training the future advantage model. Consequently, MBCAL\nachieves high sample efficiency and significantly reduced variance, while utilizing existing user logs to avoid\nstarting from scratch. Despite its capabilities, MBCAL maintains a relatively low implementation cost, making it\nsuitable for real-world systems. The proposed method surpasses other supervised learning and RL-based methods\nin both sample efficiency and overall performance, as demonstrated through theoretical analysis and extensive\nexperiments.",
  "introduction": "Recommender systems are essential for delivering personalized content and improving the efficiency of information retrieval.\nModern recommender systems, like news feeds, must consider multiple user-system interactions within a single session. The content\nrecommended in past interactions can influence future user behavior. For example, exploring new topics might pique a user\u2019s interest\nin related areas, while repeatedly showing similar content could lead to a rapid decline in user engagement. Traditional recommender\nsystems rely on collaborative filtering or neural networks to predict immediate user actions, such as clicks. However, solely focusing\non immediate actions can result in issues like recommendation redundancy, ultimately harming the user\u2019s long-term experience.\n\nRecently, deep reinforcement learning (Deep RL) has gained attention for its potential in recommender systems. Deep RL models\nuser-system interactions as Markov Decision Processes (MDPs). Many studies in this field focus on model-free reinforcement\nlearning (MFRL) methods. However, challenges persist, including the substantial data consumption during training, also known as\nlow sample efficiency. Another challenge is the practical risks associated with implementing MFRL. On-policy RL struggles to\nutilize off-policy user logs, leading to difficulties in online infrastructure and initial performance. Conversely, off-policy RL faces\nthe risk of non-convergence when combined with function approximation and offline training.\n\nModel-based RL (MBRL) offers an alternative with improved sample efficiency and reduced practical risks. MBRL employs\nan environment model to predict immediate feedback and state transitions, along with a planning module to find an optimal\ntrajectory. However, MBRL can be computationally intensive during inference. Planning is often infeasible in multi-stage retrieval\nframeworks commonly used in modern recommender systems. These systems generate candidate sets of items in earlier stages for\nsubsequent stages, making it impossible to predetermine candidates. To address these issues, Dyna algorithms have been proposed\nfor recommender systems. The Dyna algorithm accelerates convergence by generating virtual interactions using the environment\nmodel. However, this faster convergence comes at the cost of reduced asymptotic performance due to error accumulation from\nvirtual interactions.\n\nAnother significant challenge in deploying RL is the excessive variance of gradients during optimization. This variance can stem\nfrom stochastic transitions, noisy rewards, and stochastic policies. Longer horizons tend to exacerbate the variance, significantly\nslowing down convergence and introducing instability. Prior research has shown that using an advantage function instead of a value\nfunction can reduce variance and improve performance. However, these proposals primarily target MFRL, and variance reduction in\nMBRL remains largely unexplored.\n\nIn recommender systems, variance can arise from various factors. First, there is substantial noise in observed user feedback. Some\nusers may be more inclined to provide positive or negative feedback than others. Even individual users may exhibit different\n\n\fbehaviors at different times of the day. Second, for stochastic policies, resampling trajectories from any state can lead to varying\nlong-term returns. While a large amount of data can mitigate the impact of variance, it still negatively affects performance due to\ndata sparsity for specific users and items.\n\nTo address variance reduction, our work introduces the concept of comparing an observed trajectory with a counterfactual trajectory.\nThis counterfactual trajectory shares all contexts with the original, including the user, historical interactions, and follow-up items,\nexcept for the current action being replaced. By comparing these trajectories, we can make more informed judgments about the\nadvantage of taking a specific action. While finding such counterfactual records in user logs is impossible, we can leverage the\nenvironment model to simulate future rollouts and generate these trajectories.\n\nBuilding on this idea, we propose a novel MBRL solution for recommender systems called Model-based Counterfactual Advantage\nLearning (MBCAL). MBCAL decomposes overall utility into immediate utility (rewards from the current step) and future utility\n(rewards from future steps). The environment model naturally predicts immediate utility, while future utility is approximated through\nsimulated rollout. To further reduce variance in future utilities, we perform two comparative simulated rollouts. We introduce a\nmasking item to the environment model, enabling us to generate simulated rollouts by masking the action of interest. We then\ncalculate the counterfactual future advantage (CFA) as the difference in future utility with and without masking. Finally, we introduce\nthe future advantage model to approximate the CFA.\n\nWe conducted experiments using three real-world datasets and compared our method with supervised learning, MFRL, and MBRL\napproaches. We also focused on Batch-RL and Growing Batch-RL settings, which are more aligned with practical infrastructures.\nThe experimental results demonstrate the superiority of our proposed method.",
  "related_work": "",
  "methodology": "The core concept of MBCAL is illustrated by employing two models: the Masked Environment Model (MEM) and the Future\nAdvantage Model (FAM). These models are designed to estimate immediate user behavior and future advantages, respectively. The\ntraining process begins with optimizing the environment model to predict user behaviors, incorporating a masking item into the\nmodel. Using the MEM, we compute the Counterfactual Future Advantage (CFA) by contrasting the future utility derived from\nmasking the action against not masking it. The CFA then serves as the target for training the FAM. During inference, we combine\nboth models to select actions.\n\nWe first formalize the environment model and then detail the MEM, FAM, and the overall learning process. Following this, we\nprovide a theoretical analysis of the proposed method.\n\n2.1 Environment Modeling\n\nTypically, an environment model predicts transitions and rewards separately. Here, we use approximations for the transition\nprobability and the reward function. Specifically, to formulate the environment model in a recommender system context, we can\nexpress the transition probability as the probability of observing the next user behavior given the past trajectory and the current\naction. This means that predicting the transition simplifies to predicting the immediate user behavior. Since the reward also depends\nsolely on user behavior, a single model can replace the separate transition and reward approximations. We introduce a function with\ntrainable parameters to approximate the probability of the next user behavior. The transition and reward are then approximated using\nthis function.\n\n2.2 Masked Environment Model\n\nTo mitigate the intractable noise in user feedback, we introduce a masking item into the model. This allows us to create a\ncounterfactual comparison to the current trajectory, answering the question: \"What would the future behavior be if this action were\nnot taken?\" We introduce a virtual item represented by a trainable embedding vector. Given an observation trajectory, we denote the\ntrajectory where actions at specific positions are replaced by this virtual item as a masked trajectory.\n\nTraining is straightforward. We sample random positions for each trajectory, replacing each position with a uniform probability. The\nMEM aims to recover the user behavior as closely as possible when some items are masked. Using the collected masked trajectories,\nwe maximize the likelihood or minimize the negative log-likelihood (NLL).\n\nTo model sequential observations, the MEM\u2019s architecture follows that of session-based recurrent recommender systems. We use a\nGated Neural Network to encode the trajectory. Since we need to encode both the trajectory and the current action, we concatenate\nthe input in a staggered manner. For each step, the model takes the previous behavior and the current action as input and outputs the\nprobability of the next possible behavior. An additional start symbol is introduced as the beginning of the observed user behavior.\nThe architecture is formulated as follows: a representation layer, a concatenation operation, a multilayer perceptron, and a Gated\nRecurrent Unit.\n\n2\n\n\f2.3 Counterfactual Future Advantage\n\nUsing the Masked Environment Model (MEM), we can estimate the difference in future utilities between the original trajectory and\nits counterfactual counterpart, which we term the Counterfactual Future Advantage (CFA). Given a trained MEM, we first define the\nSimulated Future Reward (SFR) for an observed trajectory at a specific time step. We then calculate the CFA by subtracting the SFR\nof the counterfactual comparison from the original one. Finally, we introduce the Future Advantage Model (FAM), with its own set\nof trainable parameters, to approximate this CFA. To train the FAM, we minimize the mean square error.\n\nThe FAM uses a similar neural architecture to the MEM, except for the final layer, but with different parameters. Instead of predicting\na distribution, the FAM\u2019s last layer predicts a scalar value representing the advantage.\n\n2.4 Summary of MBCAL\n\nFor inference, we select the item (action) based on both the MEM and FAM. Formally, given user information and the observation\ntrajectory, we choose the next action by maximizing the sum of the immediate reward predicted by the MEM and the future advantage\npredicted by the FAM. To avoid local optima in policy improvement, we use an \u03b5-greedy strategy. With probability \u03b5, we select a\nrandom action; otherwise, we select the action that maximizes the combined reward and advantage.\n\nMBCAL aligns well with the Growing Batch-RL settings. The algorithm involves iterative data collection and policy updates.\nAlthough we use the term \"policy,\" we do not require an explicit policy formulation, unlike common policy gradient methods, which\nare often challenging to define in many recommender systems.\n\nThe variance reduction in MBCAL is primarily achieved through the subtraction in the CFA calculation, which eliminates noise\nfrom user feedback and other sources. While we borrow ideas from the advantage function concept, our CFA differs in that we do\nnot resample the trajectory but keep the remaining part unchanged. Although this could introduce bias in many MDP problems, we\nargue that recommender systems exhibit weaker correlations between sequential decisions compared to other domains (e.g., robot or\ngame control). Additionally, since the FAM averages the CFA across different trajectories, the bias becomes negligible compared to\nthe benefits of variance reduction.",
  "experiments": "3.1 Datasets\n\nEvaluating RL-based recommender systems is challenging. The most reliable metric involves online A/B tests, but these are often\ntoo costly and risky for comparing all baselines in an online system. Offline evaluation of long-term utility using user logs is difficult\nbecause we lack feedback for actions not present in the log. To thoroughly assess the performance of the proposed systems, we\nfollow previous works and construct simulators. However, instead of synthetic simulators, we use real-data-driven simulators. The\ndatasets used include MovieLens, Netflix Prize, and NewsFeed.\n\n\u2022 MovieLens: This dataset contains 5-star rating activities from MovieLens. User behavior corresponds to star ratings, with\n\nrewards matching these ratings. There are three types of features: movie-id, movie-genre, and movie-tag.\n\n\u2022 Netflix Prize: This dataset consists of 5-star ratings from Netflix. Rewards follow the same setup as MovieLens. It includes\n\nonly one type of feature: movie-id.\n\n\u2022 NewsFeed: This dataset is collected from a real online news recommendation system. We focus on predicting the dwelling\ntime on clicked news, partitioned into 12 levels, each corresponding to a different user behavior. Rewards range from\n1 to 12. There are seven types of features: news-id, news-tag, news-title, news-category, news-topics, news-type, and\nnews-source.\n\n3.2 Experimental Settings\n\nTo ensure a fair evaluation, it is crucial to prevent the agent in the evaluated system from exploiting the simulator. We implement\ntwo specific settings in the evaluation process. First, all agents are restricted to using only a subset of features, while the simulator\nuses the full feature set. In MovieLens and Netflix, agents use only the movie-id feature. In NewsFeed, agents use four out of seven\nfeatures (news-id, category, news-type, and news-source). Second, we intentionally set the model architecture of the simulator to\ndiffer from that of the agents. We use LSTM units for the simulators, while agents use GRU units.\n\nTo gauge the simulator\u2019s accuracy, we report micro-F1, weighted-F1, and RMSE for user behavior classification. The properties of\nthe datasets and simulators are detailed in Table 2. For the NewsFeed dataset, we also analyzed over 400 historical A-B test records.\nThe correlation between our simulator\u2019s predictions of long-term rewards (e.g., total clicks or session dwelling time) and the actual\noutcomes is above 0.90.\n\n3\n\n\f3.2.1 Evaluation Settings\n\nThe evaluation process consists of two types of iterations: training rounds and test rounds. During a training round, the agent\ngenerates actions using an \u03b5-greedy policy (\u03b5 = 0.1 for all experiments) and updates its policy based on feedback from the simulator.\nIn the test round, the agent uses a greedy policy, and the generated data is not used for training. Each session in both training and test\nrounds involves 20 steps of interaction between the simulator and the agent. Each round includes 256,000 sessions.\n\nFor each experiment, we report the average reward per session in the test round, calculated as the sum of rewards over all sessions in\nthe test round divided by the number of sessions. Each experiment is repeated three times with different random seeds, and we\nreport the mean and variance of the scores. We simulate both Batch RL and Growing Batch-RL evaluations separately. In Batch RL\nevaluation, the agent trains only on static user logs and interacts with the simulator during testing. In Growing Batch RL evaluation,\nthe agent interacts with the simulator during both training and test rounds, with the training round repeating up to 40 times.\n\n3.3 Methods for Comparison\n\nWe compare various methods, including Supervised Learning (GRU4Rec), bandits (GRU4Rec (\u03b5-greedy)), MFRL (MCPE, DQN,\nDDQN, and DDPG), and MBRL (Dyna-Q). For bandits, LinUCB is a common baseline, but it performs poorly in our environments\ndue to the limited representational power of linear models. Therefore, we use the \u03b5-greedy version of NN models (GRU4Rec\n(\u03b5-greedy)) instead of LinUCB.\n\nThe methods for comparison are:\n\n\u2022 GRU4Rec: Uses GRU to encode interactive history to predict immediate user behavior, with an architecture equivalent to\n\nthe environment model. We use entropy loss in GRU4Rec.\n\n\u2022 GRU4Rec (\u03b5-greedy): Applies \u03b5-greedy item selection in GRU4Rec during training rounds.\n\n\u2022 DQN: A classic off-policy learning algorithm. We use GRU for state representation to ensure fair comparison, similar to\n\nGRU4Rec and our method.\n\n\u2022 DDQN: Double DQN, which uses a different action selection for value backup to avoid value overestimation in off-policy\n\nlearning. The model architecture is equivalent to GRU4Rec.\n\n\u2022 DDPG: Deep Deterministic Policy Gradient, an off-policy learning algorithm for continuous action spaces. The inferred\naction selects the nearest neighbor item for display. We use the same neural structure as GRU4Rec for both actor and critic\nnetworks.\n\n\u2022 MCPE: Monte Carlo Policy Evaluation, a straightforward value iteration algorithm using the whole trajectory for value\n\nbackup. The model architecture is the same as other baselines.\n\n\u2022 Dyna-Q: An MBRL method that augments DQN with imagined rollouts from an environment model. The ratio of imagined\n\nrollouts to real trajectories is 1:1.\n\n\u2022 MBCAL: The full version of our proposed method.\n\n\u2022 MBCAL (w/o variance reduction): An ablated version of MBCAL where we use SFR instead of CFA as the label for FAM.\n\nAll parameters are optimized using the Adam optimizer with a learning rate of 10-3, \u03b21 = 0.9, and \u03b22 = 0.999. The discount factor for\nlong-term rewards is \u03b3 = 0.95. Embedding sizes for item-id and other id-type features are set to 32. The hidden size for MLP is 32.\nFor training MEM in MBCAL, we use pmask = 0.20 to generate masked trajectories. In DDPG, we use a 4-dimensional action space\ndue to poor performance with higher dimensions, and an additional layer maps item representations to this 4-dimensional space.\n\n3.4 Experimental Results\n\n3.4.1 Results of Batch-RL Evaluation\n\nThe results of the Batch-RL evaluation are presented in Table 3. We evaluate the reward per session based on the rewards generated\nby the simulator. The results indicate that MFRL methods cannot outperform MBRL methods across all three environments. Due to\nits sample inefficiency, MFRL tends to exhibit poor initial performance. Notably, DDPG demonstrates the weakest performance\nacross all environments. Upon closer examination of the value functions in DDPG, we observed significant overestimation compared\nto other MFRL methods. This overestimation likely arises from value backups based on continuous actions that may not correspond\nto actual items.\n\nAs anticipated, MBCAL outperforms all other tested systems by substantial margins, showcasing its sample efficiency. However, the\nadvantage of our method over the supervised learning method is less pronounced in the MovieLens and Netflix datasets compared to\nNewsFeed. This suggests that long-term rewards play a more significant role in the NewsFeed environment.\n\nFurthermore, while learning to predict long-term utility requires more data than immediate rewards, the dominance of RL is not\nyet fully apparent in Batch-RL settings. Nevertheless, it is crucial that MBCAL\u2019s initial performance is already state-of-the-art,\nunderscoring its low risk and high sample efficiency.\n\n4\n\n\fTable 1: Average reward per session of different algorithms and datasets in Batch-RL evaluation.\n\nAlgorithms\n\nMovieLens\n\nNetflix\n\nNewsFeed\n\nGRU4Rec\nDDPG\nDQN\nDDQN\nMCPE\nDyna-Q\nMBCAL\nMBCAL (w/o variance reduction)\n\n77.93 \u00b1 0.06\n70.99 \u00b1 0.70\n77.27 \u00b1 0.06\n77.23 \u00b1 0.02\n77.20 \u00b1 0.10\n77.25 \u00b1 0.05\n78.02 \u00b1 0.03\n77.70 \u00b1 0.04\n\n79.63 \u00b1 0.02\n72.50 \u00b1 0.35\n77.75 \u00b1 0.01\n77.70 \u00b1 0.04\n77.70 \u00b1 0.03\n77.81 \u00b1 0.02\n79.71 \u00b1 0.04\n79.50 \u00b1 0.04\n\n11.58 \u00b1 0.14\n10.90 \u00b1 0.42\n12.44 \u00b1 0.33\n12.48 \u00b1 0.17\n13.21 \u00b1 0.53\n13.04 \u00b1 0.33\n16.32 \u00b1 0.24\n15.61 \u00b1 0.38\n\nTable 2: Properties of Datasets and Simulators.\n\nProperties\n\nMovieLens Netflix NewsFeed\n\n# of Users\n# of Items\n# of Different Labels\n# of Types of Features\nSize of Training Set\nSize of Validation Set\nSimulator Macro-F1\nSimulator Weighted-F1\nSimulator RMSE\n\n130K\n20K\n6\n3\n2.48M\n1.27M\n0.545\n0.532\n0.770\n\n480K\n17K\n6\n1\n4.53M\n2.27M\n0.511\n0.498\n0.848\n\n920K\n110K\n12\n7\n9.41M\n4.70M\n0.923\n0.887\n1.810\n\n3.4.2 Results of Growing Batch-RL Evaluation\n\nIn all environments, GRU4Rec(\u03b5-greedy) slightly outperforms the purely supervised GRU4Rec, highlighting the advantages of\nexploration in online systems. The performance of DDPG remains surprisingly poor across all three environments.\n\nWith the aid of the environment model, Dyna-Q initially gains some advantages but gradually diminishes as learning progresses.\nThis observation aligns with expectations since the virtual experience loses its benefits as sufficient real user feedback accumulates.\nMBCAL maintains its performance lead over other methods in all environments. Even in Netflix and MovieLens, where other\nRL-based systems fail to outperform traditional GRU4Rec, MBCAL achieves a considerable margin. In NewsFeed, where long-term\nrewards are more critical, MBCAL further extends its lead.\n\nMCPE, DQN, DDQN, and Dyna-Q lag behind other methods, including supervised learning baselines in MovieLens and Netflix, but\nnot in NewsFeed. Investigating further, we modified GRU4Rec to output the immediate reward instead of user behavior classification,\nturning the task into regression and replacing entropy loss with mean square error loss. This change resulted in a significant\nperformance drop in GRU4Rec, aligning more closely with the NewsFeed results. These findings suggest that classification and\nentropy loss benefit the system more than regression, and that user behavior contains richer information than rewards, giving MBRL\nan edge over MFRL.\n\n3.4.3 Analysis of the variance\n\nThe critical aspect of MBCAL is variance reduction through counterfactual comparisons. Previous research indicates that the\nmean square error (MSE) in a well-trained model comprises model bias and label variance (noise). Since we use equivalent neural\narchitectures across all comparison methods, they share the same model bias. Thus, the MSE is primarily influenced by noise. To\nassess whether CFA effectively reduces variance, we compare the MSE from the value backup equation and the CFA equation. We\nanalyze the MSE of MCPE, DQN, Dyna-Q, MBCAL (w/o variance reduction), and MBCAL using interactive logs from the test\nround of Batch-RL evaluation.\n\nTable 3: The mean square error (MSE) loss of different algorithms in different environments.\n\nAlgorithms\n\nMovieLens Netflix NewsFeed\n\nDQN\nMCPE\nDyna-Q\nMBCAL\nMBCAL (w/o variance reduction)\n\n1.50\n17.1\n0.94\n0.004\n3.45\n\n1.22\n9.21\n1.04\n0.009\n3.29\n\n4.29\n46.9\n7.87\n0.07\n3.07\n\n5\n\n\fThe average MSE is presented in Table 4. Consistent with theoretical analysis, longer horizon value backups exhibit higher variance.\nMCPE has a higher variance than DQN and Dyna-Q due to using the entire trajectory for backup. MBCAL (w/o variance reduction)\nhas the second-largest variance, lower than MCPE because the environment model\u2019s simulated rollout partially eliminates noise.\nDQN and Dyna-Q have smaller variances due to one-step value backup. Compared to other methods, MBCAL shows significantly\nlower variance, confirming the expected variance reduction.",
  "results": "",
  "conclusion": "In conclusion, our work focuses on sequential decision-making problems in recommender systems. To maximize long-term utility,\nwe propose a sample-efficient and variance-reduced reinforcement learning method called MBCAL. This method incorporates a\nmasked environment model to capture immediate user behavior and a future advantage model to predict future utility. By employing\ncounterfactual comparisons, MBCAL significantly reduces learning variance. Experiments conducted on real-data-driven simulations\ndemonstrate that our proposed method surpasses existing approaches in both sample efficiency and asymptotic performance. Future\nwork could involve theoretically calculating the error bound and extending the fixed horizon settings to infinite and dynamic horizon\nrecommender systems.\n\n6"
}
{
  "title": "Enhancing Self-Consistency and Performance of\nPre-Trained Language Models through Natural\nLanguage Inference",
  "abstract": "While large pre-trained language models are powerful, their predictions often\nlack logical consistency across test inputs. For example, a state-of-the-art Macaw\nquestion-answering (QA) model answers Yes to Is a sparrow a bird? and Does\na bird have feet? but answers No to Does a sparrow have feet?. To address this\nfailure mode, we propose a framework, Consistency Correction through Relation\nDetection, or ConCoRD, for boosting the consistency and accuracy of pre-trained\nNLP models using pre-trained natural language inference (NLI) models without\nfine-tuning or re-training. Given a batch of test inputs, ConCoRD samples several\ncandidate outputs for each input and instantiates a factor graph that accounts for\nboth the model\u2019s belief about the likelihood of each answer choice in isolation and\nthe NLI model\u2019s beliefs about pair-wise answer choice compatibility. We show that\na weighted MaxSAT solver can efficiently compute high-quality answer choices\nunder this factor graph, improving over the raw model\u2019s predictions. Our experi-\nments demonstrate that ConCoRD consistently boosts accuracy and consistency of\noff-the-shelf closed-book QA and VQA models using off-the-shelf NLI models,\nnotably increasing accuracy of LXMERT on ConVQA by 5",
  "introduction": "Reliable and trustworthy AI systems should demonstrate internal self-consistency, in the sense that\ntheir predictions across inputs should imply logically compatible beliefs about the world. However,\neven powerful large language models are known to lack self-consistency. For example, a question-\nanswering (QA) model that answers the question Is a sparrow a bird? and Does a bird have feet?\nwith Yes is implicitly expressing the belief that A sparrow is a bird and A bird has feet. If the\nsame model answers the question Does a sparrow have feet? with No, the model expresses the\nlogically incompatible belief A sparrow does not have feet. In such cases, ascertaining the model\u2019s\n\u02d8201ctrue\u02d8201d belief is difficult, making interpreting and validating its behavior correspondingly\nchallenging.\n\nPrior work has improved model self-consistency by training with specialized loss functions or data\naugmentation, or alternatively re-ranking model predictions based on their mutual self-consistency\nusing pre-written logical constraints, such as \u02d8201call mammals have fur\u02d8201d. However, the first class\nof methods requires expensive fine-tuning which might be impractical for many practitioners for\nvery large pre-trained models, and re-ranking methods require an explicit collection of the logical\nrelations of interest, making scaling a challenge. Still, re-ranking-based approaches have the benefit\nof not requiring fine-tuning, and we hypothesize that their scalability limitations may be addressed by\nestimating logical relationships between model predictions on the fly. Specifically, we hypothesize\nthat existing pre-trained natural language inference (NLI) models can estimate logical relationships\nbetween an arbitrary pair of model predictions well enough to provide an effective, scalable substitute\nfor explicit collection of such constraints. Leveraging these estimated constraints, we can construct\n\n\fa factor graph representing a probability distribution over model outputs that incorporates both the\noriginal model\u2019s confidence scores and the NLI model\u2019s beliefs about logical relationships.\n\nOur primary contribution is Consistency Correction through Relation Detection, or ConCoRD, a\nframework to improve the consistency and performance of a pre-trained base language model without\nfine-tuning by using more confident and better attested model predictions to override less confident\nmodel beliefs. To enable propagation of model beliefs, we estimate pair-wise logical relationships\nbetween model predictions using a pre-trained NLI model. Using these pair-wise relationships, we\ndefine an undirected graphical model representing a distribution over responses accounting for both\nthe base model\u2019s beliefs and the NLI model\u2019s estimates of answer compatibility. We efficiently find\nthe approximate mode of this distribution among the base model\u2019s top answer choices for each input\nas the solution of a MaxSAT problem, which consistently produces more accurate and self-consistent\npredictions than using the raw model predictions. We find that ConCoRD produces an 8.1",
  "related_work": "Prior work for maintaining consistency in the question-answering space often involves additional\ntraining to improve performance. Some work generates questions from unlabeled texts, then filters\nthem to ensure roundtrip consistency; pre-training on this synthetic set improves performance on\nSQuAD 2.0 and Natural Questions. Other work augments QA-pairs with their logically symmetric\nand transitive counterparts through linguistic approaches to enhance cross-dataset QA performance.\nConCoRD differs significantly from these question-answering-specific approaches because no fine-\ntuning of the base model is needed and the methodology is not specific to question-answering.\n\nSimilarly to ConCoRD, other work re-rank model predictions by solving an optimization problem\ndefined by a combination of the base model confidence scores and pair-wise constraints representing\nthe logical compatibility of different model predictions stored in a persistent memory, which they\ncall BeliefBank. The key distinguishing property of ConCoRD is the fact that pair-wise constraints\nbetween model predictions are dynamically estimated by a pre-trained NLI model, rather than drawn\nfrom a fixed, pre-collected set of constraints. Dynamically estimating the constraints has a variety of\nbenefits, eliminating the need for manually collecting the logical constraints of interest, automating\nthe process of determining whether a particular constraint applies to a particular pair of predictions,\nand likely inheriting improvements in Natural language inference (NLI) models over time.\n\nNLI has long been used to maintain logical consistency in generated dialogue utterances, radiology\nreport domain entities, and summarization. Perhaps most similarly, other work uses NLI to estimate\nconstraints between factual statements produced by GPT-3. These prior approaches support our\nintuition for using NLI models to improve logical consistency among batches of answers. While the\nauthors explore applications of this framework to multi-step reasoning for True/False questions or\nstatements, our work focuses on applying this methodology to more general settings, such as VQA,\nopen-ended QA, and model editing.\n\n3 Consistency Correction through Relation Detection\n\nConCoRD contains three key components, the base model, a relation model (typically a pre-trained\nNLI model), and an inference procedure that combines the predictions of the two models into a more\naccurate and self-consistent set of beliefs. Importantly, both the base model and relation model are\npre-trained, off-the-shelf models; ConCoRD does not update any weights or require training data\nfor either model, using only a small validation set for hyperparameter tuning. We next explain the\nfunction of each of these components when executing ConCoRD.\n\n3.1 Base Model\n\nThe core function of the base model in ConCoRD is generating a set of candidate outputs for a given\ninput, which are ultimately re-ranked by the inference process (Sec. 3.3). Given a batch of N model\nqueries Q = {qi}, the first step of ConCoRD is to generate a set of J candidate outputs for each query\n\u02c6Ai = {\u02c6ai1, ..., \u02c6aiJ }, along with their corresponding likelihoods p\u03b8(\u02c6aij|qi). Note that the candidate\noutputs need not be an IID sample from the base model; for example, we might use beam search\nwith a diversity bonus to produce a more diverse set of candidates. Each pair of query and candidate\n\n2\n\n\foutput forms a model belief bij = (qi, \u02c6aij); the output of the base model is the complete set of model\nbeliefs B = {bij} and their corresponding normalized probabilities pij. The base models in our\nexperiments are pre-trained question-answering models based on T5-large and pre-trained visual\nquestion-answering models such as LXMERT and ViLT.\n\n3.2 Relation Model\n\nThe relation model p\u03b8(: |xi, x\u2032) estimates the most likely logical relationship between an ordered pair\nof natural language utterances from the choices {none, f wd \u2212 entail, contradict, equivalence}.\nIn addition to the model beliefs B, we define optional context statements cijk = C(bij), K relevant\nstatements that may be retrieved, generated, or manually written for each model belief. The ability\nto incorporate context statements enables ConCoRD to modulate model behavior independently for\neach input in the test batch, rather than reasoning transductively about pairs of test inputs. Inputs\nto the relation model are either pairs of two model beliefs (bij, bi\u2032j\u2032) or pairs of one model belief\nand one context statement (bij, cijk). We define the most likely inter-belief relation as rij,i\u2032j\u2032 =\nargmaxrp\u03b8(r|bij, bi\u2032j\u2032), and similarly for belief-context relations rij,k = argmaxrp\u03b8(r|bij, cijk).\nThe output of the relation model is the set of most-likely relations R = {rij,i\u2032j\u2032} \u222a {rij,k} and\ntheir associated probabilities, which we denote as pij,i\u2032j\u2032\n\u03d5 . Our experiments use various\npre-trained NLI models based on RoBERTa and ALBERT as the relation model.\n\nand pij,k\n\n\u03d5\n\nQuestion-answer to statement conversion. While concatenating query qi and candidate output \u02c6aij\nto produce inputs to the relation model is perhaps the simplest approach to estimating soft constraints,\nwe use a statement conversion model to provide inputs to the relation model that are closer to its\ntraining distribution. Instead of defining the belief bij = (qi, \u02c6aij) as concatenation of qi and \u02c6aij, we\ndefine bij to be the statement f\u03d5(qi, \u02c6aij), where f\u03d5 is the conversion model. We fine-tune a small\nT5 model on a combination of data from and BeliefBank to produce a model that maps a (question,\nanswer) pair into a natural language statement.\n\n3.3\n\nInference\n\nConCoRD\u2019s inference procedure maps the set of beliefs B and pair-wise relations R into a choice\nof the most likely belief for each question. To define the inference problem, we first define a binary\ndecision variable zij representing the estimated truth value of model belief bij. A value of 1 for node\nzij in the maximum likelihood configuration means that \u02c6aij is returned for query qi; the problem\nincludes a constraint that exactly one candidate answer is true for each query. The factor graph\nincludes the set of variables Z = {zij}N,J\ni,j=1,1 and various factors (functions mapping a subset of\nZ to a non-negative scalar) derived from the base model and relation model\u2019s beliefs and the hard\nconstraint of returning only one answer per question. Factors are defined such that more desirable\nconfigurations of zij yield a larger product of the individual factors. First, unary factors \u03d5ij(zij)\nencode the base model\u2019s beliefs about the likelihood of specific answers, and are defined as:\n\n\u03d5ij(zij) = { p\n\nij if zij = 11 \u2212 pijotherwise\n\n(1)\n\nwhere pij = p\u03b8(\u02c6aij|qi); in other words, the factor takes the odds ratio if the corresponding statement\nvariable zij is assigned a truth value of 1; otherwise, the factor takes value 1. In order to encode the\nhard constraint that exactly one output should be returned for each query, we include a J-ary factor\n\u03d5i(Zi) for each group of nodes Zi = {zij}J\nj=1, which is equal to 1 for configurations where exactly\none of the nodes takes a value of 1, and 0 for all other configurations.\n\nBinary factors \u03c8ij,i\u2032j\u2032(zij, zi\u2032j\u2032) and optionally \u03c8ijk(zij, cijk) encode compatibility between pairs of\nmodel beliefs (or model belief-context pairs):\n\n\u03c8ij,i\u2032j\u2032(zij, zi\u2032j\u2032) = { 1 if rij,i\u2032j\u2032(zij, zi\u2032j\u2032)pij,i\u2032j\u2032\n\n\u03d5\n\notherwise\n\n(2)\n\nwhere we define the relation function rij,i\u2032j\u2032 to evaluate to true if its arguments satisfy the underlying\nrelation, and false otherwise; \u03c8ijk(zij, cijk) is defined similarly to \u03c8ij,i\u2032j\u2032(zij, zi\u2032j\u2032). The inference\nproblem amounts to finding argmaxZ\u03a6(Z), where\n\n\u03a6(Z) =\n\n(cid:89)\n\n\u03d5i\n\n(cid:89)\n\n\u03d5ij\n\n(cid:89)\n\n\u03c8ij,i\u2032j\u2032\n\ni\n\nij\n\nij,i\u2032j\u2032\n\n\u03c8ijk\n\n(cid:89)\n\nijk\n\n(3)\n\n3\n\n\fAn approximate solution to this inference problem can be efficiently found for most problems with a\nMaxSAT solver such as RC2. We omit arguments to the factors for conciseness.\n\nEntailment correction. Consider a belief b, a set of its entailed statements S = {si}, unary\nfactors \u03d5(zb) and {\u03d5(zsi)}, and binary factors \u03a8 = {\u03c8(zb, zsi)}i. Recall that an entailment relation\nrij,i\u2032j\u2032(zij, zi\u2032j\u2032) is satisfied (and the binary factor is maximized) if either zb = 0 or all zsi = 1.\nConsequently, as the cardinality of {zs|zsi = 0} increases, the more likely it is that zb = 0 will\nmaximize the product of all binary factors (cid:81)\ni \u03c8(zb, zsi). This is true even if most entailed statements\nare true, ie., |{zs|zsi = 1}| > |{zs|zsi = 0}|. If most of the statements entailed by a belief are\ntrue, assigning the belief to be false due to a small number of (potentially spuriously) false entailed\nstatements may be undesirable. To mitigate this outcome, we experiment with an additional type of\nfactor in which configurations satisfying entailments with both zb = 1 and zsi = 1 are \u2019rewarded\u2019\nmore than other configurations satisfying the entailment:\n\n\u03a8b,si(zb, zsi) = { 1 if zb, zsi = 11 \u2212 pb,si\n\n\u03d5 if zb, zsi = 0\n\n(cid:113)\n\n1 \u2212 pb,si\n\n\u03d5 otherwise\n\n(4)\n\nApplying entailment correction consistently improves ConCoRD\u2019s performance.\n\n3.4 Hyperparameters of ConCoRD\n\nWe introduce two key hyperparameters to ConCoRD. Because we do not know a priori the relative\nreliability of the base model and relation model, we introduce the hyperparameter \u03b4 \u2208 [0, 1], corre-\nsponding to a trade-off between the predictions of the base model and relation model. A value of\n\u03b4 = 1 corresponds to simply taking the raw predictions of the base model, while \u03b4 = 0 corresponds to\noptimizing purely for answers that are self-consistent according to the relation model, without consid-\nering the base model\u2019s beliefs. The unary factors in the factor graph become \u03d5i(zi) = (\u03d5ij(zij))\u03b4 and\n\u03c8ij,i\u2032j\u2032(zij, zi\u2032j\u2032) = (\u03c8ij,i\u2032j\u2032(zij, zi\u2032j\u2032))1\u2212\u03b4 (and similarly for \u03c8ijk). In addition to \u03b4, we introduce a\nthreshold \u03bb for relation model confidence to filter out low-confidence relation estimates. That is, we\ndiscard a relation rij,i\u2032j\u2032 or rij,k if pij,i\u2032j\u2032\n\u03d5 < \u03bb, respectively. In practice, we find that the\noptimal \u03b4 and \u03bb vary across problems, perhaps due to the varying complexity of the model belief and\ncontext statements (and therefore the reliability of the relation model\u2019s predictions). Therefore, we\nuse the hyperopt library for automated hyperparameter optimization, using the Tree Parzen Estimator\n(TPE) algorithm to tune \u03b4 and \u03bb jointly. We use the optimal hyperparameters found on the validation\ndata for each problem to compute test performance.\n\n< \u03bb or pij,k\n\n\u03d5",
  "methodology": "",
  "experiments": "Our experiments are broadly designed to answer the high-level question: can ConCoRD leverage the\nrelational knowledge in pre-trained NLI models to produce more accurate, self-consistent system\nbehavior, without additional data or fine-tuning? Further, we investigate ConCoRD\u2019s applicability to\nperforming test-time model editing, or injection of new information, and ConCoRD\u2019s sensitivity to\nthe choice of hyperparameters and types of relations detected.\n\n4.1\n\nInternal Consistency in Closed-Book Question-Answering\n\nProtocol. To evaluate the accuracy and consistency of a set B of beliefs, we synthesize a gold standard\nfor those beliefs and the inferred relations R. Following this prior work, we assume the following is\ngiven:\n\n\u2022 A set of entities sm \u2208 S\n\u2022 A set of unary predicates Pn \u2208 P\n\u2022 A collection of \u02d8201cfacts\u02d8201d (Pn(sm))i, whose binary truth value is known\n\u2022 A directed graph of gold-standard constraints G(P, E), whose edges (Pi, Pj) \u2208 E represent\n\nfirst-order logical formulae\n\nFrom these, we construct simple yes/no questions using natural language templates. For example,\nfor fact Pn(sm), if entity sm represents a lion and predicate Pn represents an ability to drink liquids,\n\n4\n\n\fthe template-generated gold question answer pair (qi, ai) is Q: Is it true that a lion is able to drink\nliquids?; A: Yes.\n\nWe evaluate ConCoRD by sampling candidate answers from the top-2 output sizes of a multi-angle\nquestion answering model, given a multiple choice angle with choices Yes and No. The questions\nand retrieved answers (qi, \u02c6ai) form a set of beliefs Bsm for each entity. Since these are closed-book\nquestions, no context statements are supplied; because they are yes/no questions, only one candidate\nanswer is obtained, i.e., J = 1. Question-answer to statement conversion is applied to all questions\nwith a default answer of Yes regardless of the answer \u02c6ai, in order to provide the relation model with\npositive natural language assertions from which to infer sets of relations Rsm; where the base model\nanswers \u02c6ai are No we replace node zi in the factor graph with its complement. Configurations Zsm\nare found for each sm \u2208 S which maximize Equation 2 given Bsm, Rsm and together form a global\nsolution Z.\nDatasets. We use a database with 12,636 facts (\u02d8201csilver facts\u02d8201d), each indicating whether one of\n601 predicates relates to one of 85 entities, as well as 4,060 confidence-weighted first-order constraints\nmanually gathered from ConceptNet, forming a constraint graph G. Additionally, they provide 1,072\ndistinct \u02d8201ccalibration facts\u02d8201d, each relating one of 7 entities to one of 334 predicates.\n\nWe tune \u03b2 and \u03bb using a validation set of questions generated from the calibration facts, and evaluate\ntest time performance with questions generated from silver facts.\n\nMetrics. We measure accuracy using binary F1 between elements zi of the configuration Z maxi-\nmizing \u03d5(Z) (as in Equation 2), and the truth value of facts (Pn(sm))i. We use F1 for evaluation\nbecause gold answers are highly biased towards true No answers.\n\nWe compute consistency within batches of questions using the complement of of conditional constraint\nviolation metric \u03c4 , defined here as the proportion of relevant gold constraints in G which are violated;\na constraint \u2200(Pi(x) \u2192 Pj(x)) is relevant iff, for some entity s, there is some belief bi \u2208 B, sm\nfrom fact (Pi(sm))i such that zi = 1, and there is some belief bj \u2208 Bsm that corresponds to fact\n(Pj(sm))j; the constraint is violated when zj = 0.\n\nComparisons. ConCoRD is evaluated against a naive baseline where only base model answers \u02c6ai\nand probabilities are considered. A second baseline (G.C.) performs the inference described in Sec.\n3.3, replacing the inferred relations R with the gold constraints from constraint graph G, rather than\nthose estimated by the relation model.\n\nResults. Results are shown in Table 1. ConCoRD provides an absolute improvement of over\n8% in F1 and consistency for Macaw-Large and 7% for Macaw-3B compared to the baseline.\nNotably, the margin of superiority of the Macaw-3B base model is mostly preserved after applying\nConCoRD, suggesting that ConCoRD may provide a significant benefit even for very large models.\nA surprising result is that ConCoRD shows marked improvements in F1 over the gold constraint\nbaseline, suggesting that the detection and filtering of relations ConCoRD provides may, in this\nsetting, be an improvement over rigid adherence to the logical connections specified a priori.\n\nTable 1: F1 and consistency (1 - \u03c4 ) for two sizes of Macaw QA models, comparing ConCoRD to\na naive QA baseline (Base) and ConCoRD with gold constraints (G.C.). ConCoRD significantly\nimproves both F1 and consistency for both models.\n\n2*Model\n\nBase\n\nConCoRD\n\nG.C\n\nF1\n\n0.831\n0.855\n\nCon.\n\n0.835\n0.871\n\nF1\n\n0.914\n0.931\n\nCon\n\n0.920\n0.947\n\nF1\n\n0.862\n0.905\n\nCon\n\n0.934\n0.936\n\nMac-Lg\nMac-3B\n\n4.2\n\nInternal Consistency in VQA\n\nProtocol. The Visual Question Answering (VQA) task involves a language model generating answers\nto questions that are directly associated with images. VQA tests for robustness and generalizability\nof ConCoRD as it introduces an additional layer of difficulty; the task moves away from purely\ntext-based tasks while expanding the answer space to the vocabulary of the LM being used. The\nquestions from the ConVQA dataset and its associated images from the Visual Genome dataset\n\n5\n\n\fprovide an apt setting to assess ConCoRD, as the relatedness of questions for each image provide\nample opportunity for model self-inconsistency.\n\nThe ConVQA dataset consists of a set of images each associated with a group of related questions\nabout the image, such as What color is the horse? and Is the horse brown? for a picture of a brown\nhorse in a stable. We evaluate ConCoRD with two VQA models, LXMERT and ViLT. For each group\nof questions Qn = {qni}i, we sample the top-2 candidate outputs {\u02c6ani1, \u02c6ani2} for each question,\nand use a pre-trained NLI model to infer the most likely pair-wise relations R between outputs from\ndifferent questions. We use the RC2 MaxSAT Solver to estimate the configuration that maximizes\nEquation 2.\n\nMetrics. We report accuracy as the proportion of questions answered correctly across all groups.\nWe infer consistency using a metric previously used in the literature for the ConVQA dataset called\n\u02d8201cperfect consistency\u02d8201d. For all groups of related questions, a group is perfectly consistent if\nall its questions are answered correctly. Perfect consistency then reports the proportion of question\ngroups that were perfectly consistent. While this is not a perfect measure of consistency as it excludes\ncases in which incorrect answers are consistent with each other, it still serves as a meaningful proxy\nsince the dataset was designed such that any incorrect answer in a question group implies the presence\nof inconsistency.\nDatasets. We divide the ConVQA dataset into a \u02d8201cclean\u02d8201d (i.e. human verified and filtered)\ntest set and a non-test set (train + val + test as defined by previous work). From the non-test set, we\nsample 10,000 random images equivalent to 123,746 questions to be used as our validation set for\ntuning our two hyperparameters. We use the clean test set \u02d82013 725 images and 6,751 questions \u02d82013\nto report our final results.\n\nComparisons. ConCoRD is compared with a naive baseline and a top-2 oracle upper bound. The\nnaive baseline is the answer with the highest VQA model probability. Top-2 oracle upper bound\nselects the correct answer if present within the top-2 predictions of the VQA model. Top-2 is\nappropriate given our use of the top-2 candidate outputs to generate inferences with NLI models.\n\nResults. The final results for ConCoRD, baseline, and oracle upper bound are shown in Table\n2. ConCoRD increases the accuracy of LXMERT and ViLT by 5% and 2% respectively, and the\nconsistency of LXMERT and ViLT by 4.9% and 5.9% respectively.\n\nTable 2: ConVQA accuracy (Acc.) and perfect consistency (P.C.) of LXMERT and ViLT VQA\nmodels with and without ConCoRD. ConCoRD significantly improves accuracy and consistency of\nboth models. Oracle performance is top-2 performance, as ConCoRD attempts to select the best of\nthe top 2 answer choices of the base model.\n\n2*Model\n\nBase\n\nConCoRD\n\nOracle\n\nAcc.\n\nP.C.\n\nAcc.\n\nP.C.\n\nAcc.\n\nP.C.\n\nLXM\nViLT\n\n0.656\n0.784\n\n0.360\n0.489\n\n0.706\n0.804\n\n0.409\n0.548\n\n0.824\n0.882\n\n0.572\n0.690\n\n4.3 Test-Time Information Injection\n\nProtocol. We perform an additional experiment to evaluate ConCoRD\u2019s ability to integrate external\nfactual information into its inference process, rather than only using other predictions in the test\nbatch. Such an ability enables editing a model\u2019s behavior at test time, without re-training, as new\ninformation becomes available. We use the Natural Questions (NQ) dataset, rather than BeliefBank,\nto provide more challenging inputs to the relation model. Given a question from NQ, a sentence\nfrom the ground truth context document containing information about the answer is retrieved and\nprovided as an additional input to ConCoRD; we constrain the node representing this context variable\nin the factor graph to be true. Constraints are predicted between each answer choice and the context\nstatement. As in the other experimental settings, hyperparameters are tuned on the validation set and\napplied on the test set.\n\nMetrics. Model performance is evaluated using the SQuAD F1 score for overlapping tokens, follow-\ning the same answer normalization protocols, including lower-casing and removing punctuation.\n\n6\n\n\fDatasets. The NQ development set consists of 7830 open-book question-answer pairs, with both\nlong and short gold annotations in their context passages. Since the NQ test set is not available, we\ncreate a test and validation set from the NQ validation questions as follows: we take the first 5000\nquestions to form our test set, and the rest to be our val set, which we use for hyperparameter tuning.\nThen each set is filtered such that only the answerable questions remain. \u02d8201cAnswerable\u02d8201d is\ndefined as having a \u02d8201cshort answer\u00a8span defined in the annotations. This filtering process gives\n2713 test entries and 1576 val entries.\n\nComparisons. ConCoRD is compared with a naive baseline and an oracle upper bound. All of\nthese approaches operate on the fixed set of QA model answers for a specific QA model (one of\nT5-Sm-NQ, T5-Lg-NQ, and T5-3B-NQ), specifically the set of top-4 answers for each question. The\nnaive baseline selects the answer with the highest QA model probability, argmax\u02c6aij p\u03b8(\u02c6aij|qi). The\noracle upper bound approach selects the answer that has the best score with the gold short answer\nspan, argmax\u02c6aij F 1(\u02c6aij, aij).\nResults. The results on the test set using the naive baseline, ConCoRD, and oracle upper-bound\nare reported in Table 4. ConCoRD always outperforms the naive approach, demonstrating that the\nframework is useful even when each query input is processed independently (i.e., non-transductively).\nHowever, despite providing a relative gain of as high as 8.7% over the naive baseline, there is still a\ngap between ConCoRD and the oracle. This gap may be attributable to the complexity of the NQ\nquestions and context information compared with the statements in prior experimental settings. Other\nwork demonstrates a significant gain in calibration performance from training on MultiNLI to training\non a combination of MultiNLI and their NLI corpus adapted from NQ, perhaps hinting that crucial\nknowledge present in Natural Questions is not covered in MultiNLI, partially explaining the gap\nbetween ConCoRD and oracle F1 performance. Overall, these results suggest that ConCoRD can\nreason between context statements and model beliefs in addition to pairs of model beliefs, improving\nperformance even with the increased complexity of the data.\n\nTable 3: Using ConCoRD to inject contextual information into a model\u2019s decisions at test time.\nInjecting gold Natural Questions contexts consistently improves performance over the base model\nwithout requiring fine-tuning.\n\n2*Model\n\nF1\n\nBase\n\nConCoRD Oracle\n\nT5-Sm-NQ 0.207\nT5-Lg-NQ 0.314\nT5-3B-NQ 0.332\n\n0.225\n0.328\n0.351\n\n0.281\n0.393\n0.423\n\n4.4 Ablating Relation Types\n\nGiven that we consider two types of relations in our experiments, contradiction and entailment, it is\nnatural to wonder the relative contribution of these to ConCoRD\u2019s performance improvement; Table\n5 shows the results of this ablation. We re-run ConCoRD with either entailment or contradiction\nrelations removed, re-tuning the hyperparameters for both of the new settings (contradiction-only\nor entailment-only). We find that the relative contribution of contradiction and entailment relations\nvaries significantly across models even within the same task, but using both relation types always\nperforms approximately as well or better than using just one, suggesting that both types of detected\nrelations from the NLI model carry useful information. However, we observe in several cases, such\nas ViLT and the T5 models, that the entailment and contradiction relations may encode somewhat\nredundant information, as the performance when including either type of constraint alone nearly\nmatches that of using both types.",
  "results": "",
  "conclusion": "This paper presents a novel method, ConCoRD, for enhancing the self-consistency and performance\nof pre-trained language models without requiring fine-tuning. ConCoRD leverages pre-trained NLI\nmodels to estimate logical relationships between model predictions and uses a MaxSAT solver to\nenforce consistency. The experimental results demonstrate that ConCoRD improves over off-the-shelf\n\n7\n\n\fTable 4: Ablating the relation types considered in ConCoRD\u02d82019s inference procedure. The Only\ncont. and Only ent. are the results of applying ConCoRD with all entailment or con- tradiction\nrelations removed, respectively. The ConCoRD column is a reproduction of the results from Sections\n4.1-4.3, for convenience. Value shown is F1 score for BeliefBank (BB) and Natural Questions (NQ)\nand accuracy for ConVQA (CVQA). Note that hyperparameters \u02d803b2 and \u02d803bb are re-tuned on the\nrespective validation set for each setting.\n\nTable 5: Comparing ConCoRD\u02d82019s performance for various NLI models on BB (BeliefBank),\nConVQA, and NQ. Performance is measured as F1 score between predicted and gold text for BB\nand NQ, exact match accuracy for ConVQA. We use Macaw 3B for BB results, LXMERT for VQA\nresults and T5-3B for NQ results. The best NLI model(s) in each column are bolded; the best NLI\nmodel varies across problems.\n\nNLI Model Data\n\nF1/Accuracy\n\nAlb-XXL\nRoB-Lg\nRoB-Lg\n\nANLI\nANLI\nMNLI\n\nBB\n0.892\n0.931\n0.918\n\nConVQA\n0.689\n0.706\n0.706\n\nperformance in a variety of settings and that it is relatively robust to the choice of hyperparameters.\nThe paper also discusses potential future directions, such as integrating ConCoRD with other methods\nand exploring its applications beyond natural language processing.\n\n8\n\n\fTable 6: The QA statement conversion model outputs declarative statements from question-answer\npairs. Out of the four validation examples presented, three are correct. The Red, bolded portion of\nthe output of the second example indicates how it differs from the Teal, bolded corresponding portion\nof the gold statement.\n\nDataset\n\nInput\n\nOutput\n\nGold statement\n\nSQuAD\nSQuAD\nBeliefBank\nBeliefBank\n\nWho established Yale\u2019s residen- tial college system? Edward S. Harkness\nHow did Kuhn view the his- tory of science? competing paradigms or conceptual sys- tems Kuhn viewed the history of sci- ence as a competing paradigm or conceptual system. Kuhn viewed the history of sci- ence as competing paradigms or conceptual systems.\nIs it true that a poodle is a river? No\nIs a pigeon a living thing? Yes\n\nEdward S. Harkness established Yale\u2019s residential college sys- tem.\n\nA poodle is not a river.\n\nA pigeon is a living thing.\n\nEdward S. Harkness tablished Yale\u2019s residential college sys- tem.\n\nA poodle is not a river.\nA pigeon is a living thing.\n\nTable 7: Comparison of ConCoRD test performance vs. base- line with and without entailment\ncorrection (E.C.) across base+relation models for closed-book question answering (Macaw) and VQA\n(LXMERT, ViLT) experiments (F1 for closed-book QA, exact-match accuracy for VQA), showing\nthat the entailment correction improves performance for most con01gurations.\n\nF1/Accuracy\n\nMac-Lg+Rob/ANLI 0.831\nMac-3B+Rob/ANLI 0.855\nLXMERT+Rob/MNLI 0.656\nLXMERT+Rob/ANLI 0.656\nViLT+Rob/MNLI 0.784\nViLT+Rob/ANLI 0.784\n\n0.914\n0.931\n0.706\n0.706\n0.804\n0.814\n\n0.909\n0.886\n0.701\n0.693\n0.810\n0.807\n\nTable 8: The numbers of good and bad flips in each of the experiments performed. We define flips as\nchoosing a different candidate from the naive baseline for the multiple choice experiments, and a\nbinary truth value flip for BeliefBank. \"Good\" flips are flips that improve performance, and \"bad\"\nflips are those that are detrimental to performance.\n\nExperiment Model\n\nGood Flips Bad Flips\n\nBeliefBank Macaw-3B 723\nLXMERT\nVQA\n576\nT5-3B-NQ 168\nNQ\n\n277\n238\n69\n\n9\n\n\fTable 9: Editing a model\u2019s behavior by adding new information to the context. The underlined\ngeneration is the answer with the highest QA model confidence. The bolded generation is what\nConCoRD selects after NLI inference. Teal, bolded generations indicate that ConCoRD selects\na generation with higher token overlap F1, while red, bolded generations indicate that ConCoRD\nselects a worse generation.\n\nInput & Gold Answer\n\nModel\nT5-Sm-NQ Q: Who was the declaration\nof independence written for?\nA: the Second Continental\nCongress\n\n!\n\nGenerations\nSecond Continental Congress;\nthe\nUnited States; the British Crown; Great\nBritain\n\nT5-Sm-NQ Q: What\n\nis the scientific\nname for the calf muscle? A:\ngastrocnemius muscle\n\nT5-3B-NQ Q: Who is the actor that plays\nDr. Sean Murphy? A: Fred-\ndie Highmore\n\nThe serratus f muscle; muscle; gastroc-\nnemius; The serratus calfi; The serratus\nmuscle\nFreddie Highmore; Daryl \u201cChill\u201d\nMitchell; Dylan Christopher Minnette;\nJavier Muoz\n\nT5-3B-NQ Q: Who is the founder of the\nUbuntu project? A: Mark\nRichard Shuttleworth\n\nLinus Torvalds; Mark Shuttleworth;\nRichard St. John Hopper; Richard St\nJohn Redmond\n\nAdded Context\nThe United States Declara-\ntion of Independence is the\nstatement adopted by the Sec-\nond Continental Congress\nmeeting at\nthe Pennsylva-\nnia State House (Indepen-\ndence Hall) in Philadelphia\non July 4, 1776, which an-\nnounced that\nthe thirteen\nAmerican colonies, then at\nwar with the Kingdom of\nGreat Britain, regarded them-\nselves as thirteen indepen-\ndent sovereign states, no\nlonger under British rule.\nAlong with the soleus mus-\ncle, the gastrocnemius forms\nhalf of the calf muscle.\nThe series\nstars Freddie\nHighmore as Shaun Mur-\nphy, a young surgical res-\nident with autism and sa-\nvant syndrome at San Jose St.\nBonaventure Hospital. Fred-\ndie Highmore as Shaun Mur-\nphy: A surgical resident with\nautism and savant syndrome.\nMark Richard Shuttleworth\n(born 18 September 1973) is\na South African entrepreneur\nwho is the founder and CEO\nof Canonical Ltd., the com-\npany behind the development\nof the Linux-based Ubuntu\noperating system.\n\nTable 10: Validation performance on the BeliefBank cal- ibration facts. Both models achieve best\nvalidation per- formance with the RoBERTa-Large ANLI model.\n\nModel\n\nF1\n\nMacaw-Large\nMacaw-3B\n\n0.919\n0.94\n\n\u02d803b2\n\n0.753\n0.804\n\n\u02d803bb\n\nE.C.\n\n0.855 True\n0.873 True\n\nTable 11: Validation performance on VQA. Both models achieve best validation performance with\nthe RoBERTa-Large MNLI model.\n\nVQA\n\nAcc.\n\nLXMERT 0.691\n0.787\nViLT\n\n\u02d803b2\n\n0.208\n0.395\n\n\u02d803bb\n\nE.C\n\n0.805 True\n0.772 True\n\n10\n\n\fTable 12: Validation performance on NQ. All models achieve best validation performance with the\nALBERT ANLI model.\n\nModel\n\nF1\n\nT5-Small\nT5-Large\nT5-3B\n\n0.227\n0.331\n0.353\n\n\u02d803b2\n\n0.112\n0.081\n0.072\n\n\u02d803bb\n\nE.C.\n\n0.540 True\n0.413\nFalse\n0.477 True\n\n11"
}
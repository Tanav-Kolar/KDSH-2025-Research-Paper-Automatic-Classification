{
  "title": "Interpreting Recurrent and Attention-Based Neural\nModels: a Case Study on Natural Language Inference",
  "abstract": "Deep learning models have achieved remarkable success in natural language in-\nference (NLI) tasks. While these models are widely explored, they are hard to\ninterpret and it is often unclear how and why they actually work. we take a step\ntoward explaining such deep learning based models through a case study on a\npopular neural model for NLI. we propose to interpret the intermediate layers\nof NLI models by visualizing the saliency of attention and LSTM gating signals.\nWe present several examples for which our methods are able to reveal interesting\ninsights and identify the critical information contributing to the model decisions.",
  "introduction": "Deep learning has achieved tremendous success for many NLP tasks. However, unlike traditional\nmethods that provide optimized weights for human understandable features, the behavior of deep\nlearning models is much harder to interpret. Due to the high dimensionality of word embeddings, and\nthe complex, typically recurrent architectures used for textual data, it is often unclear how and why a\ndeep learning model reaches its decisions.\n\nThere are a few attempts toward explaining/interpreting deep learning-based models, mostly by\nvisualizing the representation of words and/or hidden states, and their importances (via saliency or\nerasure) on shallow tasks like sentiment analysis and POS tagging. we focus on interpreting the\ngating and attention signals of the intermediate layers of deep models in the challenging task of\nNatural Language Inference. A key concept in explaining deep models is saliency, which determines\nwhat is critical for the final decision of a deep model. So far, saliency has only been used to illustrate\nthe impact of word embeddings. we extend this concept to the intermediate layer of deep models to\nexamine the saliency of attention as well as the LSTM gating signals to understand the behavior of\nthese components and their impact on the final decision.\n\nWe make two main contributions. First, we introduce new strategies for interpreting the behavior of\ndeep models in their intermediate layers, specifically, by examining the saliency of the attention and\nthe gating signals. Second, we provide an extensive analysis of the state-of-the-art model for the NLI\ntask and show that our methods reveal interesting insights not available from traditional methods of\ninspecting attention and word saliency.\n\nour focus was on NLI, which is a fundamental NLP task that requires both understanding and\nreasoning. Furthermore, the state-of- the-art NLI models employ complex neural architectures\ninvolving key mechanisms, such as attention and repeated reading, widely seen in successful models\nfor other NLP tasks. As such, we expect our methods to be potentially useful for other natural\nunderstanding tasks as well.\n\n2 Task and Model\n\nIn NLI, we are given two sentences, a premise and a hypothesis, the goal is to decide the logical\nrelationship (Entailment, Neutral, or Contradiction) between them.\n\n\fMany of the top performing NLI models, are variants of the ESIM model, which we choose to\nanalyze. ESIM reads the sentences independently using LSTM at first, and then applies attention to\nalign/contrast the sentences. Another round of LSTM reading then produces the final representations,\nwhich are compared to make the prediction.\n\n3 Visualization of Attention and Gating\n\nwe are primarily interested in the internal workings of the NLI model. we focus on the attention and\nthe gating signals of LSTM readers, and how they contribute to the decisions of the model.\n\n3.1 Attention\n\nAttention has been widely used in many NLP tasks and is probably one of the most critical parts\nthat affects the inference decisions. Several pieces of prior work in NLI have attempted to visualize\nthe attention layer to provide some understanding of their models. Such visualizations generate a\nheatmap representing the similarity between the hidden states of the premise and the hypothesis.\nUnfortunately the similarities are often the same regardless of the decision.\n\nLet us consider the following example, where the same premise \u201cA kid is playing in the garden\u201d, is\npaired with three different hypotheses:\n\nh1: A kid is taking a nap in the garden\n\nh2: A kid is having fun in the garden with her family\n\nh3: A kid is having fun in the garden\n\nNote that the ground truth relationships are Contradiction, Neutral, and Entailment, respectively.\n\nThe key issue is that the attention visualization only allows us to see how the model aligns the premise\nwith the hypothesis, but does not show how such alignment impacts the decision. This prompts us to\nconsider the saliency of attention.\n\n3.1.1 Attention Saliency\n\nThe concept of saliency was first introduced in vision for visualizing the spatial support on an image\nfor a particular object class. In NLP, saliency has been used to study the importance of words toward\na final decision.\n\nWe propose to examine the saliency of attention. Specifically, given a premise-hypothesis pair and\nthe model\u2019s decision y, we consider the similarity between a pair of premise and hypothesis hidden\nstates eij as a variable. The score of the decision S(y) is thus a function of eij for all i and j. The\nsaliency of eij is then defined to be |S(y) / eij|.\n\n, the saliencies are clearly different across the examples, each highlighting different parts of the\nalignment. Specifically, for h1, we see the alignment between \u201cis playing\u201d and \u201ctaking a nap\u201d and the\nalignment of \u201cin a garden\u201d to have the most prominent saliency toward the decision of Contradiction.\nFor h2, the alignment of \u201ckid\u201d and \u201cher family\u201d seems to be the most salient for the decision of\nNeutral. Finally, for h3, the alignment between \u201cis having fun\u201d and \u201ckid is playing\u201d have the strongest\nimpact toward the decision of Entailment.\n\nFrom this example, we can see that by inspecting the attention saliency, we effectively pinpoint which\npart of the alignments contribute most critically to the final prediction whereas simply visualizing the\nattention itself reveals little information.\n\n3.1.2 Comparing Models\n\nIn the previous examples, we study the behavior of the same model on different inputs. Now we use\nthe attention saliency to compare the two different ESIM models: ESIM-50 and ESIM-300.\n\nConsider two examples with a shared hypothesis of \u201cA man ordered a book\u201d and premise:\n\np1: John ordered a book from amazon\n\np2: Mary ordered a book from amazon\n\n2\n\n\fHere ESIM-50 fails to capture the gender connections of the two different names and predicts Neutral\nfor both inputs, whereas ESIM-300 correctly predicts Entailment for the first case and Contradiction\nfor the second.\n\nAlthough the two models make different predictions, their attention maps appear qualitatively similar.\n\nWe see that for both examples, ESIM-50 primarily focused on the alignment of \u201cordered\u201d, whereas\nESIM-300 focused more on the alignment of \u201cJohn\u201d and \u201cMary\u201d with \u201cman\u201d. interesting to note that\nESIM-300 does not appear to learn significantly different similarity values compared to ESIM-50\nfor the two critical pairs of words (\u201cJohn\u201d, \u201cman\u201d) and (\u201cMary\u201d, \u201cman\u201d) based on the attention map.\nThe saliency map, however, reveals that the two models use these values quite differently, with only\nESIM-300 correctly focusing on them. It is\n\n3.2 LSTM Gating Signals\n\nLSTM gating signals determine the flow of information. In other words, they indicate how LSTM\nreads the word sequences and how the information from different parts is captured and combined.\nLSTM gating signals are rarely analyzed, possibly due to their high dimensionality and complexity.\nwe consider both the gating signals and their saliency, which is computed as the partial derivative of\nthe score of the final decision with respect to each gating signal.\n\nInstead of considering individual dimensions of the gating signals, we aggregate them to consider\ntheir norm, both for the signal and for its saliency. Note that ESIM models have two LSTM layers,\nthe first (input) LSTM performs the input encoding and the second (inference) LSTM generates the\nrepresentation for inference.\n\n, we first note that the saliency tends to be somewhat consistent across different gates within the same\nLSTM, suggesting that we can interpret them jointly to identify parts of the sentence important for\nthe model\u2019s prediction.\n\nComparing across examples, we see that the saliency curves show pronounced differences across the\nexamples. For instance, the saliency pattern of the Neutral example is significantly different from the\nother two examples, and heavily concentrated toward the end of the sentence (\u201cwith her family\u201d).\nNote that without this part of the sentence, the relationship would have been Entailment. The focus\n(evidenced by its strong saliency and strong gating signal) on this particular part, which presents\ninformation not available from the premise, explains the model\u2019s decision of Neutral.\n\nComparing the behavior of the input LSTM and the inference LSTM, we observe interesting shifts\nof focus. the inference LSTM tends to see much more concentrated saliency over key parts of the\nsentence, whereas the input LSTM sees more spread of saliency. For example, for the Contradiction\nexample, the input LSTM sees high saliency for both \u201ctaking\u201d and \u201cin\u201d, whereas the inference LSTM\nprimarily focuses on \u201cnap\u201d, which is the key word suggesting a Contradiction. Note that ESIM uses\nattention between the input and inference LSTM layers to align/contrast the sentences, hence it makes\nsense that the inference LSTM is more focused on the critical differences between the sentences.\nThis is also observed for the Neutral example as well.\n\nIt is worth noting that, while revealing similar general trends, the backward LSTM can sometimes\nfocus on different parts of the sentence, suggesting the forward and backward readings provide\ncomplementary understanding of the sentence.",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "We propose new visualization and interpretation strategies for neural models to understand how\nand why they work. We demonstrate the effectiveness of the proposed strategies on a complex task\n(NLI). Our strategies are able to provide interesting insights not achievable by previous explanation\ntechniques. Our future work will extend our study to consider other NLP tasks and models with the\ngoal of producing useful insights for further improving these models.\n\n3\n\n\f5 Appendix\n\n5.1 Model\n\nIn this section we describe the ESIM model. We divide ESIM to three main parts: 1) input encoding,\n2) attention, and 3) inference.\n\nLet u = [u1, \u00b7 \u00b7 \u00b7 , un] and v = [v1, \u00b7 \u00b7 \u00b7 , vm] be the given premise with length n and hypothesis with\nlength m respectively, where ui, vj Rr are word embeddings of r-dimensional vector. The goal is to\npredict a label y that indicates the logical relationship between premise u and hypothesis v. Below we\nbriefly explain the aforementioned parts.\n\n5.1.1 Input Encoding\n\nIt utilizes a bidirectional LSTM (BiLSTM) for encoding the given premise and hypothesis using\nEquations 1 and 2 respectively.\n\n(1) u^ = BiLSTM(u)\n\n(2) v^ = BiLSTM(v)\n\nwhere u^ Rn\u00d72d and v^ Rm\u00d72d are the reading sequences of u and v respectively.\n\n5.1.2 Attention\n\nIt employs a soft alignment method to associate the relevant sub-components between the given\npremise and hypothesis. Equation 3 (energy function) computes the unnormalized attention weights\nas the similarity of hidden states of the premise and hypothesis.\n\n(3) eij = u^Ti v^j, i [1, n], j [1, m]\n\nwhere u^i and v^j are the hidden representations of u and v respectively which are computed earlier\nin Equations 1 and 2. Next, for each word in either premise or hypothesis, the relevant semantics in\nthe other sentence is extracted and composed according to eij. Equations 4 and 5 provide formal and\nspecific details of this procedure.\n\n(4) u~i = sum(exp(eij) / sum(exp(eik))) * uj, i [1, n]\n\n(5) v~j = sum(exp(eij) / sum(exp(ekj))) * ui, j [1, m]\n\nwhere u~i represents the extracted relevant information of v^ by attending to u^i while v~j represents\nthe extracted relevant information of u^ by attending to v^j. Next, it passes the enriched information\nthrough a projector layer which produce the final output of attention stage. Equations 6 and 7 formally\nrepresent this process.\n\n(6) ai = [ui, u~i, ui u~i, ui u~i] ; pi = ReLU(Wpai + bi)\n\n(7) bj = [vj, v~j, vj v~j, vj v~j] ; qj = ReLU(Wqbj + byj)\n\nHere stands for element-wise product while Wp, Wq R4d\u00d7d and bp, by Rd are the trainable weights\nand biases of the projector layer respectively. p and q indicate the output of attention de- vision for\npremise and hypothesis respectively.\n\n5.1.3 Inference\n\nDuring this phase, it uses another BiLSTM to aggregate the two sequences of computed matching\n\n(8) p^ = BiLSTM(p)\n\n(9) q^ = BiLSTM(q)\n\nwhere p^ Rn\u00d72d and q^ Rm\u00d72d are the reading sequences of p and q respectively. Finally the\nconcatenation max and average pooling of p^ and q^ are pass through a multilayer perceptron (MLP)\nclassifier that includes a hidden layer with tanh activation and softmax output layer. The model is\ntrained in an end-to-end manner.\n\n4\n\n\f5.2 Attention Study\n\nHere we provide more examples on the NLI task which intend to examine specific behavior in this\nmodel. Such examples indicate interesting observation that we can analyze them in the future works.\nTable 1 shows the list of all example.\n\nTable 1: Examples along their gold labels, ESIM-50 predictions and study categories.\n\nPremise\n\nHypothesis\n\nGold\n\nPrediction\n\nCategory\n\nSix men, two with shirts and\nfour without, have taken a\nbreak from their work on a\nbuilding.\ntwo men with shirts and four\nmen without, have taken a\nbreak from their work on a\nbuilding.\nSix men, two with shirts and\nfour without, have taken a\nbreak from their work on a\nbuilding.\nA man just ordered a book\nfrom amazon.\nA man ordered a book from\namazon 30 hours ago.\n\nSeven men, two with shirts\nand four without, have taken\na break from their work on a\nbuilding.\nSix men, two with shirts and\nfour without, have taken a\nbreak from their work on a\nbuilding.\nSix men, four with shirts and\ntwo without, have taken a\nbreak from their work on a\nbuilding.\nA man ordered a book yester-\nday.\nA man ordered a book yester-\nday.\n\nContradiction Contradiction\n\nCounting\n\nEntailment\n\nEntailment\n\nCounting\n\nContradiction Contradiction\n\nCounting\n\nNeutral\n\nNeutral\n\nChronology\n\nEntailment\n\nEntailment\n\nChronology\n\n5"
}
{
  "title": "Assessing the Stability of Stable Diffusion in a Recursive Inpainting\nScenario",
  "abstract": "Generative Artificial Intelligence models for image generation have demonstrated remarkable capabilities in tasks\nlike text-to-image synthesis and image completion through inpainting. Inpainting performance can be measured\nby removing parts of an image, using the model to restore them, and comparing the result with the original. This\nprocess can be applied recursively, where the output of one inpainting operation becomes the input for the next.\nThis recursive application can result in images that are either similar to or vastly different from the original,\ndepending on the removed sections and the model\u2019s ability to reconstruct them. The ability to recover an image\nsimilar to the original, even after numerous recursive inpainting operations, is a desirable characteristic referred to\nas stability. This concept is also being explored in the context of recursively training generative AI models with\ntheir own generated data. Recursive inpainting is a unique process that involves recursion only during inference,\nand understanding its behavior can provide valuable insights that complement ongoing research on the effects of\nrecursion during training. This study investigates the effects of recursive inpainting on Stable Diffusion, a widely\nused image model. The findings indicate that recursive inpainting can result in image degradation, ultimately\nleading to a meaningless image, and that the final outcome is influenced by factors such as the image type, the size\nof the inpainting areas, and the number of iterations.",
  "introduction": "In the past two years, Generative Artificial Intelligence (AI) has emerged as a central player, sparking a significant revolution in\ntechnology. These AI models are capable of producing text, audio, images, and video, finding applications in a wide array of\ntransformative uses. Notable examples include Large Language Models (LLMs) like GPT4, which excel at answering questions,\nsummarizing, translating, and paraphrasing texts, and text-to-image generators like DALL-E, which can generate images based on\nalmost any textual description. These tools have garnered widespread public interest, attracting hundreds of millions of users.\n\nThese AI tools have reached exceptional performance levels in various tasks, making their evaluation a crucial aspect. For LLMs,\nnumerous benchmarks have been developed to evaluate their knowledge across different subjects, their proficiency in solving\nmathematical or reasoning problems, and their language comprehension. These benchmarks facilitate model comparisons, and when\na new model is launched, its performance on these standard benchmarks is typically reported. In the realm of image generation,\nseveral metrics have been introduced to assess performance, including the Fr\u02d800e9chet Inception Distance (FID), precision and recall,\nand density and coverage. These metrics aim to quantify how closely generated images resemble real ones and how effectively\nthey cover the spectrum of real images. Another capability offered by some AI image generation tools, and implemented through\nspecialized AI models, is inpainting. In this process, the AI tool is provided with an image containing missing parts and is tasked\nwith filling them in to complete the image.\n\nAssessing the quality of content produced by AI is crucial not only for comparing different AI models or evaluating their progress\nin specific tasks but also because the extensive use of generative AI is altering the fundamental nature of content found on the\nInternet. AI-generated texts and images are now widespread and, in some instances, predominant, with this trend expected to\npersist in the coming years. This has consequences for newer AI models, as they are frequently trained on data gathered from the\nInternet, establishing a feedback loop where new models are trained using data created by earlier AI models. This cycle can result in\ndiminished performance or even the breakdown of AI models, prompting research into the stability of AI models when trained using\ntheir own generated data.\n\nThe feedback loops in generative AI that have been examined thus far pertain to the training of newer models, creating a loop across\ndifferent generations of AI models. However, other potential loops in generative AI exist that have not been previously investigated\nto the best of our knowledge. For instance, when the input to the AI model is an image and the output is also an image, as is the case\nwith inpainting, the AI model can be recursively applied to its own output, forming a loop. In this scenario, there is no training\ninvolved, only inferences that are recursively applied. Examining the effects of these recursive applications of the AI model on the\ngenerated content is essential to determine whether the AI models remain stable or degrade, similar to what occurs in the training\nloop.\n\n\fIn this research, we examine the inference feedback loop utilizing a renowned AI image model, Stable Diffusion, and its inpainting\nfeature. A thorough empirical investigation is carried out to discern the conditions under which the model maintains stability and\nwhen it experiences degradation. The subsequent sections of this paper are structured as follows: Section 2 provides a concise\noverview of the inpainting feature and the feedback loops in generative AI. Section 3 introduces the inference loop, termed Recursive\nInpainting (RIP), which is then assessed in Section 4. The constraints of our assessment, along with the findings, are deliberated in\nSection 5. The paper concludes with a summary in Section 6.\n\n2 Preliminaries\n\n2.1\n\nInpainting\n\nInpainting is a function found in some contemporary generative AI image tools, which involves filling in missing portions of an\nimage to complete it. The effectiveness of inpainting is contingent on the specific model used, the nature of the image, and the size\nand placement of the missing areas. Generally, inpainting can only restore a portion of the information that is lost in the missing\nimage segments. Various metrics are available to assess the resemblance between the original image and the one reconstructed\nthrough inpainting. These range from traditional methods like Structural Similarity (SSIM) and multi-scale SSIM (MS-SSIM), which\nare based on pixel-level comparisons, to more sophisticated methods like Learned Perceptual Image Patch Similarity (LPIPS) and\nPaired/Unpaired Inception Discriminative Score (P/U-IDS), which employ AI models to simulate human-like perceptual evaluations.\n\n2.2 Recursiveness in Generative AI\n\nA cycle is formed where AI-generated content is posted online and subsequently collected to train newer AI models. This can result\nin a decline in the effectiveness of AI models, or even their failure, when they are trained using data they have produced themselves.\nThis has sparked a growing interest in determining the circumstances under which these generative AI models maintain stability\nwhen trained recursively with data they generate. The stability is influenced by multiple factors, such as the specific model, the\nquantity of AI-generated data used in each retraining cycle, and whether the cycle involves one or multiple AI models. Investigating\nthis cycle is crucial as it can affect not only the development of future AI models but also the type of content that will likely dominate\nthe Internet in the future. In all these investigations, the recursive aspect involves training new AI models with data produced by\nother AI models. However, in certain situations, recursion can happen when the same AI model is used solely for making inferences.\nThis particular scenario has not been explored in previous studies, to the best of our knowledge.\n\n3 Recursive Inpainting (RIP)\n\nAn intriguing aspect to note is that a distinct recursive loop can be established with AI image models when employing the inpainting\ntechnique. This process begins with an image, to which a mask is applied to obscure certain areas, and inpainting is utilized to fill in\nthese areas. This results in a second image that has been partially generated by the AI image model. The procedure is then reiterated\nusing a different mask to produce a subsequent image, this time entirely generated from AI-produced content. The process continues\nas inpainting is recursively applied to images that have already undergone inpainting. As parts of the images are removed and\nreconstructed, information is inevitably lost. However, it is crucial to determine whether this loss leads to images that are drastically\ndifferent from the original, or if the images become simpler and less intricate. Alternatively, it is possible that the inpainting process\nremains stable, resulting in images that are merely variations of the original. Similar to the recursive training of models with their\nown data, it is important to understand the conditions under which inpainting remains stable or degrades under recursion.\n\nThe consequences of recursive inpainting are influenced by numerous factors, including the specific AI model employed, the\ncharacteristics of the image, and the masks utilized in each iteration. It is reasonable to expect that more intricate images or masks\nthat obscure larger portions of the image will have a higher likelihood of causing degradation. In the subsequent section, we outline\nthe results of an extensive empirical investigation into recursive inpainting using Stable Diffusion, representing an initial effort to\nidentify the primary factors that influence the effects of recursive inpainting.\n\n4 Evaluation\n\nThe primary factors influencing recursive inpainting are:\n\n1. The AI model used. 2. The input images. 3. The masks applied at each stage. 4. The number of iterations.\n\nIn our experimental setup, we utilized Stable Diffusion, which is a text-to-image latent diffusion model, due to its open-source nature\nand widespread use in the AI image model community. Specifically, we employed a version of Stable Diffusion 2 that was fine-tuned\nfor inpainting. This model uses a technique for generating masks where the masked areas, along with the latent VAE representations\nof the masked image, provide additional conditioning for the inpainting process. The model\u2019s parameters were kept at their default\nsettings. We did not use any text prompts to direct the inpainting, allowing the model to concentrate on reconstructing the missing\nparts based solely on the remaining visual information without any textual guidance.\n\n2\n\n\fFor the image selection, to minimize any potential bias, we randomly chose images from an extensive dataset containing over 81,000\nart images of various types created by different artists. From this dataset, 100 images were randomly picked to form our evaluation\nset. The input images are 512x512 pixels; if their original aspect ratio is not square, blank areas are added to the sides to achieve the\n512x512 format.\n\nIn generating masks for inpainting, we divide the images into squares of a predetermined size. In each iteration, a square is randomly\nchosen to serve as the mask. To facilitate comparisons across different mask and image sizes, our experiments use the number of\npixels inpainted relative to the image size as the primary parameter, rather than the number of inpainting operations.\n\nTo assess the similarity to the original image across iterations, we employ the Learned Perceptual Image Path Similarity (LPIPS)\nmetric, which is frequently used to evaluate inpainting quality. In our implementation, we utilize the features from three neural\nnetworks to calculate the metric: SqueezeNet, AlexNet, and VGG.\n\nWe conducted recursive inpainting, altering 400% of the pixels, using masks of sizes 64x64, 128x128, and 256x256. To measure the\ndegradation as inpainting operations are performed, we calculated the LPIPS metric between the original image and each subsequent\ngeneration using the features from the three neural networks (SqueezeNet, AlexNet, and VGG). The average distances for the 100\nimages at each 50% inpainting step are presented. The bars represent the standard deviation observed across the samples for each\ndata point. Several initial observations can be drawn from these results:\n\n1. As the recursive inpainting progresses, the distance from the original image increases, potentially leading to an image that bears\nno resemblance to the original. 2. The rate at which the distance increases tends to decrease, but it does not appear to stabilize even\nwhen the distance becomes substantial. 3. The discrepancy with the original image is more pronounced when larger masks are used\nfor inpainting, which aligns with the expectation that larger blocks are more challenging to inpaint. 4. The three networks used for\ncomputing the LPIPS (SqueezeNet, AlexNet, and VGG) yield comparable results. 5. The significant standard deviation indicates\nthat different images will exhibit varying behaviors.\n\nTo gain a better understanding of the variability in distances for each image, scatter plots of the LPIPS distances for the 100 images\nfor each neural network are presented. It is evident that there is considerable variability across images, but the general trends are\nconsistent with those observed in the mean: the distance increases with more inpainting and with larger masks. Among the three\nnetworks (SqueezeNet, AlexNet, and VGG), VGG shows the fewest outliers. Given that VGG is the most complex network, it\nis expected to capture the image features more effectively. Consequently, we will only report results for VGG moving forward,\nalthough all metrics are available in the repository along with the images.\n\nTo investigate whether the degradation is consistent across different runs, we selected 10 images from the set of 100 and performed\n10 runs on each. The LPIPS metrics across these runs for three different images are displayed, using the VGG network, which\ngenerally exhibits the lowest deviations. It is noticeable that variations are more significant with larger masks, which is anticipated\nsince larger masks require fewer iterations to reach a given percentage of inpainting, thus introducing more variability. The variations\nalso decrease as the percentage of inpainting increases, indicating that a higher number of inpainting operations leads to reduced\nvariability. This suggests that recursive inpainting tends to converge in terms of LPIPS distance as the process advances.\n\n5 Conclusion and Future Work\n\nIn this study, we have introduced and empirically examined the impact of recursive inpainting on AI image models. The findings\nreveal that recursion can result in the deterioration and eventual breakdown of the image, a phenomenon akin to the model collapse\nobserved when training generative AI models with their own data. This issue is currently a focal point in the research community.\nConsequently, this paper introduces a new dimension to the study of the effects of recursive application of generative AI, specifically\nin the inference phase. This can enhance current research endeavors and offer deeper insights into the underlying causes of collapse,\npotentially leading to advancements in AI models that can lessen the adverse effects of recursion.\n\nThe presented analysis of recursive inpainting represents an initial step in this area. Further investigation involving different AI\nmodels, a variety of images, and diverse model configurations is necessary to gain a more comprehensive understanding of the effects\nof recursive inpainting. Developing theoretical models that can account for these effects is also a crucial area for future research.\nAdditionally, exploring the connections between recursive training and recursive inpainting could provide valuable insights.\n\n3",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": ""
}
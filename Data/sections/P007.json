{
  "title": "Joint Syntacto-Discourse Parsing and the\nSyntacto-Discourse Treebank",
  "abstract": "Discourse parsing has long been treated as a stand-alone problem independent from\nconstituency or dependency parsing. Most attempts at this problem are pipelined\nrather than end-to-end, sophisticated, and not self-contained: they assume gold-\nstandard text segmentations (Elementary Discourse Units), and use external parsers\nfor syntactic features. In this paper we propose the first end-to-end discourse\nparser that jointly parses in both syntax and discourse levels, as well as the first\nsyntacto-discourse treebank by integrating the Penn Treebank with the RST Tree-\nbank. Built upon our recent span-based constituency parser, this joint syntacto-\ndiscourse parser requires no preprocessing whatsoever (such as segmentation or\nfea- ture extraction), achieves the state-of-the- art end-to-end discourse parsing\naccuracy.",
  "introduction": "Distinguishing the semantic relations between segments in a document can be greatly beneficial to\nmany high-level NLP tasks, such as summarization, sentiment analysis, question answering, and\ntextual quality evaluation.\n\nThere has been a variety of research on discourse parsing. But most of them suffer from the following\nlimitations:\n\n1. pipelined rather than end-to-end: they assume pre-segmented discourse, and worse yet, use\n\ngold-standard segmentations\n\n2. not self-contained: they rely on external syntactic parsers and pretrained word vectors;\n3. complicated: they design sophisticated features, including those from parse-trees.\n\nWe argue for the first time that discourse parsing should be viewed as an extension of, and be\nperformed in conjunction with, constituency parsing. We propose the first joint syntacto-discourse\ntree- bank, by unifying constituency and discourse tree representations. Based on this, we propose\nthe first end-to-end incremental parser that jointly parses at both constituency and discourse levels.\nOur algo- rithm builds up on the span-based parser; it employs the strong general- ization power\nof bi-directional LSTMs, and parses efficiently and robustly with an extremely simple span-based\nfeature set that does not use any tree structure information.\n\nWe make the following contributions:\n\n1. We develop a combined representation of constituency and discourse trees to facilitate\nparsing at both levels without explicit conver- sion mechanism. Using this representation,\nwe build and release a joint treebank based on the Penn Treebank and RST Treebank.\n2. We propose a novel joint parser that parses at both constituency and discourse levels.\n3. Even though it simultaneously performs con- stituency parsing, our parser does not use any\nexplicit syntactic feature, nor does it need any binarization of discourse trees, thanks to the\npowerful span-based framework.\n\n\f4. Empirically, our end-to-end parser outperforms the existing pipelined discourse pars- ing\nefforts. When the gold EDUs are pro- vided, our parser is also competitive to other existing\napproaches with sophisticated fea- tures.\n\n2 Combined Representation & Treebank\n\nWe first briefly review the discourse structures in Rhetorical Structure Theory, and then discuss how to\nunify discourse and constituency trees, which gives rise to our syntacto-discourse treebank PTB-RST.\n\n2.1 Review: RST Discourse Structures\n\nIn an RST discourse tree, there are two types of branchings. Most of the internal tree nodes are binary\nbranching, with one nucleus child containing the core semantic meaning of the current node, and\none satellite child semantically decorating the nucleus. Like dependency labels, there is a relation\nannotated between each satellite-nucleus pair, such as \u201cBackground\u201d or \u201cPurpose\u201d. There are also\nnon- binary-branching internal nodes whose children are conjunctions, e.g., a \u201cList\u201d of semantically\nsimilar EDUs (which are all nucleus nodes).\n\n2.2 Syntacto-Discourse Representation\n\nIt is widely recognized that lower-level lexical and syntactic information can greatly help determin-\ning both the boundaries of the EDUs (i.e., dis- course segmentation) as well as the semantic relations\nbetween EDUs. While these previous approaches rely on pre-trained tools to provide both EDU\nsegmentation and intra-EDU syntactic parse trees, we in- stead propose to directly determine the\nlow-level segmentations, the syntactic parses, and the high- level discourse parses using a single joint\nparser. This parser is trained on the combined trees of constituency and discourse structures.\n\nWe first convert an RST tree to a format similar to those constituency trees in the Penn Treebank. For\neach binary branching node with a nucleus child and a satellite child, we use the relation as the label\nof the converted parent node. The nucleus/satellite relation, along with the direction (either \u2190 or \u2192,\npointing from satellite to nucleus) is then used as the label. For a conjunctive branch (e.g. \u201cList\u201d), we\nsimply use the relation as the label of the converted node.\n\nAfter converting an RST tree into the constituency tree format, we then replace each leaf node (i.e.,\nEDU) with the corresponding syntactic (sub)tree from PTB. Given that the sentences in the RST\nTreebank is a subset of that of PTB, we can always find the corresponding constituency subtrees for\neach EDU leaf node. In most cases, each EDU corresponds to one sin- gle (sub)tree in PTB, since the\ndiscourse bound- aries generally do not conflict with constituencies. In other cases, one EDU node\nmay correspond to multiple subtrees in PTB, and for these EDUs we use the lowest common ancestor\nof those subtrees in the PTB as the label of that EDU in the con- verted tree. E.g., if C\u2013D is one EDU\nin the PTB tree A it might be converted to Purpose\u2192DCB A based on the Penn Treebank and RST\nTreebank. This PTB-RST treebank is released as a set of tools to generate the joint trees given Penn\nTree- bank and RST Treebank data. During the align- ment between the RST trees and the PTB trees,\nwe only keep the common parts of the two trees.\n\nWe follow the standard training/testing split of the RST Treebank. In the training set, there are 347\njoint trees with a total of 17,837 tokens, and the lengths of the discourses range from 30 to 2,199\ntokens. In the test set, there are 38 joint trees with a total of 4,819 tokens, and the lengths vary from\n45 to 2,607. Figure 3 shows the distribution of the discourse lengths over the whole dataset, which on\naverage is about 2x of PTB sen- tence length, but longest ones are about 10x the longest lengths in\nthe Treebank.\n\n3 Joint Syntacto-Discourse Parsing\n\nGiven the combined syntacto-discourse treebank, we now propose a joint parser that can perform\nend-to-end discourse segmentation and parsing.\n\n2\n\n\f3.1 Extending Span-based Parsing\n\nAs mentioned above, the input sequences are sub- stantially longer than PTB parsing, so we choose\nlinear-time parsing, by adapting a popular greedy constituency parser, the span-based constituency\nparser.\n\n3.2\n\nJoint PTB-RST Treebank\n\nUsing the conversion strategy described above we build the first joint syntacto-discourse treebank.\n\nAs in span-based parsing, at each step, we main- tain a a stack of spans. Notice that in conventional\nincremental parsing, the stack stores the subtrees constructed so far, but in span-based constituency\nparsing, the stack only stores the boundaries of subtrees, which are just a list of indices ...i k j. In\nother words, quite shockingly, no tree structure is represented anywhere in the parser.\n\nSimilar to span-based constituency parsing, we alternate between structural (either shift or combine)\nand label (labelX or nolabel) actions in an odd-even fashion. But different from previous work, after a\nstructural action, we choose to keep the last branching point k, i.e., i k j (mostly for combine, but also\ntrivially for shift). This is because in our parsing mechanism, the dis- course relation between two\nEDUs is actually de- termined after the previous combine action. We need to keep the splitting point\nto clearly find the spans of the two EDUs to determine their relations. This midpoint k disappears\nafter a label ac- tion; therefore we can use the shape of the last span on the stack (whether it contains\nthe split point, i.e., i k j or i j) to determine the par- ity of the step and thus no longer need to carry the\nstep z in the state .\n\nThe nolabel action makes the binarization of the discourse/constituency tree unnecessary, because\nnolabel actually combines the top two spans on the stack into one span, but without annotating the\nnew span a label. This greatly simplifies the pre- processing and post-processing efforts needed.\n\nPrec. Recall\n\nF1\n\n87.6\n46.5\n83.5\nTable 1: Accuracies on PTB-RST at constituency and discourse levels.\n\nConstituency\nDiscourse\nOverall\n\n87.2\n43.0\n82.5\n\n86.9\n40.2\n81.6\n\n3.3 Recurrent Neural Models and Training\n\nThe scoring functions in the deductive system are calculated by an underlying neu- ral model, which\nis similar to the bi-directional LSTM model that evaluates based on span boundary features. Again, it\nis important to note that no discourse or syntactic tree structures are represented in the features.\n\nDuring the decoding time, a document is first passed into a two-layer bi-directional LSTM model,\nthen the outputs at each text position of the two layers of the bi-directional LSTMs are con- catenated\nas the positional features. The spans at each parsing step can be represented as the fea- ture vectors\nat the boundaries. The span features are then passed into fully connected networks with softmax to\ncalculate the likelihood of performing the corresponding action or marking the cor- responding label.\n\nWe use the \u201ctraining with exploration\u201d strategy and the dynamic oracle mechanism to make sure the\nmodel can handle unseen parsing configurations properly.",
  "related_work": "",
  "methodology": "",
  "experiments": "We use the treebank described in Section 2 for empirical evaluation. We randomly choose 30\ndocuments from the training set as the development set.\n\nWe tune the hyperparameters of the neural model on the development set. For most of the hyperpa-\nrameters we settle with the same values sug- gested previously. To alleviate the overfitting problem\nfor training on the relative small RST Treebank, we use a dropout of 0.5.\n\n3\n\n\fOne particular hyperparameter is that we use a value to balance the chances between training\nfollowing the exploration (i.e., the best action cho- sen by the neural model) and following the correct\npath provided by the dynamic oracle. We find that = 0.8, i.e., following the dynamic oracle with a\nprobability of 0.8, achieves the best performance. One explanation for this high chance to follow the\noracle is that, since our combined trees are significantly larger than the constituency trees in Penn\nTreebank, lower makes the parsing easily divert into wrong trails that are difficult to learn from.\n\nSince our parser essentially performs both constituency parsing task and discourse parsing task. We\nalso evaluate the performances on sentence constituency level and discourse level separately. The\nresult is shown in Table 1. Note that in constituency level, the accuracy is not directly comparable\nwith the accuracy reported previously, since: a) our parser is trained on a much smaller dataset (RST\nTreebank is about 1/6 of Penn Treebank); b) the parser is trained to optimize the discourse-level\naccuracy.\n\nTable 2 shows that, in the perspective of end- to-end discourse parsing, our parser first outper- forms\nthe state-of-the-art segmentator, and furthermore, in end-to-end pars- ing, the superiority of our parser\nis more pronounced comparing to the previously best parser.\n\nOn the other hand, the majority of the conven- tional discourse parsers are not end-to-end: they rely\non gold EDU segmentations and pre-trained tools like Stanford parsers to generate features. We\nperform an experiment to compare the per- formance of our parser with them given the gold EDU\nsegments (Table 3). Note that most of these parsers do not handle multi-branching discourse nodes\nand are trained and evaluated on binarized discourse trees, so their performances are actually not\ndirectly comparable to the results we reported.\n\ndescription\n\nsyntactic feats.\n\nsegmentation\n\nstructure\n\n+nuclearity\n\n+relation\n\njoint syntactic & discourse parsing\n\nsegmentation only\nend-to-end pipeline\n-\nTable 2: F1 scores of end-to-end systems. \u201c+nuclearity\u201d indicates scoring of tree structures with\nnucle- arity included. \u201c+relation\u201d has both nuclearity and relation included (e.g., \u2190Elaboration).\n\nStanford\nPenn Treebank\n95.4\n\n95.1\n94.0\n78.8\n\n-\n72.3\n65.0\n\n-\n59.1\n52.2\n\n-\n47.3\n\nsyntactic feats\n\nstructure\n\n+nuclearity\n\n+relation\n\nhuman\n\n6*sparse\n\n5*neural\n\nannotation\n\n-\n\nPenn Treebank\nCharniak (retrained)\nCharniak (retrained)\nStanford\nZPar (retraied)\nStanford\n\n+ sparse features\n\n+ sparse features\nspan-based discourse parsing\n\nStanford\nMALT\nMALT\n-\n\n88.7\n\n83.0\n82.7\n-\n85.7\n83.5\n86.0\n\n82.4\n84.0\n80.5\n81.6\n84.2\n\n77.7\n\n68.4\n68.4\n-\n71.0\n68.1\n72.4\n\n69.2\n70.8\n68.6\n71.1\n67.7\n\n65.8\n\n54.8\n55.7\n57.3\n58.2\n55.1\n59.7\n\n56.8\n58.6\n58.3\n61.8\n56.0\n\nTable 3: Experiments using gold segmentations. The column of \u201csyntactic feats\u201d shows how the\nsyntactic features are calculated in the corresponding systems. Note that our parser predicts solely\nbased on the span features from bi-directionaly LSTM, instead of any explicitly designed syntactic\nfeatures.",
  "results": "",
  "conclusion": "We have presented a neural-based incremental parser that can jointly parse at both constituency and\ndiscourse levels. To our best knowledge, this is the first end-to-end parser for discourse parsing task.\n\n4\n\n\fOur parser achieves the state-of-the-art per- formance in end-to-end parsing, and unlike previ- ous\napproaches, needs little pre-processing effort.\n\n5"
}
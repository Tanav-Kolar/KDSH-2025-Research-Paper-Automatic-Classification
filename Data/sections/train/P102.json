{
  "title": "A Large-Scale Car Dataset for Fine-Grained\nCategorization and Verification",
  "abstract": "This paper aims to highlight vision related tasks centered around \u201ccar\u201d, which has\nbeen largely neglected by vision community in comparison to other objects. We\nshow that there are still many interesting car-related problems and applications,\nwhich are not yet well explored and researched. To facilitate future car-related\nresearch, in this paper we present our on-going effort in collecting a large-scale\ndataset, \u201cCompCars\u201d, that covers not only different car views, but also their dif-\nferent internal and external parts, and rich attributes. Importantly, the dataset is\nconstructed with a cross-modality nature, containing a surveillance- nature set and\na web-nature set. We further demonstrate a few important applications exploiting\nthe dataset, namely car model classification, car model verification, and attribute\nprediction. We also discuss specific challenges of the car-related problems and\nother potential applications that worth further investigations.\n** Update: This technical report serves as an extension to our earlier work published\nin CVPR 2015. The experiments shown in Sec. 5 gain better performance on\nall three tasks, i.e. car model classification, attribute prediction, and car model\nverification, thanks to more training data and better network structures. The\nexperimental results can serve as baselines in any later research works. The settings\nand the train/test splits are provided on the project page.\n** Update 2: This update provides preliminary experiment results for fine-grained\nclassification on the surveillance data of CompCars. The train/test splits are\nprovided in the updated dataset. See details in Section 6.",
  "introduction": "Cars represent a revolution in mobility and convenience, bringing us the flexibility of moving from\nplace to place. The societal benefits (and cost) are far-reaching. Cars are now indispensable from our\nmodern life as a vehicle for transportation. In many places, the car is also viewed as a tool to help\nproject someone\u2019s economic status, or reflects our economic stratification. In addition, the car has\nevolved into a subject of interest amongst many car enthusiasts in the world. In general, the demand\non car has shifted over the years to cover not only practicality and reliability, but also high comfort\nand design. The enormous number of car designs and car model makes car a rich object class, which\ncan potentially foster more sophisticated and robust computer vision models and algorithms.\n\nCars present several unique properties that other objects cannot offer, which provides more challenges\nand facilitates a range of novel research topics in object categorization. Specifically, cars own large\nquantity of models that most other categories do not have, enabling a more challenging fine-grained\ntask. In addition, cars yield large appearance differences in their unconstrained poses, which demands\nviewpoint-aware analyses and algorithms (see Fig. 1(b)). Importantly, a unique hierarchy is presented\nfor the car category, which is three levels from top to bottom: make, model, and released year.\nThis structure indicates a direction to address the fine-grained task in a hierarchical way, which is\nonly discussed by limited literature. Apart from the categorization task, cars reveal a number of\ninteresting computer vision problems. Firstly, different designing styles are applied by different\ncar manufacturers and in different years, which opens the door to fine-grained style analysis and\n\n.\n\n\ffine-grained part recognition (see Fig. 1(c)). Secondly, the car is an attractive topic for attribute\nprediction. In particular, cars have distinctive attributes such as car class, seating capacity, number\nof axles, maximum speed and displacement, which can be inferred from the appearance of the cars\n(see Fig. 1(a)). Lastly, in comparison to human face verification, car verification, which targets at\nverifying whether two cars belong to the same model, is an interesting and under- researched problem.\nThe unconstrained viewpoints make car verification arguably more challenging than traditional face\nverification.\n\nAutomated car model analysis, particularly the fine- grained car categorization and verification, can be\nused for innumerable purposes in intelligent transportation sys- tem including regulation, description\nand indexing. For instance, fine-grained car categorization can be exploited to inexpensively automate\nand expedite paying tolls from the lanes, based on different rates for different types of vehicles.\nIn video surveillance applications, car verification from appearance helps tracking a car over a\nmultiple camera network when car plate recognition fails. In post-event in- vestigation, similar\ncars can be retrieved from the database with car verification algorithms. Car model analysis also\nbears significant value in the personal car consumption. When people are planning to buy cars, they\ntend to observe cars in the street. Think of a mobile application, which can instantly show a user\nthe detailed information of a car once a car photo is taken. Such an application will provide great\nconvenience when people want to know the information of an unrecognized car. Other applications\nsuch as predicting popularity based on the appearance of a car, and recommending cars with similar\nstyles can be beneficial both for manufacturers and consumers.\n\nDespite the huge research and practical interests, car model analysis only attracts few attentions\nin the computer vision community. We believe the lack of high quality datasets greatly limits the\nexploration of the community in this domain. To this end, we collect and organize a large-scale\nand comprehensive image database called \u201cComprehensive Cars\u201d, with \u201cCompCars\u201d being short.\nThe \u201cCompCars\u201d dataset is much larger in scale and diversity compared with the current car image\ndatasets, containing 208, 826 images of 1, 716 car models from two scenarios: web-nature and\nsurveillance-nature. In addition, the dataset is carefully labelled with viewpoints and car parts, as well\nas rich attributes such as type of car, seat capacity, and door number. The new dataset dataset thus\nprovides a comprehensive platform to validate the effectiveness of a wide range of computer vision\nalgorithms. It is also ready to be utilized for realistic applications and enormous novel research topics.\nMoreover, the multi-scenario nature en- ables the use of the dataset for cross modality research. The\ndetailed description of CompCars is provided in Section 3.\n\nTo validate the usefulness of the dataset and to encourage the community to explore for more novel\nresearch topics, we demonstrate several interesting applications with the dataset, including car model\nclassification and verification based on convolutional neural network (CNN). An- other interesting\ntask is to predict attributes from novel car models (see details in Section 4.2). The experiments reveal\nseveral challenges specific to the car-related problems. We conclude our analyses with a discussion\nin Section 7.",
  "related_work": "Most previous car model research focuses on car model classification. propose an evolutionary\ncomputing framework to fit a wireframe model to the car on an image. Then the wireframe model is\nemployed for car model recognition. construct 3D space curves using 2D training images, then match\nthe 3D curves to 2D image curves using a 3D view-based alignment technique. The car model is\nfinally determined with the alignment result. optimize 3D model fitting and fine-grained classification\njointly. All these works are restricted to a small number of car models. Recently, propose to extract\n3D car representation for classifying 196 car models. The experiment is the largest scale that we\nare aware of. Car model classification is a fine-grained categorization task. In contrast to general\nobject classification, fine-grained categorization targets at recognizing the subcategories in one object\nclass. Fol- lowing this line of research, many studies have proposed different datasets on a variety\nof categories: birds, dogs, cars, flowers, etc. But all these datasets are limited by their scales and\nsubcategory numbers.\n\nTo our knowledge, there is no previous attempt on the car model verification task. Closely related to\ncar model verification, face verification has been a popular topic. The recent deep learning based\nalgorithms first train a deep neural network on human identity clas- sification, then train a verification\n\n2\n\n\fmodel with the feature extracted from the deep neural network. Joint Bayesian is a widely-used\nverification model that models two faces jointly with an appropriate prior on the face representation.\nWe adopt Joint Bayesian as a baseline model in car model verification.\n\nAttribute prediction of humans is a popular research topic in recent years. However, a large portion\nof the labeled attributes in the current attribute datasets, such as long hair and short pants lack strict\ncriteria, which causes annotation ambiguities. The attributes with ambiguities will potentially harm\nthe effectiveness of evaluation on related datasets. In contrast, the attributes provided by CompCars\n(e.g. maximum speed, door number, seat capacity) all have strict criteria since they are set by the car\nmanufacturers. The dataset is thus advantageous over the current datasets in terms of the attributes\nvalidity.\n\nOther car-related research includes detection, track- ing, joint detection and pose estimation, and 3D\nparsing. Fine-grained car models are not explored in these studies. Previous research related to car\nparts includes car logo recognition and car style analysis based on mid-level features.\n\nSimilar to CompCars, the Cars dataset also targets at fine-grained tasks on the car category. Apart\nfrom the larger-scale database, our CompCars dataset offers several significant benefits in comparison\nto the Cars dataset. First, our dataset contains car images diversely distributed in all viewpoints\n(annotated by front, rear, side, front-side, and rear-side), while Cars dataset mostly consists of front-\nside car images. Second, our dataset contains aligned car part images, which can be utilized for many\ncomputer vision algorithms that demand precise alignment. Third, our dataset provides rich attribute\nannotations for each car model, which are absent in the Cars dataset.\n\n3 Properties of CompCars\n\nThe CompCars dataset contains data from two scenarios, including images from web-nature and\nsurveillance-nature. The images of the web-nature are collected from car forums, public websites,\nand search engines. The images of the surveillance-nature are collected by surveillance cameras. The\ndata of these two scenarios are widely used in the real-world applications. They open the door for\ncross-modality analysis of cars. In particular, the web-nature data contains 163 car makes with 1, 716\ncar models, covering most of the commercial car models in the recent ten years. There are a total of\n136, 727 images capturing the entire cars and 27, 618 images capturing the car parts, where most\nof them are labeled with attributes and viewpoints. The surveillance-nature data contains 44, 481\ncar images captured in the front view. Each image in the surveillance-nature partition is annotated\nwith bounding box, model, and color of the car. Fig. 2 illustrates some examples of surveillance\nimages, which are affected by large variations from lightings and haze. Note that the data from the\nsurveillance-nature are significantly different from the web-nature data in Fig. 1, suggesting the great\nchallenges in cross-scenario car analysis. Overall, CompCars dataset offers four unique features in\ncomparison to existing car image databases, namely car hierarchy, car attributes, viewpoints, and car\nparts. the\n\nCar Hierarchy The car models can be organized into a large tree structure, consisting of three layers\n, namely car make, car model, and year of manufacture, top to bottom as depicted in Fig. 3. The\ncomplexity is further compounded by the fact that each car model can be produced in different years,\nyielding subtle difference in their appearances. For instance, three versions of \u201cAudi A4L\u201d were\nproduced between 2009 to 2011 respectively. from\n\nCar Attributes Each car model is labeled with five at- tributes, including maximum speed, displace-\nment, number of doors, number of seats, and type of car. These attributes provide rich information\nwhile learning the relations or similarities between different car models. For example, we define\ntwelve types of cars, which are MPV, SUV, hatchback, sedan, minibus, fastback, estate, pickup, sports,\ncrossover, convertible, and hardtop convertible, as shown in Fig. 4. Furthermore, these attributes\ncan be partitioned into two groups: explicit and implicit attributes. The former group contains door\nnumber, seat number, and car type, which are represented by discrete values, while the latter group\ncontains maximum speed and displacement (volume of an engine\u2019s cylinders), represented by contin-\nuous values. Humans can easily tell the numbers of doors and seats from a car\u2019s proper viewpoint,\nbut hardly recognize its maximum speed and displacement. We conduct interesting experiments to\npredict these attributes in Section 4.2.\n\n3\n\n\fViewpoints We also label five viewpoints for each car model, including front (F), rear (R), side (S),\nfront-side (FS), and rear-side (RS). These viewpoints are labeled by several professional annotators.\nThe quantity distribution of the labeled car images is shown in Table 1. Note that the numbers of\nviewpoint images are not balanced among different car models, because the images of some less\npopular car models are difficult to collect.\n\nCar Parts We collect images capturing the eight car parts for each car model, including four exterior\nparts (i.e. headlight, taillight, fog light, and air intake) and four interior parts (i.e. console, steering\nwheel, dashboard, and gear lever). These images are roughly aligned for the convenience of further\nanalysis. A summary and some examples are given in Table 2 and Fig. 5 respectively.\n\nTable 1: Quantity distribution of the labeled car images in different viewpoints.\n\nViewpoint No. in total No. per model\n\nF\nR\nS\nFS\nRS\n\n18431\n13513\n23551\n49301\n31150\n\n10.9\n8.0\n14.0\n29.2\n18.5\n\nTable 2: Quantity distribution of the labeled car part images.\n\nPart\n\nNo. in total No. per model\n\nheadlight\ntaillight\nfog light\nair intake\nconsole\nsteering wheel\ndashboard\ngear lever\n\n3705\n3563\n3177\n3407\n3350\n3503\n3478\n3435\n\n2.2\n2.1\n1.9\n2.0\n2.0\n2.1\n2.1\n2.0\n\n4 Applications\n\nIn this section, we study three applications using CompCars, including fine-grained car classification,\nattribute prediction, and car verification. We select 78, 126 images from the CompCars dataset and\ndivide them into three subsets without overlaps. The first subset (Part-I) contains 431 car models with\na total of 30, 955 images capturing the entire car and 20, 349 images capturing car parts. The second\nsubset (Part-II) consists 111 models with 4, 454 images in total. The last subset (Part-III) contains 1,\n145 car models with 22, 236 images. Fine-grained car classification is conducted using images in the\nfirst subset. For attribute prediction, the models are trained on the first subset but tested on the second\none. The last subset is utilized for car verification.\n\nWe investigate the above potential applications using Convolutional Neural Network (CNN), which\nachieves great empirical successes in many computer vision prob- lems, such as object classification,\ndetection, face alignment, and face verification. Specifically, we employ the Overfeat model, which\nis pretrained on ImageNet classification task, and fine-tuned with the car images for car classification\nand attribute prediction. For car model verification, the fine-tuned model is employed as a feature\nextractor.\n\n4.1 Fine-Grained Classification\n\nWe classify the car images into 431 car models. For each car model, the car images produced in\ndifferent years are considered as a single category. One may treat them as different categories, leading\nto a more challenging problem because their differences are relatively small. Our experiments have\ntwo settings, comprising fine-grained classification with the entire car images and the car parts. For\nboth settings, we divide the data into half for training and another half for testing. Car model labels\nare regarded as training target and logistic loss is used to fine-tune the Overfeat model.\n\n4\n\n\f4.1.1 The Entire Car Images\n\nWe compare the recognition performances of the CNN models, which are fine-tuned with car images\nin specific viewpoints and all the viewpoints respectively, denoted as \u201cfront (F)\u201d, \u201crear (R)\u201d, \u201cside\n(S)\u201d, \u201cfront-side (FS)\u201d, \u201crear- side (RS)\u201d, and \u201cAll-View\u201d. The performances of these six models are\nsummarized in Table 3, where \u201cFS\u201d and \u201cRS\u201d achieve better performances than the performances\nof the other viewpoint models. Surprisingly, the \u201cAll- View\u201d model yields the best performance,\nalthough it did not leverage the information of viewpoints. This result reveals that the CNN model is\ncapable of learning discriminative representation across different views. To verify this observation,\nwe visualize the car images that trigger high responses with respect to each neuron in the last fully-\nconnected layer. As shown in Fig. 6, these neurons capture car images of specific car models across\ndifferent viewpoints.\n\nSeveral challenging cases are given in Fig. 7, where the images on the left hand side are the testing\nimages and the images on the right hand side are the examples of the wrong predictions (of the\n\u201cAll-View\u201d model). We found that most of the wrong predictions belong to the same car makes as the\ntest images. We report the \u201ctop- 1\u201d accuracies of car make classification in the last row of Table 3,\nwhere the \u201cAll-View\u201d model obtain reasonable good result, indicating that a coarse-to-fine (i.e. from\ncar make to model) classification is possible for fine-grained car recognition.\n\nTo observe the learned feature space of the \u201cAll-View\u201d model, we project the features extracted\nfrom the last fully- connected layer to a two-dimensional embedding space using multi-dimensional\nscaling. Fig. 8 visualizes the projected features of twelve car models, where the images are chosen\nfrom different viewpoints. We observe that features from different models are separable in the 2D\nspace and features of similar models are closer than those of dissimilar models. For instance, the\ndistances between \u201cBWM 5 Series\u201d and \u201cBWM 7 Series\u201d are smaller than those between \u201cBWM 5\nSeries\u201d and \u201cChevrolet Captiva\u201d.\n\nWe also conduct a cross-modality experiment, where the CNN model fine-tuned by the web-nature\ndata is evaluated on the surveillance-nature data. Fig. 9 illustrates some predictions, suggesting that\nthe model may account for data variations in a different modality to a certain extent. This experiment\nindicates that the features obtained from the web-nature data have potential to be transferred to data\nin the other scenario.\n\nTable 3: Fine-grained classification results for the models trained on car images. Top-1 and Top-5\ndenote the top-1 and top-5 accuracy for car model classification, respectively. Make denotes the make\nlevel classification accuracy.\n\nViewpoint\n\nF\n\nR\n\nS\n\nFS\n\nRS\n\nAll-View\n\nTop-1\nTop-5\nMake\n\n0.524\n0.748\n0.710\n\n0.431\n0.647\n0.521\n\n0.428\n0.602\n0.507\n\n0.563\n0.769\n0.680\n\n0.598\n0.777\n0.656\n\n0.767\n0.917\n0.829\n\n4.1.2 Car Parts\n\nCar enthusiasts are able to distinguish car models by examining the car parts. We investigate if\nthe CNN model can mimic this strength. We train a CNN model using images from each of the\neight car parts. The results are reported in Table 4, where \u201ctaillight\u201d demonstrates the best accuracy.\nWe visualize taillight images that have high responses with respect to each neuron in the last fully-\nconnected layer. Fig. 10 displays such images with respect to two neurons. \u201cTaillight\u201d wins among\nthe different car parts, mostly likely due to the relatively more distinctive designs, and the model\nname printed close to the taillight, which is a very informative feature for the CNN model.\n\nWe also combine predictions using the eight car part models by voting strategy. This strategy\nsignificantly improves the performance due to the complementary nature of different car parts.\n\n4.2 Attribute Prediction\n\nHuman can easily identify the car attributes such as numbers of doors and seats from a proper\nviewpoint, without knowing the car model. For example, a car image captured in the side view\n\n5\n\n\fTable 4: Fine-grained classification results for the models trained on car parts. Top-1 and Top-5\ndenote the top-1 and top-5 accuracy for car model classification, respectively.\n\nExterior parts\n\nInterior parts\n\nHeadlight Taillight\n\nFog light Air intake Console\n\nSteering wheel Dashboard Gear lever Voting\n\nTop-1\nTop-5\n\n0.479\n0.690\n\n0.684\n0.859\n\n0.387\n0.566\n\n0.484\n0.695\n\n0.535\n0.745\n\n0.540\n0.773\n\n0.502\n0.736\n\n0.355\n0.589\n\n0.808\n0.927\n\nprovides sufficient information of the door number and car type, but it is hard to infer these attributes\nfrom the frontal view. The appearance of a car also provides hints on the implicit attributes, such\nas the maximum speed and the displacement. For instance, a car model is probably designed for\nhigh-speed driving, if it has a low under-pan and a streamline body.\n\nIn this section, we deliberately design a challenging experimental setting for attribute recognition,\nwhere the car models presented in the test images are exclusive from the training images. We fine-tune\nthe CNN with the sum- of-square loss to model the continuous attributes, such as \u201cmaximum speed\u201d\nand \u201cdisplacement\u201d, but a logistic loss to predict the discrete attributes such as \u201cdoor number\u201d, \u201cseat\nnumber\u201d, and \u201ccar type\u201d. For example, the \u201cdoor number\u201d has four states, i.e. 2, 3, 4, 5 doors, while\n\u201cseat number\u201d also has four states, i.e. 2, 4, 5, > 5 seats. The attribute \u201ccar type\u201d has twelve states as\ndiscussed in Sec. 3.\n\nTo study the effectiveness of different viewpoints for attribute prediction, we train CNN models for\ndifferent viewpoints separately. Table 5 summarizes the results, where the \u201cmean guess\u201d represents\nthe errors computed by using the mean of the training set as the prediction. We observe that the\nperformances of \u201cmaximum speed\u201d and \u201cdisplacement\u201d are insensitive to viewpoints. However, for\nthe explicit attributes, the best accuracy is obtained under side view. We also found that the the\nimplicit attributes are more difficult to predict then the explicit attributes. Several test images and\ntheir attribute predictions are provided in Fig. 11.\n\nTable 5: Attribute prediction results for the five single viewpoint models. For the continuous attributes\n(maximum speed and displacement), we display the mean difference from the ground truth. For the\ndiscrete attributes (door and seat number, car type), we display the classification accuracy. Mean\nguess denotes the mean error with a prediction of the mean value on the training set.\n\nViewpoint\n\nF\n\nR\n\nS\n\nFS\n\nRS\n\nmean difference\nMaximum speed\n(mean guess)\nDisplacement\n(mean guess)\n\n20.8\n38.0\n0.811\n1.04\nclassification accuracy\n\nDoor number\nSeat number\nCar type\n\n0.674\n0.672\n0.541\n\n21.3\n38.5\n0.752\n0.922\n\n0.748\n0.691\n0.585\n\n20.4\n39.4\n0.795\n1.04\n\n0.837\n0.711\n0.627\n\n20.1\n40.2\n0.875\n1.13\n\n0.738\n0.660\n0.571\n\n21.3\n40.1\n0.822\n1.08\n\n0.788\n0.700\n0.612\n\n4.3 Car Verification\n\nIn this section, we perform car verification following the pipeline of face verification. In particular,\nwe adopt the classification model in Section 4.1.1 as a feature extractor of the car images, and then\napply Joint Bayesian to train a verification model on the Part-II data. Finally, we test the performance\nof the model on the Part-III data, which includes 1, 145 car models. The test data is organized into\nthree sets, each of which has different difficulty, i.e. easy, medium, and hard. Each set contains 20,\n000 pairs of images, including 10, 000 positive pairs and 10, 000 negative pairs. Each image pair in\nthe \u201ceasy set\u201d is selected from the same viewpoint, while each pair in the \u201cmedium set\u201d is selected\nfrom a pair of random viewpoints. Each negative pair in the \u201chard set\u201d is chosen from the same car\nmake.\n\n6\n\n\fDeeply learned feature combined with Joint Bayesian has been proven successful for face verification.\nJoint Bayesian formulates the feature x as the sum of two independent Gaussian variables\n\nx = p + e,\n\n(1)\n\nwhere p \u223c N (0, \u03a3p) represents identity information, and e \u223c N (0, \u03a3e) the intra-category variations.\nJoint Bayesian models the joint probability of two objects given the intra or extra-category varia-\ntion hypothesis, P (x1, x2|HI ) and P (x1, x2|HE). These two probabilities are also Gaussian with\nvariations\n\n\u03a3I = \u03a3p + \u03a3e, \u03a3E = \u03a3p + \u03a3e\n\n(2)\n\nand\n\n\u03a3I = \u03a3p + \u03a3e, \u03a3E = \u03a3e\n(3)\nrespectively. \u03a3p and \u03a3e can be learned from data with EM algorithm. In the testing stage, it calculates\nthe likelihood ratio\n\nr(x1, x2) = log\n\n,\n\n(4)\n\nP (x1, x2|HI )\nP (x1, x2|HE)\n\nwhich has closed-form solution. The feature extracted from the CNN model has a dimension of 4,\n096, which is reduced to 20 by PCA. The compressed features are then utilized to train the Joint\nBayesian model. During the testing stage, each image pair is classified by comparing the likelihood\nratio produced by Joint Bayesian with a threshold. This model is denoted as (CNN feature + Joint\nBayesian).\n\nThe second method combines the CNN features and SVM, denoted as CNN feature + SVM. Here,\nSVM is a binary classifier using a pair of image features as input. The label \u20181\u2019 represents positive\npair, while \u20180\u2019 represents negative pair. We extract 100, 000 pairs of image features from Part-II data\nfor training.\n\nThe performances of the two models are shown in Table 6 and the ROC curves for the \u201chard set\u201d\nare plotted in Fig. 14. We observe that CNN feature + Joint Bayesian outperforms CNN feature\n+ SVM with large margins, indicating the advantage of Joint Bayesian for this task. However, its\nbenefit in car verification is not as effective as in face verification, where CNN and Joint Bayesian\nnearly saturated the LFW dataset and approached human performance. Fig. 12 depicts several pairs\nof test images as well as their predictions by CNN feature + Joint Bayesian. We observe two major\nchallenges. First, for the image pair of the same model but different viewpoints, it is difficult to\nobtain the correspondences directly from the raw image pixels. Second, the appearances of different\ncar models of the same car make are extremely similar. It is difficult to distinguish these car models\nusing the entire images. Part localization or detection is crucial for car verification.\n\nTable 6: The verification accuracy of three baseline models.\n\nEasy Medium Hard\n\nCNN feature + Joint Bayesian\nCNN feature + SVM\nrandom guess\n\n0.833\n0.700\n\n0.824\n0.690\n0.500\n\n0.761\n0.659\n\n5 Updated Results: Comparing Different Deep Models\n\nAs an extension to the experiments in Section 4, we conduct experiments for fine-grained car\nclassification, at- tribute prediction, and car verification with the entire dataset and different deep\nmodels, in order to explore the different capabilities of the models on these tasks. The split of the\ndataset into the three tasks is similar to Section 4, where three subsets contain 431, 111, and 1, 145\ncar models, with 52, 083, 11, 129, and 72, 962 images respectively. The only difference is that we\nadopt full set of CompCars in order to establish updated baseline experiments and to make use of the\ndataset to the largest extent. We keep the testing sets of car verification same to those in Section 4.3.\n\nWe evaluate three network structures, namely AlexNet, Overfeat, and GoogLeNet for all three tasks.\nAll networks are pre-trained on the ImageNet classification task, and fine-tuned with the same\nmini-batch size, epochs, and learning rates for each task. All predictions of the deep models are\nproduced with a single center crop of the image. We use Caffe as the platform for our experiments.\n\n7\n\n\fThe experimental results can serve as baselines in any later research works. The train/test splits can\nbe downloaded from CompCars webpage.\n\n5.1 Fine-Grained Classification\n\nIn this section, we classify the car images into 431 car models as in Section 4.1.1. We divide the data\ninto 70\n\nTable 7: The classification accuracies of three deep models.\n\nModel AlexNet Overfeat GoogLeNet\n\nTop-1\nTop-5\n\n0.819\n0.940\n\n0.879\n0.969\n\n0.912\n0.981\n\nTable 8: Attribute prediction results of three deep models. For the continuous attributes (maximum\nspeed and displacement), we display the mean difference from the ground truth (lower is better). For\nthe discrete attributes (door and seat number, car type), we display the classification accuracy (higher\nis better).\n\nModel\n\nAlexNet Overfeat GoogLeNet\n\nmean difference\nMaximum speed\n(mean guess)\nDisplacement\n(mean guess)\n\n21.3\n\n0.803\n\nclassification accuracy\n\nDoor number\nSeat number\nCar type\n\n0.750\n0.691\n0.602\n\n19.4\n36.9\n0.770\n1.02\n\n0.780\n0.713\n0.631\n\n19.4\n\n0.760\n\n0.796\n0.717\n0.643\n\n5.2 Attribute Prediction\n\nWe predict attributes from 111 models not existed in the training set. Different from Section 4.2\nwhere models are trained with cars in single viewpoints, we train with images in all viewpoints to\nbuild a compact model. Table 8 summarizes the results for the three networks, where \u201cmean guess\u201d\nrepresents the prediction with the mean of the values on the training set. GoogLeNet performs the\nbest for all attributes and Overfeat is a close running-up.\n\n5.3 Car Verification\n\nThe evaluation pipeline follows Section 4.3. We evaluate the three deep models combined with two\nverification models: Joint Bayesian and SVM with polynomial kernel. The feature extracted from the\nCNN models is reduced to 200 by PCA before training and testing in all experiments.\n\nThe performances of the three networks combined with the two verification models are shown in\nTable 9, where each model is denoted by name of the deep model + name of the verification model.\nGoogLeNet + Joint Bayesian achieves the best performance in all three settings. For each deep model,\nJoint Bayesian outperforms SVM consistently. Compared to Table 6, Overfeat + Joint Bayesian\nyields a performance gain of 2 4\n\n8",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": ""
}
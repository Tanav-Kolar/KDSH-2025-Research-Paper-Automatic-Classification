{
  "title": "Subspace Constraint Method of Feature Tracking",
  "abstract": "Feature tracking in video is a crucial task in computer vision. Usually, the tracking\nproblem is handled one feature at a time, using a single-feature tracker like the\nKanade-Lucas-Tomasi algorithm, or one of its derivatives. While this approach\nworks quite well when dealing with high- quality video and \u201cstrong\u201d features, it\noften falters when faced with dark and noisy video containing low-quality features.\nWe present a framework for jointly tracking a set of features, which enables sharing\ninformation between the different features in the scene. We show that our method\ncan be employed to track features for both rigid and non- rigid motions (possibly\nof few moving bodies) even when some features are occluded. Furthermore, it can\nbe used to significantly improve tracking results in poorly-lit scenes (where there\nis a mix of good and bad features). Our approach does not require direct modeling\nof the structure or the motion of the scene, and runs in real time on a single CPU\ncore.",
  "introduction": "Feature tracking in video is an important computer vision task, often used as the first step in finding\nstructure from motion or simultaneous location and mapping (SLAM). The celebrated Kanade-Lucas-\nTomasi algorithm tracks feature points by searching for matches between templates representing\neach feature and a frame of video. Despite many other alternatives and improvement, it is still one\nof the best video feature tracking algorithms. However, there are several realistic scenarios when\nLucas-Kanade and many of its alternatives do not perform well: poor lighting conditions, noisy video,\nand when there are transient occlusions that need to be ignored. In order to deal with such scenarios\nmore robustly it would be useful to allow the feature points to communicate with each other to decide\nhow they should move as a group, so as to respect the underlying three dimensional geometry of the\nscene.\n\nThis underlying geometry constrains the trajectories of the track points to have a low-rank structure\nfor the case when tracking a single rigid object under an affine camera model, and for non-rigid\nmotion and the perspective camera. In this work we will combine the low-rank geometry of the\ncohort of tracked features with the successful non-linear single feature tracking framework of Lucas\nand Kanade by adding a low-rank regularization penalty in the tracking optimization problem. To\naccommodate dynamic scenes with non-trivial motion we apply our rank constraint over a sliding\nwindow, so that we only consider a small number of frames at a given time (this is a common idea\nfor dealing with non-rigid motions). We demonstrate very strong performance in rigid environments\nas well as in scenes with multiple and/or non- rigid motion (since the trajectories of all features are\nstill low rank for short time intervals). We describe experiments with several choices of low-rank\nregularizers (which are local in time), using a unified optimization framework that allows real time\nregularized tracking on a single CPU core.\n\n2 On Low-Rank Feature Trajectories\n\nUnder the affine camera model, the feature trajectories for a set of features from a rigid body should\nexist in an affine subspace of dimension 3, or a linear subspace of dimension 4. However, subspaces\n\n.\n\n\fcorresponding to very degenerate motion are lower-dimensional those corresponding to general\nmotion.\n\nFeature trajectories of non-rigid scenarios exhibit significant variety, but some low-rank models\nmay still be successfully applied to them. we consider a sliding temporal window, where over\nshort durations the motion is simple and the feature trajectories are of lower rank. The restriction\non the length of feature trajectories can also help in satisfying an approximate local affine camera\nmodel in scenes which violate the affine camera model. In general, depth disparities give rise to\nlow-dimensional manifolds which are only locally approximated by linear spaces.\n\nAt last, even in the case of multiple moving rigid objects, the set of trajectories is still low rank\n(confined to the union of a few low rank subspaces). In all of these scenarios the low rank is unknown\nin general.\n\n3 Feature Tracking\n\nNotation: A feature at a location z1 \u2208 R2 in a given N1 \u00d7 N2 frame of an N1 \u00d7 N2 \u00d7 N3 video is\ncharacterized by a template T, which is an n \u00d7 n sub-image of that frame centered at z1 (n is a small\ninteger, generally taken to be odd, so the template has a center pixel). If z1 does not have integer\ncoordinates, T is interpolated from the image. We denote \u2126 = 1, ..., n \u00d7 1, ..., n and we parametrize T\nso that its pixel values are obtained by T (u)u\u2208\u2126.\n\nA classical formulation of the single-feature tracking problem is to search for the translation x1 that\nminimizes some distance between a feature\u2019s template T at a given frame and the next frame of video\ntranslated by x1; we denote this next frame by I. That is, we minimize the single-feature energy\nfunction c(x1):\n\nc(x1) =\n\n1\n2\n\n(cid:88)\n\nu\u2208\u2126\n\n\u03c8(T (u) \u2212 I(u + x1))\n\nwhere, for example, \u03c8(x) = |x| or \u03c8(x) = x2. To apply continuous optimization we view x1 as a\ncontinuous variable and we thus view T and I as functions over continuous domains (implemented\nwith bi-linear interpolation).\n\n3.1 Low Rank Regularization Framework\n\nIf we want to encourage a low rank structure in the trajectories, we cannot view the tracking of\ndifferent features as separate problems. For f \u2208 1, 2, ..., F, let xf denote the position of feature f in\nthe current frame (in image coordinates), and let x = (x1, x2, ..., xF ) \u2208 R2F denote the joint state of\nall features in the scene. We define the total energy function as follows:\n\nC(x) =\n\n1\nFn2\n\nF\n(cid:88)\n\n(cid:88)\n\nf =1\n\nu\u2208\u2126\n\n\u03c8(Tf (u) \u2212 I(u + xf ))\n\nwhere Tf (u) is the template for feature f. Now, we can impose desired relationships between features\nin a scene by imposing constraints on the domain of optimization.\n\nInstead of enforcing a hard constraint, we add a penalty term to, which increases the cost of states\nwhich are inconsistent with low-rank motion. Specifically, we define:\n\nC(x) = \u03b1\n\nF\n(cid:88)\n\n(cid:88)\n\nf =1\n\nu\u2208\u2126\n\n\u03c8(Tf (u) \u2212 I(u + xf )) + P (x)\n\nwhere P(x) is an estimate of, or proxy for, the dimensionality of the set of feature trajectories over the\nlast several frames of video (past feature locations are treated as constants, so this is a function only\nof the current state, x). Notice that we have replaced the scale factor 1/(F n2) from with the constant\n\u03b1, as this coefficient is now also responsible for controlling the relative strength of the penalty term.\nWe will give explicit examples for P in section 3.2.\n\nThis framework gives rise to two different solutions, characterized by the strength of the penalty\nterm (definition of \u03b1). Each has useful, real-world tracking applications. In the first case, we assume\n\n2\n\n\fthat most (but not necessarily all) features in the scene approximately obey a low rank model. This\nis appropriate if the scene contains non-rigid or multiple moving bodies. We can impose a weak\nconstraint by making the penalty term small relative to the other terms. If a feature is strong, it will\nconfidently track the imagery, ignoring the constraint (regardless of whether the motion is consistent\nwith the other features in the scene). If a feature is weak in the sense that we cannot fully determine\nits true location by only looking at the imagery, then the penalty term will become significant and\nencourage the feature to agree with the motion of the other features in the scene.\n\nIn the second case, we assume that all features in the scene are supposed to agree with a low rank\nmodel (and deviations from that model are indicative of tracking errors). We can impose a strong\nconstraint by making the penalty term large relative to the other terms. No small set of features can\noverpower the constraint, regardless of how strong the features are. This forces all features to move is\na way that is consistent with a simple motion. Thus, a small number of features can even be occluded,\nand their positions will be predicted by the motion of the other features in the scene.\n\n3.2 Specific Choices of the Low-Rank Regularizer\n\nThere is now a large body of work on low rank regularization. We will restrict ourselves to showing\nresults using three choices for P described below. Each choice we present defines P(x) in terms\nof a matrix M. It is the 2(L + 1) \u00d7 F matrix whose column f contains the feature trajectory for\nfeature f within a sliding window of L + 1 consecutive frames (current frame and L past frames).\nSpecifically, M = [mi,j], where (m0,f , m1,f )T is the current (variable) position of feature f and\n(m2l+1,f , m2l+2,f )T , l = 1, ..., L contains the x and y pixel coordinates of feature f from l frames\nin the past (past feature locations are treated as known constants). One may alternatively center the\ncolumns of M by subtracting from each column the average of all columns. Most constraints derived\nfor trajectories actually confine trajectories to a low rank affine subspace (as opposed to a linear\nsubspace). Centering the columns of M transforms an affine constraint into a linear one. Alternatively,\none can forgo centering and view an affine constraint as a linear constraint in one dimension higher.\nWe report results for both approaches.\n\n3.2.1 Explicit Factorizations\n\nA simple method for enforcing the structure constraint is to write M = BC, where B is a 2(L+1)\u00d7d\nmatrix, and C is a d \u00d7 F matrix. However, as mentioned in the previous section, because the feature\ntracks often do not lie exactly on a subspace due to deviations from the camera model or non- rigidity,\nan explicit constraint of this form is not suitable.\n\nHowever, an explicit factorization can be used in a penalty term by measuring the deviation of M, in\nsome norm, from its approximate low rank factorization. For example, if we let\n\nM = U \u03a3V T\n\ndenote the SVD of M, we can take P(x) to be ||BC M||, where B is the first three or four columns of U,\nand C is the first three or four rows of V T . Then this P corresponds to penalizing M via (cid:80)F\ni=d+1 \u03c3i,\nwhere \u03c3i = \u03bbii is the ith singular value of M. As above, since the history is fixed, U, \u03a3, and V T are\nfunctions of x.\n\nThis approach assumes knowledge of the low-rank d. For simplicity, we assume a local rigid model\nand thus set d = 3 when centering M and d = 4 when not centering.\n\n3.2.2 Nuclear Norm\n\nA popular alternative to explicitly keeping track of the best fit low-dimensional subspace to M is to\nuse the matrix nuclear norm and define\n\nP (x) = ||M ||\u2217 = ||\u03c3||1\n\nThis is a convex proxy for the rank of M. Here \u03c3 = (\u03c31 \u03c32 . . . \u03c32(L+1)\u2227F )T is the vector of singular\nvalues of M, and || \u00b7 ||1 is the l1 norm. Unlike explicit factorization, where only energy outside the first\nd principal components of M is punished, the nuclear norm will favor lower-rank M over higher-rank\nM even when both matrices have rank d. Thus, using this kind of penalty will favor simpler track\npoint motions over more complex ones, even when both are technically permissible.\n\n3\n\n\f3.2.3 Empirical Dimension\n\nEmpirical Dimension refers to a class of dimension estimators depending on a parameter \u03f5 \u2208 (0,1].\nThe empirical dimension of M is defined to be:\n\nd\u03f5(M ) =\n\n||\u03c3||\u03f5\n1\n||\u03c3||\u03f5\n\u03f5\n\nNotice that we use norm notation, although || \u00b7 ||\u03f5 is only a pseudo-norm. When \u03f5 = 1, this is sometimes\ncalled the \u201ceffective rank\u201d of the data matrix.\n\nEmpirical dimension satisfies a few important properties. First, empirical dimension is invariant\nunder rotation and scaling of a data set. Additionally, in the absence of noise, empirical dimension\nnever exceeds true dimension, but it approaches true dimension as the number of measurements goes\nto infinity for spherically symmetric distributions. Thus, d\u03f5 is a true dimension estimator (whereas\nthe nuclear norm is a proxy for dimension). To use empirical dimension as our regularizer, we define\nP(x) = d\u03f5(M).\n\nEmpirical dimension is governed by its parameter, \u03f5. An \u03f5 near 0 results in a \u201cstrict\u201d estimator, which\nis appropriate for estimating dimension in situations where you have little noise and you expect your\ndata to live in true linear spaces. If \u03f5 is near 1 then d\u03f5 is a lenient estimator. This makes it less\nsensitive to noise, and more tolerant of data sets that are only approximately linear. In all of the\nexperiments we present, we use \u03f5 = 0.6, although we found that other tested values also worked well.\n\n3.3\n\nImplementation Details\n\nWe fix L = 10 for the sliding window and let (x) = |x|. We use this form for so that all terms in the total\nenergy function behave linearly in a known range of values. If our fit terms behaved quadratically,\nit would be more challenging to balance them against a penalty term. We also tested a Huber loss\nfunction for and have concluded that such a regularization is not needed.\n\nWe fix a parameter m for each penalty form (selected empirically - see the supplementary material\nfor our procedure), which determines the strength of the penalty. The weak and strong regularization\nparameters are set as follows:\n\n\u03b1weak =\n\n1\nmn2 and \u03b1strong =\nThe weak scaling implies that a perfectly-matched feature will contribute 0 to the total energy, and a\npoorly-matched feature will contribute an amount on the order of 1/m to the total energy. The penalty\nterm will contribute on the order of 1 to the total energy. Since we do not divide the contributions of\neach feature by the number of features, the penalty terms contribution is comparable in magnitude to\nthat of a single feature. The strong scaling implies that the penalty term is on the same scale as the\nsum of the contributions of all of the features in the scene.\n\n1\nmF n2\n\n3.3.1 Minimization Strategy\n\nThe total energy function we propose for constrained tracking is non-convex since the contributions\nfrom the template fit terms are not convex (even if P is convex); this is also the case with other feature\ntracking methods, including the Lucas-Kanade tracker. We employ a 1st-order descent approach for\ndriving the energy to a local minimum.\n\nTo reduce the computational load of feature tracking, some trackers use 2nd-order methods for\noptimization. This works well when tracking strong features, but in our experience it can be\nunreliable when dealing with weak or ambiguous features. Since we are explicitly trying to improve\ntracking accuracy on poor features we opt for a 1st-order descent approach instead.\n\nThe simplest 1st-order descent method is (sub)gradient descent. Unfortunately, because there can be\na very large difference in magnitude between the contributions of strong and weak features to our\ntotal energy, our problem is not well-conditioned. If we pursue standard gradient descent, the strong\nfeatures dictate the step direction and the weak features have very little effect on it. Ideally, once the\nstrong features are correctly positioned, they will no longer dominate the step direction. If we were\nable to perfectly measure the gradient of our objective function, this would be the case. In practice,\nthe error in our numerical gradient estimate can be large enough to prevent the strong features from\n\n4\n\n\fever relinquishing control over the step direction. The result is that in a scene with both very strong\nand very weak features, the weak features may not be tracked.\n\nTo remedy this, we compute our step direction by blending the gradient of the energy function with a\nvector that corresponds to taking equal-sized gradient descent steps separately for each feature. We\nuse a fast line search in each iteration to find the nearest local minimum in the step direction. This\ncompromise approach allows for efficient descent while ensuring that each feature has some control\nover the step direction (regardless of feature strength).\n\nBecause the energy is not convex, it is important to choose a good initial state. We use a combination\nof two strategies to initialize the tracking: first, we generate our initial guess of x by registering an\nentire frame of video with the previous (at lower resolution). Secondly, we use multi-resolution, or\npyramidal tracking so that approximate motion on a large scale can help us get close to the minimum\nbefore we try tracking on finer resolution levels.\n\nWe now explain the details of the algorithm. Let I denote a full new frame of video and let xprev be\nthe concatenation of feature positions in the previous frame. We form a pyramid for I where level 0\nis the full-resolution image and each higher level m (1 through 3) has half the vertical and half the\nhorizontal resolution of level m 1. To initialize the optimization, we take the full frame (at resolution\nlevel 3) and register it against the previous frame (also at resolution level 3) using gradient descent\nand an absolute value loss function. We initialize each features position in the current frame by taking\nits position in the previous frame and adding the offset between the frames, as found through this\nregistration process). Once we have our initial x, we begin optimization on the top pyramid level.\nWhen done on the top level, we use the result to initialize optimization on the level below it, and\nso on until we have found a local minimum on level 0. On any given pyramid level, we perform\noptimization by iteratively computing a step direction and conducting a fast line search to find a local\nminimum in the search direction. We impose a minimum and maximum on the number of steps to be\nperformed on each level (mini and maxi, respectively). Our termination condition (on a given level)\nis when the magnitude of the derivative of C is not significantly smaller than it was in the previous\nstep. To compute our search direction in each step, we first compute the gradient of C (which we will\ncall DC) and set a =\n\nThis is done by breaking it into a collection of 2-vectors (elements 1 and 2 are together, elements\n3 and 4 are together, and so on) and normalizing each of them. We then recombine the normalized\n2-vectors to get b. We blend a with c to compute our step direction. Algorithm 1 summarizes the full\nprocess.\n\n3.3.2 Efficiency and Complexity\n\nWe have found that our algorithm typically converges in about 20 iterations or less at each pyramid\nlevel (with fewer iterations on lower pyramid levels). In our experiments, we used a resolution\nof 640-by-480 (we have also done tests at 1000 \u00d7 562), and we found that 4 pyramid levels were\nsufficient for reliable tracking. Thus, on average, less than 80 iterations are required to track from\none frame to the next. A single iteration requires one gradient evaluation and multiple evaluations\nof C. The complexity of a gradient evaluation is k1F n2 + k2LF 2, and the complexity of an energy\nevaluation is k3F n2 + k4L2F . Our C++ implementation (which makes use of OpenCV) can run\non 35 features of size 7-by-7 with a temporal window of 6 frames (L = 5) on a 3rd-generation\nIntel i5 CPU at approximately 16 frames per second. SIMD instructions are used in places, but no\nmulti-threading was used, so faster processing rates are possible. With a larger window of L = 10 our\nalgorithm still runs at 2-5 frames per second.",
  "related_work": "",
  "methodology": "",
  "experiments": "To evaluate our method, we conducted tests on several real video sequences in circumstances that are\ndifficult for feature tracking. These included shaky footage in low-light environments. The resulting\nvideos contained dark regions with few good features and the unsteady camera motion and poor\nlighting introduced time-varying motion blur.\n\nIn these video sequences it proved very difficult to hand-register features for ground-truth. In order to\npresent a quantitative numerical comparison we also collected higher-quality video sequences and\nsynthetically degraded their quality. We used a standard Lucas-Kanade tracker on the non-degraded\n\n5\n\n\fvideos to generate ground-truth (the output was human-verified and corrected). We therefore present\nqualitative results on real, low-quality video sequences, as well as quantitative results on a set of\nsynthetically degraded videos.\n\n4.1 Qualitative Experiments on Real Videos\n\nIn our tests on real video sequences containing low- quality features, single-feature tracking does\nnot provide acceptable results. When following a non-distinctive feature, the single-feature energy\nfunction often flattens out in one or more directions. A tracker may move in any ambiguous direction\nwithout realizing a better or worse match with the features template. This results in the tracked\nlocation drifting away from a features true location (i.e. \u201cwandering\u201d). This is not a technical\nlimitation of one particular tracking implementation. Rather, it is a fundamental problem due to the\nfact that the local imagery in a small neighborhood of a feature does not always contain enough\ninformation to deduce the features motion between frames. This claim can be verified by attempting\n\n6",
  "results": "",
  "conclusion": ""
}
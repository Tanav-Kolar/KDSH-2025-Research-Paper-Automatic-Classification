{
  "title": "Examining Machine Learning\u2019s Impact on Personal\nPrivacy",
  "abstract": "This paper delves into the growing concerns surrounding the use of machine\nlearning and its impact on personal privacy. It highlights the potential for misuse in\nsurveillance technologies and proposes various strategies to counter these threats,\nemphasizing the need for collaboration between machine learning experts and\nhuman-computer interaction (HCI) researchers.",
  "introduction": "The intersection of machine learning and privacy has become a significant area of study within the\nfield of computer science. While privacy-preserving techniques such as differential privacy offer\npotential solutions, some machine learning systems, particularly those designed for biometric analysis\nor behavioral profiling, inherently compromise individual privacy. Therefore, there is a crucial need\nto explore methods beyond these traditional approaches.\n\nAlthough various definitions and frameworks for privacy have been proposed, a universal consensus\nremains elusive. This paper focuses on specific harms to privacy caused or made worse by machine\nlearning systems. In an era of powerful algorithms and massive datasets, maintaining privacy is\nincreasingly challenging, given that facial recognition systems can identify individuals in public\nspaces, targeted advertising can exploit user profiles, and predictive policing algorithms can single\nout individuals for surveillance. This paper addresses these unique threats to privacy that machine\nlearning systems enable.\n\nThis research provides an overview of strategies developed to combat privacy-threatening machine\nlearning systems and advocates for increased collaboration between the machine learning community\nand experts in the field of human-computer interaction (HCI). Two main approaches are discussed:\nfirst, challenging the data that feeds these models through obfuscation or data withholding, and\nsecond, directly challenging the model itself through public pressure or regulation. This paper\nsuggests that computer scientists have an important role to play in both these approaches.\n\n2 Challenging Data\n\nMachine learning systems depend on data for both training and operation. Data is used to train\nmachine learning models, and new data is fed into the models to generate predictions. These training\nand deployment stages can be iterative; models can be updated using new data over time. One way to\noppose a machine learning system is by disrupting the data it relies on. This involves strategies such\nas data obfuscation or withholding of data.\n\n2.1 Obfuscation\n\nOne method for avoiding machine learning surveillance is by altering either the data used to make\npredictions or the data used to train the system. For example, research has shown that glasses can\nbe designed to deceive facial recognition systems. This type of method uses adversarial examples,\nwhere a slight modification to a data point is enough to cause misclassification by a machine learning\n\n.\n\n\fmodel but is imperceptible to humans. Various strategies have been developed for evading facial\nrecognition using adversarial examples, with the aim to help individuals avoid surveillance. However,\nthese approaches often lack strong guarantees.\n\nAnother approach involves altering the training data used for machine learning models, known as\ndata poisoning attacks. For example, systems can create altered images to reduce the accuracy of\ndeep learning models. Additionally, some vendors sell clothing designed to trigger automated license\nplate readers by injecting junk data, furthering this method.\n\nBeyond image classification, similar obfuscation tactics have also been used to counter web tracking\nand loyalty card-based tracking. Obfuscation can also have an expressive function, as illustrated by\ngroups who use unusual makeup to challenge facial recognition. These acts serve a dual purpose of\nboth evading surveillance and protesting against its use.\n\nWhile adversarial examples and data poisoning are ongoing topics of study, these technologies need\nfurther evaluation before being adopted as anti-surveillance tools. Accessibility, evaluation methods,\nand communication of risks are areas that require further work and collaboration between machine\nlearning experts, HCI researchers, activists, and other relevant stakeholders.\n\n2.2 Withholding Data\n\nAn alternative approach to altering data is to withhold it entirely. This can be achieved through\nprivacy-enhancing technologies that block web tracking. While tracker-blocking browser extensions\ncan provide some privacy to individuals, data can also be withheld collectively. Data strikes, a form\nof digital boycott, can apply pressure to technology companies. Protest non-use is another way of\nwithholding data, where people stop using platforms due to privacy concerns. These methods go\nbeyond simple evasion, using the act of withholding data as a way to launch broader campaigns\nagainst surveillance systems.\n\n3 Challenging Models\n\nWhile data-oriented approaches are helpful, policy solutions may offer a more effective way to\nresist machine learning surveillance systems. For example, while strategies can help evade facial\nrecognition, banning the technology would render those strategies unnecessary. There are many\nforms that regulation can take and many roles that computer scientists can play in this process.\n\nOne method of pressuring companies that develop surveillance technologies is through auditing.\nResearch audits of facial recognition systems have shown they perform poorly on darker-skinned\nsubjects, which has led to wrongful arrests. These audits have led some companies to stop selling\nfacial recognition technology. However, audits do have limitations, as they can sometimes normalize\nharmful tasks for certain communities.\n\nSome technologies are difficult to audit due to restricted access. Nevertheless, these systems can\nsometimes be reverse-engineered to show potential societal harms. Predictive policing systems, for\ninstance, can amplify existing biases. Algorithmic audits or reverse engineering should focus on\nbroader societal implications of the technology to avoid merely shifting goal posts and algorithmic\nreformism.\n\nResearchers have partnered with community organizations to resist surveillance technologies, debunk-\ning the myth that critics do not understand the technology, and demystifying complex algorithms. It\nis important for researchers to approach these collaborations with humility, as community organizers\nbring their own areas of expertise.\n\nIt is also crucial to recognize the academic community\u2019s role in creating and upholding surveillance\ntechnologies. Computer science educators should make computing\u2019s role in injustice more visible.\nStudent-led efforts can help educate future computer scientists about the consequences of their work.",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "This paper has outlined various methods for resisting machine learning-based surveillance technolo-\ngies. It emphasizes the need for participatory methods when developing anti-surveillance technologies.\n\n2\n\n\fWhile these participatory methods are common in HCI research, the machine learning community\nhas paid less attention to it. The impact of surveillance technologies is disproportionately borne by\nalready marginalized groups. Therefore, it is critical that the design of anti-surveillance technologies\nbe led by those who are most affected.\n\n3"
}
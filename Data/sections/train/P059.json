{
  "title": "Large Vocabulary Handling in Recurrent Neural\nNetworks Enhanced by Positional Encoding",
  "abstract": "This research presents a counterintuitive discovery: positional encoding, a high-\ndimensional representation of time indices on input data, improves the learning\ncapabilities of recurrent neural networks (RNNs). Although positional encoding is\nwidely recognized for complementing Transformer neural networks by enabling\nthem to process data order, its application to RNNs seems unnecessary because\nRNNs inherently encode temporal information. However, our analysis using syn-\nthetic benchmarks shows that combining positional encoding with RNNs offers\nadvantages, especially when dealing with extensive vocabularies that include low-\nfrequency tokens. Further investigation reveals that these infrequent tokens cause\ninstability in the gradients of standard RNNs, and positional encoding helps to miti-\ngate this instability. These findings highlight a new function of positional encoding\nbeyond its well-known role as a timekeeping mechanism for Transformers.",
  "introduction": "Since their introduction, Transformer neural networks have become the preferred method for pro-\ncessing and generating time series data, surpassing traditional recurrent neural networks (RNNs). A\nsignificant difference between these models is their handling of temporal information, that is, the\nsequence of data points or tokens. RNNs process temporal information by adjusting their internal\nstate based on new inputs and their existing state. Conversely, Transformers lack an intrinsic mecha-\nnism for understanding data sequence order and, therefore, depend on an external system known as\npositional encoding to keep track of time.\n\nPositional encoding represents time indices in a high-dimensional format. A common method\ninvolves using sinusoidal waves of predetermined frequencies. This method marks input tokens by\nadding or appending these vectors to the input embeddings. Unlike RNNs, positional encoding\u2019s time\nrepresentation remains constant regardless of input values until processed by a network.\n\nAlthough positional encoding is often viewed as a way to represent time that can replace RNNs when\nused with Transformers, it is not incompatible with RNNs. Inputs to RNNs can be augmented with\nposition-encoding vectors. Autonomous activities in biological neurons, such as oscillations, are\nbelieved to be important for time perception and other perceptual processes, as well as motor control.\n\nThis study, therefore, investigates the effects of adding positional encoding to the inputs of RNNs,\nusing synthetic benchmarks. The results demonstrate that positional encoding helps RNNs manage a\nmore extensive range of discrete inputs, or a larger vocabulary, compared to those without positional\nencoding.\n\nThe key contributions of this research are outlined below:\n\n\u2022 It illustrates the challenges faced when training RNNs on large vocabularies using carefully\ndesigned benchmark tasks, a problem that has not been widely recognized or addressed in\nprevious research, despite its potential impact on practical applications.\n\n.\n\n\f\u2022 It explains that the difficulties in training RNNs with extensive vocabularies are due to\ngradient instability caused by infrequent tokens, which inevitably occur as vocabulary size\nincreases.\n\n\u2022 It introduces a novel use of positional encoding, beyond its typical role in timing for\nTransformers, by integrating it with RNNs. It shows that positional encoding helps alleviate\nissues related to large vocabularies by stabilizing RNN gradients against the disruptions\ncaused by infrequent tokens.\n\n2 Related Studies\n\n2.1 Theoretical and Empirical Computational Power of (Vanilla) RNNs\n\nMathematically, RNNs are recognized as being Turing-complete, capable of simulating Turing\nmachines if their weights are infinitely precise and perfectly tuned. In practice, however, RNN\nweights are limited by finite precision and the need to optimize based on a finite set of observations.\nThese constraints impose practical limitations on the capabilities of RNNs. For instance, empirical\nRNNs cannot store an infinite number of observations in their memory, and the memorized information\ntends to degrade over time.\n\nMore recently, research into extending memory retention has explored continuous-time models.\nInstead of modifying a latent state in discrete-time steps, these models use a linear combination\nof orthogonal polynomials in a continuous-time domain to approximate the input history. The\ncoefficients of these polynomials provide a finite-dimensional representation of the input sequence,\nknown as the High-Order Polynomial Projection Operator (HiPPO), and the dynamics of these\ncoefficients can be described by an ordinary differential equation (ODE). This concept has been\nfurther developed into neural state-space models by replacing the fixed state matrix in the ODE\nwith a learnable one, constrained to a diagonal structure plus a row-rank matrix. With additional\nenhancements, the latest state-space models have shown language modeling performance that rivals\nTransformer-based models.\n\n2.2 Positional Encoding\n\nPositional encoding serves as a high-dimensional representation of the temporal structures present\nin input data. This method is particularly crucial for Transformers, which, unlike RNNs, do not\ninherently capture the order of inputs. Therefore, input tokens to a Transformer are \"time-stamped\"\nby adding or concatenating a position-encoding vector.\n\nIn the initial implementation of the Transformer, token positions were represented using sinusoidal\nwaves of various predefined frequencies. Although this method is effective for a wide range of tasks,\nresearchers have explored other encoding schemes as well. For instance, the well-known BERT\npretraining for natural language processing used learnable embeddings to indicate token positions.\nSome studies have suggested that combining sinusoidal and learnable encodings can enhance model\nperformance. Another approach is to encode the distance between tokens instead of the time elapsed\nfrom the sequence\u2019s beginning.\n\nBeyond Transformers, positional encoding is used to indicate elapsed time in diffusion processes.\nIts effectiveness is not limited to temporal information; studies on three-dimensional mesh and\npoint-cloud modeling have shown that sinusoidal transformation of spatial data outperforms raw\ncoordinate representation.\n\nDespite its widespread use across various areas of machine learning, the application of positional\nencoding to pure RNNs has been largely unexplored. To the author\u2019s knowledge, only a few studies\nhave investigated position-encoded RNNs. The time index in time series data has rarely been directly\nused by RNNs, likely due to perceived redundancy alongside RNN functionalities.\n\n2\n\n\f3 Methods\n\n3.1 Task\n\nThe impact of positional encoding on RNNs was examined using a reverse-ordering task. In this task,\nRNNs were trained to reconstruct a sequence of random integers in reverse order (e.g., given 8, 29, 2,\n11, the output should be 11, 2, 29, 8).\n\n3.2 Model Architecture\n\nThis study\u2019s investigations were based on single-layer gated recurrent units (GRUs), long short-term\nmemory (LSTM) networks, and a neural state-space model, S4D. Each integer in the input sequences\nwas first embedded, concatenated with its positional encoding, and then fed into the RNN or S4D.\nAfter processing the entire input sequence, the network received a command to produce the output,\nrepresented by a time-invariant learnable vector. The outputs from the RNN or S4D module were\nlinearly projected into classification logits, and the cross-entropy loss against the target sequence was\nused to optimize the entire network. Model predictions during testing were determined by the argmax\nof these logits for each time step.\n\nThe canonical sinusoidal positional encoding used for Transformers was adopted in this study.\nSpecifically, each time step t was encoded by a Dpos-dimensional vector, (P Et,1, ..., P Et,Dpos )T ,\ndefined as follows:\n\nP Et,2i := sin\n\nP Et,2i+1 := cos\n\n(cid:33)\n\n(cid:33)\n\n(cid:32)\n\n(cid:32)\n\nt \u2212 1\n\n10000\n\n2(i\u22121)\nDpos\n\nt \u2212 1\n\n10000\n\n2(i\u22121)\nDpos\n\n(1)\n\n(2)\n\nFor learning stability, the positional encoding was normalized by dividing it by (cid:112)Dpos/2, ensuring\nthe encoding vectors had a unit L2-norm. The time step t incremented throughout both input and\noutput phases (i.e., t = 1, ..., L, L + 1, ..., 2L, where L is the input length), without any hard-coded\nlink between input and output positions.\n\n3.3\n\nImplementation Details\n\nAcross the experiments, the dimensionality of the hidden layer of the RNNs was set to 512. The\nembedding of the input integers and the memory cell of the LSTM also had the same dimensionality\nof 512. Similarly, the hidden dimensionality of S4D was set to 512, while its state size (or the order\nof the Legendre polynomials) was maintained at the default value of 64.\n\nThe models were trained for 300,000 iterations using the Adam optimizer with parameters (\u03b21, \u03b22) :=\n(0.9, 0.999) and no weight decay. The learning rate was linearly warmed up from 0.0 to 0.001 for the\nfirst 1,000 iterations, and then annealed according to the cosine schedule. The batch size was 512.\n\nAll experiments were implemented in PyTorch (ver. 2.1.1).",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "4.1 Key Findings\n\nPositional encoding improved the ability of RNNs to handle a larger vocabulary in the reverse-ordering\ntask. The position-encoded GRU and LSTM successfully reversed input sequences of 64 integers\ndrawn uniformly at random from vocabularies of size 32-256 and 256-16,384, respectively, achieving\ntoken-wise accuracy above 95%. In contrast, the performance of the vanilla models without positional\nencoding degraded as the vocabulary size increased. Similarly, positional encoding enhanced the\ncapacity of S4D to handle large vocabularies. These improvements are also evident in the reduced\nsequence-wise reconstruction errors, measured by the Damerau-Levenshtein distance. Neither extra\ntraining iterations nor greater batch sizes improved the performance of the vanilla models.\n\n3\n\n\f4.2 Frequency Matters\n\nThe most apparent consequence of the increased vocabulary size was the reduced chance of observing\nindividual vocabulary items. Accordingly, additional experiments were conducted with non-uniformly\ndistributed tokens to investigate the relation between their frequency and RNN performance. Specif-\nically, the input vocabulary was evenly divided into Frequent and Rare groups, and the Frequent\ntokens had three times the probability of the Rare tokens.\n\nThe training data consisted of 64 independent samples from this dual-frequency vocabulary. By\ncontrast, the test data were systematically constructed so that each sequence included a single\n\"target\" token (Frequent/Rare) whose retrieval was evaluated for accuracy assessment, along with\n63 \"disturbants\" that were either all Frequent or all Rare. The experiment revealed that it was the\ndisturbant tokens whose frequency significantly impacted the performance of the vanilla RNNs and\nS4D. On the one hand, the Rare targets were successfully retrieved as long as they were surrounded\nby the Frequent disturbants. On the other hand, the vanilla GRU struggled to recover the Frequent\ntargets when the other input tokens were filled with the Rare disturbants. The LSTM performance was\nalso degraded, especially when the targets were positioned in the first quarter of the input sequence (1\n\u2264 t \u2264 16). Similarly, the Rare disturbants were detrimental to the S4D; unlike the RNNs, however,\nthe accuracy was worst when the targets were located in the middle of the input sequences (17 \u2264 t \u2264\n32).\n\nIn contrast, the position-encoded RNNs exhibited robustness to the frequency of the target and\ndisturbant tokens. They achieved nearly perfect accuracies in most cases, except when the GRU\nprocessed the fully Rare data whose target was located in the first half of the sequence (1 \u2264 t \u2264\n32). Likewise, positional encoding enhanced the resilience of the S4D against the influence of Rare\ndisturbants.\n\n4.3 Analysis of Gradient Stability\n\nTo delve deeper into the influence of token frequency on RNN performance, the gradients of the\nRNN latent states were scrutinized. In the analysis, pairs of input sequences were processed by the\nRNNs trained on the dual-frequency vocabulary (comprising Frequent and Rare items). Each pair\nof sequences shared the same initial token (t = 1; \"target\") but varied in the subsequent tokens (2\n\u2264 t \u2264 L; \"disturbants\"). Then, gradients were computed for the distant mapping between the first\nand last updated states (i.e., at time t = 1 and 2L) of the RNNs using backpropagation through time.\nThe stability of RNN learning was assessed by measuring the dot-product similarity of the gradients\nbetween the paired input sequences (after normalization over output dimensions).\n\nFormally, the paired input sequences, denoted as A and B, established two distinct, but ideally similar\nmappings, f (A) and f (B), from the first to the last latent state of the RNNs (\u02dch(s)\n2L = f (s)(\u02dcz1), where\ns \u2208 {A, B}). The gradient stability of the RNNs was defined by the dot-product similarities between\nthe normalized gradients of these paired mappings:\n\nStability(A, B) :=\n\nD\n(cid:88)\n\ni=1\n\n\u27e8\u03b1(A)\n\ni \u2207f (A)\n\ni\n\n(\u02dcz1), \u03b1(B)\n\ni \u2207f (B)\n\ni\n\n(\u02dcz1)\u27e9 =\n\nD\n(cid:88)\n\ni \u03b1(B)\n\u03b1(A)\n\ni\n\nwhere the coefficients \u03b1(s)\ni := 1, ..., D:\n\ni\n\nnormalized the raw gradients \u2207f (s)\n\ni\n\n(cid:32) \u2202h(A)\n2L,i\n\u2202z1,j\n\n\u2202h(B)\n2L,i\n\u2202z1,j\n\n\u00b7\n\n(cid:33)\n\ni=1\n\n(1)\n(\u02dcz1) over the output dimensions\n\n\u03b1(s)\ni\n\n:=\n\n(cid:118)\n(cid:117)\n(cid:117)\n(cid:117)\n(cid:116)\n\n2D\n(cid:88)\n\nj=1\n\n(cid:32) \u2202h(s)\n2L,i\n\u2202z1,j\n\n(cid:33)2(cid:44)\n\n(cid:118)\n(cid:117)\n(cid:117)\n(cid:117)\n(cid:116)\n\nD\n(cid:88)\n\n2D\n(cid:88)\n\nk=1\n\nj=1\n\n(cid:33)2\n\n(cid:32) \u2202h(s)\n2L,k\n\u2202z1,j\n\n(2)\n\nMonitoring the gradients at training checkpoints revealed that Rare disturbants destabilize the learning\nof vanilla RNNs. The similarity of the paired gradients decreased gradually (GRU) or rapidly (LSTM)\nwhen the networks were exposed to the Rare disturbants. Positional encoding endowed the RNNs\nwith robustness to these RARE disturbants. Both the GRU and LSTM maintained the high similarity\nof the paired gradients across the different target/disturbant conditions. By contrast, the impact of\npositional encoding on the gradient stability of the S4D was marginal; unlike the RNNs, the vanilla\nS4D was highly stable by itself against Rare disturbants throughout the training, even though there\n\n4\n\n\fwas a visible relative destabilization due to Rare disturbants compared to Frequent disturbants in the\nearly stages of training, as well as an observable improvement by positional encoding.\n\n5 Discussion\n\n5.1 Difficulties in Handling a Large Vocabulary\n\nThis study introduces a novel challenge in training (vanilla) RNNs: managing large vocabularies.\nWhile the manageable vocabulary size of RNNs is a pertinent research area, crucial for empirical\napplications like natural language processing, previous studies have primarily focused on evaluating\nand improving the memory duration of RNNs, typically with small vocabulary sizes.\n\nThis research examined RNN gradients and identified their destabilization when processing low-\nfrequency tokens, which are necessarily included in a large vocabulary. Specifically, inputs that do\nnot contribute to gradient-based optimization at a target time step were found to be detrimental.\n\nIn general time series processing, data points carrying crucial information for specific time steps\nbecome irrelevant otherwise. Consequently, each token exhibits a dual nature\u2014both crucial and\nnoisy\u2014throughout the task. Processing rare tokens is particularly challenging, presumably because\nthey are irrelevant most of the time while making a large impact on learning due to their greater loss,\ncompensating for fewer learning opportunities. Dealing with such \"unignorable noise\" presents a\npervasive challenge for RNNs.\n\n5.2 Functionality of Positional Encoding beyond the Timekeeper for Transformers\n\nAlthough low-frequency tokens destabilize the gradient-based learning of RNNs, this study also\ndiscovered that positional encoding can alleviate this issue. This enhancement of RNNs via positional\nencoding is noteworthy because RNNs were specifically designed to process time series data on\ntheir own. Unlike Transformers, they are presumed to function without relying on an \"external\nclock\". Consequently, position-encoded RNNs have remained largely unexplored. The findings of\nthe present study\u2014namely, the improvement in the manageable vocabulary size due to enhanced\ngradient stability\u2014broaden the currently limited understanding of the impact of positional encoding\non RNNs.\n\nAdditionally, the results of this study shed new light on the utility of positional encoding. While\npositional encoding has been viewed as nothing more than input timestamps for Transformers, the\npresent study demonstrated its efficacy in stabilizing the gradients of RNNs against disruption by\nlow-frequency tokens. This novel functionality of positional encoding would not have been visible in\nTransformer studies, as the model can dynamically adjust the relevance of input tokens through their\nattention mechanism, thus inherently mitigating the impact of disturbant tokens.\n\n5.3 Limitations and Future Directions\n\nA primary unresolved question in this study pertains to the mechanism behind the gradient stabilization\nby positional encoding. All the findings here are based on experimental investigations, lacking\nrigorous mathematical explanations for how and why the gradients of RNNs are destabilized by\ninfrequent tokens and stabilized by positional encoding. Moreover, the present study primarily focused\non the canonical implementation of sinusoidal positional encoding designed for Transformers, leaving\nopen which parameters of the sinusoidal waves (i.e., frequencies and phases) are critical for gradient\nstabilization. Future research may broaden its scope to encompass more general forms of positional\nencoding, such as wavelets and non-periodic signals.\n\nMoreover, the analysis of gradient stability did not fully address the enhanced performance of\nthe position-encoded state-space model (S4D). In terms of accuracy, the positioned-encoded S4D\nexhibited greater robustness to infrequent tokens compared to the vanilla model, resembling the\nbehavior observed in RNNs. However, the gradients of the vanilla S4D were too stable to account for\nthis decline in performance. This leaves open the question of how positional encoding influences\ngradient-based learning of state-space models. Additionally, future studies may investigate a broader\nrange of state-space models to achieve a comprehensive understanding of the interplay between\npositional encoding and these models.\n\n5\n\n\fIn addition to these scientifically oriented questions, future studies could also address practical\napplications of position-encoded RNNs and neural state-space models. Although positional encoding\nenhanced model performance across different synthetic tasks, the extent of this enhancement is task-\ndependent. Thus, positional encoding is not a panacea for arbitrary tasks, and further investigations\nare necessary to determine when it is effective.\n\n6 Appendix\n\n6.1 Other Tasks\n\nThis section demonstrates the effectiveness of positional encoding on RNNs across different tasks,\nbesides the reverse ordering task discussed in the main text.\n\n6.1.1 Reverse-Ordering + Delayed-Addition\n\nThis section reports the performance of position-encoded RNNs on a more complicated, combinatorial\ntask than the reverse ordering of input sequences. Extending the reverse-ordering task, the models\nreceived additional random input integers during the output phase, and added each of them to the\ncorresponding token in the reverse-ordered input sequence (modulo the vocabulary size, so that the\noutput range was bounded). This task was too challenging to GRUs\u2014even after reducing the input\nlength to L = 16\u2014so only the results from LSTMs are reported below. Also, the network was trained\nfor 600,000 iterations (i.e., twice longer than the other tasks) for ensuring the convergence. The other\nconditions/hyperparameters were the same as reported in the main text. Consequently, positional\nencoding improved the model performance as the vocabulary size grew from 896 to 1088.\n\n6.1.2 Sorting\n\nIn the reverse ordering task, the order of input integers was important information for accomplishing\nthe task. Thus, positional encoding may play its originally intended role in encoding the temporal\ninformation.\n\nThis section reports the effectiveness of positional encoding for a task in which the order of input\nobservations was completely irrelevant; the learning objective was to simply sort the input integers in\ntheir inherent ascending order (e.g. 8, 29, 2, 11 -> 2, 8, 11, 29). The input integers were uniformly\nrandomly sampled with replacement, allowing for ties in the sorting process.\n\nAs a result, positional encoding also proved effective for RNNs to handle a larger vocabulary in the\nsorting task, though the improvement remained marginal compared to the reverse-ordering task.\n\n6.1.3 Predecessor Query\n\nFinally, this section presents benchmark results for the predecessor-query task. The network first\nreceived a sequence of non-repeating random integers, x1, ..., xL. Subsequently, one of the non-initial\ninput integers, xtquery (2 \u2264 tquery \u2264 L), was randomly selected and reintroduced to the network\nat time t = L + 1. The learning objective is to return the predecessor of the reviewed integer (=\nxtquery\u22121). The predecessor-query task evaluates the capacity of RNNs to integrate information\nregarding both the order and content of input sequences.\n\nAs in the reverse-ordering + delayed-addition task, the input sequence was reduced to L = 16 due\nto the complexity of the task, and the experiment focused on the LSTM. The number of training\niterations was maintained at 300,000. Similar to the other benchmarks, positional encoding improved\nthe LSTM\u2019s capacity to manage the larger vocabularies.\n\n6.2 Robustness to Variations in Input Length\n\nSo far, all the tasks were experimented using fixed-length inputs (L = 64). One might wonder if\npositional encoding is exceptionally effective under this setting, informing RNNs with the exact\ntiming when each input token should be returned as the output. Thus, it remains unclear whether\nor not position-encoded RNNs can also handle a larger vocabulary even when the input length is\nvariable and, thus, the exact timing of the output emission is not identifiable from the positional\nencoding attached to the inputs.\n\n6\n\n\fTo assess the robustness to variations in the input length, an additional experiment was conducted on\nthe LSTM, with the input length varied between 32 and 64. In this setup, the maximum input length\n(= 64) covers the entirety of the shortest input sequence plus its reversed reconstruction (= 32 + 32).\nConsequently, the positional encoding per se cannot even distinguish the input vs. output phases at t\n= 33, ..., 64. The vocabulary size was set to 16,384.\n\nAs a result, the positional encoding still improved the LSTM\u2019s performance on the reverse-ordering\ntask against the perturbations in the input length. This result suggests that the effectiveness of the\npositional encoding for RNNs is not limited to strictly scheduled tasks.\n\n6.3 Effects of Additional Parameters in Position-Encoded RNNs\n\nThe concatenation of positional encoding with input embeddings inflates the number of learnable\nparameters in the input-to-hidden projection weights. This additional parameterization per se does\nnot influence the learning of the input embeddings, and therefore does not elucidate the enhanced\nperformance of position-encoded RNNs. This section substantiates this argument by equalizing the\nnumber of learnable parameters between the vanilla and position-encoded models.\n\nSpecifically, the equalization was achieved by concatenating two identical copies of the input\nembeddings and feeding them to the LSTM. This configuration\u2014henceforth termed \"double\nvanilla\"\u2014effectively doubled the size of the input- to-hidden weight for each gate in the LSTM,\naligning it with that of the position-encoded LSTM, while maintaining all other parameters, including\nthe dimensionality of the (non-repeated) input embeddings.\n\nAs illustrated, the double vanilla LSTM did not yield any improvements in the reverse-ordering or\nsort- ing tasks. These results affirm that the reported enhancement of RNNs is not merely attributable\nto the additional parameterization associated with the positional encoding.\n\n6.4 Alternative Implementations of Positional Encoding\n\nWhile this study implemented positional encoding by sinusoidal waves, there are alternative imple-\nmentations proposed in the previous studies. For instance, the BERT-based models typically encode\neach token position by a learnable embedding. Moreover, the original study of Transformer pointed\nout that even random vectors can function as positional encoding.\n\nAccordingly, these two alternative forms of positional encoding were tested on the LSTM performing\nthe reverse- ordering task. The random position-encoding vectors were uniformly and independently\nsampled from the (512 1)- dimensional hypersphere. The learnable embeddings were implemented\nusing the canonical embedding module of PyTorch (torch.nn.Embedding). The input length and\nvocabulary size were set to 64 and 16,384 respectively. Both the random vectors and learnable\nembeddings improved the performance of LSTM.\n\nAmong the different implementations of positional encoding, the sinusoidal encoding outperformed\nthe two alterna- tives. The advantage of the sinusoidal encoding became more apparent when the input\nlength was variable between 32 and 64; the sinusoidal encoding was more robust to the variations in\nthe input length than the others.\n\n6.5 Language Modeling\n\nThis section reports benchmark results for the language modeling task. Single-layer LSTMs with\nand without sinusoidal positional encoding were trained and tested on the WikiText-103 dataset.\nDue to constraints in computational resources, the vocabulary was reduced from the original size of\n267,735 to 32,768 by retokenizing the raw data using SentencePiece. The headings were removed,\nand the main text was segmented by paragraphs (separated by the line break). Additionally, only the\nfirst 1024 tokens of each paragraph were utilized for training and testing, ensuring that the absolute\npositional encoding always aligned with the beginning of each paragraph. The hyperparameters were\nconfigured as specified in \u00a73.3.\n\nAs illustrated, positional encoding proved effective only for marginally faster learning during the\ninitial phase of training. The difference diminished around 10,000/30,000 iterations, and the test\nperplexities of the position-encoded model were inferior to those of the vanilla model.\n\n7\n\n\fTable 1: Test perplexities on the WikiText-103 dataset. The minimum, mean, and maximum are\nobtained from five trials with different random seeds.\n\nModel\n\nMin\n\nMean\n\nMax\n\nVanilla LSTM\n36.8257\nPosition-Encoded LSTM 38.0685\n\n37.7731\n38.5384\n\n38.916589\n38.893656\n\n8",
  "conclusion": ""
}
{
  "title": "Learning Genomic Sequence Representations using\nGraph Neural Networks over De Bruijn Graphs",
  "abstract": "The rapid increase of genomic sequence data requires new methods for creating ro-\nbust sequence representations. Existing techniques often neglect detailed structural\ninformation, focusing mainly on contextual information. We addressed this issue\nby developing k-mer embeddings that combine contextual and structural string\ninformation, by enriching De Bruijn graphs with structural similarity connections.\nWe also crafted a self-supervised method using Contrastive Learning, employing a\nheterogeneous Graph Convolutional Network encoder and constructing positive\npairs based on node similarities. Our embeddings consistently outperform prior\nmethods for Edit Distance Approximation and Closest String Retrieval tasks.",
  "introduction": "Genomic sequence data is growing at an unprecedented rate, requiring the development of novel\nmethods that can provide both accurate and scalable sequence representations. These representations\nare essential for various computational biology tasks, including gene prediction and multiple sequence\nalignment. Methods from Natural Language Processing (NLP), such as Word2Vec and Transformers,\nhave been adopted to improve the representation of genomic sequences. These NLP-based approaches\nare effective at capturing the context within a sequence, which is important because the semantics of\nwords often outweigh their precise letters.\n\nCharacter-level n-gram models might be used to capture structural nuances. However, a uniform\nrepresentation of each n-gram across all sequences can oversimplify the problem. Applying techniques\nlike transformer-based models on n-grams can escalate computational demands. Consequently, these\nmethods may overlook nuanced k-mer variations important for understanding single-nucleotide\npolymorphisms and other minor sequence changes. These SNPs can influence disease susceptibility,\nphenotypic traits, and drug responses.\n\nTherefore, we developed a k-mer embedding approach that combines metagenomic context and string\nstructure. In our method, contextual information refers to the relationships between k-mers closely\nsituated within sequences, and structural information examines nucleotide patterns within a k-mer\nand their relations to other k-mers. We constructed a metagenomic graph that builds upon the De\nBruijn Graph to capture k-mer transitions and structural similarities.\n\nGiven the advances in Graph Neural Networks (GNNs), we grounded our method in GNNs but\ndesigned for heterogeneous graphs. This approach effectively recognizes and uses both contextual\nand structural connection types. Drawing from the success of self-supervised pre-training in NLP\nand Computer Vision, we designed a self-supervised objective for genomic graph data. We employed\ncontrastive loss aiming to align k-mers with similar context and structure in representation space.\n\nFinally, we tested our technique on two downstream tasks: Edit Distance Approximation and Closest\nString Retrieval. The former estimates the minimum changes needed to transform one genomic\nsequence into another, avoiding quadratic computational complexity. The latter task, Closest String\nRetrieval, involves finding sequences similar to a query.\n\n.",
  "related_work": "2.1 Genomic Sequence Representation\n\nMachine learning methods have emerged in computational biology to represent genomic sequences.\nA key component is the k-mer: a continuous nucleotide sequence of length k. The Word2Vec method,\nwhich represents words as vectors using their context, treats overlapping k-mers in genomic sequences\nas words in sentences. Building on this, kmer2vec was introduced to apply Word2Vec to genomic\ndata for Multiple Sequence Alignment. Another strategy is to use the De Bruijn graph, where k-mers\nare nodes and their overlaps are edges, in conjunction with Node2Vec, which derives node features\nfrom the contextual information of biased random walks. This method underpins GRaDL for early\nanimal genome disease detection. K-mers also pair well with transformer-based models: DNABERT\nleverages a BERT-inspired objective and k-mer tokenization to predict genome-wide regulatory\nelements. Metagenome2Vec blends Node2Vec with transformers to analyze metagenomes with\nlimited labeled data. Given the high computational demands of these transformer-based approaches,\nthey are outside the scope of our benchmarks in this study.\n\n2.2 Graph Neural Networks\n\nGraph Convolutional Networks (GCNs) are foundational to several innovations in graph-based\nmachine learning. In genomics, GNNs have been applied in metagenomic binning. Because we aim\nto enhance our node embeddings with structural similarity, both heterogeneity and heterophily are\nkey considerations. Recognizing the ubiquity of heterogeneity in real-world graphs, Relational GCNs\n(R-GCNs) were developed. These networks expand upon GCNs by generalizing the convolution\noperation to handle different edge types. To tackle heterophily, where distant nodes in a graph may\nbear similar features, Geom-GCN maps nodes to a latent space, while another approach suggests a\ndistinct encoding approach for node embeddings and neighborhood aggregations.\n\n2.3 Self-Supervised Learning\n\nSelf-supervised learning (SSL) enables effective use of unlabeled data and reduces dependence on\nannotated labels. Among SSL methods, contrastive learning has made a significant impact. At its\ncore, contrastive learning seeks to bring similar data instances closer in the embedding space while\npushing dissimilar ones apart. When applied to graph data, several techniques have been proposed\nfor obtaining positive pairs, including uniform sampling, node dropping, and random walk sampling.",
  "methodology": "3.1 Metagenomic Graph\n\nThe De Bruijn Graph, which is created from metagenomic sequences, forms the basis of our method.\nIn this graph, each k-mer, a substring of length k from the sequences, is represented by a different\nnode. An edge from node vi to node vj in the graph indicates that the k-mer at node vi directly\nprecedes the k-mer at node vj in one of the sequences of the metagenome.\n\nWhen used, edge weights represent the frequency of these transitions, capturing genomic structures\nwithin the graph.\n\nAlthough Node2Vec captures the sequential context in De Bruijn graphs, it overlooks structural k-mer\nsimilarities. To address this, we expand the graph to include connections based on these similarities.\nWe formulate two edge types for our graph, where nodes vi, vj, ... represent k-mers.\n\nDe Bruijn Graph\u2019s edges The first edge type is designed to capture contextual information. Let\nT (vi, vj) be the count of transitions between k-mers within a dataset of genomic sequences. The\nweight of an edge connecting nodes vi and vj, w(dBG)\n\n, is defined by,\n\nij\n\nw(dBG)\nij\n\n=\n\n(cid:80)\n\nT (vi,vj )\nvk \u2208\u03b4+ (vi) T (vi,vk)\n\nwhere \u03b4+(vi) denotes nodes adjacent to vi via outgoing edges.\n\n2\n\n\fSub-k-mer Frequency edges To capture the structural similarity between strings, we introduce\na method using sub-k-mer frequency vectors, denoted as y(KFsub_k). This vector quantifies the\noccurrences of each sub-k-mer of length sub_k within a given k-mer. The i-th entry indicates the\nfrequency of the i-th sub-k-mer,\ny(KFsub_k)[i] = (cid:80)k\u2212sub_k+1\n\nI[kmer[j : j + sub_k \u2212 1] = si]\u2200si, s \u2208 (cid:80)sub_k\n\nj=1\n\nThe k-mer similarity is determined by the cosine similarity between the sub-k-mer frequency vectors,\n\nw(KFsub_k)\n\nij\n\n=\n\n||y\n\n(KFsub_k )\ni\n\ny\n(KFsub_k )\ni\n\nT y\n\n||2||y\n\n(KFsub_k )\nj\n(KFsub_k )\nj\n\n||2\n\nThis method, scaling linearly with the frequency vector size per weight, provides a computational\nadvantage over the direct Edit Distance calculation for k-mers. We apply edge-filtering at threshold t,\nretaining only the links with the highest similarity. The filtered set of weights is then,\nW (KFsub_k) = {w(KFsub_k)\nTo accommodate graphs for larger k values, we have developed a more scalable approximation of the\nabove approach. It utilizes approximate nearest neighbor search on the sub-k-mer frequency vectors,\nwhich replaces the computationally demanding pairwise cosine similarity calculations.\n\n|w(KFsub_k)\n\n\u2265 t}\n\nij\n\nij\n\nThe metagenomic graph is defined as G = (V, E, W ). Nodes V correspond to individual k-mers.\nThe edges E can be categorized into two sets: De Bruijn Graphs\u2019s edges E(dBG) and Sub-k-mer\nFrequency edges E(KF ). Edges in E(KF ) may be further subdivided based on various sub_k values.\nEdge weights W can contain W (dBG) and several W (KFsub_k).\n\n3.2 Encoder\n\nWe tailored GNNs for a heterogeneous metagenomic graph to capture nuanced k-mer relationships.\nThe design employs varying depths of message passing: deeper for De Bruijn edges to capture\nbroader context and shallower for similarity measures. Central to this GNN is the adapted Graph\nConvolutional Layer, formulated as:\nH (l+1) = \u03c3( \u02dcD(edge_type)\u2212 1\nwhere \u02dcW (edge_type) includes added self-loops and \u02dcDii is its diagonal degree matrix. The term\nedge_type refers to either dBG or KFsub_k. The GCN layout consists of multiple layers, each\ncharacterized by a unique edge feature type and the number of channels.\n\n2 \u02dcW (edge_type) \u02dcD(edge_type)\u2212 1\n\n2 H (l)\u0398(l))\n\n3.3 Self-Supervised Task\n\nWe use a contrastive learning method for k-mer representations. Graph nodes are initialized using\na sub-k-mer frequency vector. Positive and negative pairs are sampled and, along with the k-mer\nrepresentations from the encoder, are used to compute the loss.\n\nBiased Random Walk Sampling We employ Biased Random Walk Sampling to capture k-mer\ncontextual information. This approach uses w(dBG) edges to conduct walks, implemented exactly\nas in Node2Vec. Given a walk of a set length, we extract positive pairs by applying a window of\nsize m. Using a shrink factor \u03b4, drawn uniformly from 1, ..., m, we determine the range i \u00b1 \u03b4 within\nwhich nodes are considered positive pairs to node vi. Repeating this across multiple random walks,\nwe gather a comprehensive set of positive pairs.\n\nStructural Similarity Sampling To capture the structural notion of k-mers, we sample pairs with\nprobability proportional to sub-k-mer frequency similarity, w(KFsub_k). The goal is for k-mers linked\nby higher similarity to have similar representations. The probability of sampling is given by,\nP (vi, vj) \u221d w(KFsub_k)\nNegative Sampling We randomly select negative pairs from all node pairs in the graph, leveraging\nthe assumption that most pairs lack a high similarity edge. This approach ensures diversity in learned\nrepresentations.\n\nij\n\n3\n\n\fLoss Function Having established both positive (Ppos) and negative (Pneg) pair types, we apply the\ncontrastive loss function. Using \u03c3(x) as the sigmoid function, the loss function is:\nlij = \u2212log(\u03c3(zT\n\nlog(1 \u2212 \u03c3(zT\n\ni zj)) \u2212 (cid:80)\n\ni zl))\n\n(vi,vl)\u2208Pneg\n\nTo reduce memory usage, we employed Neighborhood Sampling for mini-batching during training.\n\n4 Bioinformatics Tasks\n\n4.1 Edit Distance Approximation\n\nThe task is to calculate the edit distance without quadratic complexity. The NeuroSEED framework\noffers a solution using sequence representations trained on a ground truth set of edit distances. In our\napproach, we began with sequence representations derived from k-mer embeddings and fine-tuned\nthem with a single linear layer. Our experiments were tested against One-Hot encoding (for k =\n1), Word2Vec, and Node2Vec. To find optimal hyperparameters, we executed a grid search on\nthe validation set. Based on previous work, we used the hyperbolic function. Our primary metric\nfor evaluation was the percentage Root Mean Squared Error (percent RMSE), where l denotes the\ndataset\u2019s maximum sequence length, h represents the hyperbolic distance function, and f\u03b8 indicates\nthe downstream model,\n\n%RM SE(D) = 100\nl\n\n(cid:113)(cid:80)\n\ns1,s2\u2208D(EditDistance(s1, s2) \u2212 h(f\u03b8(s1), f\u03b8(s2)))2\n\n4.2 Closest String Retrieval\n\nThe task is to find the sequence from a reference set that is closest to a query. We assessed embeddings\nfine-tuned on the edit distance approximation task using Convolutional Neural Networks (CNNs).\nThese embeddings were contrasted with ones directly derived from our Self-supervised method,\nOne-Hot, Word2Vec, or Node2Vec, through concatenation or taking the mean of k-mer embeddings.\nFor performance assessment, we used top-n percent accuracies, measuring how often the actual\nsequence appears within the top n percent of positions based on the closeness of embedding vectors\nin hyperbolic space. We selected the optimal model for the embeddings based on the validation loss\nobserved for the previous Edit Distance task.\n\n5 Results and Analysis\n\nIn all our experiments, the memory requirements of the One-Hot method increased exponentially,\nleading to its exclusion from our results for k > 7. When pre-training exclusively on the training set,\nour method, thanks to the GCN encoder, can generalize beyond k-mers present in the training set. In\ncontrast, Node2Vec and Word2Vec can only handle k-mer sizes up to the diversity of the training\ndataset. Therefore, for k > 6, where the test set introduces new k-mers, we excluded these methods.\n\n5.1 Edit Distance Approximation\n\nTable 1 presents the results obtained by using our pre-trained embeddings to estimate edit distances\nbetween sequences on the RT988 and Qiita datasets. For the RT988 dataset, our Contrastive Learning\n(CL) and Node2Vec techniques surpassed Word2Vec and One-Hot. The increased losses in Qiita\nhighlight its greater complexity. In this context, our method\u2019s integration of k-mer structural similarity\nbecomes even more beneficial, outperforming all other tested methods. This benefit becomes more\nevident as k increases, underscoring our embedding\u2019s capability to adapt to new nodes.\n\n5.2 Closest String Retrieval\n\nTables 2a and 2b present the performance of our zero-shot sequence embeddings, directly derived\nfrom the aggregation of our k-mer embeddings, in retrieving the nearest sequences in the Qiita dataset.\nThe tables also showcase a comparison with the embeddings that were specifically fine-tuned for the\nEdit Distance Task.\n\n4\n\n\fFor direct k-mer aggregation, our Contrastive Learning (CL) embeddings are obtained through\nconcatenation, while for k-mer aggregation with One-Hot, Word2Vec, and Node2Vec, we report the\nresults of the better performing method, either concatenation or averaging. The superior zero-shot\nnon-parametric retrieval performance of our CL method emphasizes the combined utility of both\ncontext and structural similarity during self-supervised pre-training. Notably, while k-mers of size\naround three are optimal for Top 1 percent retrieval, larger k-mers excel in the Top 10 percent metrics.\nThis suggests that smaller k-mers are better at discerning local sequence distances, while larger ones\ncapture broader sequence distances.\n\nFor embeddings fine-tuned using CNNs for Edit Distance Approximation, the complexity of CNNs\nobscures differences between the embeddings. Our method based solely on zero-shot concatenated k-\nmer embeddings outperforms this complex fine-tuning. This shows the advantage of our embeddings\nover the method by previous work.",
  "experiments": "",
  "results": "",
  "conclusion": "In our study, we introduced a novel k-mer embedding technique that seamlessly integrates metage-\nnomic contextual and structural nuances, achieved through the enhancement of the De Bruijn graph\nand the use of contrastive learning. In the Edit Distance Approximation task, our technique con-\nsistently demonstrated superior performance compared to One-Hot, Word2Vec, and Node2Vec.\nMoreover, without requiring any downstream fine-tuning, our aggregated k-mer embeddings outper-\nformed the prior method in the Closest String Retrieval task. These findings suggest potential broader\nuses in computational biology.\n\n5"
}
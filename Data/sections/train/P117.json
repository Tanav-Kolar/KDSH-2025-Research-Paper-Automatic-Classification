{
  "title": "Rapid Image Annotation Through Zero-Shot Learning",
  "abstract": "Recent experiments on word analogies demonstrate that contemporary word vectors\neffectively encapsulate subtle linguistic patterns through linear vector displace-\nments. However, the extent to which these straightforward vector displacements\ncan represent visual patterns across words remains uncertain. This research in-\nvestigates a particular image-word relevance relationship. The findings indicate\nthat, for a given image, word vectors of pertinent tags are positioned higher than\nthose of unrelated tags along a primary axis within the word vector space. Drawing\ninspiration from this insight, we suggest addressing image tagging by determining\nthe main axis for an image. Specifically, we utilize linear mappings and intricate\ndeep neural networks to deduce the primary axis from an input image. The re-\nsultant tagging model exhibits remarkable adaptability. It operates swiftly on test\nimages, with a processing time that remains constant regardless of the training set\u2019s\nsize. Furthermore, it showcases exceptional performance not only in conventional\ntagging tasks using the NUS-WIDE dataset but also in comparison to competitive\nbaselines when assigning tags to images that haven\u2019t been seen during training.",
  "introduction": "Recent advancements in representing words in vector spaces have proven advantageous for both\nNatural Language Processing and various computer vision applications, including zero-shot learning\nand image caption generation. The rationale behind using word vectors in NLP is rooted in the\nobservation that detailed linguistic patterns among words are represented by linear offsets of word\nvectors. This pivotal insight emerged from well-known word analogy studies. For example, syntactic\nrelationships like \"dance\" to \"dancing\" parallel \"fly\" to \"flying,\" and semantic connections like \"king\"\nto \"man\" mirror \"queen\" to \"woman.\" Nevertheless, it is yet to be determined whether the visual\npatterns across words, implicitly employed in the aforementioned computer vision tasks, can similarly\nbe represented by these basic vector offsets.\n\nThis paper focuses on the task of image tagging, where an image necessitates the division of a word\nlexicon into two distinct groups based on image-word relevance. For example, an image of a zoo might\nhave relevant tags like \"people,\" \"animal,\" and \"zoo,\" while irrelevant tags might include \"sailor,\"\n\"book,\" and \"landscape.\" This lexical division fundamentally differs from the nuanced syntactic or\nsemantic relationships examined in word analogy tests. Instead, it concerns the connection between\ntwo sets of words as prompted by a visual image. This type of word relationship is semantic and\ndescriptive, emphasizing visual association, albeit at a broader level. Given this context, it is worth\ninvestigating whether word vectors maintain the property where simple linear vector offsets can\ndepict visual or image-based associative relationships between words. In the zoo example, while it\u2019s\neasy for humans to recognize that words like \"people,\" \"animal,\" and \"zoo\" are more related to the\nzoo than words like \"sailor,\" \"book,\" and \"landscape,\" the question is whether such a zoo-association\nrelationship can be represented by the nine pairwise vector offsets: \"people\" minus \"sailor,\" \"people\"\nminus \"book,\" and so on, up to \"zoo\" minus \"landscape,\" between the vectors of relevant and irrelevant\ntags.\n\nA primary contribution of this research is an empirical investigation of these questions. Each image\nestablishes a visual association rule over words, represented as a pair (Y, Y). Leveraging the extensive\n\n.\n\n\fimage collections in benchmark datasets designed for image tagging, we can explore numerous\ndistinct visual association rules in words and the corresponding vector offsets in the word vector\nspace. Our findings uncover a significant correlation: the offsets between the vectors of relevant tags\n(Y) and those of irrelevant tags (Y) predominantly align in a consistent direction, which we term the\n\"principal direction\". In other words, within the word vector space, there exists at least one vector\n(direction), denoted as w, such that its inner products with the vector offsets between Y and Y are\ngreater than 0. This can be expressed as:\n(w,p \u02d82014 n) > 0 equivalently, (w,p) > (w,n)\n\nThis implies that the vector w ranks all relevant words Y ahead of irrelevant ones Y.\n\nThe visual association patterns among words manifest as the linear rank-abilities of their correspond-\ning word vectors. This observation corroborates findings from word analogy studies, suggesting that\nmultiple relationships for a single word are embedded within a high-dimensional space. Furthermore,\nthese relationships can be articulated using basic linear vector arithmetic.\n\nBuilding on this discovery, we propose a solution to the image tagging challenge by identifying the\nprimary axis along which relevant tags are ranked higher than irrelevant ones within the word vector\nspace. We employ both linear mappings and deep neural networks to infer this primary axis from\neach input image. This unique perspective on image tagging yields a highly adaptable tagging model.\nThe model processes test images rapidly, maintaining a constant processing time irrespective of the\ntraining dataset\u2019s size. It not only delivers outstanding results in traditional tagging tasks but also\nexcels at assigning new tags from a broad vocabulary that were not encountered during training. Our\nmethod does not rely on prior knowledge of these new tags, as long as they exist within the same\nvector space as the tags used during training. Consequently, we designate our technique as \"fast\nzero-shot image tagging\" (Fast0Tag), acknowledging its strengths in both speed and its zero-shot\nlearning capabilities.\n\nIn stark contrast to our approach, prior methods for image tagging are limited to assigning only those\ntags to test images that were seen during training, with a notable exception. These methods are\nconstrained by the fixed and often limited number of tags present in the training data, which poses\npractical challenges. For example, Flickr hosts approximately 53 million tags, and this number is\nrapidly increasing. The work of Fu et al. represents a pioneering effort to extend an image tagging\nmodel to previously unseen tags. However, when compared to our proposed method, it depends on\ntwo extra assumptions. Firstly, it assumes that unseen tags are known beforehand to enable model\nadjustment toward these tags. Secondly, it assumes that test images are known in advance for model\nregularization. Moreover, this method is restricted to a very limited number, U, of unseen tags, as it\nneeds to account for all 2U possibletagcombinations.\n\nTo recap, our primary contribution lies in analyzing visual association patterns in words as they relate\nto images and how these patterns are reflected in word vector offsets. We posit and confirm through\nexperiments that a main direction exists in the word vector space for each visual association rule\n(Y, Y), where vectors of relevant words are ranked higher than others. Building on this, our second\ncontribution is an innovative image tagging model, Fast0Tag, which is both swift and capable of\nhandling an open vocabulary of unseen tags. Lastly, we explore three distinct image tagging scenarios:\ntraditional tagging, which assigns seen tags to images; zero-shot tagging, which annotates images\nwith numerous unseen tags; and seen/unseen tagging, which uses both seen and unseen tags. Existing\nresearch either addresses traditional tagging or zero-shot tagging with a limited number of unseen\ntags. Our Fast0Tag method surpasses competitive baselines across all three scenarios.",
  "related_work": "Image Tagging. The objective of image tagging is to allocate pertinent tags to an image or to generate\na ranked list of tags. Within the academic community, this challenge has predominantly been tackled\nfrom the standpoint of tag ranking. Generative approaches, which incorporate topic models and\nmixture models, inherently rank candidate tags based on their conditional probabilities relative to the\ntest image. Conversely, non-parametric, nearest-neighbor-based techniques frequently rank tags for\na test image by aggregating votes from a selection of training images. Although nearest-neighbor\nmethods generally exhibit superior performance compared to those reliant on generative models,\nthey are plagued by substantial computational demands during both training and testing phases.\n\n2\n\n\fThe recently introduced FastTag algorithm offers a significant speed advantage while maintaining\nperformance levels on par with nearest-neighbor methods. Our Fast0Tag method mirrors the reduced\ncomplexity of FastTag. Embedding techniques, on the other hand, determine tag ranking scores via\na cross-modal mapping between images and tags. This concept has been further developed using\ndeep neural networks. Notably, aside from certain exceptions, the majority of these methods do not\ntrain their models with an explicit ranking objective, despite ultimately ranking candidate tags for\ntest images. This discrepancy between the trained models and their practical application contravenes\nthe principle of Occam\u2019s razor. We incorporate a ranking loss in our approach, similar to these\nexceptions.\n\nUnlike our Fast0Tag, which is capable of ranking both known and an unlimited number\nof previously unseen tags for test\nthe methods mentioned earlier are restricted\nto assigning tags to images from a predetermined vocabulary encountered during train-\ning. An exception to this is the work by Fu et al., where they address a predefined\nnumber, U, of unseen tags by developing a multi-label model that considers all possible\n2U combinationsof thesetags.However, thisapproachisconstrainedbythesmallnumberU of unseentagsitcanhandle.\n\nimages,\n\nWord Embedding. Diverging from the conventional one-hot vector representation of words, word\nembedding maps each word to a continuous-valued vector, primarily learning from the statistical\npatterns of word co-occurrences. While earlier studies on word embedding exist, our research\nemphasizes the latest GloVe and word2vec vectors. As demonstrated in the well-known word analogy\nexperiments, both types of word vectors effectively capture detailed semantic and syntactic patterns\nthrough vector offsets. In this study, we further reveal that basic linear offsets can also represent the\nbroader visual association patterns among words.\n\nZero-Shot Learning. The term \"zero-shot learning\" is frequently used interchangeably with \"zero-shot\nclassification,\" although the latter is actually a subset of the former. In contrast to weakly-supervised\nlearning, which acquires new concepts by extracting information from noisy samples, zero-shot\nclassification aims to classify objects from unseen classes by learning classifiers from seen classes.\nAttributes and word vectors are two primary semantic sources that enable zero-shot classification.\n\nOur Fast0Tag, together with Fu et al., expands the domain of zero-shot learning to include zero-\nshot multi-label classification. Fu et al. approach this by converting the problem into zero-shot\nclassification, where each combination of multiple labels is treated as a separate class. We, on the\nother hand, model the labels directly, allowing us to assign or rank a large number of unseen tags for\nan image.\n\n3 The Linear Rank-Ability of Word Vectors\n\nOur Fast0Tag method is enhanced by the discovery that the visual relationship between words,\nspecifically how a lexicon is divided based on relevance to an image, manifests in the word vector\nspace as a main direction. Along this direction, words or tags that are relevant to the image are ranked\nhigher than those that are not. This section elaborates on this discovery.\n\n3.1 The Regulation Over Words Due to Image Tagging\n\nLet\u2019s denote S as the set of seen tags available for training image tagging models, and U as the set\nof tags unseen during the training phase. The training data is structured as (xm, Ym); m = 1, 2, ...,\nM, where xm represents the feature vector of image m in RD, and Ym is a subset of S, containing\nthe seen tags relevant to that image. For simplicity, we also use Ym to represent the collection of\ncorresponding word or tag vectors.\n\nTraditional image tagging seeks to assign seen tags from S to test images. Zero-shot tagging, as\ndefined by Fu et al., aims to annotate test images using a predetermined set of unseen tags, U. Beyond\nthese two scenarios, this paper introduces seen/unseen image tagging, which identifies both relevant\nseen tags from S and relevant unseen tags from U for test images. Furthermore, the set of unseen\ntags, U, can be open and continuously expanding.\n\nWe define Ym as the complement of Ym in S, representing irrelevant seen tags. An image m\nestablishes a visual association rule among words, essentially partitioning seen tags into two distinct\nsets: Ym and Ym. Recognizing that various detailed syntactic and semantic patterns among words\n\n3\n\n\fcan be depicted through linear word vector offsets, we proceed to investigate the characteristics these\nvector offsets might exhibit for this novel visual association rule.\n\n3.2 Principal Direction and Cluster Structure\n\nFigure 2 offers a visual representation of vector offsets (p - n), where p belongs to Ym and n belongs\nto Ym, using both t-SNE and PCA for two different visual association rules over words. One rule is\ndefined by an image associated with 5 relevant tags, and the other by an image with 15 relevant tags.\nFrom these vector offsets, we identify two key structures:\n\nPrincipal Direction: For a given visual association rule (Ym, Ym) in words for image m, the vector\noffsets predominantly point in a similar direction, which we refer to as the principal direction. This\nsuggests that along this principal direction, relevant tags Ym are ranked higher than irrelevant ones\nYm.\n\nCluster Structure: Within each visual association rule over words, there are discernible cluster\nstructures in the vector offsets. Moreover, all offsets that point to the same relevant tag in Ym are\ngrouped within the same cluster. In Figure 2, we distinguish offsets pointing to different relevant tags\nby using different colors.\n\nThe question remains whether these two observations can be generalized. Specifically, do they remain\nvalid in the high-dimensional word vector space for a broader range of visual association rules defined\nby other images? To address this, we designed an experiment to confirm the existence of principal\ndirections in word vector spaces, or equivalently, the linear rank-ability of word vectors. We defer the\ninvestigation of the cluster structure to future research.\n\n3.3 Testing the Linear Rank-Ability Hypothesis\n\nThe experiments in this section are performed using the validation set of the NUS-WIDE dataset,\nwhich includes 26,844 images, 925 seen tags (S), and 81 unseen tags (U). The number of relevant\nseen/unseen tags associated with an image varies from 1 to 20/117, with an average of 1.7/4.9. Further\ndetails can be found in Section 5.\n\nOur goal is to explore whether a primary direction exists for any visual association rule (Ym, Ym)\ncreated by image m, along which relevant tags Ym rank higher than irrelevant tags Ym. This can be\nconfirmed if we find a vector w in the word vector space that fulfills the ranking conditions (w, p) >\n(w, n) for all p in Ym and n in Ym.\n\nTo achieve this, we train a linear ranking SVM for each visual association rule using all corresponding\npairs (p, n). We then rank word vectors using the SVM and assess the number of violated constraints.\nSpecifically, we use MiAP, with higher values being preferable, to compare the SVM\u2019s ranking list\nagainst the ranking constraints. This process is repeated for all validation images, resulting in 21,863\nunique visual association rules.\n\nRanking SVM Implementation. We utilize the primal formulation of ranking SVM for our experi-\nments, which is defined as:\nmin 1/2 ||w||2 + max(0, 1 \u2212 (w, yi) + (w, yj))f oryiY m, yjY m\n\nHere, is a hyperparameter that balances the objective and regularization.\n\nResults. The average MiAP outcomes across all distinct regulations are presented in Figure 3(left).\nWe evaluate 300D GloVe vectors and word2vec vectors of dimensions 100, 300, 500, and 1000. The\nhorizontal axis represents various regularizations used for training the ranking SVMs, with higher\nvalues indicating stronger regularization. In the 300D GloVe space and word2vec spaces of 300, 500,\nand 1000 dimensions, more than two ranking SVMs, with low values, produce nearly ideal ranking\nresults (MiAP 1). This demonstrates that seen tags S are linearly rankable under almost every visual\nassociation rule, satisfying all ranking constraints set by relevant Ym and irrelevant Ym tags for\nimage m.\n\nHowever, caution is advised before extending conclusions beyond the experimental vocabulary S\nof seen tags. While an image m imposes a visual association rule over all words, this rule leads\nto different partitions of distinct experimental vocabularies (e.g., seen tags S and unseen tags U).\n\n4\n\n\fTherefore, we anticipate that the principal direction for seen tags should also apply to unseen tags\nunder the same rule, if the questions at the end of Section 3.2 are answered affirmatively.\n\nGeneralization to Unseen Tags. We investigate whether the same principal direction applies to both\nseen and unseen tags under each visual association rule induced by an image. This is partially\nvalidated by applying the previously trained ranking SVMs to unseen tag vectors, as the \"true\"\nprincipal directions are unknown. We use the 81 unseen tags U as \"test data\" for the trained ranking\nSVMs, each resulting from an image-induced visual association. NUS-WIDE provides annotations\nfor these 81 tags. The results, shown in Figure 3(right), significantly outperform the basic baseline of\nrandom tag ranking, indicating that the directions produced by SVMs are generalizable to the new\nvocabulary U of words.\n\nObservation. We conclude that word vectors are an effective medium for transferring knowl-\nedge\u2014specifically, rank-ability along the principal direction\u2014from seen to unseen tags. We have\nempirically confirmed that the visual association rule (Ym, Ym) in words due to an image m can be\nrepresented by the linear rank-ability of corresponding word vectors along a principal direction. Our\nexperiments involve a total of |S| + |U| = 1,006 words. Future work should include larger-scale and\ntheoretical studies.\n\n4 Approximating the Linear Ranking Functions\n\nThis section introduces our Fast0Tag approach for image tagging. Initially, we explain how to address\nimage tagging by approximating the principal directions, based on their existence and generalization,\nas confirmed in the previous section. Subsequently, we describe the detailed approximation methods\nused.\n\n4.1\n\nImage Tagging by Ranking\n\nBased on the findings from Section 3, which indicate the existence of a principal direction, wm, in the\nword vector space for each visual association rule (Ym, Ym) generated by an image m, we propose a\ndirect solution for image tagging. The core idea is to approximate this principal direction by learning\na mapping function, f(\u02d800b7), that connects the visual space to the word vector space, such that:\n\nf(xm) wm\n\nHere, xm is the visual feature representation of image m. Consequently, given a test image x, we\ncan promptly suggest a list of tags by ranking the word vectors of the tags along the direction f(x),\nspecifically by the ranking scores:\n\nt S U, (f(x), t)\n\nThis applies whether the tags are from the seen set S or the unseen set U.\n\nWe investigate both linear and nonlinear neural networks to implement the approximation function\nf(x) w.\n\n4.2 Approximation by Linear Regression\n\nIn this approach, we assume a linear function from the input image representation x to the output\nprincipal direction w, defined as:\n\nf(x) := Ax\n\nHere, A can be determined in a closed form through linear regression. Thus, from the training data,\nwe have:\n\nwm = Axm+m, f orm = 1, 2, ..., M\n\nwhere wmistheprincipaldirectionf orallof f setvectorsof theseentags, correspondingtothevisualassociationrule(Ym, Ym)f orimagem, andmrepresentstheerrors.M inimizingthemeansquarederrorsprovidesuswithaclosed\u2212\nf ormsolutionf orA.\n\na\n\nchallenge\n\nHowever,\nwm.T hetrainingdataonlyprovideimagesxmandrelevanttagsYm.W eoptf orastraightf orwardalternative, usingthedirectionsderivedf romrankingSV M sinSection3inequation(5).Hence, theprocessinvolvestwostagestolearnthelinearf unctionf (x) =\nAx.T hef irststagetrainsarankingSV M overthewordvectorsof seentagsf oreachvisualassociation(Ym, Ym).T hesecondstagecomputesthemappingmatrixAvialinearregression, usingthedirectionsf romtherankingSV M sastargets.\n\nas we do not know the\n\nexact principal directions\n\narises\n\n5\n\n\fDiscussion. The use of linear transformation between visual and word vector spaces has been\npreviously explored, for instance, in zero-shot classification and image annotation/classification. This\nwork distinguishes itself by the clear interpretation of the mapped image f(x) = Ax as the principal\ndirection for tag assignment, which has been empirically validated. We further extend this to a\nnonlinear transformation using a neural network.\n\n4.3 Approximation by Neural Networks\n\nWe also explore a nonlinear mapping f(x; ) using a multi-layer neural network, where represents the\nnetwork parameters. The network architecture, illustrated in Figure 4, includes two RELU layers\nfollowed by a linear layer that outputs the approximated principal direction, w, for an input image\nx. We anticipate that the nonlinear mapping function f(x; ) will provide greater modeling flexibility\ncompared to the linear approach.\n\nTraining the neural network by regressing to the M directions obtained from ranking SVMs is not\nideal, as confirmed by both intuition and experiments. The number of training instances, M, is small\nrelative to the network\u2019s parameter count, increasing the risk of overfitting. Moreover, the directions\nfrom ranking SVMs are not the true principal directions, making it unnecessary to rely on them.\n\nInstead, we integrate the two stages from Section 4.2. We aim for the neural network\u2019s output f(xm; )\nto represent the principal direction, where all relevant tag vectors p Ym rank higher than irrelevant\nones n Ym for an image m. Let\u2019s define:\n\nv(p, n; ) = (f(xm; ), n) - (f(xm; ), p)\n\nas the degree of violation of these ranking constraints.\n\nWe then minimize the following loss function to train the neural network:\n\n* = argmin wm \u2217 l(xm, Ym; )l(xm, Ym; ) = log(1 + expv(p, n; ))f orpYm, nYm\n\nwhere wm = 1/(|Ym|\u2217|Ym|)normalizestheper\u2212imageRankN etlossbythenumberof rankingconstraintsimposedbyimagemoverthetags.T hissetupallowsthef unctionf (x)todirectlyconsidertherankingconstraintsf romrelevantandirrelevanttags, anditcanbeoptimizedef f ectivelyusingstandardmini\u2212\nbatchgradientdescent.\n\nPractical Considerations.\nof 1,000 images.\nstraints, which are all used in the optimization.\nimagerankinglosshelpsbalancetheinf luenceof imageswithmanypositivetags, addressingtheissueof unbalancednumbersof relevanttagsacrossimages.W ithoutnormalization, M iAP resultsdropbyabout2%inourexperiments.F orregularization, weemployearlystoppingandadropoutlayerwitha30%droprate.Optimizationhyperparametersarechosenusingthevalidationset.\n\nWe use Theano for optimization, with a mini-batch size\nimposes 4,600 pairwise ranking con-\nThe normalization wmf ortheper \u2212\n\nEach image, on average,\n\nBesides the RankNet loss, we tested other per-image loss options, including hinge loss, Crammer-\nSinger loss, and pairwise max-out ranking. Hinge loss performed the worst, likely because it\u2019s\nnot designed for ranking. Crammer-Singer, pairwise max-out, and RankNet yielded comparable\nresults, with RankNet slightly outperforming the others by about 2% in MiAP, possibly due to easier\noptimization control. Listwise ranking loss could also be considered.\n\n5 Experiments on NUS-WIDE\n\nThis section details our experimental results, comparing our method against several strong baselines\nfor traditional image tagging on the large-scale NUS-WIDE dataset. Additionally, we evaluate our\nmethod on zero-shot and seen/unseen image tagging scenarios, extending some existing zero-shot\nclassification algorithms and exploring variations of our approach for comparison.\n\n5.1 Dataset and Configuration\n\nNUS-WIDE Dataset. We primarily utilize the NUS-WIDE dataset for our experiments. This dataset\nis a standard benchmark for image tagging, originally containing 269,648 images. We were able\nto retrieve 223,821 images, as some were either corrupted or removed from Flickr. Following\nthe recommended protocol, we divide the dataset into a training set of 134,281 images and a test\nset of 89,603 images. We further allocate 20% of the training set as a validation set for tuning\nhyperparameters in both our method and the baselines, and for conducting the empirical analyses in\nSection 3.\n\n6\n\n\fAnnotations of NUS-WIDE. NUS-WIDE provides three sets of tags for its images. The first set\nincludes 81 \"ground truth\" tags, carefully selected to represent Flickr tags, encompassing both general\nterms (e.g., \"animal\") and specific ones (e.g., \"dog,\" \"flower\"), and corresponding to frequent Flickr\ntags. These tags are annotated by students and are less noisy than those directly collected from the\nWeb, serving as the ground truth for evaluating image tagging methods. The second and third sets\ncontain 1,000 popular and nearly 5,000 raw Flickr tags, respectively.\n\nImage Features and Word Vectors. We extract and normalize image feature representations using\nVGG-19. Both GloVe and Word2vec word vectors are used in our empirical analysis in Section 3,\nwith 300D GloVe vectors used for the remaining experiments. Word vectors are also normalized.\n\nEvaluation. We assess tagging results using two types of metrics: mean image average precision\n(MiAP), which considers the entire ranking list, and precision, recall, and F1-score for the top K tags\nin the list (K = 3 and K = 5). Both metrics are commonly used in image tagging research. For details\non calculating MiAP and top-K precision and recall, we refer readers to Section 3.3 of Li et al. (2015)\nand Section 4.2 of Gong et al. (2013), respectively.\n\n5.2 Conventional Image Tagging\n\nIn this section, we present experimental results for traditional image tagging, using the 81 \"ground\ntruth\" annotated concepts in NUS-WIDE to benchmark various methods.\n\nBaselines. We include TagProp as a primary competitive baseline, representing nearest-neighbor-\nbased methods that generally outperform parametric methods built from generative models and have\nshown state-of-the-art results in experimental studies. We also compare against two recent parametric\nmethods, WARP and FastTag, both based on deep architectures but using different models. For a\nfair comparison, we use the same VGG-19 features across all methods, with code for TagProp and\nFastTag provided by the authors and WARP implemented based on our neural network architecture.\nAdditionally, we compare to WSABIE and CCA, which correlate images and relevant tags in a\nlow-dimensional space. Hyperparameters for all methods are selected using the validation set.\n\nResults. Table 4 presents the comparison results among TagProp, WARP, FastTag, WSABIE, CCA,\nand our Fast0Tag models, implemented with both linear mapping and a nonlinear neural network.\nTagProp significantly outperforms WARP and FastTag, but its training and testing complexities are\nhigh, at O(M 2) and O(M ) respectively, relative to the training set size M. In contrast, WARP and\nFastTag are more efficient, with O(M) training complexity and constant testing complexity due to\ntheir parametric nature. Our Fast0Tag with linear mapping yields results comparable to TagProp,\nwhile Fast0Tag with the neural network surpasses the other methods. Both implementations maintain\nlow computational complexities similar to WARP and FastTag.\n\nTable 1: Comparison results of the conventional image tagging with 81 tags on NUS-WIDE.\n\nMethod\n\nMiAP\n\nK = 3\n\nK = 5\n\nCCA\nWSABIE\nTagProp\nWARP\nFastTag\nFast0Tag (lin.)\nFast0Tag (net.)\n\nP\n\n9\n16\n29\n27\n23\n29\n31\n\nR F1\n\n15\n27\n50\n45\n39\n50\n52\n\n11\n20\n37\n34\n29\n37\n39\n\nP\n\n7\n12\n22\n20\n19\n21\n23\n\nR F1\n\n20\n35\n62\n57\n54\n60\n65\n\n11\n18\n32\n30\n28\n31\n34\n\n19\n28\n53\n48\n41\n52\n55\n\n5.3 Zero-Shot and Seen/Unseen Image Tagging\n\nThis section presents results for two novel image tagging scenarios: zero-shot and seen/unseen\ntagging.\n\nFu et al. formalised the zero-shot image tagging problem, which aims to annotate test images using a\npre-defined set U of unseen tags. Our Fast0Tag naturally applies to this scenario by simply ranking\nthe unseen tags with equation (3). Furthermore, this paper also considers seen/unseen image tagging,\n\n7\n\n\fwhich finds both relevant seen tags from S and relevant unseen tags from U for the test images. The\nset of unseen tags U could be open and dynamically growing.\n\nIn our experiments, we treat the 81 concepts with high-quality user annotations in NUS-WIDE as\nthe unseen set U for evaluation and comparison. We use the remaining 925 out of the 1000 frequent\nFlickr tags to form the seen set S - 75 tags are shared by the original 81 and 1,000 tags.\n\nBaselines. Our Fast0Tag models can be readily applied to the zero-shot and seen/unseen image\ntagging scenarios. For comparison, we study the following baselines.\n\nSeen2Unseen. We first propose a simple method that extends an arbitrary traditional image tagging\nmethod to also work with previously unseen tags. It originates from our analysis experiment in\nSection 3. First, we use any existing method to rank the seen tags for a test image. Second, we train a\nranking SVM in the word vector space using the ranking list of the seen tags. Third, we rank unseen\n(and seen) tags using the learned SVM for zero-shot (and seen/unseen) tagging.\n\nLabelEM. The label embedding method achieves impressive results on zero-shot classification for\nfine-grained object recognition. If we consider each tag of S U as a unique class, though this implies\nthat some classes will have duplicated images, the LabelEM can be directly applied to the two new\ntagging scenarios. LabelEM+. We also modify the objective loss function of LabelEM when we train\nthe model, by carefully removing the terms that involve duplicated images. This slightly improves\nthe performance of LabelEM. ConSE. Again by considering each tag as a class, we include a recent\nzero-shot classification method, ConSE in the following experiments. Note that it is computationally\ninfeasible to compare with Fu et al., which might be the first work to our knowledge on expanding\nimage tagging to handle unseen tags, because it considers all the possible combinations of the unseen\ntags. Results. Table 5 summarizes the results of the baselines and Fast0Tag when they are applied to\nthe zero-shot and seen/unseen image tagging tasks. Overall, Fast0Tag, with either linear or neural\nnetwork mapping, performs the best.\n\nAdditionally, in the table, we add two special rows whose results are mainly for reference. The\nRandom row corresponds to the case when we return a random list of tags in U for zero-shot tagging\n(and in U S for seen/unseen tagging) to each test image. We compare this row with the row of\nSeen2Unseen, in which we extend TagProp to handle the unseen tags. We can see that the results of\nSeen2Unseen are significantly better than randomly ranking the tags. This tells us that the simple\nSeen2Unseen is effective in expanding the labeling space of traditional image tagging methods. Some\ntag completion methods may also be employed for the same purpose as Seen2Unseen. Another\nspecial row in Table 5 is the last one with RankSVM for zero-shot image tagging. We obtain its\nresults through the following steps. Given a test image, we assume the annotation of the seen tags,\nS, are known and then learn a ranking SVM with the default regularization = 1. The learned SVM\nis then used to rank the unseen tags for this image. One may wonder that the results of this row\nshould thus be the upper bound of our Fast0Tag implemented based on linear regression because the\nranking SVM models are the targets of the linear regression. However, the results show that they are\nnot. This is not surprising, but rather it reinforces our previous statement that the learned ranking\nSVMs are not the \"true\" principal directions. The Fast0Tag implemented by the neural network is an\neffective alternative for seeking the principal directions. It would also be interesting to compare the\nresults in Table 5 (zero-shot image tagging) with those in Table 4 (conventional tagging), because the\nexperiments for the two tables share the same testing images and the same candidate tags; they only\ndiffer in which tags are used for training. We can see that the Fast0Tag (net.) results of the zero-shot\ntagging in Table 5 are actually comparable to the conventional tagging results in Table 4, particularly\nabout the same as FastTag\u2019s. These results are encouraging, indicating that it is unnecessary to use\nall the candidate tags for training in order to have high-quality tagging performance. Annotating\nimages with 4,093 unseen tags. What happens when we have a large number of unseen tags showing\nup at the test stage? NUS-WIDE provides noisy annotations for the images with over 5,000 Flickr\ntags. Excluding the 925 seen tags that are used to train models, there are 4,093 remaining unseen\ntags. We use the Fast0Tag models to rank all the unseen tags for the test images, and the results\nare shown in Table 3. Noting that the noisy annotations weaken the credibility of the evaluation\nprocess, the results are reasonably low but significantly higher than the random lists. Qualitative\nresults. Figure 6 shows the top five tags for some exemplar images, returned by Fast0Tag under\nthe conventional, zero-shot, and seen/unseen image tagging scenarios. Those by TagProp under the\nconventional tagging are shown on the rightmost. The tags in green color appear in the ground truth\n\n8\n\n\fannotation; those in red color and italic font are the mistaken tags. Interestingly, Fast0Tag performs\nequally well for traditional and zero-shot tagging and makes even the same mistakes.\n\n6 Experiments on IAPRTC-12\n\nWe present another set of experiments conducted on the widely used IAPRTC-12 dataset. We use\nthe same tag annotation and image training-test split as described in prior work for our experiments.\nThere are 291 unique tags and 19,627 images in IAPRTC-12. The dataset is split into 17,341 training\nimages and 2,286 testing images. We further separate 15\n\n6.1 Configuration\n\nSimilar to the experiments in the previous section, we evaluate our methods in three distinct tasks:\nconventional tagging, zero-shot tagging, and seen/unseen tagging. Unlike NUS-WIDE, where a\nrelatively small set of 81 tags is considered the ground truth annotation, all 291 tags of IAPRTC-12\nare typically used in prior work to compare different methods. Therefore, we also use all of them\nfor conventional tagging. For the zero-shot and seen/unseen tagging tasks, we exclude 20The visual\nfeatures, evaluation metrics, word vectors, and baseline methods remain the same as described in the\nmain text.\n\n6.2 Results\n\nTables 4 and 5 display the results for all three image tagging scenarios (conventional, zero-shot, and\nseen/unseen tagging). The proposed Fast0Tag continues to outperform the other competitive baselines\non this new IAPRTC-12 dataset. A notable observation, which is less apparent on NUS-WIDE\nprobably due to its noisier seen tags, is the significant performance gap between LabelEM+ and\nLabelEM. This indicates that traditional zero-shot classification methods may not be directly suitable\nfor either zero-shot or seen/unseen image tagging tasks. However, performance can be improved\nby tweaking LabelEM and carefully removing terms in its formulation that involve comparisons of\nidentical images.\n\n7 More Qualitative Results\n\nIn this section, we provide additional qualitative results from different tagging methods on both the\nNUS-WIDE and IAPRTC-12 datasets. These are presented to supplement the findings discussed in\nthe main text. Due to the incompleteness and noise in tag ground truth, many accurate tag predictions\nare often incorrectly assessed as mistakes because they don\u2019t match the ground truth. This issue is\nparticularly evident in the 4k zero-shot tagging results, where a wide variety of tag candidates are\nconsidered.",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "We have conducted a thorough examination of a specific visual pattern in words: the visual association\nrule that divides words into two distinct groups based on their relevance to an image. We also\ninvestigated how this rule is captured by vector offsets within the word vector space. Our empirical\nfindings demonstrate that for any given image, there exists a main direction in the word vector\nspace along which vectors of relevant tags are ranked higher than those of irrelevant tags. While\nour experimental analyses involved 1,006 words, future research should encompass larger-scale\nand theoretical investigations. Based on this discovery, we developed a Fast0Tag model to address\nimage tagging by estimating the primary directions for input images. Our method is as efficient as\nFastTag and is capable of annotating images with a large number of previously unseen tags. Extensive\nexperiments confirm the effectiveness of our Fast0Tag approach.\n\n9"
}
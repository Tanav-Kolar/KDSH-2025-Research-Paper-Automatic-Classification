{
  "title": "Representation Transferability in Neural Networks\nAcross Datasets and Tasks",
  "abstract": "Deep neural networks, which are built from multiple layers with hierarchical\ndistributed representations, tend to learn low-level features in their initial layers\nand shift to high-level features in subsequent layers. Transfer learning, multi-task\nlearning, and continual learning paradigms leverage this hierarchical distributed\nrepresentation to share knowledge across different datasets and tasks. This paper\nstudies the layer-wise transferability of representations in deep networks across\nseveral datasets and tasks, noting interesting empirical observations.",
  "introduction": "Deep networks, constructed with multiple layers and hierarchical distributed representations, learn\nlow-level features in initial layers and shift to high-level features as the network becomes deeper.\nGeneric hierarchical distributed representations allow for the sharing of knowledge across datasets\nand tasks in paradigms such as transfer learning, multi-task learning, and continual learning. In\ntransfer learning, for example, the transfer of low-level features from one dataset to another can\nboost performance on the target task when data is limited, provided that the datasets are related.\nTransferring high-level features, with the learning of low-level features, can also be useful when the\ntasks are similar but the data distributions differ slightly.\n\nThis paper studies the layer-wise transferability of representations in deep networks across several\ndatasets and tasks, and reports some interesting observations. First, we demonstrate that the layer-wise\ntransferability between datasets or tasks can be non-symmetric, with features learned from a source\ndataset being more relevant to a target dataset, despite similar sizes. Secondly, the characteristics of\nthe datasets or tasks and their relationship have a greater effect on the layer-wise transferability of\nrepresentations than factors such as the network architecture. Third, we propose that the layer-wise\ntransferability of representations can be a proxy for measuring task relatedness. These observations\nemphasize the importance of curriculum methods and structured approaches to designing systems\nfor multiple tasks that maximize knowledge transfer and minimize interference between datasets or\ntasks.\n\n2 Citation Networks\n\n2.1 Methods\n\nWe have produced a citation graph using citation data from NeurIPS papers from SemanticScholar,\nand institutional information about authors from AMiner. From the NeurIPS website, we first gathered\nall paper titles from 2012 to 2021. We then mapped the paper titles to their Semantic Scholar paper\nIDs using the Semantic Scholar Academic Graph (S2AG) API. Unmatched papers were manually\nsearched for, with all but one being found in the Semantic Scholar database. For each paper, we used\nthe S2AG API to identify authors, and the authors of their references.\n\nWe used AMiner to identify institutional information for each author. The 9460 NeurIPS papers\ncontain 135,941 authors, with institutions found for 83,515 (61%) of them. Papers lacking author\n\n.\n\n\finformation were removed from our dataset. We then marked institutes automatically by country\nname and common cities and regions in China. We supplemented automatic annotations with existing\nregional matchings and added 364 additional rules for regional matching. We also removed major\nmultinational corporate labs. Of the remaining 5422 papers, we removed papers that were not from\nChina, the US, or Europe, or included collaborators from multiple regions, leaving us with 1792\npapers. Finally, we calculated the average number and proportion of citations between papers from\neach region.\n\n2.2 Results\n\nOur results show how American and Chinese papers fail to cite each other. While 60% of the data set\ncomes from American papers, they only compose 34% of Chinese citations. American citations of\nChinese papers are even more dramatic, with the 34% of the dataset coming from Chinese papers only\naccounting for 9% of American citations. These numbers are even more significant when compared\nto American citations of European papers; we found that American institutions cite European papers\nmore often than Chinese papers despite our dataset containing six times more Chinese papers than\nEuropean.\n\nEach region tends to cite its own papers more often: China 21%, the USA 41%, and Europe 14%.\nThe separation between American and Chinese research is more pronounced than would be expected\nbased solely on regional preference. American and European research communities demonstrate\nsimilar citation patterns with few citations to Chinese papers. Chinese institutions, on the other hand,\ncite both American and European papers less than either of those regions.\n\nUSA China Europe\n\nUSA\nChina\nEurope\n\n41\n34\n15\n\n9\n21\n9\n\n12\n6\n14\n\nTable 1: Proportion of papers from given regions citing other regions or endogenously. Values are in\npercentage.\n\n3 Limitations\n\nThe results presented here have some limitations. Firstly, while we have labeled the work of any\nuniversity located in the United States as American, it is possible that such labs still have close ties to\nChina, leading to an underestimate of the divide between US and Chinese AI research. Secondly, we\nhave excluded papers where author information was not available on AMiner, a Chinese company,\nand therefore, there could be more Chinese papers in our dataset than we have determined. The 43%\nof discarded papers due to missing author information also likely represent a biased sample.\n\n4 Consequences\n\nWhile American and Chinese researchers publish in the same venues, they represent two parallel\ncommunities with limited impact on each other\u2019s research. This can, partly, be attributed to differing\nresearch interests arising from distinct cultural norms that influence research priorities. For instance,\nmulti-object tracking is an active area of research in China with large scale benchmarks, whereas,\nconcerns surrounding misuse of biometric data in North America have led researchers there to avoid\nsuch research. Likewise, US researchers are heavily represented at conferences regarding fairness in\nAI, while the Chinese are not.\n\nThis separation impacts not only the research topics, but also how they evolve. In addition, abstract\ntopics or architectures that are popular in one region may not be popular in the other. For example,\nPCANet which is a popular image classification architecture has most of its 1200 citations from East\nAsian institutions, while Deep Forests has most of its 600 citations from Chinese institutions.\n\nAnother limitation is related to differences in the approach to ethics. The North American and Euro-\npean AI communities have begun to publish research on the ethics of AI and have included systems\n\n2\n\n\ffor reviewers to flag ethical concerns and ask authors to provide ethics statements. Engagement\nwith Chinese researchers in this topic remains limited, even though ethics statements from Chinese\nAI institutions show many similarities to western ones. A clear example of this disconnect is the\nProvisional Draft of the NeurIPS Code of Ethics where, at the time of initial publication, all the\nauthors were based in the US or Australia, but none were based in Asia. Although similar statements\nexist across regions, disagreements in research practice still arise. One such example is where Duke\nUniversity stopped using the Duke-MTMC dataset because researchers had not obtained consent\nfrom the students they collected images from, yet similar datasets like Market-1501 from China\ncontinue to be used.\n\nThe divide between these two communities impacts individual researchers, the machine learning\ncommunity as a whole, and potentially the societies impacted by AI research, highlighting the need\nfor a discussion to overcome this barrier.\n\n3",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": ""
}
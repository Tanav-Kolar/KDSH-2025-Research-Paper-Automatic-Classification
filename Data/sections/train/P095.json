{
  "title": "JueWu-MC: Achieving Sample-Efficient Minecraft\nGameplay through Hierarchical Reinforcement\nLearning",
  "abstract": "Learning rational behaviors in open-world games such as Minecraft continues to\npose a challenge to Reinforcement Learning (RL) research, due to the combined\ndifficulties of partial observability, high-dimensional visual perception, and delayed\nrewards. To overcome these challenges, we propose JueWu-MC, a sample-efficient\nhierarchical RL method that incorporates representation learning and imitation\nlearning to handle perception and exploration. Our approach has two levels of\nhierarchy: the high-level controller learns a policy to manage options, while the\nlow-level workers learn to solve each sub-task. To boost learning of sub-tasks,\nwe propose a combination of techniques including: 1) action-aware represen-\ntation learning, which captures relations between action and representation; 2)\ndiscriminator-based self-imitation learning for efficient exploration; and 3) ensem-\nble behavior cloning with consistency filtering for policy robustness. Extensive\nexperiments demonstrate that JueWu-MC significantly enhances sample efficiency\nand outperforms several baselines. We won the championship of the MineRL 2021\nresearch competition and achieved the highest performance score.",
  "introduction": "Deep reinforcement learning (DRL) has achieved great success in numerous game genres including\nboard games, Atari games, simple first-person-shooter (FPS) games, real-time strategy (RTS) games,\nand multiplayer online battle arena (MOBA) games. Recently, open-world games have garnered\nattention due to their playing mechanisms and their resemblance to real-world control tasks. Minecraft,\nas a typical open-world game, has been increasingly explored in recent years.\n\nCompared to other games, the properties of Minecraft make it an ideal testbed for RL research,\nbecause it emphasizes exploration, perception, and construction in a 3D open world. Agents are given\npartial observability and face occlusions. Tasks in the game are chained and long-term. Humans can\ntypically make rational decisions to explore basic items and construct more complex items with a\nreasonable amount of practice, while it can be challenging for AI agents to do so autonomously. To\nfacilitate the effective decision-making of agents in playing Minecraft, MineRL has been developed\nas a research competition platform, which provides human demonstrations and encourages the\ndevelopment of sample-efficient RL agents for playing Minecraft. Since its release, many efforts\nhave been made to develop Minecraft AI agents.\n\nHowever, it remains difficult for current RL algorithms to acquire items in Minecraft due to several\nfactors, which include the following. First, in order to reach goals, the agent is required to complete\nmany sub-tasks that highly depend on each other. Due to the sparse reward, it is difficult for agents\nto learn long-horizon decisions efficiently. Hierarchical RL from demonstrations has been explored\nto take advantage of the task structure to accelerate learning. However, learning from unstructured\ndemonstrations without any domain knowledge remains difficult. Second, Minecraft is a flexible\n3D first-person game which revolves around gathering resources and creating structures and items.\nIn this environment, agents are required to handle high-dimensional visual input to enable efficient\n\n.\n\n\fcontrol. However, the agent\u2019s surroundings are varied and dynamic, which makes it difficult to learn\na good representation. Third, with partial observability, the agent needs to explore in the right way\nand collect information from the environment in order to achieve goals. Naive exploration can waste\na lot of samples on useless actions. Self-imitation learning (SIL) is a simple method that learns to\nreproduce past good behaviors to incentivize exploration, but it is not sample efficient. Lastly, human\ndemonstrations are diverse and often noisy.\n\nTo address these combined challenges, we propose an efficient hierarchical RL approach, equipped\nwith novel representation and imitation learning techniques. Our method leverages human demonstra-\ntions to boost the learning of agents, enabling the RL algorithm to learn rational behaviors with high\nsample efficiency.\n\nHierarchical Planning with Prior. We propose a hierarchical RL (HRL) framework with two\nlevels of hierarchy. The high-level controller extracts sub-goals from human demonstrations and\nlearns a policy to control options, while the low-level workers learn sub-tasks to achieve sub-goals\nby leveraging demonstrations and interactions with environments. Our approach structures the\ndemonstrations and learns a hierarchical agent, which enables better decisions over long-horizon\ntasks. We use the following key techniques to boost agent learning.\n\nAction-aware Representation Learning. We propose action-aware representation learning (A2RL)\nto capture the relations between action and representation in 3D visual environments such as Minecraft.\nA2RL enables effective control and improves the interpretability of the learned policy.\n\nDiscriminator-based Self-imitation Learning. We propose discriminator-based self-imitation\nlearning (DSIL), which leverages self-generated experiences to learn self-correctable policies for\nbetter exploration.\n\nEnsemble Behavior Cloning with Consistency Filtering. We propose consistency filtering to\nidentify common human behaviors, and then perform ensemble behavior cloning to learn a robust\nagent with reduced uncertainty.\n\nOur contributions are as follows: 1) We propose JueWu-MC, a sample-efficient hierarchical RL\napproach, equipped with action-aware representation learning, discriminator-based self-imitation,\nand ensemble behavior cloning with consistency filtering. 2) Our approach outperforms competitive\nbaselines and achieves the best performance throughout the history of the competition.",
  "related_work": "2.1 Game AI\n\nGames have long been a testing ground for artificial intelligence research. AlphaGo mastered the\ngame of Go with DRL and tree search. Since then, DRL has been used in other sophisticated games,\nincluding StarCraft, Google Football, VizDoom, and Dota. Recently, the 3D open-world game\nMinecraft has been attracting attention. Previous research has shown that existing RL algorithms\ncan struggle to generalize in Minecraft and a new memory-based DRL architecture was proposed to\naddress this. Another approach combines a deep skill array and a skill distillation system to promote\nlifelong learning and transfer knowledge among different tasks. Since the MineRL competition began\nin 2019, many solutions have been proposed to learn to play in Minecraft. These works can be\ngrouped into two categories: 1) end-to-end learning; 2) hierarchical RL with human demonstrations.\nOur approach belongs to the second category, which leverages the structure of the tasks and learns\na hierarchical agent to play in Minecraft. ForgER proposed a hierarchical method with forgetful\nexperience replay, and SEIHAI fully takes advantage of human demonstrations and task structure.\n\n2.2 Sample-efficient Reinforcement Learning\n\nOur work aims to create a sample-efficient RL agent for playing Minecraft, and we thereby develop a\ncombination of efficient learning techniques. We discuss the most relevant works below.\n\nOur work is related to HRL research that builds upon human priors. One approach proposes to\nwarm-up the hierarchical agent from demonstrations and fine-tune it with RL algorithms. Another\napproach proposes to learn a skill prior from demonstrations to accelerate HRL algorithms. Compared\nto existing works, we are faced with highly unstructured demos in 3D first-person video games played\n\n2\n\n\fby the crowds. We address this challenge by structuring the demonstrations and defining sub-tasks\nand sub-goals automatically.\n\nRepresentation learning in RL has two broad directions: self-supervised learning and contrastive\nlearning. Self-supervised learning aims to learn rich representations for high-dimensional unlabeled\ndata to be useful across tasks. Contrastive learning learns representations that obey similarity\nconstraints. Our work proposes a self-supervised representation learning method that measures action\neffects in 3D video games.\n\nExisting methods use curiosity or uncertainty as a signal for exploration so that the learned agent\nis able to cover a large state space. The exploration-exploitation dilemma drives us to develop self-\nimitation learning (SIL) methods that focus on exploiting past good experiences for better exploration.\nWe propose discriminator-based self-imitation learning (DSIL).\n\n3 Method\n\nIn this section, we first introduce our overall HRL framework, and then describe each component in\ndetail.\n\n3.1 Overview\n\nOur overall framework is shown in Figure 1. We define human demonstrations as D = {\u03c40, \u03c41, \u03c42, ...}\nwhere \u03c4i is a long-horizon trajectory containing states, actions, and rewards. The provided demon-\nstrations are unstructured, without explicit signals that specify sub-tasks and sub-goals.\n\nWe define an atomic skill as a skill that gets a non-zero reward. We define sub-tasks and sub-goals\nbased on the atomic skills. To define sub-tasks, we examine the reward delay for each atomic skill,\nkeeping those with long reward delays as individual sub-tasks and merging those with short reward\ndelays into one sub-task. Through this process, we have n sub-tasks in total. To define sub-goals for\neach sub-task, we extract the most common human behavior pattern and use the last state in each\nsub-task as its sub-goal. Through this, we have structured demonstrations (D \u2192 {D0, D1, ..., Dn\u22121}\n) with sub-tasks and sub-goals used to train the hierarchical agent. With the structured demonstrations,\nwe train the meta-policy using imitation learning and train sub-policies to solve sub-tasks using\ndemonstrations and interactions with the environment.\n\n3.2 Meta- and Sub-policies\n\nMeta-policy. We train a meta-policy that maps continuous states to discrete indices (0, 1, ..., n - 1)\nthat specify which option to use. Given state space S and discrete option o \u2208 O, the meta-policy is\ndefined as \u03c0m(\u03b8)(o|s), where s \u2208 S, o \u2208 O, and \u03b8 represents the parameters. \u03c0m(\u03b8)(o|s) specifies\nthe conditional distribution over the discrete options. To train the meta-policy, we generate training\ndata (s, i) where i represents the i-th stage and s \u2208 Di is sampled from the demonstrations of the i-th\nstage. The meta-policy is trained using negative log-likelihood (NLL) loss:\nLm = \u2212 (cid:80)n\u22121\nDuring inference, the meta-policy generates options by taking\n\ni=0 log \u03c0m(i|s)\n\n\u03c3 = argmaxo\u03c0m(o|s)\n\nSub-policy. In Minecraft, sub-tasks can be grouped into two main types: gathering resources, and\ncrafting items. In the first type (gathering resources), agents need to navigate and gather sparse\nrewards by observing high-dimensional visual inputs. In the second type (crafting items), agents need\nto execute a sequence of actions robustly.\n\nIn typical HRL, the action space of the sub-policies is predefined. However, in the competition, a\nhandcrafted action space is prohibited. Additionally, the action space is obfuscated in both human\ndemonstrations and the environment. Learning directly in this continuous action space is challenging\nas exploration in a large continuous space can be inefficient. We use KMeans to cluster actions for\neach sub-task using demonstration Di, and perform reinforcement learning and imitation learning\nbased on the clustered action space.\n\n3\n\n\fIn the following section, we describe how to learn sub-policies efficiently to solve these two kinds of\nsub-tasks.\n\n3.3 Learning Sub-policies to Gather Resources\n\nTo efficiently solve these sub-tasks, we propose action-aware representation learning and\ndiscriminator-based self-imitation learning to facilitate the learning of sub-policies. The model\narchitecture is shown in Figure 2.\n\nAction-aware Representation Learning. To learn compact representations, we observe that in 3D\nenvironments, different actions have different effects on high-dimensional observations. We propose\naction-aware representation learning (A2RL) to capture the relation with actions.\n\nWe learn a mask net on a feature map for each action to capture dynamic information between the\ncurrent and next states. Let the feature map be f\u03b8(s) \u2208 RC\u00d7H\u00d7W and the mask net be m\u03d5(s, a) \u2208\n[0, 1]H\u00d7W , where \u03b8 and \u03d5 represent parameters of the policy and mask net. Given a transition tuple\n(s, a, s\u2032), the loss function for training the mask is:\nLm(\u03d5) = \u2212Es,a,s\u2032\u223cD[log(\u03c3((f\u03b8(s\u2032) \u2212 g\u03c8(f\u03b8(s))) \u2299 m\u03d5(s, a))) + \u03b7(1 \u2212 m\u03d5(s, a))]\nwhere g\u03c8 is a linear projection function parameterized by learnable parameters \u03c8; \u2299 represents\nelement-wise product, and \u03b7 is a hyper-parameter that balances two objectives.\n\nTo optimize the above loss function, we use a two-stage training process. In the first stage, we train\nthe linear projection network g\u03c8a using the following objective:\nLg(\u03c8a) = Es,a,s\u2032\u223cD[||f\u03b8(s\u2032) \u2212 g\u03c8a (f\u03b8(s))||2]\nThis objective learns to recover information of s\u2032 from s in latent space, which is equal to learning a\ndynamic model to predict the next state given the current state and action. Note that the parameter \u03c8\nis dependent on the action a. In the second stage, we fix the learned linear function g\u03c8a and optimize\nthe mask net.\n\nBy minimizing the loss function, the mask net will learn to focus on local parts of the current image\nthat are uncertain to the dynamic model. This is similar to human curiosity, which focuses on that\nwhich is uncertain.\n\nFor policy-based methods, we integrate our learned representations into policy networks. For value-\nbased methods, we combine our learned representations directly with Q-value functions. The learning\nof the Q-value function can be done using any Q-learning based algorithms.\n\nDiscriminator-based Self-imitation Learning. We propose discriminator-based self-imitation\nlearning (DSIL). Unlike ASIL, DSIL does not use advantage clipping. Our intuition is that the agent\nshould be encouraged to visit the state distribution that is more likely to lead to goals.\n\ni and B\u2212\n\ni as positive samples and data from B\u2212\n\nTo do so, DSIL learns a discriminator to distinguish between states from successful and failed\ntrajectories, and then uses the learned discriminator to guide exploration. We maintain two replay\nbuffers B+\nto store successful and failed trajectories. During learning, we treat data from\nB+\ni as negative samples to train the discriminator. Let the\ndiscriminator be D\u03be : S \u2192 [0, 1] which is parameterized by parameters \u03be. We train the discriminator\nwith the objective:\nmax\u03beEs\u2208B+\n\n[log D\u03be(s)] + Es\u2208B\u2212\n\n[1 \u2212 log D\u03be(s)]\n\ni\n\ni\n\ni\n\nThe discriminator is encouraged to output high values for good states and low values for bad states.\nFor states that are not distinguishable, D\u03be(s) tends to output 0.5.\n\nWe use the trained discriminator to provide intrinsic rewards for policy learning to guide exploration.\nThe intrinsic reward is defined as:\nr(s, a, s\u2032) = { + 1if D\u03be(s\u2032) > 1 \u2212 \u03f5\n\u2212 1if D\u03be(s\u2032) < \u03f5\nwhere \u03f5 \u2208 (0, 0.5) is a hyper-parameter to control the confidence interval of D\u03be. This reward drives\nthe policy to explore in regions that previously led to successful trajectories. DSIL encourages the\npolicy to stay close to a good state distribution, reproduce past decisions, and also be self-correctable.\n\n4\n\n\f3.4 Learning Sub-policies to Craft Items\n\nIn this type of sub-task, agents must learn a sequence of actions to craft items. To finish such tasks,\nagents need to learn a robust policy to execute a sequence of actions.\n\nWe explore pure imitation learning (IL) to reduce the need for interactions with the environment, due\nto the limited sample and interaction usage. We propose ensemble behavior cloning with consistency\nfiltering (EBC).\n\nConsistency Filtering. Human demonstrations can be diverse and noisy. Directly imitating such noisy\ndata can cause confusion for the policy. Therefore, we perform consistency filtering by extracting\nthe most common pattern of human behaviors. We extract the most common action sequence from\ndemonstrations Di. For each trajectory, we keep those actions that lead to a state change while\nappearing for the first time to form an action sequence, and count the occurrences of each pattern.\nWe then get the most common action pattern. Afterward, we conduct consistency filtering using the\nextracted action pattern.\n\nEnsemble Behavior Cloning. Learning policy from offline datasets can lead to generalization\nissues. Policies learned through behavior cloning may become uncertain when encountering unseen\nout-of-distribution states. To mitigate this, EBC learns a population of policies on different subsets of\ndemonstrations to reduce the uncertainty of the agent\u2019s decision. Specifically, we train K policies on\ndifferent demonstrations with NLL loss:\nmin\u03b8k Es,a\u223c \u00afDk\nwhere \u03b8k parameterizes the k-th policy. During inference, EBC adopts a majority voting mechanism\nto select an action that is the most confident among the policies.\n\ni \u2282 \u00afDi, k = 1, 2, ..., K\n\n[\u2212 log \u03c0\u03b8k (a|s)], \u00afDk\n\ni\n\n4 Experiment\n\nWe conduct experiments using the MineRL environment. Our approach is built based on RL\nalgorithms including SQIL, PPO, and DQfD.\n\n4.1 Main Results\n\nTable 1 shows all the MineRL competition results since 2019. The competition settings in 2020 and\n2021 were more difficult than in 2019. In these years, participants had to focus on the algorithm\ndesign itself. The scores in 2020 and 2021 are lower than in 2019. Our approach outperforms all\nprevious solutions. End-to-end baselines cannot achieve a decent result, showing it is difficult to\nsolve long-horizon tasks with end-to-end learning. Compared to the results of the 2020 competition,\nour method outperforms other solutions with a score (76.97) that is 3.4x higher than the second place\nscore (22.97). Table 2 shows the conditional success rate of each stage between our approach and\nSEIHAI. Our approach outperforms SEIHAI in every stage.\n\nFigure 3(a) shows the training curves. Due to a version update of MineRL 2021, our online score\ndropped compared with the performance in our training curve. Our approach is sample-efficient and\noutperforms prior best results with 0.5 million training samples. Our score reaches 100 with 2.5\nmillion training samples, which is less than the 8 million samples of the MineRL competition.\n\n4.2 Ablation Study\n\nTo examine the effectiveness of our proposed techniques, we consider three variants of our approach:\n1) without A2RL, 2) without DSIL, and 3) without EBC. Figure 3(b) shows the training curves. Each\ntechnique contributes to the overall performance. EBC and A2RL contribute more than DSIL. DSIL\nmainly boosts the performance for later sub-tasks, while A2RL and EBC have earlier effects on\nthe overall pipeline. EBC contributes significantly, demonstrating that learning a robust policy is\nimportant for solving long-horizon tasks.\n\n5\n\n\f4.3 Visualization\n\nTo understand why our techniques work, we conduct an in-depth analysis. To understand the learned\nmask in A2RL, we compute saliency maps. For each action, we show the current state, the next state,\nand the saliency map of the learned mask on the current state. We find that the learned mask captures\nthe dynamic information between two adjacent states, revealing curiosity on the effect of actions. The\nmask net learns to focus on uncertain parts of the current state. For the \u2019attack\u2019 action, the learned\nmask focuses on the objects in front of the agent. For the \u2019turn left\u2019 and \u2019turn down\u2019 actions, the mask\nnet focuses on the parts that have major changes due to the rotation and translation of the agent\u2019s\nperspective. Our learned mask assists the agent in better understanding the 3D environment.\n\nTo understand how DSIL works, we visualize the state distribution that the agent visits. We compare\nPPO, PPO+SIL, and PPO+DSIL. At the early training stage, both methods explore randomly and\nsometimes reach the goal state successfully. After getting samples and training, PPO+DSIL starts to\nexplore in a compact region, while PPO and PPO+SIL still explore in a wider region. DSIL pushes\nthe agent to stay close to a good state distribution, reproducing its past behaviors and exploring in a\nbetter way, which incentivizes deep exploration for successful trajectories.\n\nTable 1: MineRL Competition Results. Our solution (JueWu-MC) significantly outperforms all other\ncompetitive solutions.\n\nBaselines\n\nName\n\nSQIL\nDQfD\nRainbow\nPDDDQN\nBC\n\nScore\n\n2.94\n2.39\n0.42\n0.11\n2.40\n\n2019 Competition Results\nScore\n\nTeam Name\n\nCDS (ForgER)\nmcrl\nI4DS\nCraftRL\nUEFDRL\nTD240\n\n61.61\n42.41\n40.8\n23.81\n17.9\n15.19\n\n2020 Competition Results\n\nTeam Name\n\nScore\n\n2021 Competition Results\nScore\n\nTeam Name\n\nHelloWorld (SEIHAI)\nmichal_opanowicz\nNoActionWasted\nRabbits\nMajiManji\nBeepBoop\n\n39.55 X3 (JueWu-MC)\n13.29 WinOrGoHome\n12.79\n5.16\n2.49\n1.97\n\nMCAgent\nsneakysquids\nJBR_HSE\nzhongguodui\n\n76.97\n22.97\n18.98\n14.35\n10.33\n8.84\n\nTable 2: The conditional success rate of each stage.\n\nMethods\n\nStage 1\n\nStage 2\n\nStage 3\n\nStage 4\n\nStage 5\n\nStage 6\n\nStage 7\n\nSEIHAI\nJueWu-MC\n\n64%\n92%\n\n78.6% 78.3% 84.7%\n87%\n96%\n96%\n\n23%\n46%\n\n0%\n11%\n\n0%\n0%",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": "In this paper, we present JueWu-MC, a sample-efficient hierarchical reinforcement learning frame-\nwork designed to play Minecraft. With a high-level controller and several auto-extracted low-level\nworkers, our framework can adapt to different environments and solve sophisticated tasks. Our\nnovel techniques in representation learning and imitation learning improve both the performance and\nlearning efficiency of the sub-policies. Experiments show that our pipeline outperforms all baseline\nalgorithms and previous solutions from MineRL competitions. In future work, we would like to apply\nJueWu-MC to other Minecraft tasks, as well as other open-world games.\n\n6"
}
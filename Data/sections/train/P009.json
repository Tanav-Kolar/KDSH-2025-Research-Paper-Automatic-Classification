{
  "title": "Flexible Online Aggregations Using Basis Function Expansions",
  "abstract": "Bayesian learning often necessitates online inference, adaptive models, and the combination of multiple distinct\nmodels. Recent advancements have demonstrated the use of random feature approximations for scalable, online\naggregation of Gaussian processes, which possess favorable theoretical characteristics and practical uses. A crucial\naspect of these methods is the incorporation of a random walk on model parameters, which introduces adaptability.\nWe demonstrate that these methods can be readily extended to any model using basis function expansion and that\nemploying alternative basis expansions, such as Hilbert space Gaussian processes, frequently leads to enhanced\nperformance. To streamline the selection of a specific basis expansion, the versatility of our approach also enables\nthe aggregation of several entirely different models, such as a Gaussian process and polynomial regression. Lastly,\nwe introduce an innovative technique for combining both static and dynamic models.",
  "introduction": "Numerous machine learning applications demand real-time, online data processing, a scenario that frequently requires substantial\nalterations to conventional techniques. Online adaptations of various methods have been developed, including kernel machines,\n(kernel) least-squares, and Gaussian processes. The field of online learning has also been thoroughly investigated from an optimization\nstandpoint.\n\nOnline learning can be further complicated when model selection is needed, as the best-performing model is rarely evident at the\noutset of the learning process. One solution involves training multiple models concurrently and then combining them. In a Bayesian\nframework, Bayesian model averaging (BMA) has long been employed to combine online models, functioning by assigning weights\nto each \"expert\" model based on its supporting evidence.\n\nMore recently, it was shown how to adapt BMA to online Gaussian processes (GPs) in a technique called incremental ensembles\nof GPs. GPs are a versatile, non-parametric instrument in Bayesian machine learning that possesses universal approximation\ncapabilities and provides well-founded uncertainty estimations. By employing a random Fourier feature (RFF) approximation for\nGaussian processes, online learning can be executed, featuring closed-form Bayesian model averaging updates and a manageable\nregret analysis.\n\nBesides an online ensemble of GPs, the advantages of incorporating random walks on model parameters were illustrated, which they\nterm dynamic IE-GPs (DIE-GPs). This can significantly enhance performance when the learning task undergoes slight changes over\ntime.",
  "related_work": "The concept of combining random feature GPs, as introduced by IE-GPs, has demonstrated adaptability and effectiveness. Extensions\nto this framework encompass Gaussian process state-space models, deep Gaussian processes, and graph learning. Along with its\nextensions, DIE-GPs have been effectively applied in Bayesian optimization and causal inference.\n\nHowever, the dependence on the RFF approximation implies that IE-GPs also inherit the limitations of random feature GPs.\nSpecifically, the RFF approximation is a direct Monte Carlo approximation of the Wiener-Khinchin integral and thus is significantly\nimpacted by the curse of dimensionality. Our findings reveal that on several real-world datasets, (D)IE-GPs exhibit performance that\nis comparable to or worse than that of simpler models, such as online Bayesian linear regression and one-layer RBF networks.",
  "methodology": "In this paper, we present online ensembles of basis expansions (OEBEs), a generalization of IE-GPs that overcomes their dependence\non RFF GPs and enhances performance across multiple real datasets. Our specific contributions are as follows:\n\n\f1. We observe that the derivation of DIE-GPs does not rely on the RFF approximation, except for its role as a linear basis\nexpansion. The same derivations and code can be reused to combine arbitrary Bayesian linear models with any design matrix.\nThis allows for the combination of not only models of the same type but also various distinct basis expansions (e.g., B-splines,\none-layer RBF networks, etc.). 2. We contend that a GP with a generalized additive model (GAM) structure is often more\nsuitable when GP regression is the focus. To this end, we employ GAM Hilbert space Gaussian processes (HSGPs), which can\nbe interpreted as a quadrature rule for the same integral that the RFF approximation addresses through direct Monte Carlo. Apart\nfrom theoretical considerations, empirical evidence indicates that HSGPs converge to the true approximated GP more rapidly (in\nterms of the number of basis functions) than RFF GPs. We offer a similar empirical evaluation. 3. We introduce a new method for\nintegrating static and dynamic models, enabling the use of principled posteriors of static methods when appropriate and extending\nthe expressiveness of dynamic methods otherwise. We demonstrate the necessity of this method by providing a constructive example\non real data where the naive approach to combining static and dynamic methods is unsuccessful. 4. We provide Jax/Objax code\nat https://www.github.com/danwaxman/DynamicOnlineBasisExpansions that only requires the user to specify the design\nmatrix, with several choices already implemented.\n\nThe remainder of this paper is organized as follows: Section 2 reviews foundational concepts in linear basis expansions, GP regression,\nspectral approximations of GPs, and BMA. These concepts are put into practice in Section 3, where we present the OEBEs and\nseveral extensions, including applications to non-Gaussian likelihoods, and provide some concise theoretical observations. We offer\nfurther practical insights regarding the development of OEBEs, including a discussion on the composition of an ensemble and how\nto combine static and dynamic models in Section 4. The proposed models are empirically evaluated in Section 5. Finally, we present\nconcluding remarks and suggest future directions in Section 6.",
  "experiments": "We present three distinct experiments in the main text, with supplementary experiments in the appendices. In the first experiment\n(Section 5.1), we assess ensembles of several different basis expansions, demonstrating that the best-performing model varies\nconsiderably. In the second experiment (Section 5.2), we illustrate how model collapse can occur between static and dynamic models\nand how the model introduced in Section 4.2 mitigates this issue. Lastly, we demonstrate that E-DOEBE can effectively combine\nmethods that are both static and dynamic, and of different basis expansions (Section 5.3).\n\nThe metrics we employ are the normalized mean square error (nMSE) and the predictive log-likelihood (PLL). The nMSE is defined\nas the MSE of yt with the predictive mean, divided by the variance of y1:T. Specifically, at time t, the nMSE is calculated as:\n\nnM SEt =\n\n(cid:80)t\n\n\u03c4 =1(\u00b5y\u03c4 \u2212y\u03c4 )2\nt\u00b7V ar(y1:T )\n\nThe predictive log-likelihood (PLL) is the average value of log p(yt+1|X1:t, y1:t), i.e.,\n\nP LLt =\n\n(cid:80)t\n\n\u03c4 =1 log p(y\u03c4 +1|X1:\u03c4 ,y1:\u03c4 )\nt\n\n.\n\nAcross all experiments, we utilize several publicly available datasets, varying in both size and the number of features. A summary\nof dataset statistics is provided in Table 1. Friedman 1 and Friedman 2 are synthetic datasets designed to be highly nonlinear and,\nnotably, are i.i.d. The Elevators dataset pertains to controlling the elevators on an aircraft. The SARCOS dataset uses simulations of\na robotic arm, and Kuka 1 is a similar real dataset derived from physical experiments. CaData comprises California housing data,\nand the task of CPU Small is to predict a type of CPU usage based on system properties.\n\nAll hyperparameter optimization was performed on the first 1,000 samples of each dataset; since we already assume access, each\ndataset was additionally standardized in both x and y using the statistics of the first 1,000 samples. We follow prior work in setting a\nweight to 0 when it falls below the threshold of 10-16.\n\n4.1 Comparing Different Basis Expansions\n\nTo demonstrate that having a diverse set of basis expansion models available is beneficial, we evaluate several model types on each\ndataset listed in Table 1. Furthermore, we examine both static and dynamic versions of models to assess their performance.\n\nModels used for comparison include an additive HSGP model [(D)OE-HSGP], an RFF GP [(D)OE-RFF], an ensemble of quadratic,\ncubic, and quartic polynomials with additive structure [(D)OE-Poly], linear regression [(D)OE-Linear], and a one-layer RBF network\n[(D)OE-RBF]. Apart from additional hyperparameter tuning in an ARD kernel, the (D)OE-RFF model is identical to the (D)IE-GP.\nFor RFF GPs, 50 Fourier features were employed (resulting in F = 2 \u00d7 50), and for HSGPs, \u02d8230a100/D\u02d8230b features were used for\neach dimension (resulting in F \u02d82272 100). An SE-ARD kernel was utilized in both cases. For RBF networks, 100 locations were\ninitialized using K-means and subsequently optimized with empirical Bayes, along with ARD length scales. For all models except\nRBF networks, ensembles were generated using the process outlined in Section 4.1 \u02d82014 for RBF networks, the computation of the\nHessian was too computationally demanding, so parameters were randomly perturbed by white Gaussian noise with variance 10-3\ninstead.\nFor dynamic models, \u02d803c32 was set to 10-3. The initial values of \u02d803c32 \u02d803b8 and \u02d803c32 \u02d803f5 were 1.0 and 0.25, respectively.\nOptimization was carried out using Adam.\n\n2\n\n\fResults of the average nMSE and PLL are presented in Table 2 and Table 3. We observe that the best-performing class of models\nvaries significantly across datasets. Specifically, in terms of both nMSE and PLL, HSGPs, RFF GPs, and RBF networks each achieve\nthe best performance on at least one dataset. This reinforces the notion that combining several different models is advantageous, as\nno single method consistently outperforms the others.\n\nMoreover, as anticipated, dynamic models can substantially outperform static models in specific scenarios (e.g., on SARCOS and\nKuka 1) but yield a lower PLL on datasets where the data is reasonably i.i.d. (e.g., Friedman 1).\n\nAs expected, when an additive structure is a reasonable approximation, additive HSGP methods surpass RFF GPs, for instance, on\nKuka 1 and CaData. The RFF GP approximation rarely exhibits particularly poor performance, making it a consistently \"good\"\nestimator, and it achieves the highest PLL on Friedman 2, SARCOS, and CPU Small. However, it is also occasionally outperformed\nby simpler methods, such as the RBF network, highlighting the potential advantages of employing diverse basis expansions.\n\nKey Takeaways Key takeaways from this experiment include: (1) neither dynamic nor static methods are strictly superior across all\nsettings, (2) no single basis expansion is superior across all datasets, and (3) RFF GPs consistently provide good performance, but\nthis performance can often be improved upon by using other basis expansions.\n\n4.2 The Necessity of Ensembles of Dynamic Ensembles\n\nIn this experiment, we demonstrate that the E-DOEBE model introduced in Section 4.2 can indeed prevent the premature collapse of\nBMA weights. While this premature collapse of BMA weights does not appear to be common in real datasets, it is not difficult to\nillustrate its possibility, even on real datasets with high-performing methods.\n\nAs a constructive example, we can create an ensemble of additive HSGPs on the Kuka 1 dataset, where dynamic models performed\nsignificantly better in Section 5.1. Specifically, we created an ensemble of two additive HSGPs, with the first model being dynamic\n(\u02d803c3(1)rw2 = 10-3) and the second model being static (\u02d803c3(2) = 0). The ensemble hyperparameters were determined using\nempirical Bayes, with initial length scale values set to the vector of ones. Subsequently, the resulting ensemble was trained online as\na DOEBE and as an E-DOEBE, with \u02d803b4 = 10-2. Note that in this carefully controlled setting, each basis expansion is entirely\ndeterministic given the hyperparameters, so the results are purely deterministic and cannot be attributed to poor random seeds.\n\nThe resulting weights demonstrate that premature collapse of BMA weights can be a problem. Numerically, the log-likelihood of the\nE-DOEBE model is dramatically better than that of the DOEBE model (Table 2), showing this collapse can be catastrophic.\nThis issue can be partially averted by eliminating the threshold of 10-16 when ensembling. Indeed, in this example, the weights\nreach a minimum of approximately 10-72. However, with any finite precision arithmetic, there is always the potential for this type\nof collapse to occur due to numerical underflow. It is trivial to construct such examples by generating the first N1 samples with\n\u02d803c3(m)rw = 0 until weight collapse occurs, and the rest of the dataset with \u02d803c3(m)rw > 0.\n\nKey Takeaway The key takeaway of this experiment is that an ensemble of dynamic and static models can catastrophically collapse\n\u02d82014 even when the discrepancy in performance along the entire dataset is large \u02d82014 and that the E-DOEBE approach proposed in\nSection 4.2 can avoid this collapse.\n\n4.3 E-DOEBE Outperforms Other Methods\n\nThe ultimate goal of the E-DOEBE model is to combine static and dynamic models of several different types. To do so, we repeat\nthe experiments of Section 5.1 while comparing to an E-DOEBE model. We restrict our attention to static and dynamic versions of\nthe three best-performing families of models in Experiment 1 ((D)OE-HSGP, (D)OE-RFF, and (D)OE-RBF), and an E-DOEBE\nensemble containing all of them. The E-DOEBE model is created with \u02d803b4 = 10-2, which was not tuned.\n\nAs desired, the E-DOEBE model can effectively ensemble dynamic and static models of different basis expansions. Across all\nexperiments, the E-DOEBE model performs the best in terms of PLL, and is the best in terms of NMSE for all but one dataset\n(Friedman 2).\n\nKey Takeaway The E-DOEBE can effectively ensemble several different ensembles of high-performing basis expansions, resulting\nin consistently better performance than any single method.",
  "results": "",
  "conclusion": "In this paper, we demonstrated that recent advancements in online prediction using RFF GPs can be extended to arbitrary linear basis\nexpansions. This included several basis expansions that surpass RFF GPs on real and synthetic datasets. We show how different\nlinear basis expansions can be combined within a simple framework, enhancing ensemble diversity. While several common choices\nof basis expansions were employed, it would be worthwhile to expand the tests even further, particularly with splines.\n\nWe also demonstrated that the premature collapse of BMA weights can be a concern in online combining. We introduced the\nE-DOEBE model, which mitigates this issue, and demonstrated its effectiveness. However, this meta-combining may be perceived\n\n3\n\n\fas adding a complex workaround to BMA rather than addressing the underlying problems. Further research could explore the\nincorporation of other Bayesian combining methods, such as Bayesian (hierarchical) stacking.\n\nWhile we provide guidance on initializing ensembles given a set of basis expansions, determining which basis expansions to use is\nan important open topic. A naive approach would be to expand on the existing use of the marginal likelihood for model selection,\nbut this may be \"unsafe\" when using different basis expansions and therefore requires caution. We additionally presented several\nideas for inference with non-Gaussian likelihoods, for example, for classification tasks. Determining which, if any, of these tasks is\nsuperior to the Laplace approximation is another interesting topic for future study.\n\nFinally, it could be beneficial to modify or add new basis expansions in the online setting. Indeed, recent progress in GPs has worked\ntowards selecting and adapting kernels online to great benefit. If such techniques could be adapted to DOEBE, it could eliminate the\npre-training period and allow for adapting the domain of approximations when new data arrives.\n\n6 Tables\n\nTable 1: Dataset statistics, including the number of samples, the number of features, and the original source. In addition to the\noriginal sources above, several of these datasets were curated by the UCI Machine Learning Repository or LibSVM.\n\nDataset Name Number of Samples Dimensionality d\n\nFriedman 1\nFriedman 2\nElevators\nSARCOS\nKuka 1\nCaData\nCPU Small\n\n40,000\n40,000\n16,599\n44,484\n197,920\n20,640\n8,192\n\n10\n4\n17\n21\n21\n8\n12\n\nTable 2: Predictive log-likelihood of DOEBE and E-DOEBE models in Experiment 2 (higher is better).\n\nMethod\n\nPredictive Log-Likelihood\n\nDOEBE\nE-DOEBE\n\n-403.41\n0.55\n\nTable 3: Predictive likelihood (higher is better) and normalized MSE (lower is better) of type-II MLE and Laplace-approximated\ninitialization, plus/minus one standard deviation over 100 trials. Bolded entries denote superior performance significant at the p =\n0.05 level according to a one-sided Wilcoxon rank-sum test.\n\n2*Method\n\nElevators\n\nPredictive Log-Likelihood\nSARCOS\n\nCaData\n\nNormalized Mean Square\nSARCOS\nElevators\n\nError\nCaData\n\nDOE-HSGP-MLE\nDOE-HSGP-Sample\nDOE-RFF-MLE\nDOE-RFF-Sample\n\n-0.753 \u00b1 0.000\n-0.748 \u00b1 0.003\n-0.640 \u00b1 0.007\n-0.639 \u00b1 0.007\n\n0.421 \u00b1 0.000\n0.466 \u00b1 0.010\n0.756 \u00b1 0.018\n0.766 \u00b1 0.019\n\n0.081 \u00b1 0.000\n0.120 \u00b1 0.010\n0.243 \u00b1 0.009\n0.247 \u00b1 0.009\n\n0.221 \u00b1 0.000\n0.219 \u00b1 0.001\n0.178 \u00b1 0.003\n0.177 \u00b1 0.004\n\n0.017 \u00b1 0.000\n0.018 \u00b1 0.000\n0.018 \u00b1 0.001\n0.018 \u00b1 0.001\n\n0.055 \u00b1 0.000\n0.052 \u00b1 0.001\n0.040 \u00b1 0.001\n0.040 \u00b1 0.002\n\n4\n\n\fTable 4: Dataset statistics, including the number of samples and the number of features for datasets used in Delbridge et al. (2020).\nAll datasets are available on the UCI Machine Learning Repository.\n\nDataset Name Number of Samples Dimensionality d\n\nautos\nservo\nmachine\nyacht\nautompg\nhousing\nstock\nenergy\nconcrete\nairfoil\ngas\nskillcraft\nsml\npol\nbike\nkin40k\n\n25\n4\n7\n6\n7\n13\n11\n8\n8\n5\n128\n19\n26\n26\n17\n8\n\n159\n167\n209\n308\n392\n506\n536\n768\n1,030\n1,503\n2,565\n3,338\n4,137\n15,000\n17,379\n40,000\n\n5"
}
{
  "title": "Learning Explanations from Language Data",
  "abstract": "PatternAttribution is a recent method, introduced in the vision domain, that explains\nclassifications of deep neural networks. We demonstrate that it also generates\nmeaningful interpretations in the language domain.",
  "introduction": "In the last decade, deep neural classifiers achieved state-of-the-art results in many domains, among\nothers in vision and language. Due to the complexity of a deep neural model, however, it is difficult\nto explain its decisions. Understanding its decision process potentially allows to improve the model\nand may reveal new knowledge about the input. Recently, it was claimed that \u201cpopular explanation\napproaches for neural networks (...) do not provide the correct explanation, even for a simple linear\nmodel.\u201d They show that in a linear model, the weights serve to cancel noise in the input data and thus\nthe weights show how to extract the signal but not what the signal is. This is why explanation methods\nneed to move beyond the weights, the authors explain, and they propose the methods \u201cPatternNet\u201d\nand \u201cPatternAttribution\u201d that learn explanations from data. We test their approach in the language\ndomain and point to room for improvement in the new framework.",
  "related_work": "Many of the approaches used to explain and interpret models in NLP mirror methods originally\ndeveloped in the vision domain.\nIn this paper we implemented a similar strategy. Following\nKindermans et al., however, our approach improves upon the latter methods for the reasons outlined\nabove. Furthermore, PatternAttribution is related to work who make use of Taylor decompositions to\nexplain deep models. PatternAttribution reveals a good root point for the decomposition, the authors\nexplain.",
  "methodology": "Kindermans et al. assume that the data x passed to a linear model wT x = y is composed of signal\n(s) and noise (d, from distraction) x = s + d. Furthermore, they also assume that there is a linear\nrelation between signal and target y as = s where as is a so called signal base vector, which is in fact\nthe \u201cpattern\u201d that PatternNet finds for us. As mentioned in the introduction, the authors show that in\nthe model above, w serves to cancel the noise such that\n\nwT d = 0, wT s = y.\n\n(1)\n\nThey go on to explain that a good signal estimator S(x) = \u02c6s should comply to the conditions in Eqs.\n1 but that these alone form an ill-posed quality criterion since S(x) = u(wT u)\u22121y already satisfies\nthem for any u for which wT u \u0338= 0. To address this issue they introduce another quality criterion\nover a batch of data x:\n\n\u03c1(S) = 1 \u2212 max\n\nv\n\ncorr(y, vT (x \u2212 S(x)))\n\n(2)\n\nand point out that Eq. 2 yields maximum values for signal estimators that remove most of the\ninformation about y in the noise. We argue that Eq. 2 still is not exhaustive. Consider the artificial\nestimator\n\nSm(x) = mx + (1 \u2212 m)s = s + md\n(3)\nwhich arguably is a a bad signal estimator for large m as its estimation contains scaled noise, md.\nNevertheless, it still satisfies Eqs. 1 and yields maximum values for Eq. 2 since\n\nx \u2212 Sm(x) = (1 \u2212 m)(x \u2212 s) = (1 \u2212 m)d\n\n(4)\n\nis again just scaled noise and thus does not correlate with the output y. To solve this issue, we propose\nthe following criterion:\n\n\u03c1\u2032(S) := max\n\nv1\n\ncorr(wT x, vT\n\n1 S(x)) \u2212 max\nv2\n\ncorr(wT x, vT\n\n2 (x \u2212 S(x))).\n\n(5)\n\n\fThe minuend measures how much noise is left in the signal, the subtrahend measures how much\nsignal is left in the noise. Good signal estimators split signal and noise well and thus yield large\n\u03c1\u2032(S). We leave it to future research to evaluate existing signal estimators with our new criterion. For\nour experiments, the authors equip us with expressions for the signal base vectors as for simple linear\nlayers and ReLU layers. For the simple linear model, for instance, it turns out that as = cov(x, y)/\u03c32\ny.\nTo retrieve contributions for PatternAttribution, in the backward pass, the authors replace the weights\nby w \u00b7 as.",
  "experiments": "To test PatternAttribution in the NLP domain, we trained a CNN text classifier on a subset of the\nAmazon review polarity data set. We used 150 bigram filters, dropout regularization and a dense FC\nprojection with 128 neurons. Our classifier achieves an F1 score of 0.875 on a fixed test split. We\nthen used PatternAttribution to retrieve neuron-wise signal contributions in the input vector space. To\nalign these contributions with plain text, we summed up the contribution scores over the word vector\ndimensions for each word and used the accumulated scores to scale RGB values for word highlights\nin the plain text space. Positive scores are highlighted in red, negative scores in blue. This approach\nis inspired by similar work. Example contributions are shown in Figs. 1 and 2.",
  "results": "We observe that bigrams are highlighted, in particular no highlighted token stands isolated. Bigrams\nwith clear positive or negative sentiment contribute heavily to the sentiment classification. In contrast,\nstop words and uninformative bigrams make little to no contribution. We consider these meaningful\nexplanations of the sentiment classifications.",
  "conclusion": "We successfully transferred a new explanation method to the NLP domain. We were able to demon-\nstrate that PatternAttribution can be used to identify meaningful signal contributions in text inputs.\nOur method should be extended to other popular models in NLP. Furthermore, we introduced an\nimproved quality criterion for signal estimators. In the future, estimators can be deduced from and\ntested against our new criterion.\n\n2"
}
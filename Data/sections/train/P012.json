{
  "title": "Harmonizing Scaling Laws: Bridging the Gap\nBetween Kaplan and Chinchilla",
  "abstract": "Studies by Kaplan et al. (2020) and Hoffmann et al. (2022) examined the scaling\ncharacteristics of transformers in next-token language prediction, yielding different\nrecommendations for configuring the number of parameters (N) and training tokens\n(D) to minimize loss within a set compute budget (C). Kaplan suggested an optimal\nparameter count scaling with Noptimal \u221d C0.73, whereas Chinchilla proposed\nNoptimal \u221d C0.50. This paper demonstrates that a significant portion of this\ndifference can be traced back to Kaplan\u2019s focus on non-embedding parameters,\nrather than the total parameter count, along with their study\u2019s concentration on a\nsmaller scale. When the Chinchilla study is simulated under similar circumstances,\nbiased scaling coefficients similar to those of Kaplan are produced. As a result, this\nwork confirms Chinchilla\u2019s scaling coefficients by clarifying the primary reason for\nKaplan\u2019s initial overestimation. Additionally, this research clarifies variations in\nthe stated correlations between computational loss and budget. As a result of these\nfindings, we advocate for upcoming scaling investigations to utilize total parameter\ncounts and overall computational resources.",
  "introduction": "Two important studies by Kaplan et al. (2020) and Hoffmann et al. (2022) examined how scale\naffects large language models (LLMs). Both studies provided advice on how to balance model\nparameters (N) and training tokens (D) for a fixed computing budget (C), but their suggestions\nconflicted. The conclusion drawn from Kaplan\u2019s discovery that Noptimal \u221d C0.73 and Doptimal\n\u221d C0.27 was that \"large models might be more crucial than extensive data.\" Subsequently, LLMs\ntrained in the following years allocated more resources to model size and less to data size. The\nChinchilla research that came after that discovered that Noptimal \u221d C0.50 and Doptimal \u221d C0.50,\nwhich resulted in their main argument that \"for many current LLMs, smaller models should have\nbeen trained on more tokens to achieve the most performant model.\" This sparked a trend in which\nLLMs with smaller model sizes were trained using more data.\n\nWhat caused the discrepancy in these scaling coefficient estimates, which resulted in a significant\nwaste of computer resources, emissions, and money? There have been theories suggesting that\nvariations in optimization techniques or datasets might account for the differences. This paper argues\nthat these explanations are insufficient and proposes a straightforward substitute: the majority of\nthe discrepancy is caused by Kaplan\u2019s decision to count non-embedding parameters instead of total\nparameters, together with the limited scale of their investigation.\n\nAdditionally, it is discovered that this methodological discrepancy contributes to variations in the\nstated correlation between loss and compute.\n\nSpecifically, this research provides the following:\n\n\u2022 An analytical method is created to assess the scaling relationships described in the studies\n(Section 3). If non-embedding parameters are utilized, and at a smaller scale, this method\ndemonstrates that Kaplan\u2019s documented relationship is locally compatible with Chinchilla\u2019s.\n\n.\n\n\f\u2022 We investigate the stated correlations between processing power and loss (Section 5). Once\nmore, the cause of Kaplan\u2019s skewed estimate is the use of non-embedding parameters and\nsmaller scale models, together with the lack of an offset term in their compute-loss equation.\n\u2022 It is suggested that the scaling community use total parameters, total compute, and an offset\n\nin the compute-loss equation going forward.\n\n2 Preliminaries\n\nThis section provides some foundational information and definitions (Section 2.1), summarizes\nthe analytical method used for our primary finding (Section 2.2), and documents our assumptions\n(Section 2.3).\n\n2.1 Set Up\n\nKaplan et al. (2020) and Hoffmann et al. (2022) conducted empirical studies to model the relationships\nbetween the number of parameters (N), training tokens (D), training compute (C), and loss (L) in\ntransformers used for language modeling. The primary functional relationship explored was a power\nlaw, y = axb, which is frequently employed in various scientific fields to illustrate the connection\nbetween two quantities (x and y) that span multiple orders of magnitude.\n\nThe two studies differed in their definitions of N and C. Kaplan investigated relationships regarding\nnon-embedding parameters (N\nE) and non-embedding compute (C\nE), excluding contributions from embedding layers for vocabulary and position indices (NE). In\ncontrast, Chinchilla studied total parameters (NT) and total compute (CT). We define,\n\nN T = N E + N E,\nN E = (h + v)d,\n\n(1)\n(2)\n\nwhere d represents the transformer residual stream\u2019s dimension, v denotes the vocabulary size, and\nh stands for the context length (included only when positional embeddings are learned). Utilizing\nthe typical approximation for training compute FLOPs C = 6ND (where a factor of 6 accounts for a\nforward and backward pass), we establish total and non-embedding compute as:\n\nCT = 6N T D = 6(N E + N E)D,\nCE = 6N ED.\n\n(3)\n(4)\n\nThe definition of compute, C = 6ND, indicates a direct trade-off between the number of parameters\nand training tokens for a specified compute budget. The focus of the two research studies is on\n\"compute optimal\" configurations, which are the parameter and token combinations that result in the\nlowest loss for a given compute budget. This is expressed as follows for total parameters (using \u22c6 to\ndenote \"optimal\"):\n\nSubject to:\n\nN T = argminL(N T, CT ).\n\nCT = 6N T D\n\nWith this notation, the estimated scaling coefficients can be written more precisely as:\n\nKaplan : N EC0.73E, Chinchilla : N T C0.50T.\n\n(5)\n\n(6)\n\n(7)\n\n(It should be noted that although this study concentrates on the scaling coefficient for parameters, the\ndata coefficient is inferred by subscribing to C = 6ND; N \u221d Ca \u2192 C/D \u221d Ca \u2192 D \u221d C1\u2212a.)\n\nAn important functional form relating NT, D, and L, as used in the Chinchilla study, is:\n\nL(N T, D) = N cN T + DcD\u03a603b2 + E,\n\n(8)\n\n2\n\n\fwhere Nc, Dc, \u03b1, \u03b2 > 0 are empirically determined constants, and E represents the irreducible loss\ninherent in language. This equation conveniently generates power-law relationships: N T \u221d Ca T\nwith a = \u03b2 / (\u03b1 + \u03b2), D T \u221d Cb T with b = \u03b1 / (\u03b1 + \u03b2), and L T - E \u221d C\u2212\u03b3 T with \u03b3 = \u03b1\u03b2 / (\u03b1 + \u03b2).\n\nThere are two possible specifications based on the constants in Equation 8: those originally reported\nin the Chinchilla study and those from a re-analysis by Besiroglu et al. (2024), which claims to\ncorrect minor errors in the fitting procedure. Our work presents results using both specifications.\n\nChinchilla : N c = 406.4, Dc = 410.7, = 0.3392,\u03a603b2 = 0.2849, E = 1.693 = \u03a621d2N T C0.46T,\n(9)\nEpochAI : N c = 482.0, Dc = 2085.43, = 0.3478,\u03a603b2 = 0.3658, E = 1.817 = \u03a621d2N T C0.51T.\n(10)\n\n2.2 Analysis Overview\n\nIn our analysis, we use data and insights from the Chinchilla and Kaplan studies to predict the scaling\nlaws that would result if the Chinchilla relationship were stated in terms of N\nE and C\nE, and this was done using the smaller model sizes used in Kaplan\u2019s study.\n\nIt will be demonstrated that when NT is large, N\nE becomes an insignificant component of the model\u2019s parameters and computing cost. As a result, the\ntwo coefficients are in direct opposition to one another in the large parameter regime. The embedding\nparameters are not insignificant when NT is smaller (this is the regime examined in Kaplan\u2019s study,\nwhich used parameters ranging from 768 to 1.5B). We discover that the relationship between N\nE and C\nE is not, in fact, a power law at the lower end of this range. However, fitting a \"local\" power law at\nthis modest scale yields a coefficient that is comparable to Kaplan\u2019s, roughly reconciling these two\nfindings.\n\nOur approach in Section 3 is broken down as follows:\n\n\u2022 Step 1. Fit a suitable function predicting N\n\nE from NT.\n\n\u2022 Step 2. Incorporate this function into a model predicting loss in terms of NT and CT.\n\n\u2022 Step 3. Analytically derive the relationship between N\n\nE and C\nE.\n\n\u2022 Step 4. Simulate synthetic data from the Chinchilla loss model over the model sizes used in\n\nthe Kaplan study. Fit a local power law for N\nE in terms of C\nE.\n\nSection 4 provides experimental validation of our analysis by training a set of language models at a\nvery small scale and examining scaling laws under different settings. Simply changing the basis from\nNT to N\nE yields coefficients consistent with Chinchilla and Kaplan, respectively, while varying token budgets\nand decay schedules does not.\n\nA second, connected contribution is made in Section 5. The two studies\u2019 suggested relationships\nbetween loss and computation are reconciled by us. In order to examine the relationship between the\nideal loss L\nE and compute C\nE, Steps 3 and 4 are carried out once more using a similar analysis as before. To do this, we start with\nChinchilla data and adjust for the smaller model sizes utilized in Kaplan\u2019s investigation, the exclusion\nof embedding parameters and compute, and a different fitting function option. We are able to roughly\nrecover Kaplan\u2019s compute-loss coefficient and reconcile the two studies by making these adjustments.\n\n3\n\n\f2.3 Assumptions\n\nFor transparency, we list the assumptions and approximations made in our analysis.\n\n\u2022 We assume C\n\nE = 6N\nED and CT = 6NT D.\n\n\u2022 We assume a fixed functional form between total and non-embedding parameters in Equation\n\n11, and fit \u03c9 empirically using Chinchilla model configurations.\n\n\u2022 We assume a fixed functional form between loss, total parameters, and training data given\nby Equation 8. We report results using both the Chinchilla (Equation 9) and Epoch AI\n(Equation 10) fitted constants.\n\n\u2022 We approximate Kaplan\u2019s models with 20 logarithmically spaced model sizes from 0.79k to\n\n1.58B non-embedding parameters.\n\n3 Analysis: Compute-Parameter Scaling Coefficient\n\nThis section presents our core analysis. We demonstrate that a local scaling coefficient ranging from\n0.74 to 0.78 (close to Kaplan\u2019s 0.73) can emerge when calculated in terms of non-embedding parame-\nters within the small-parameter regime, while remaining consistent with Chinchilla\u2019s coefficient.\n\nStep 1. Fit a suitable function predicting N\nE from NT.\n\nWe need a suitable function connecting non-embedding and total parameters. We propose to use the\nform:\n\nN T = N E + \u03a603c9N 1/3E\n\n(11)\n\nfor some constant \u03c9 > 0. Apart from having several desirable properties (strictly increasing and lim\nNT \u2192 \u221e NT = N\nE2), it can be supported by findings from both the Kaplan and Chinchilla studies.\n\nKaplan perspective. Consider Kaplan\u2019s approach to parameter counting:\n\nN T = 12ld2 + N E,\n\n(12)\n\nwhere l represents the number of layers. While Kaplan does not explicitly list their model configura-\ntions, they do explore varying the aspect ratio A = d/l for a fixed model size. They determine that\nmodels of a given size exhibit similar performance across a range of aspect ratios, and this is not\ninfluenced by model scale (their Figure 5). Consequently, we could propose a sizing scheme with a\nfixed aspect ratio (A \u2248 40 appears reasonable from their plots). Assuming this sizing allows us to\nstate (with l = d/A in Equation 12):\n\nN T =\n\n12\nA\n\nd3 + N E.\n\nObserving that N\nE = 12\nE A\n\nA d3 \u2192 d = (N\n\n12 )1/3, and combining with NE = (v + h)d,\n\nN T \u2248 N E + (v + h)(\n\nA\n12\n\n)1/3N 1/3E.\n\n(13)\n\n(14)\n\nThis takes the same form as Equation 11 with \u03c9 = (v + h)( A\n\n12 )1/3.\n\nChinchilla perspective. We empirically fit a function NT = N\nE + \u03c9N\u03b4\nE (note the learnable exponent) to the Chinchilla model configurations listed in Table A9 of Hoffmann\n\n4\n\n\fet al. (2022) for a range of NT (44M to 16B). We calculate NE from Equation 2, using the reported\nvocabulary size of 32,000, but disregard the context length of 2,048 since Chinchilla used non-\nlearnable position embeddings (though their inclusion only slightly affects the coefficients).\n\nFitting a model with numpy\u2019s polyfit yields coefficients \u03c9 = 47491 and \u03b4 = 0.34. The exponent is\nclose to 1/3, with an implied aspect ratio A = 39.2 (inferred from \u03c9). This further supports the form\nin Equation 11.\n\nStep 2. Incorporate this function into a model predicting loss in terms of NT and CT.\n\nIt should be remembered that although we are interested in how N T depends on CT, this only occurs\nbecause of how they both relate to loss.\n\nSubject to:\n\nN T = argminL(N T, CT ).\n\nCT = 6N T D\n\n(15)\n\n(16)\n\nTo analytically examine their scaling relationship, we need a mathematical expression for loss, for\nwhich we utilize the functional form from the Chinchilla study. Substituting CT = 6NT D into\nEquation 8 yields:\n\nL(N T, CT ) = N cN T + Dc(CT /6N T )\u03a603b2 + E.\n\n(17)\n\nBy differentiating Equation 16 with respect to NT, setting the result to zero, and rearranging in terms\nof NT, we obtain:\n\nN T = CT\n\n\u03b2\n\u03b1+\u03b2 (\n\n\u03b2Dc\n\u03b16\u03b2N c\n\n)\n\n1\n\n\u03b1+\u03b2 , orsimplyN T \u221d C\n\n\u03b2\n\u03b1+\u03b2\n\n(18)\n\nWe now modify Equation 16 to be in terms of non-embedding parameters and compute. While NT\nrequires Equation 11 from step 1, the second term avoids this because D = CT / 6NT = C\nE / 6N\nE.\n\nL(N E, CE) = N c(N E + \u03a603c9N 1/3E)\u03b1 + Dc(CE/6N E)\u03b2 + E\n\n(19)\n\nStep 3. Analytically derive the relationship between N\nE and C\nE.\n\nTo determine the relationship between N\nE and C\nE, we take the derivative of Equation 18 with respect to N\nE, set it to zero, and rearrange:\n\nCE = 6N E(N E + \u03c9(N E)1/3)\u03b1(\n\n\u03b2Dc\n\u03b1N c\n\n)(\n\n1\n\n1 + \u03c9\n\n3 (N E)\u22122/3\n\n+ \u03b1)\u22121 (20)\n\nThis indicates that, generally, the relationship between N\nE and C\nE is not a power law. Nevertheless, we can think about a \"local\" power law approximation. That is,\nfor a specific value of N\nE, there exists a constant g that provides a first-order approximation (denoted by \u221d) N\nE, where g is defined as:\n\ng :=\n\ndlog(C E)\ndlog(N E)\n\n=\n\n1\n\n1 \u2212 1\n\u03b2\n\n\u03c9\n3 (N E)\u22122/3\n\n5\n\n\f1+ \u03c9\n\n3 (N E)\u22122/3 + \u03b1 + 1 \u03b2 \u03c9\n\n3 (N E)\u22122/3 1+ \u03c9\n\n3 (N E)\u22122/3 . (21)\n\nThe derivation is detailed in Appendix A.1. There are three phases.\n\n\u2022 At a small scale, lim N\n\u03b2 \u2192 N\n\nE \u2192 0 g = \u03b1/3+\u03b2\n\n\u03b2\n\u03b1/3+\u03b2\n\nE \u221d C\nE.\n\n\u2022 At a large scale, lim N\n\u03b2 \u2192 N\n\nE \u2192 \u221e g = \u03b1+\u03b2\n\n\u03b2\n\u03b1+\u03b2\n\nE \u221d C\nE, consistent with the NT case in Equation 17.\n\n\u2022 A transition phase exists where g briefly increases. This occurs between the two limits when\n\nN2/3\nE is of the same order as \u03c9. Indeed, at exactly the point N2/3\nE = \u03c9, we have NT = N\nE + \u03c9N1/3\nE = NT = 2N\nE, indicating a 50/50 split between embedding and non-embedding parameters.\n\nStep 4. Simulate synthetic data from the Chinchilla loss model over the model sizes used in the\nKaplan study. Fit a local power law for N\nE in terms of C\nE.\n\nBy reading g, we could estimate a local power law and thus a scaling coefficient for a specific value\nof N\nE. However, it is unclear which N\nE value is representative of the Kaplan study. We choose a more accurate estimation approach,\ncreating synthetic training curves from Equation 18 over the range of model sizes employed in the\nKaplan study, and fitting coefficients using models that lie on the compute-efficient frontier. This will\nalso validate our analytical expression for N\nE and C\nE in Equation 19.\n\nWe simulated 20 models with N\nE ranging from 790 parameters to 1.58B (Kaplan reports using model sizes \"ranging in size from\n768 to 1.5 billion non-embedding parameters\"). For other constants in Equation 18, we adopt the\nEpoch AI specification (Equation 10) and \u03c9 = 47491, though we also report results for the Chinchilla\nspecification (Equation 9).\n\nMain result. The estimated scaling coefficient is shown when a power law is fitted to the compute\noptimal frontier (Chinchilla\u2019s Method 1) generated by these synthetic training curves. This represents\nour primary finding - by starting with a model from the Chinchilla study and modifying two aspects\nto match Kaplan\u2019s study (NT \u2192 N\nE, small model sizes 0.79k - 1.58B parameters), we obtain local scaling coefficients:\n\nEpochAI : N EC0.78E,\n\nChinchilla : N EC0.74E,\n\n(22)\n\n(23)\n\nwhich are close to the Kaplan coefficient of 0.73. Therefore, this demonstrates that the Chinchilla co-\nefficient is largely consistent with Kaplan\u2019s coefficient, given these two adjustments. This constitutes\nthe paper\u2019s main result, reconciling these two apparently conflicting results.\n\n6\n\n\f4 Experiments: Compute-Parameter Scaling Coefficient\n\nWe offer concise experiments to confirm that our assertions are valid for models trained on a limited\nscale (millions of parameters).\n\nExperiment 1. First, we confirm if scaling coefficients approximate those of Chinchilla and Kaplan\nwhen employing NT and N\nE, respectively.\n\nFive models with sizes NT \u2208 [0.8M, 1.6M, 2.1M, 3.3M, 4.6M] were trained using the BookCorpus\ndataset. The GPT-2 tokenizer was used, with a vocabulary size of 50,257 and a context length of\n16 (although this is much less than normal, our tests indicate that context length has no impact on\nscaling coefficients). To estimate scaling coefficients, Chinchilla\u2019s Method 1 was applied, using the\napproximation C = 6ND.\n\nModels were trained for updates \u2208 [4000, 4000, 4000, 8000, 8000], with a batch size of 65,536\ntokens per update, for a total of training tokens D \u2208 [262M, 262M, 262M, 524M, 524M]. For each\nmodel size, the optimal learning rate was selected from \u2208 [0.001, 0.005, 0.01, 0.05], and no annealing\nwas implemented.\nResult 1. When coefficients are fitted to NT, we obtain NT \u221d C0.49T, and for N\nE, we obtain N\nE \u221d C0.74\nE. These closely match the Chinchilla and Kaplan coefficients, respectively.\n\nExperiment 2. We present an ablation of optimization schemes, demonstrating that using multi-\nple training budgets per model has a negligible impact on coefficients (contrary to Chinchilla\u2019s\nexplanation).\n\n\u2022 Scheme 1. A single learning rate of 0.001 is set for all models. A single model is trained per\n\nsize, and no annealing is applied.\n\n\u2022 Scheme 2. The best learning rate is chosen per model. A single model is trained per size,\n\nand no annealing is applied. (As in our NT vs. N\nE comparison.)\n\n\u2022 Scheme 3. The best learning rate is chosen per model. A single model is trained per size,\n\nand cosine annealing is applied at the update budget. (Kaplan study used this.)\n\n\u2022 Scheme 4. The best learning rate is chosen per model. Six models are trained per size at\ndifferent budgets \u2208 [0.25D, 0.5D, 0.75D, 1.0D, 1.5D, 2.0D], and cosine annealing is applied.\n(Chinchilla study used this.)\n\nResult 2. The optimization technique has less of an influence on scaling coefficients than switching\nfrom NT to N\nE. Using a single set of models without annealing (scheme 2) yields coefficients that are identical to\nthose of the more computationally demanding scheme 4. In contrast to Chinchilla\u2019s assertion that\nswitching from Kaplan\u2019s scheme 3 to scheme 4 would lower the scaling coefficient, our research\nindicates the opposite, with an increase from 0.46 to 0.49. This might account for our minor\noverestimation of the scaling coefficients in Equations 21 and 22.\n\nTable 1: Comparison of different scaling coefficients from our experiments. Note that the change\nmoving from NT to N\nE has a much larger effect than moving between optimization schemes.\n\n5 Analysis: Compute-Loss Scaling Coefficient\n\nIn addition to examining the compute-optimal parameter scaling, Kaplan and Chinchilla also char-\nacterized the scaling relationship between compute and loss, assuming optimal parameter scaling.\nKaplan expressed this optimal loss in terms of non-embedding compute, while Chinchilla used total\ncompute.\n\nLE = minL(N E, CE), s.t.CE = 6N ED,\n\n(24)\n\n7\n\n\fLT = minL(N T, CT ), s.t.CT = 6N T D.\n\n(25)\n\nSpecifically, the two studies reported the following forms and coefficients linking optimal loss and\ncompute:\n\nKaplan : LE = (\n\nCE\nCo\n\n)\u03b3\n\nKaplan : LEC0.057E\nCT\nCo\n\nChinchilla : LT E = (\n\n)\u2212\u03b3\n\nChinchilla : LT EC0.155T\n\nEpochAI : LT EC0.178T\n\n(26)\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\n(Refer to Section A.3 for Chinchilla\u2019s compute coefficient.) Similar to the compute-parameter scaling\ncoefficient, Kaplan\u2019s coefficient of 0.057 initially appears significantly different from Chinchilla\u2019s\nrange of 0.155 to 0.178. However, we will again demonstrate that by starting with the Chinchilla\nstudy and adjusting for Kaplan\u2019s non-embedding compute, smaller scale, and their compute-loss\nform, these two coefficients can be largely reconciled.\n\nOur analysis follows the same four-step approach as in Section 3. We can directly reuse Steps 1 and\n2, while Steps 3 and 4 are now modified to study the relationship between optimal loss and compute,\nrather than optimal parameters and compute as previously.\n\nStep 3. Analytically derive the relationship between L\nE and C\nE.\n\nWe determine that the relationship between L\nE and C\nE is not a power law (derived in Section A.2).\n\ndlog(LE)\ndlog(CE)\n\n= N E(N E + \u03c9(N E)1/3\n\n)\u03b1 (1 1+ \u03c9\n\n3 (N E)\u22122/3 ) LE(6N E)\u03b2 (31)\n\nNevertheless, we can once more take into account a local first-order approximation, L\nE \u221d Ck\nE, where k = dlog(LE)\ndlog(CE) .\n\nStep 4. Simulate synthetic data from the Chinchilla loss model over the model sizes used in the\nKaplan study. Fit a local power law for L\nE in terms of C\nE, using Kaplan\u2019s compute-loss form.\n\nAs in Section 3, we could use Equation 30 to calculate a point estimate for k in the relationship L\nE \u221d Ck\nE, and then fit. However, we again opt for the more faithful procedure of simulating data from the\nloss curves.\n\nUsing Kaplan\u2019s compute-loss form L\nE = ( CE\nCo\n\n)\u03b3, we obtain the following models for the two specifications:\n\nEpochAI : LECE,\n\nChinchilla : LECE,\n\n(32)\n\n(33)\n\nwhich are roughly in line with Kaplan\u2019s reported coefficient of L\nE \u221d C\u22120.057\nE.\n\n8\n\n\fWe observe that Kaplan\u2019s form provides a good fit of the data in the non-embedding compute plot\nat a small scale, over the range of model sizes they considered. We speculate that this might be the\nmotivation for Kaplan\u2019s selection of this simpler compute-loss form.",
  "related_work": "After early research that established how language models get better with parameters, data, and\ntraining computation, there has been research into the theoretical underpinnings of these scaling laws\nand whether they apply to other domains.\n\nSeveral concurrent studies that have looked at how different design decisions affect scaling law\nanalyses are more closely related to the spirit of our work. The methodology for determining scaling\ncoefficients is revisited by Su et al. (2024). Hagele et al. (2024) discovered that multiple short\ndecays with a constant learning rate or stochastic weight averaging may be used to recreate numerous\nindependent cosine schedules more effectively. Our discovery is subtly different; a straightforward\nfixed learning rate will recover extremely comparable compute-parameter scaling coefficients as\nmany cosine schedules. The impact of different hyperparameters on scaling laws is examined by Bi\net al. (2024). They point out that different text datasets yield somewhat different optimal coefficients,\nwith \"cleaner\" data exhibiting more parameter-hungry scaling behavior, which they believe may\npartially account for the discrepancy between the Kaplan and Chinchilla coefficients.\n\nThe goal of Porian et al. (2024)\u2019s concurrent work is to clarify the discrepancies between the Kaplan\nand Chinchilla coefficients, which is the same goal as that of our paper. They conduct a number\nof large-scale experiments that replicate Kaplan\u2019s study, and they come to the conclusion that the\ndiscrepancy is caused by, in decreasing order of importance: 1) Kaplan\u2019s use of non-embedding\ncompute rather than total compute; 2) Kaplan\u2019s use of an excessively long fixed-length warmup\nperiod for smaller models, which made them appear less efficient; and 3) Kaplan\u2019s failure to fully\noptimize hyperparameters. We believe that these findings complement our own. We have used\nan entirely analytical method to identify the main \"first order\" cause using just the data that was\nmade publicly available in the two papers. (As a form of verification, tiny-scale experiments were\nconducted post-hoc.) This shows how mathematical techniques can be used in scaling\u2019s empirical\nscience.\n\n7 Discussion\n\nThis study sought to account for the disparity between the scaling coefficients of Kaplan and\nChinchilla. We discovered two problems with Kaplan\u2019s study that, when taken together, biased their\nestimated scaling coefficients: they focused on smaller model sizes and only counted coefficients:\nthey focused on smaller model sizes and only counted non-embedding parameters. This implies\na curvature in the actual relationship between Nand NT (Figure 5). At greater values of NT, the\nembedding parameter counts become negligible, NT = N, and differences would not arise. Alterna-\ntively, had Kaplan investigated relationships directly in terms of NT, this issue would also not occur,\neven at this smaller scale (confirmed by our Experiment 1 finding NT (\u221d)C(0.49)T evenf orN T <\n5M ).T hef ormKaplanusedtopredictlossf romcomputef urthercontributedtodif f erencesinthereportedcompute\u2212\nlossscalingcoef f icients.\n\nInconsistency across scaling studies. Existing literature on scaling is not consistent in its use of\nnon-embedding vs. total compute. Some studies follow Kaplan\u2019s approach, using non-embedding\nparameters or compute, while others adhere to the Chinchilla approach, using total parameters. Our\nwork indicates that this choice can substantially alter scaling exponents, complicating cross-study\ncomparisons. Similarly, the choice of compute-loss equation varies through the literature. Studies\nsuch as opt for the Kaplan compute-loss form without offsets. In contrast, employ the Chinchilla\ncompute-loss form with non-zero offsets. Again, our work suggests that these methodological\ndifferences can lead to significant variations in scaling predictions and interpretations.\n\nThe lack of a standardized approach in scaling studies risks making comparisons misleading and\ninsights less clear. We see our work as helping to understand certain decisions made in previous\nstudies that should be standardized. Concretely, we advise future studies to report total, rather than\nnon-embedding, parameters, and to include an offset in the compute-loss fitting models. We discuss\nmotivation for these choices below. Furthermore, our initial evidence does not support using multiple\n\n9\n\n\fcosine decays per model size \u2013 we find a single fixed learning rate per model size is sufficient for\nmeasuring compute-optimal parameter coefficients.\n\nWhy should embedding parameters contribute to scaling behavior? Several works provide evidence\nthat embedding parameters capture meaningful language properties. Word embeddings can be\nfactorized into semantically interpretable factors (even the shallow Word2vec). LLMs learn linear\nembeddings of space and time across scales. Developing such meaningful embedding structures\nallows LLMs to perform high-level language operations, such as arithmetic. Therefore, if one believes\nthat the embedding layer does more than just \u2018translate\u2019 tokens to a vector of the correct dimension,\nwe see no reason to exclude them in the parameter count.\n\nWhy should a non-zero offset be used in loss-compute predictions? The Chinchilla compute-loss form\nwith a non-zero offset (Equation 27) is a more appropriate form from the perspective of statistical\nlearning. This approach accounts for the concept of irreducible risk, which posits a lower bound on\nachievable loss regardless of model or dataset size. This may arise from various factors: inherent\nbiases or limitations in the learning algorithm, or noise in the original task. As a concrete example in\nlanguage modeling, the best a model can do for the prediction of the first token in a sequence is to\nestimate the marginal distribution of all tokens, which leads to a non-zero loss.\n\nLimitations. We acknowledge several limitations of our analysis. We have aimed to capture the\nprimary \u2018first order\u2019 reason for the difference between the Kaplan and Chinchilla scaling coefficients.\nBut there are multiple other differences between the two studies that likely also affect scaling coeffi-\ncients (Section 6); datasets (Kaplan used OpenWebText2, Chinchilla used MassiveText), transformer\ndetails (Kaplan used learnable position embeddings while Chinchilla\u2019s were fixed, also differing\ntokenizers, vocabularly sizes), optimization scheme (Kaplan used scheme 3, Chinchilla scheme\n4, also differing warmup schedules), differences in computation counting (Kaplan used C = 6ND,\nChinchilla\u2019s Method 1 and 2 used a full calculation). However, our work suggested these factors\nimpact coefficients in a more minor way.\n\n8 Appendix\n\n8.1 Derivation of Equation 20\n\nThis section derives Equation 20:\n\ng :=\n\ndlog(C)\ndlog(N )\n\n=\n\n1 \u2212 1\n\u03b2\n\nFirst note that\n\n1\n\n\u03c9\n3 (N )(\u22122/3)\n3 (N )(\u22122/3) + \u03b1+1\n\n\u03b2\n\n1+ \u03c9\n\n.\n\n(34)\n\n\u03c9\n3 (N )(\u22122/3)\n\n1+ \u03c9\n\n3 (N )(\u22122/3)\n\ndlog(C)\ndlog(N )\n\n=\n\ndlog(C)\ndN\n\ndN\ndlog(N )\n\n=\n\ndlog(C)\ndN\n\nN\n\n(35)\n\nRecall the definition of Cfrom Equation 19:\n\nC = 6N (N + \u03c9(N )(1/3))(\u03b1)((\n\n\u03b2Dc\n\u03b1N c\n\n))((\n\n1\n\n1 + \u03c9\n\n3 (N )(\u22122/3) + \u03b1\n\n))(\u22121)\n\n(36)\n\nlog(C) = log(N ) \u2212\n\n1\n\u03b2\n\nlog(1 +\n\n\u03c9\n3\n\n(N )(\u22122/3)) +\n\n\u03b1 + 1\n\u03b2\n\nlog(N + \u03c9(N )(1/3)) + const\n\n(37)\n\nwhere const. does not depend on N . We now can take the derivative of each term. Derivative of term\n1:\n\nDerivative of term 2:\n\ndlog(N )\ndN\n\n=\n\n1\nN\n\n10\n\n(38)\n\n\fd\ndN\n\n(\u2212\n\n1\n\u03b2\n\nlog(1 +\n\n\u03c9\n3\n\n(N )(\u22122/3))) = \u2212\n\n1\n\u03b2\n\n1\n\n1 + \u03c9\n\n3 (N )(\u22122/3)\n\n\u03c9\n3\n\n(\u2212\n\n2\n3\n\n)(N )(\u22125/3)\n\n(39)\n\nDerivative of term 3:\n\nd\ndN\n\n(\n\n\u03b1 + 1\n\u03b2\n\nlog(N + \u03c9(N )(1/3))) =\n\n\u03b1 + 1\n\u03b2\n\n1\nN + \u03c9(N )(1/3)\n\n(1 +\n\n\u03c9\n3\n\n(N )(\u22122/3))\n\n(40)\n\nThen assemble all terms and multiply by N as per Equation 35.\n\n8.2 Derivation of compute-loss analytical form in Equation 30\n\nThis section derives k, defined as:\n\nExpanding with the chain rule we find:\n\nk =\n\ndlog(L)\ndlog(C)\n\n.\n\nk =\n\ndlog(L)\ndL\n\ndL\ndN\n\ndN\ndlog(N )\n\ndlog(N )\ndlog(C)\n\n=\n\nN\nL\n\ndL\ndN\n\ng,\n\n(41)\n\n(42)\n\nwhere we previously derived g = (d log(C) dlog(N ))inEquation20.\nThis leaves us with (dL dN )tof ind.F irstnotethatLisgivenbyEquation18whentheoptimalmodelsizeisused,i.e.,N (\u2192)N :\n\nL = N c(N + \u03c9(N )(1/3))(\u03b1) + Dc(C/6N )(\u03b2) + E.\n\n(43)\n\nBefore taking this derivative, we recall that Cis actually a function of N (via Equation 19). Hence, we\ntackle the derivative in two parts. We find the first term derivative is equal to:\n\nd\ndN\n\n(N c(N + \u03c9(N )(1/3))(\u03b1)) = \u03b1N c(1 +\n\n\u03c9\n3\n\n(N )(\u22122/3))(N + \u03c9(N )(1/3))(\u03b1\u22121)\n\n(44)\n\nThe derivative of the second term, via the product rule, and spotting that (dC\n\ndN )=( Cg\n\nN ),equals:\n\nd\ndN\n\n(Dc(C/6N )(\u03b2)) = \u03b2Dc(\n\nC\n6N\n\n)(\u03b2\u22121)((\n\n1\n6N\n\n)(\n\nCg\nN\n\n) \u2212 (\n\nC\n6(N )(2)\n\n)) = \u03b2Dc(\n\nC\n6N\n\n)(\u03b2)(\n\ng \u2212 1\nN\n\n)\n\nHence, combining these two terms we find:\n\ndL\ndN\n\n= \u03b1N c(1 +\n\n\u03c9\n3\n\n(N )(\u22122/3))(N + \u03c9(N )(1/3))(\u03b1\u22121) + \u03b2Dc(\n\nC\n6N\n\n)(\u03b2)(\n\ng \u2212 1\nN\n\n)\n\nCombining this result into to Equation 43 we get:\n\n(45)\n\n(46)\n\nk =\n\nN\nL\n\ndL\ndN\n\ng =\n\ng\nL\n\n(\u03b1N c(1 +\n\n\u03c9\n3\n\n(N )(\u22122/3))(N + \u03c9(N )(1/3))(\u03b1) + \u03b2Dc(g \u2212 1)(\n\nC\n6N\n\n)(\u03b2))\n\n(47)\n\n8.3 Compute-loss coefficient derivation\n\n\u03b1+\u03b2 ).Substitutingtheseintothelossf ormof Equation8, andf orsomenewconstants( \u00afN c), ( \u00afDc)wef ind, LT =\n\n17\n\nN\n\nT\n\n(\u221d)C(\n\n\u03b2\n\n\u03b1+\u03b2 ), andsimilarlyDT (\u221d\n\n\u03b1\n\nknow\n\nWe\n)C(\nN c(N T )(\u03b1) + Dc(DT )(\u03b2) + E(48)\n\nEquation\n\nfrom\n\n11\n\n\fLT = \u00afN cC(\n\n\u03b1\u03b2\n\n\u03b1+\u03b2 ) + ( \u00afDc)C(\n\n\u03b1\u03b2\n\u03b1+\u03b2 ) + E\n\nLT \u2212 E(\u221d)C(\n\n\u2212\u03b1\u03b2\n\u03b1+\u03b2 )\n\n(49)\n\n(50)\n\n12",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": ""
}
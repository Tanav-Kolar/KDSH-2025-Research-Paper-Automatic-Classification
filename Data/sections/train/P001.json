{
  "title": "Leveraging Clustering Techniques for Enhanced\nDrone Monitoring and Position Estimation",
  "abstract": "Drone tracking and localization are essential for various applications, including\nmanaging drone formations and implementing anti-drone strategies. Pinpointing\nand monitoring drones in three-dimensional space is difficult, particularly when\ntrying to capture the subtle movements of small drones during rapid maneuvers.\nThis involves extracting faint signals from varied flight settings and maintaining\nalignment despite swift actions. Typically, cameras and LiDAR systems are used\nto record the paths of drones. However, they encounter challenges in categorizing\ndrones and estimating their positions accurately. This report provides an overview\nof an approach named CL-Det. It uses a clustering-based learning detection strategy\nto track and estimate the position of drones using data from two types of LiDAR\nsensors: Livox Avia and LiDAR 360. This method merges data from both LiDAR\nsources to accurately determine the drone\u2019s location in three dimensions. The\nmethod begins by synchronizing the time codes of the data from the two sensors\nand then isolates the point cloud data for the objects of interest (OOIs) from the\nenvironmental data. A Density-Based Spatial Clustering of Applications with\nNoise (DBSCAN) method is applied to cluster the OOI point cloud data, and the\ncenter point of the most prominent cluster is taken as the drone\u2019s location. The\ntechnique also incorporates past position estimates to compensate for any missing\ninformation.",
  "introduction": "Unmanned aerial vehicles (UAVs), commonly referred to as drones, have gained prominence and\nsignificantly influence areas like logistics, imaging, and emergency response, offering substantial\nadvantages to society. However, the broad adoption and sophisticated features of compact, off-the-\nshelf drones have created intricate security issues that extend beyond conventional risks.\n\nRecent years have witnessed a surge in research on anti-UAV systems. Present anti-UAV methods\npredominantly utilize visual, radar, and radio frequency (RF) technologies. Despite these strides,\nrecognizing drones poses a considerable hurdle for sensors like cameras, particularly when drones\nare at significant altitudes or in challenging visual environments. These methods usually fail to spot\nsmall drones because of their minimal size, which leads to a decreased radar cross-section and a\nless noticeable visual presence. Furthermore, current anti-UAV studies primarily focus on detecting\nobjects and tracking them in two dimensions, overlooking the crucial element of estimating their\n3D paths. This omission significantly restricts the effectiveness of anti-UAV systems in practical,\nreal-world contexts.\n\nOur proposed solution, a detection method based on clustering learning (CL-Det), uses the strengths\nof both Livox Avia and LiDAR 360 to improve the tracking and position estimation of UAVs.\nInitially, the timestamps from the Livox Avia and LiDAR 360 data are aligned to maintain temporal\nconsistency. By examining the LiDAR data, which contains the spatial coordinates of objects at\nspecific times, and comparing these to the actual recorded positions of the drone at those times, the\ndrone\u2019s location within the LiDAR point cloud data is effectively pinpointed. The point cloud for\n\n.\n\n\fobjects of interest (OOIs) is then isolated from the environmental data. The point cloud of the OOIs\nis grouped using the DBSCAN algorithm, and the central point of the largest cluster is designated as\nthe UAV\u2019s position. Moreover, radar data also faces significant challenges due to missing information.\nTo mitigate potential data deficiencies, past estimations are employed to supplement missing data,\nthereby maintaining the consistency and precision of UAV tracking.",
  "related_work": "",
  "methodology": "This section details the methodology employed to ascertain the drone\u2019s spatial position utilizing\ninformation from LiDAR 360 and Livox Avia sensors. The strategy integrates data from both sensor\ntypes to achieve precise position calculations.\n\n2.1 Data Sources\n\nThe following modalities of data were utilized:\n\n\u2022 Double fisheye camera visual images\n\u2022 Livox Mid-360 (LiDAR 360) 3D point cloud data\n\u2022 Livox Avia 3D point cloud data\n\u2022 Millimeter-wave radar 3D point cloud data\n\nOnly 14 out of 59 test sequences have non-zero radar values; therefore, the radar dataset is excluded\nfrom this work due to data availability issues. Two primary sensor types are employed: LiDAR 360\nand Livox Avia, both of which supply 3D point cloud data crucial for identifying the drone\u2019s location.\nThe detailed data descriptions are outlined as follows:\n\n\u2022 LiDAR 360 offers a complete 360-degree view with 3D point cloud data. This dataset\n\nencompasses environmental details and other observable objects.\n\n\u2022 Livox Avia delivers focused 3D point cloud data at specific timestamps, typically indicating\n\nthe origin point or the drone\u2019s position.\n\n2.2 Algorithm\n\nFor every sequence, corresponding positions are recorded at specific timestamps. The procedure\ngives precedence to LiDAR 360 data, using Livox Avia data as a backup if the former is not available.\nIf neither source is accessible, the position is estimated using historical averages.\n\n2.2.1 LiDAR 360 Data Processing\n\n\u2022 Separation of Points: The LiDAR 360 data is visually examined to classify areas into two\n\nzones: environment and non-environment zones.\n\n\u2022 Removal of Environment Points: All points within the environment zone are deemed part\nof the surroundings and are thus excluded from the dataset. After removing environment\npoints, it is observed that the remaining non-environment points imply the drone position.\n\u2022 Clustering: The DBSCAN clustering algorithm is applied to the remaining points to discern\n\ndistinct clusters.\n\n\u2022 Cluster Selection: The most extensive non-environment cluster is chosen as the representa-\n\ntive group of points that correspond to the drone.\n\n\u2022 Mean Position Calculation: The drone\u2019s position is determined by calculating the mean of\n\nthe selected cluster, represented by (x, y, z) coordinates.\n\n2.2.2 Livox Avia Data Processing\n\n\u2022 Removal of Noise: Points with coordinates (0, 0, 0) are eliminated as they are regarded as\n\nnoise.\n\n\u2022 Mean Position Calculation: The mean of the residual points is computed to ascertain the\n\ndrone\u2019s position in (x, y, z) coordinates.\n\n2\n\n\f2.2.3 Fallback Method\n\nWhen neither LiDAR 360 nor Livox Avia data is available, the average location of the drone derived\nfrom training datasets is used. The average ground truth position (x, y, z) from all training datasets\nestimates the drone ground truth position, which is (0.734, -9.739, 33.353).\n\n2.3\n\nImplementation Details\n\nThe program fetches LiDAR 360 or Livox Avia data from the nearest timestamp for each sequence,\nas indicated in the test dataset. Clustering is executed using the DBSCAN algorithm with appro-\npriate parameters to guarantee strong clustering. Visual inspection is employed for the preliminary\nseparation of points, ensuring an accurate categorization of environment points.\n\nThe implementation was conducted on a Lenovo IdeaPad Slim 5 Pro (16\") running Windows 11\nwith an AMD Ryzen 7 5800H CPU and 16GB DDR4 RAM. The analysis was carried out in a\nJupyter Notebook environment using Python 3.10. For clustering, the DBSCAN algorithm from the\nScikit-Learn library was utilized. The DBSCAN algorithm was configured with an epsilon (eps)\nvalue of 2 and a minimum number of points (minPts) set to 1.",
  "experiments": "",
  "results": "The algorithm achieved a pose MSE loss of 120.215 and a classification accuracy of 0.322. Table 1\npresents the evaluation results compared to other teams.\n\nTable 1: Evaluation results on the leaderboard\n\nTeam\n\nID\n\nPose MSE (\u2193) Accuracy (\u2191)\n\nSDUCZS\nGaofen Lab\nsysutlt\ncasetrous\nNTU-ICG (ours)\nMTC\ngzist\n\n58198\n57978\n57843\n58233\n58268\n58180\n56936\n\n2.21375\n7.299575\n24.50694\n56.880267\n120.215107\n189.669428\n417.396317\n\n0.8136\n0.3220\n0.3220\n0.2542\n0.3220\n0.2724\n0.2302\n\n4 Conclusions\n\nThis paper introduces a clustering-based learning method, CL-Det, which employs advanced cluster-\ning techniques such as K-Means and DBSCAN for drone detection and position estimation using\nLiDAR data. The approach guarantees dependable and precise drone position estimation by utilizing\nmulti-sensor data and robust clustering methods. Fallback mechanisms are in place to ensure con-\ntinuous position estimation even when primary sensor data is absent. Through thorough parameter\noptimization and comparative assessment, the proposed method\u2019s effective performance in drone\ntracking and position estimation is demonstrated.\n\n3",
  "conclusion": ""
}
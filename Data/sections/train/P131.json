{
  "title": "Enhancing Disentanglement through Learned\nAggregation of Convolutional Feature Maps: A Study\non the 2019 Disentanglement Challenge",
  "abstract": "This paper details our submission for stage 2 of the 2019 disentanglement challenge.\nIt introduces a straightforward image preprocessing technique for discovering dis-\nentangled latent factors. Our approach involves training a variational autoencoder\nusing aggregated feature maps. These maps are obtained from networks that were\npretrained on the ImageNet database, and we leverage the implicit inductive bias\npresent in those features for disentanglement. This bias can be further strengthened\nby fine-tuning the feature maps with auxiliary tasks such as angle, position estima-\ntion, or color classification. Our method achieved second place in stage 2 of the\ncompetition. Code is publicly available.",
  "introduction": "Methods that are fully unsupervised are unable to learn disentangled representations unless further\nassumptions are made through inductive biases on both the model and the data. In our submission, we\nutilize the implicit inductive bias included in models pretrained on the ImageNet database, and then\nimprove it by fine-tuning such models on tasks that are relevant to the challenge such as angle, position\nestimation, or color classification. Our stage 2 submission builds upon our stage 1 submission, in\nwhich we used pretrained CNNs to extract convolutional feature maps as a preprocessing step before\ntraining a VAE. Although this approach provided adequate disentanglement scores, two weaknesses\nwere identified with the feature vectors that were extracted. First, the feature extraction network\nis trained on ImageNet, which is dissimilar to the MPI3d dataset that was used in the challenge.\nSecondly, the mechanism for feature aggregation was chosen in an ad-hoc way, and likely did not\nretain all information needed for disentanglement. We address these issues by fine-tuning the feature\nextraction network as well as by learning how to aggregate feature maps from data by using the labels\nof the simulation datasets MPI3d-toy and MPI3d-realistic.\n\n2 Method\n\nOur method includes three steps: (1) a supervised fine-tuning of the feature extraction CNN, (2)\nextracting a feature vector from each image in the dataset using the fine-tuned network, and (3)\ntraining a VAE to reconstruct the feature vectors and disentangle the latent factors of variation.\n\n2.1 Finetuning the Feature Extraction Network\n\nIn this step, we fine-tune the feature extraction network offline, before submitting to the evaluation\nserver. The aim is to adapt the network so that it produces aggregated feature vectors that retain the\nnecessary information for disentangling the latent factors of the MPI3d-real dataset. The network is\nfine-tuned by learning to predict the value of each latent factor using the aggregated feature vector of\nan image. To do so, we use the simulation datasets MPI3d-toy and MPI3d-realistic, specifically the\nimages as inputs and the labels as supervised classification targets.\n\n.\n\n\fFor the feature extraction network, we use the VGG19-BN architecture from the torchvision package.\nThe input images are standardized using mean and variance across each channel as computed from\nthe ImageNet dataset. We use the output feature maps from the last layer before the final average\npooling (dimensionality 512 x 2 x 2) as the input to a feature aggregation module which reduces\nthe feature map to a 512-dimensional vector. The aggregation module consists of three convolution\nlayers using 1024, 2048, and 512 feature maps and kernel sizes of 1, 2, and 1 respectively. Each layer\nis followed by batch normalization and ReLU activation. We also utilize layerwise dropout with a\nrate of 0.1 before each convolution layer. Finally, the aggregated feature vector is L2-normalized.\nThis was empirically found to be important for the resulting disentanglement performance. Then, for\neach latent factor, we add a linear classification layer that computes the logits of each class using the\naggregated feature vector. These linear layers are discarded after this step.\n\nWe use both MPI3d-toy and MPI3d-realistic for training to push the network to learn features that\nidentify latent factors in a robust way, regardless of details such as reflections or specific textures. We\nsplit each dataset randomly with 80\n\n2.2 Feature Map Extraction and Aggregation\n\nIn this step, we use the fine-tuned feature extraction network to produce a set of aggregated feature\nvectors. We simply run the network on each image of the dataset and store the aggregated 512-\ndimensional vectors in memory. Again, inputs to the feature extractor are standardized such that mean\nand variance across each channel correspond to the respective values from the ImageNet dataset.\n\n2.3 VAE Training\n\nFinally, we train a standard \u03b2-VAE on the set of aggregated feature vectors. The encoder network\nconsists of a single fully connected layer with 4096 neurons, followed by two fully-connected layers\nthat parameterize the means and log variances of a normal distribution N used as the approximate\nposterior q(z|x). The number of latent factors is determined experimentally. The decoder network\nhas four fully-connected layers with 4096 neurons each, followed by a fully-connected layer parame-\nterizing the means of a normal distribution N used as the conditional likelihood p(x|z). The mean is\nconstrained to the range [0, 1] using the sigmoid activation. All fully connected layers except for the\nfinal ones use batch normalization and are followed by ReLU activation functions. We use orthogonal\ninitialization for all layers and assume a factorized standard normal distribution as the prior p(z) on\nthe latent variables.\n\n(cid:80)512\n\nj=1 1 + log(\u03c32\n\nFor optimization, we use the RAdam optimizer with a learning rate of 0.001, \u03b20 = 0.999, \u03b21 = 0.9\nand a batch size of 256. The VAE is trained for 120 epochs by maximizing the evidence lower bound,\nwhich is equivalent to minimizing\ni=1 ||\u00b5i \u2212 xi||2 + 0.5\u03b2 (cid:80)C\n\n1\nj ) \u2212 \u00b52\nB\nwhere \u03b2 is a hyperparameter to balance the MSE reconstruction and the KLD penalty term. Because\nthe scale of the KLD term depends on the number of latent factors C, we normalize it by C such that \u03b2\ncan be varied independently of C. It can be harmful to start training with too much weight on the KLD\nterm. Therefore, we use the following cosine schedule to smoothly anneal \u03b2 from \u03b2start = 0.005 to\n\u03b2end = 0.4 over the course of training:\n\u03b2(t) = { \u03b2 start f ort < tstart\n2 (\u03b2end \u2212 \u03b2start)(1 + cos(\u03c0 t\u2212tstart\n1\ntend\u2212tstart\n\u03b2endf ort > tend\n\n)) + \u03b2startf ortstart \u2264 t \u2264 tend\n\nj \u2212 \u03c32\nj\n\nwhere \u03b2(t) is the value for \u03b2 in training episode t \u2208 0, ..., N \u2212 1, and annealing runs from epoch\ntstart = 10 to epoch tend = 79. This schedule allows the model to initially learn to reconstruct\nthe data well, and only then puts pressure on the latent variables to be factorized, which improved\nperformance.\n\n2\n\n\f3 Discussion\n\nOur method achieved second place in stage 2 of the competition. Compared to our stage 1 approach,\nour stage 2 approach resulted in large improvements on the FactorVAE and DCI metrics. On the\npublic leaderboard, our best submission achieved first rank on these metrics. See appendix A for\nfurther discussion of the results.\n\nIntroducing prior knowledge makes the disentanglement task considerably easier, and this is reflected\nin the improved scores. However, our method uses task-specific supervision obtained from simulation,\nwhich restricts its applicability. Nevertheless, this demonstrates that such supervision can transfer to\nbetter disentanglement on real-world data, which was a goal of the challenge.\n\n3",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": ""
}
{
  "title": "Fast Vocabulary Transfer for Language Model\nCompression",
  "abstract": "Real-world business applications require a trade-off between language model\nperformance and size. We propose a new method for model compression that relies\non vocabulary transfer. We evaluate the method on various vertical domains and\ndownstream tasks. Our results indicate that vocabulary transfer can be effectively\nused in combination with other compression techniques, yielding a significant\nreduction in model size and inference time while marginally compromising on\nperformance.",
  "introduction": "In the last few years, many NLP applications have been relying more and more on large pre-trained\nLanguage Models (LM). Because larger LMs, on average, exhibit higher accuracy, a common trend\nhas been to increase the model\u2019s size. Some LMs like GPT-3 and BLOOM have reached hundreds\nof billion parameters. However, these models\u2019 superior performance comes at the cost of a steep\nincrease in computational footprint, both for development and for inference, ultimately hampering\ntheir adoption in real-world business use-cases. Besides models that only a few hi-tech giants can\nafford, like GPT-3, even smaller LMs with hundreds of million parameters could be too expensive\nor infeasible for certain products. For one thing, despite being tremendously cheaper than their\nbigger cousins, fine-tuning, deploying and maintaining large numbers of such models (one for each\ndownstream task) soon becomes too expensive. Furthermore, latency and/or hardware requirements\nmay limit their applicability to specific use-cases. For all these reasons, significant efforts - in both\nacademic and industry-driven research - are oriented towards the designing of solutions to drastically\nreduce the costs of LMs.\n\nRecently, several attempts have been made to make these models smaller, faster and cheaper, while\nretaining most of their original performance. Knowledge Distillation (KD) is a teacher-student\nframework, whereby the teacher consists of a pre-trained large model and the student of a smaller\none. The teacher-student framework requires that both the teacher and the student estimate the same\nprobability distribution. While the outcome is a smaller model, yet, this procedure constrains the\nstudent to operate with the same vocabulary as the teacher in the context of Language Modeling.\n\nIn this work, we explore a method for further reducing an LM\u2019s size by compressing its vocabulary\nthrough the training of a tokenizer in the downstream task domain. The tokenizer is a crucial part\nof modern LMs. In particular, moving from word to subword- level, the tokenization solves two\nproblems: vocabulary explosion and unknown words. Moreover, the capability to tokenize text\neffectively in any domain is key for the massive adoption of pre-trained general-purpose LMs fine-\ntuned on downstream tasks. Indeed, tokenizers are still able to process out-of-distribution texts at the\ncost of producing frequent word splits into multiple tokens.\n\nHowever, the language varies significantly in vertical domains or, more generally, in different topics.\nHence, ad-hoc tokenizers, trained on the domain statistics, may perform a more efficient tokenization,\nreducing on average the length of the tokenized sequences. This is important since compact and\nmeaningful inputs could reduce computational costs, while improving performance. Indeed, memory\nand time complexity of attention layers grows quadratically with respect to the sequence length.\n\n\fFurthermore, a vertical tokenizer may require a smaller vocabulary, which also affects the size of the\nembedding matrix, hence further reducing the model\u2019s size.\n\nFollowing this intuition, we propose a Vocabulary Transfer (VT) technique to adapt LMs to in-domain,\nsmaller tokenizers, in order to further compress and accelerate them. This technique is complementary\nto the aforementioned model compression methods and independent of the type of tokenizer. As a\nmatter of fact, we apply it in combination with KD.\n\nOur experiments show that VT achieves an inference speed-up between x1.07 and x1.40, depending\non the downstream task, with a limited performance drop, and that a combination of VT with KD\nyields an overall reduction up to x2.76.\n\nThe paper is organized as follows. After reviewing related works in Section 2, we present the\nmethodology in Section 3, we then outline the experiments in Section 4 and draw our conclusions in\nSection 5.",
  "related_work": "The goal of Model Compression is to shrink and optimize neural architectures, while retaining most\nof their initial performance. Research on LM compression has been carried out following a variety of\napproaches like quantization, pruning knowledge distillation, and combinations thereof.\n\nA most popular distillation approach in NLP was proposed by Sanh et al. (2019). The obtained\nmodel, called DistilBERT, is a smaller version of BERT, with the same architecture but half the layers,\ntrained to imitate the full output distribution of the teacher (a pre-trained BERT model). DistilBERT\nhas a 40\n\nLittle focus has been devoted thus far to the role of tokenization in the context of model compression.\nEven in domain adaptation, the vocabulary was kept the same. Both the versatility of the subword-\nlevel tokenization, and the constraints imposed by the teacher- student framework (same output\ndistribution), discouraged such investigations. Recently, Samenko et al. (2021) presented an approach\nfor transferring the vocabulary of an LM into a new vocabulary learned from new domain, with the\npurpose of boosting the performance of the fine-tuned model. To the best of our knowledge, we are\nthe first to study VT in the scope of model compression.\n\n3 Vocabulary Transfer\n\nLet us consider a LM, trained on a general-purpose domain Dgen and associated with a vocabulary\nVgen. Such a vocabulary is used by the LM\u2019s tokenizer in order to produce an encoding of the input\nstring via an embedding matrix Egen defined on Vgen. More specifically, a tokenizer is a function\nthat maps a textual string into a sequence of symbols of a given vocabulary V . Let T be a tokenizer\nassociated with a vocabulary V and a string s, we have T : s \u2192 (t1, . . . , tn), ti \u2208 V, \u2200i = 1, . . . , n.\nHence, the vocabulary of the tokenizer determines how words in a text are split, whether as words,\nsub-words, or even characters. These symbols, which define the LM\u2019s vocabulary, are statistically\ndetermined by training the tokenizer to learn the distribution of a dataset.\n\nNow, let us consider a vertical domain Din, also referred as in-domain. For the reasons discussed\nearlier, a vocabulary Vin specialized on Din itself better fits the language distribution than Vgen.\nUnfortunately, with a new vocabulary, embedding representations associated with the tokens of Vgen\nwould be lost. Thus, VT aims to initialize Vin by re-using most of the information learned from the\nLM pre-trained on Dgen. Once the new tokenizer Tin has been trained on the in-domain dataset Din\nusing a given vocabulary size, Tin will be different from the LM\u2019s tokenizer Tgen. However, the two\ntokenizers\u2019 vocabularies Vgen and Vin may still have a large portion of their symbols in common.\nOur objective is to transfer most of the information from Vgen into Vin. To this end, we first define a\nmapping between each symbol in Vin and a set of symbols in Vgen. Then, we define an assignment\ncriterion, based on the mapping, to obtain the embeddings for the tokens of Tin.\n\nOne such criterion, called Vocabulary Initialization with Partial Inheritance (VIPI), was defined by\nSamenko et al. (2021). Whenever a token is in Vin but not in Vgen, VIPI calculates all the partitions\nof the new token with tokens from Vgen, then takes the minimal partitions and finally averages them\nto obtain an embedding for the new token. Differently, we define a simplified implementation of\n\n2\n\n\fVIPI called FVT for Fast Vocabulary Transfer. Instead of calculating all tokenizations, FVT uses a\nstraightforward assignment mechanism, whereby each token ti \u2208 Vin is partitioned using Tgen. If ti\nbelongs to both vocabularies, ti \u2208 Vin \u2229 Vgen, then Tgen(ti) = ti and the in-domain LM embedding.\n\nIf instead ti \u2208 Vin \\ Vgen, then the in-domain embedding is the average of the embeddings associated\nwith the tokens produced by Tgen:\n\nEin(ti) = Egen(ti).\n\n(1)\n\nEin(t :) =\n\n1\n|Tgen(ti)|\n\n(cid:88)\n\nEgen(tj)\n\ntj \u2208Tgen(ti)\n\n(2)\n\nPlease notice that Equation (2) is a generalization of Equation (1). Indeed, in case ti \u2208 Vin \u2229 Vgen,\nEquation (2) falls back to Equation (1).\n\nOnce embeddings are initialized with FVT, we adjust the model\u2019s weights by training it with MLM\non the in-domain data before fine-tuning it on the downstream task. MLM eases adaptation and has\nalready been found to be beneficial in (Samenko et al., 2021). We observed this trend as well during\npreliminary experiments, therefore we kept such a tuning stage in all our experiments.\n\nAs a baseline model, we also implement a method called Partial Vocabulary Transfer (PVT), whereby\nonly the tokens belonging to both vocabularies ti \u2208 Vin \u2229 Vgen are initialized with pre-trained\nembeddings, while unseen new tokens are randomly initialized.\n\n3.1 Distillation\n\nVT can be combined with other model compression methods like quantization, pruning and KD. For\nsome of the methods, the combination is trivial, since they have no impact on the vocabulary. KD,\nhowever, requires the vocabularies of the student and teacher to be aligned. Hence, its integration\nwith VT is non-trivial. Accordingly, we set up a KD procedure with VT, in order to determine the\neffects of applying both VT and KD to an LM.\n\nOur distillation consists of two steps. In the first step, we replicate the distillation process used in\n(Sanh et al., 2019) for DistilBERT, in which the number of layers of the encoder is halved and a\ntriple loss-function is applied: a distillation loss, a MLM loss, and a cosine embedding loss. However,\nunlike the original setup, we do not remove the token-type embeddings and pooler. after distilling the\nstudent on Dgen, we further distil the student using Din. However, instead of adapting the teacher\nbefore the second distillation, we simply distil the student a second time on the in-domain dataset.\nFinally, we apply VT using either FVT or PVT and fine-tune the student model on the in-domain\ndatasets.\n\nOur choice of applying VT after KD is based on findings by Kim and Hassan (2020), that different\ninput embedding spaces will produce different output embedding spaces. This difference in spaces is\nnot conducive to knowledge transfer during distillation. Hence, if VT were to be applied first to the\nstudent, its input embedding space would differ greatly from that of the pre-trained teacher during\ndistillation.",
  "methodology": "",
  "experiments": "In the experiments we measure the impact of FVT on three main KPIs: quality (F1 score), size of the\nmodels and speedup in inference.\n\n4.1 Experimental Setup\n\nWe consider for all our experiments the pre-trained cased version of BERTbase as our pre-trained\nlanguage model. Its tokenizer is composed of 28996 wordpieces. We then define four vocabulary\nsizes for retraining our tokenizers. Specifically, we take the original vocabulary size and define it\nas a vocabulary size of 100 percent. We subsequently reduce this size to 75percent, 50percent, and\n25percent, From now on, we will refer to such tokenizers as T100, T75, T50, T25 respectively, while\nthe original vocabulary will be called Tgen.\n\n3\n\n\fModels are fine-tuned for 10 epochs with early stopping on the downstream task. We set the initial\nlearning rate to 3 \u00d7 10\u22125 and batch size to 64 for each task. The sequence length is set to 64 for ADE\nand CoNLL03 and 128 for LEDGAR. Each configuration is repeated 3 times with different random\ninitializations. MLM is performed for one epoch.\n\n4.2 Datasets\n\nTo best assess the effectiveness of VT, we apply it on three different tasks from three heterogeneous\nlinguistic domains: medical (ADE), legal (LEDGAR) and news (CoNLL03). Table 4 reports the\ndataset statistics.\n\nADE. The Adverse Drug Events (ADE) corpus is a binary sentence\n\nclassification dataset in the medical domain. This domain is particularly suitable for investigating the\nbenefits of VT, since documents are characterized by the presence of frequent technical terms, such\nas drug and disease names, that are usually rare in common language. Domain-specific words are\nusually split into multiple tokens, yielding longer sequences and breaking the semantics of a word\ninto multiple pieces. An example is shown in Figure 2.\n\nLEDGAR. LEDGAR is a document classification corpus of legal provisions in contracts from\nthe US Securities and Exchange Commission (SEC). The dataset is annotated with 100 different\nmutually-exclusive labels. It is also part of LexGLUE, a benchmark for legal language understanding.\n\nCoNLL03. CoNLL03 is a popular Named Entity Recognition (NER) benchmark. It is made of news\nstories from the Reuters corpus. We chose this corpus because, differently from ADE and LEDGAR,\nthe news domain typically uses a more standard language, hence we expect its distribution to differ\nless from the one captured by a general-purpose tokenizers in the web. Statistics in Table 1 confirms\nthis hypothesis. We can observe that the sequence compression gain obtained with domain- specific\ntokenizers is less significant with respect to LEDGAR and ADE.\n\nTable 1: Number of examples of each dataset.\n\nDataset\n\nTrain Validation\n\nTest\n\nADE\n16716\nLEDGAR 60000\n14042\nCoNLL03\n\n3344\n10000\n3251\n\n836\n10000\n3454\n\n4.3 Results\n\nWe report an extensive evaluation of FVT on different setups and perspectives.\n\nIn-domain Tokenization. By retraining the tokenizer on the in-domain dataset, the average number of\ntokens per sequence decreases since the learned distribution reduces the number of word splits, as\nshown in Table 1. In the medical domain, which is particularly specialized, we notice a remarkable\n32\n\nTable 2: Average sequence length on the three datasets with different tokenizers. Tgen is the generic\ntokenizer (BERT cased), the same in each corpus, while T percent are the tokenizers trained in the\nvertical domain itself.\n\nDataset\n\nTgen T100 T75 T50 T25\n\nADE\nLEDGAR\nCoNLL03\n\n31\n155\n19\n\n21\n131\n17\n\n22\n131\n17\n\n23\n132\n18\n\n26\n135\n20\n\nVocabulary Transfer. From the results shown in Tables 2 and 3, we note a few interesting findings.\nFirst, FVT vectors initialization method consistently outperforms the baseline PVT, which confirms\nthe positive contribution of Equation 2. Second, transferring vocabulary with FVT causes limited\ndrops in performance, especially in LEDGAR (the largest one), where F1 slightly increases despite a\n75\n\n4\n\n\fTable 3: F1 results on the three benchmarks. A pre- trained language model fine-tuned on the task\n(Tgen) is compared with models having differently sized in-domain tokenizers (T100, T75, T50, T25)\nadapted by transferring information with FVT or PVT.\n\nTransfer\n\nADE\n\nLEDGAR CoNLL03\n\n90.80\nTgen\nT100 + FVT 90.77\n90.40\nT75 + FVT\n90.07\nT50 + FVT\n90.27\nT25 + FVT\nT100 + PVT 82.57\n82.47\nT75 + PVT\n83.07\nT50 + PVT\n83.57\nT25 + PVT\n\n80.93\n80.60\n80.93\n80.93\n81.03\n80.07\n80.33\n80.23\n80.20\n\n89.43\n87.87\n87.90\n86.87\n86.17\n84.53\n84.63\n84.43\n83.47\n\nTable 4: F1 results on the three benchmarks. A distilled language model fine-tuned on the task\n(Tgen) is compared with models having differently sized in-domain tokenizers (T100, T75, T50, T25)\nadapted by transferring information with FVT or PVT.\n\nADE\n\nLEDGAR CoNLL03\n\nTgen\n90.47\nT100 + FVT 89.47\n88.57\nT75 + FVT\n88.43\nT50 + FVT\nT25 + FVT\n88.23\nT100 + PVT 79.13\n78.87\nT75 + PVT\n76.30\nT50 + PVT\n77.90\nT25 + PVT\n\n78.37\n78.33\n78.90\n79.30\n78.10\n76.97\n76.93\n77.37\n77.33\n\n86.90\n84.63\n84.23\n83.80\n83.13\n81.13\n81.40\n81.63\n79.50\n\nVocabulary Transfer and Distillation. The results summarized in Table 3 clearly indicate that KD\nis complementary to VT: there is no harm in applying them together, in terms of performance on\nthe downstream task. Crucially, this guarantees a full exploitation of FVT in the scope of language\nmodel compression.\n\nCompression and Efficiency. After showcasing that VT has limited impact on performance, we\nanalyze and discuss its effects on efficiency and model compression. Table 5 reports the relative\nF1 drop on the downstream task with respect to the original LM (\u02d82206F1), the relative reduction in\nmodel size (\u02d82206Size) and the speedup gained by FVT alone and by FVT combined with KD for\nvarying vocabulary sizes. Either way, FVT achieves a remarkable 15\n\nFurthermore, the reduced input length enabled by in-domain tokenization brings a reduction in\ninference time. The more a language is specialized, the higher is the speedup with in-domain\ntokenizers. This is also confirmed by the experiments, where the major benefits are obtained on the\nmedical domain, with a x1.40 speedup. In CoNLL03 instead where language is much less specialized,\nspeedup reduces and even disappears with T25. Distillation further pushes compression and speedup\nin any benchmark and setup, up to about 55\n\nIn summary, depending on the application needs, VT enables a strategic trade-off between compres-\nsion rate, inference speed and accuracy.",
  "results": "",
  "conclusion": "The viability and success of industrial NLP applications often hinges on a delicate trade-off between\ncomputational requirements, responsiveness and output quality. Hence, language model compression\nmethods are an active area of research whose practical ramifications are self-evident. One of the\nfactors that greatly contribute to a model\u2019s inference speed and memory footprint is vocabulary size.\nVT has been recently proposed for improving performance, but never so far in the scope of model\n\n5\n\n\fTable 5: The first row (Tgen) reports absolute values of the LM fine-tuned on the downstream task\nwithout VT or KD. The rows below show values relative to Tgen.\n\n2*Transfer\n\nADE\n\u02d82206Size\n\n\u02d82206F1\n\nSpeedup\n\n\u02d82206F1\n\nLEDGAR\n\u02d82206Size\n\nSpeedup\n\n\u02d82206F1\n\nCoNLL03\n\u02d82206Size\n\nSpeedup\n\nTgen\nT100 + FVT\nT75 + FVT\nT50 + FVT\nT25 + FVT\nDistil + T100 + FVT\nDistil + T75 + FVT\nDistil + T50 + FVT\nDistil + T25 + FVT\n\n90.80\n-0.04\n-0.44\n-0.81\n-0.59\n-1.47\n-2.46\n-2.61\n-2.83\n\n433.32\n0.00\n-5.14\n-10.28\n-15.42\n-39.26\n-44.40\n-49.54\n-54.68\n\n1.00\n1.40\n1.35\n1.32\n1.20\n2.76\n2.64\n2.59\n2.37\n\n80.93\n-0.41\n0.00\n0.00\n0.12\n-3.21\n-2.51\n-2.02\n-3.50\n\n433.62\n0.00\n-5.14\n-10.27\n-15.41\n-39.24\n-44.37\n-49.51\n-54.64\n\n1.00\n1.21\n1.21\n1.10\n1.09\n2.38\n2.38\n2.16\n2.14\n\n89.43\n-1.75\n-1.71\n-2.87\n-3.65\n-5.37\n-5.81\n-6.30\n-7.04\n\n430.98\n0.00\n-5.17\n-10.33\n-15.50\n-39.48\n-44.64\n-49.81\n-54.98\n\n1.00\n1.07\n1.07\n1.02\n0.99\n2.11\n2.11\n2.01\n1.96\n\ncompression. In this work, we run an extensive experimental study on the application of a lightweight\nmethod for VT, called FVT. An analysis conducted on various downstream tasks, application domains,\nvocabulary sizes and on its possible combination with knowledge distillation indicates that FVT\nenables a strategic trade-off between compression rate, inference speed and accuracy, especially, but\nnot only, in more specialized domains. Importantly, FVT appears to be orthogonal to other model\ncompression methods.\n\nIn the future, we plan to fully integrate Vocabulary Transfer within Knowledge Distillation during the\nlearning process in order to maximize the information transfer.\n\n6"
}
{
  "title": "Advancements in Audio-Visual Active Speaker\nDetection: A Novel Approach for the ActivityNet\nChallenge",
  "abstract": "This document outlines our contribution to the ActivityNet Challenge, focusing on\nactive speaker detection. We employ a 3D convolutional neural network (CNN)\nfor feature extraction, combined with an ensemble of temporal convolution and\nLSTM classifiers to determine whether a person who is visible is also speaking.\nThe results demonstrate substantial improvements compared to the established\nbaseline on the AVA-ActiveSpeaker dataset.",
  "introduction": "The field of multimodal speech perception has garnered significant attention in recent times, with\nmajor advancements in audio-visual methodologies facilitated by deep learning. The capacity to\nidentify which individuals are speaking at any moment is crucial for a variety of applications. The\nintroduction of the AVA-ActiveSpeaker dataset has been a significant development, allowing for the\ntraining of deep-learning-based active speaker detection (ASD) models with complete supervision.\nThis document provides a concise analysis of this dataset and elaborates on the methodology behind\nour submission to the challenge.\n\n1.1 Datasets\n\nThe model is developed using the AVA-ActiveSpeaker dataset, which is divided into training, valida-\ntion, and test sets, as detailed in Table 1. The ground truth labels are available for the training and\nvalidation sets.\n\nTable 1: Statistical Overview of the AVA-ActiveSpeaker Dataset\n\nSet\n\nTrain\nVal\nTest\n\nVideos\n\nFrames\n\n120\n33\n109\n\n2,676K\n768K\n2,054K\n\nThis dataset presents several challenges. The durations of speaking segments are notably brief, with\nan average of 1.11 seconds for segments that are both spoken and audible. Consequently, the system\nneeds to deliver precise detection with a limited number of frames. Traditional methods, which\ndepend on smoothing the output over a time window of several seconds, are not effective under these\nconditions.\n\nAdditionally, the dataset includes many older videos where the audio and video recordings appear to\nhave been captured separately or are significantly out of sync. As a result, the temporal alignment\nbetween audio and visual speech representations is not a reliable indicator of a person\u2019s speaking\nstatus.\n\n.",
  "related_work": "",
  "methodology": "The active speaker detection system is composed of two primary components: front-end feature\nextractors and a back-end classifier, each discussed in detail in the subsequent sections.\n\n2.1 Front-end architecture\n\nFor the extraction of audio and video representations, pre-trained networks are employed. These\nencoder networks have undergone training for the audio-visual correspondence task through a self-\nsupervised approach on unlabeled videos.\n\nThe video encoder utilizes a convolutional neural network (CNN), processing 5 RGB image frames\nto produce a 512-dimensional representation. The architecture draws inspiration from the VGG-M\nnetwork, known for its compactness and efficiency, but incorporates a 3D convolution in the initial\nlayer instead of the conventional 2D convolution.\n\nThe audio encoder receives an input comprising 20 frames in the temporal dimension and 13 cepstral\ncoefficients in the other, generating a 512-dimensional representation that aligns with the video\nrepresentation\u2019s embedding space.\n\n2.2 Back-end architecture\n\nBoth the audio and video encoders process an input of 5 video frames (equivalent to 0.2 seconds),\nadvancing 1 video frame (0.04 seconds) at a time. Consequently, for an input of T frames, the output\ndimensions are 512 x (T - 4). In this study, two straightforward back-end classifiers are evaluated.\nAlthough our experiments utilize T = 9, no significant performance variations were noted for T values\nwithin the range of 7 to 15.\n\nLSTM classifier. The audio and video representations are channeled into two distinct bi-directional\nLSTM networks, each comprising 2 layers with a hidden size of 128. The outputs from these networks\nare merged and subsequently processed through a linear classification layer. This layer determines\nwhether the individual is speaking, and it is trained using the softmax cross-entropy loss.\n\nTC classifier. In place of LSTM layers, the encoder outputs are directed to two temporal convolution\nlayers, each equipped with 128 filters. The outputs are similarly concatenated and forwarded to the\nclassifier, mirroring the approach used with the LSTM classifier.\n\nEnsemble. Ensemble methods in machine learning have been demonstrated to frequently surpass\nthe performance of any individual classifier. In this approach, the predictions generated by both the\nLSTM and TC classifiers are averaged with equal weighting to produce the final prediction.\n\nSmoothing. To mitigate noise within the predictions, the outputs of the classifiers undergo temporal\nsmoothing using either a median or Wiener filter, both applied over 0.5-second intervals.",
  "experiments": "Our model, implemented using the PyTorch library, was trained on a single Tesla M40 card with\n24GB of memory. Training utilized the ADAM optimizer with default settings and a fixed learning\nrate of 10-2. To counteract any bias in the training data, the number of samples for positive and\nnegative classes was balanced within each mini-batch during the training process.\n\nThe evaluation metric for this task is the mean Average Precision (mAP), with the evaluation code\nsupplied by the challenge organizers.\n\nResults on the validation set for the various back-end classifiers are presented in Table 2. The best\nmodel achieved an mAP of 0.878 on the sequestered test set for the challenge. In contrast, the\nGRU-based baseline model yielded an mAP of 0.821.\n\nThe qualitative outcomes of the proposed method significantly surpass those of existing\ncorrespondence-based methods on this dataset because it does not depend on accurate audio-to-\nvideo synchronization.\n\n2\n\n\fTable 2: Performance Evaluation on the AVA-ActiveSpeaker Validation Set\n\nBack-end\n\nSmoothing mAP\n\nLSTM\nTC\nEnsemble\nEnsemble\nEnsemble\n\nX\nX\nX\nMedian\nWiener\n\n0.851\n0.855\n0.861\n0.874\n0.878\n\n3",
  "results": "",
  "conclusion": ""
}
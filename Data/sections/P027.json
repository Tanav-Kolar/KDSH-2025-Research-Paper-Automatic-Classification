{
  "title": "emoji2vec: Learning Emoji Representations from their\nDescription",
  "abstract": "Many current natural language processing applications for social media rely on\nrepresentation learning and utilize pre-trained word embeddings. There currently\nexist several publicly-available, pre-trained sets of word embeddings, but they\ncontain few or no emoji representations even as emoji usage in social media has\nincreased. emoji2vec are pre-trained embeddings for all Unicode emojis which are\nlearned from their description in the Unicode emoji standard. The resulting emoji\nembeddings can be readily used in downstream social natural language processing\napplications alongside word2vec. For the downstream task of sentiment analysis,\nemoji embeddings learned from short descriptions outperforms a skip-gram model\ntrained on a large collection of tweets, while avoiding the need for contexts in\nwhich emojis need to appear frequently in order to estimate a representation.",
  "introduction": "First introduced in 1997, emojis, a standardized set of small pictorial glyphs depicting everything\nfrom smiling faces to international flags, have seen a drastic increase in usage in social media over\nthe last decade. The Oxford Dictionary named 2015 the year of the emoji, citing an increase in usage\nof over 800% during the course of the year, and elected the \u2019Face with Tears of Joy\u2019 emoji () as the\nWord of the Year. As of this writing, over 10% of Twitter posts and over 50% of text on Instagram\ncontain one or more emojis. Due to their popularity and broad usage, they have been the subject\nof much formal and informal research in language and social communication, as well as in natural\nlanguage processing (NLP).\n\nIn the context of social sciences, research has focused on emoji usage as a means of expressing\nemotions on mobile platforms. Interestingly, although essentially thought of as means of expressing\nemotions, emojis have been adopted as tools to express relationally useful roles in conversation.\nEmojis are culturally and contextually bound, and are open to reinterpretation and misinterpretation.\nThese findings have paved the way for many formal analyses of semantic characteristics of emojis.\n\nConcurrently we observe an increased interest in natural language processing on social media\ndata. Many current NLP systems applied to social media rely on representation learning and word\nembeddings. Such systems often rely on pre-trained word embeddings that can for instance be\nobtained from word2vec or GloVe. Yet, neither resource contain a complete set of Unicode emoji\nrepresentations, which suggests that many social NLP applications could be improved by the addition\nof robust emoji representations.\n\nEmbeddings for emoji Unicode symbols are learned from their description in the Unicode emoji\nstandard. The usefulness of emoji representations trained in this way is demonstrated by evaluating on\na Twitter sentiment analysis task. Furthermore, a qualitative analysis by investigating emoji analogy\nexamples and visualizing the emoji embedding space is provided.",
  "related_work": "There has been little work in distributional embeddings of emojis. The first research done in\nthis direction was an informal blog post by the Instagram Data Team in 2015. They generated\nvector embeddings for emojis similar to skip-gram-based vectors by training on the entire corpus\nof Instagram posts. Their research gave valuable insight into the usage of emojis on Instagram,\nand showed that distributed representations can help understanding emoji semantics in everyday\nusage. The second contribution, closest to ours, trained emoji embeddings from a large Twitter\ndataset of over 100 million English tweets using the skip-gram method. These pre-trained emoji\nrepresentations led to increased accuracy on a similarity task, and a meaningful clustering of the\nemoji embedding space. While this method is able to learn robust representations for frequently-used\nemojis, representations of less frequent emojis are estimated rather poorly or not available at all. In\nfact, only around 700 emojis can be found in this corpus, while there is support of over 1600 emojis\nin the Unicode standard.\n\nThe approach differs in two important aspects. First, since the representation of emojis are estimated\ndirectly from their description, robust representations are obtained for all supported emoji symbols \u2014\neven the long tail of infrequently used ones. Secondly, the method works with much less data. Instead\nof training on millions of tweets, the representations are trained on only a few thousand descriptions.\nStill, higher accuracy results are obtained on a Twitter sentiment analysis task.\n\nIn addition, the work relates to building word representations for words and concepts based on their\ndescription in a dictionary. Similarly to their approach, representations are build for emojis based on\ntheir descriptions and keyword phrases.\n\nSome of the limitations are evident in the work who showed that different cultural phenomena and\nlanguages may co-opt conventional emoji sentiment. Since training is only on English-language\ndefinitions and ignore temporal definitions of emojis, the training method might not capture the full\nsemantic characteristics of an emoji.",
  "methodology": "The method maps emoji symbols into the same space as the 300-dimensional Google News word2vec\nembeddings. Thus, the resulting emoji2vec embeddings can be used in addition to 300-dimensional\nword2vec embeddings in any application. To this end emojis, their name and their keyword phrases\nare crawled from the Unicode emoji list, resulting in 6088 descriptions of 1661 emoji symbols.\n\n3.1 Model\n\nEmoji embeddings are trained using a simple method. For every training example consisting of an\nemoji and a sequence of words w1, ..., wN describing that emoji, we take the sum of the individual\nword vectors in the descriptive phrase as found in the Google News word2vec embeddings\n\nv =\n\nN\n(cid:88)\n\nk=1\n\nwk,\n\n(1)\n\nwhere wk is the word2vec vector for word wk if that vector exists (otherwise we drop the summand)\nand vj is the vector representation of the description. A trainable vector xi for every emoji in\nour training set is defined, and the probability of a match between the emoji representation xi\nand its description representation vj is modeled using the sigmoid of the dot product of the two\nrepresentations \u03c3(xT\n\ni vj). For training we use the logistic loss\n\nL(i, j, yij) = \u2212 log(\u03c3(yijxT\n\ni vj \u2212 (1 \u2212 yij)xT\n\ni vj))\n\n(2)\n\nwhere yij is 1 if description j is valid for emoji i and 0 otherwise.\n\n3.2 Optimization\n\nThe model is implemented in TensorFlow and optimized using stochastic gradient descent with Adam\nas optimizer. As we do not observe any negative training examples (invalid descriptions of emojis do\n\n2\n\n\fnot appear in the original training set), to increase generalization performance we randomly sample\ndescriptions for emojis as negative instances (i.e. induce a mismatched description). One of the\nparameters of our model is the ratio of negative samples to positive samples; we found that having\none positive example per negative example produced the best results. We perform early-stopping on\na held-out development set and found 80 epochs of training to give the best results. As we are only\ntraining on emoji descriptions and our method is simple and cheap, training takes less than 3 minutes\non a 2013 MacBook Pro.",
  "experiments": "The approach is quantitatively evaluated on an intrinsic (emoji-description classification) and extrinsic\n(Twitter sentiment analysis) task. Furthermore, a qualitative analysis is given by visualizing the\nlearned emoji embedding space and investigating emoji analogy examples.\n\n4.1 Emoji-Description Classification\n\nTo analyze how well the method models the distribution of correct emoji descriptions, a manually-\nlabeled test set containing pairs of emojis and phrases, as well as a correspondence label was created.\nFor instance, the test set includes the example: , \"crying\", True, as well as the example , \"fish\", False.\n\u03c3(xT\ni vi) is calculated for each example in the test set, measuring the similarity between the emoji\nvector and the sum of word vectors in the phrase.\n\nWhen a classifier thresholds the above prediction at 0.5 to determine a positive or negative correlation,\nan accuracy of 85.5% is obtained for classifying whether an emoji-description pair is valid or not.\nBy varying the threshold used for this classifier, a receiver operating characteristic curve with an\narea-under-the-curve of 0.933 is obtained, which demonstrates that high quality of the learned emoji\nrepresentations.\n\n4.2 Sentiment Analysis on Tweets\n\nAs downstream task the accuracy of sentiment classification of tweets for various classifiers with three\ndifferent sets of pre-trained word embeddings are compared: (1) the original Google News word2vec\nembeddings, (2) word2vec augmented with emoji embeddings trained by skip-gram model, and (3)\nword2vec augmented with emoji2vec trained from Unicode descriptions. A dataset is used which\nconsists of over 67k English tweets labelled manually for positive, neutral, or negative sentiment.\nIn both the training set and the test set, 46% of tweets are labeled neutral, 29% are labeled positive,\nand 25% are labeled negative. To compute the feature vectors for training, we summed the vectors\ncorresponding to each word or emoji in the text of the Tweet. The goal of this simple sentiment\nanalysis model is not to produce state-of-the-art results in sentiment analysis; it is simply to show that\nincluding emojis adds discriminating information to a model, which could potentially be exploited in\nmore advanced social NLP systems.\n\nBecause the labels are rather evenly distributed, accuracy is an effective metric in determining\nperformance on this classification task. Results are reported in Table 1. Augmenting word2vec\nwith emoji embeddings improves overall classification accuracy on the full corpus, and substantially\nimproves classification performance for tweets that contain emojis. It suggests that emoji embeddings\ncould improve performance for other social NLP tasks as well. Furthermore, emoji2vec generally\noutperforms the emoji embeddings trained by the skip-gram model, despite being trained on much\nless data using a simple model.\n\n4.3 Analogy Task\n\nA well-known property of word2vec is that embeddings trained with this method to some extent\ncapture meaningful linear relationships between words directly in the vector space. For instance, it\nholds that the vector representation of \u2019king\u2019 minus \u2019man\u2019 plus \u2019woman\u2019 is closest to \u2019queen\u2019. Word\nembeddings have commonly been evaluated on such word analogy tasks. Unfortunately, it is difficult\nto build such an analogy task for emojis due to the small number and semantically distinct categories\nof emojis. Nevertheless, a few intuitive examples were collected. For every query the closest five\n\n3\n\n\fTable 1: Three-way classification accuracy on the Twitter sentiment analysis corpus using Random\nForrests and Linear SVM classifier with different word embeddings.\n\nClassification accuracy on entire dataset, N = 12920\n\nWord Embeddings\n\nRandom Forest\n\nLinear SVM\n\nGoogle News\nGoogle News + (skip-gram model)\nGoogle News + emoji2vec\n\n57.5\n58.2*\n59.5*\n\n58.5\n60.0*\n60.5*\n\nClassification accuracy on tweets containing emoji, N = 2295\n\nWord Embeddings\n\nRandom Forest\n\nLinear SVM\n\nGoogle News\nGoogle News + (skip-gram model)\nGoogle News + emoji2vec\n\n46.0\n52.4*\n54.4*\n\n47.1\n57.4*\n59.2*\n\nClassification accuracy on 90% most frequent emoji, N = 2186\n\nWord Embeddings\n\nRandom Forest\n\nLinear SVM\n\nGoogle News\nGoogle News + (skip-gram model)\nGoogle News + emoji2vec\n\n47.3\n52.8*\n55.0*\n\n45.1\n56.9*\n59.5*\n\nClassification accuracy on 10% least frequent emoji, N = 308\n\nWord Embeddings\n\nRandom Forest\n\nLinear SVM\n\nGoogle News\nGoogle News + (skip-gram model)\nGoogle News + emoji2vec\n\n44.7\n53.9*\n54.5*\n\n43.2\n52.9*\n55.2*\n\nemojis were retrieved. Though the correct answer is sometimes not the top one, it is often contained\nin the top three.",
  "results": "",
  "conclusion": "Since existing pre-trained word embeddings such as Google News word2vec embeddings or GloVe\nfail to provide emoji embeddings, emoji2vec \u2014 embeddings of 1661 emoji symbols were released.\nInstead of running word2vec\u2019s skip-gram model on a large collection of emojis and their contexts\nappearing in tweets, emoji2vec is directly trained on Unicode descriptions of emojis. The resulting\nemoji embeddings can be used to augment any downstream task that currently uses word2vec\nembeddings, and might prove especially useful in social NLP tasks where emojis are used frequently\n(e.g. Twitter, Instagram, etc.). Despite the fact that the model is simpler and trained on much less\ndata, it outperforms the skip-gram model on the task of Twitter sentiment analysis.\n\nAs the approach directly works on Unicode descriptions, it is not restricted to emoji symbols. In\nthe future the usefulness of the method for other Unicode symbol embeddings will be investigated.\nFurthermore, plans are made to improve emoji2vec in the future by also reading full text emoji\ndescriptions and using a recurrent neural network instead of a bag-of-word-vectors approach for\nenocoding descriptions. In addition, since the approach does not capture the context-dependent\ndefinitions of emojis (such as sarcasm, or appropriation via other cultural phenomena), mechanisms\nwill be explored to efficiently capturing these nuanced meanings.\n\n4\n\n\f6 Data Release and Reproducibility\n\nPre-trained emoji2vec embeddings as well as the training data and code are released at https:\n//github.com/uclmr/emoji2vec. Note that the emoji2vec format is compatible with word2vec and can\nbe loaded into gensim or similar libraries.\n\n5"
}
{
  "title": "Analyzing Groups of Neurons in Neural Networks: Comparing\nInformation from Input and Output Perspectives",
  "abstract": "The concept of a \"modular\" structure in artificial neural networks has been suggested as beneficial for learning,\nthe ability to combine elements, and applying knowledge to new situations. However, a clear definition and\nmeasurement of modularity are still open questions. This paper reframes the identification of functional modules as\nthe identification of groups of units with similar functions. This raises the question of what constitutes functional\nsimilarity between two units. To address this, we examine two main categories of methods: those that define\nsimilarity based on how units react to variations in inputs (upstream), and those that define similarity based on\nhow changes in hidden unit activations affect outputs (downstream). We perform an empirical analysis to measure\nthe modularity of hidden layer representations in simple feedforward, fully connected networks across various\nsettings. For each model, we assess the relationships between pairs of hidden units in each layer using a range\nof upstream and downstream metrics, then group them by maximizing their \"modularity score\" with established\nnetwork science tools. We find two unexpected results: first, dropout significantly increased modularity, while\nother forms of weight regularization had smaller effects. Second, while we observe general agreement on clusters\nwithin upstream methods and within downstream methods, there is limited agreement on cluster assignments\nbetween these two categories. This has significant implications for representation learning, as it implies that\nfinding modular representations that reflect input structure (e.g., disentanglement) may be a different objective\nfrom learning modular representations that reflect output structure (e.g., compositionality).",
  "introduction": "Modularity, a principle where complex systems are broken down into simpler subsystems, allows for independent analysis, debugging,\nand recombination for new tasks. This design approach offers benefits like enhanced robustness and quicker adaptation to new\nchallenges. It is recognized that learning systems gain advantages from structures tailored to the specific problem, and many\nreal-world problems can indeed be divided into sub-problems. Consequently, modularity is viewed as a standard design principle in\nevolved biological systems, including biological neural networks, and one that can be advantageous for artificial neural networks\n(ANNs).\n\nDespite the intuitive appeal, formally defining and quantifying the modularity of a given system remains an unresolved issue. It is\ngenerally agreed that modular systems, by definition, break down into subsystems that carry out functions to solve sub-problems.\nDefining modules in ANNs, therefore, requires us to determine when two parts of a network are involved in the same \"function\". In\nthis paper, we address this question at the level of pairs of hidden units. We explore various methods for assessing the \"functional\nsimilarity\" of any two hidden units, and we define a \"module\" as a group of units with similar functions. This definition is not\nintended to be the definitive answer to what constitutes a module, but rather to offer a practical foundation for experimenting with\ndifferent concepts related to modularity, such as how regularization affects it.\n\nA key objective of this paper is to highlight the differences between \"upstream\" and \"downstream\" perspectives when considering\nneural representations and their functions. In Section 3, we provide precise definitions and detail our method for identifying and\nquantifying functional modules in the hidden layers of trained neural networks by grouping units into functionally similar sets. This\nframework enables us to directly compare various indicators of a network\u2019s modularity. Section 4 describes the experimental results.\nBesides quantitatively evaluating modularity, we further examine whether different similarity measures agree on the assignment of\nunits to modules. Surprisingly, we find that modules identified using \"upstream\" measures of functional similarity are consistently\ndifferent from those found using \"downstream\" measures. Although we do not examine regularization methods specifically designed\nto create modular designs, these initial findings call for a more in-depth examination of how the \"function\" of a representation is\ndefined, as well as why and when modules might be beneficial.",
  "related_work": "The investigation of modularity in neural networks has a rich history. A frequent source of inspiration from biology is the separation\nof \"what\" and \"where\" pathways in the ventral and dorsal streams of the brain, respectively. Each pathway can be viewed as a\nspecialized module (and can be further divided into submodules). Numerous prior experiments on modularity in artificial neural\nnetworks have investigated principles that would lead to similarly distinct what/where information processing in ANNs. A significant\ndistinction from this line of work is that, instead of predefining the functional role of modules, such as one module handling \"what\"\nand another handling \"where,\" our research aims to discover distinct functional groups in trained networks.\n\nGenerally, there are two categories of approaches to modularity in neural networks, each corresponding to a different way of\nunderstanding the function of network components. The structural modularity approach defines function based on network weights\nand the connections between sub-networks. Modules are thus defined as sub-networks with dense internal connections and sparse\nexternal connections. The functional modularity approach focuses on network activations or the information represented by those\nactivations, rather than weights. This includes concepts like disentanglement, compositionality, and invariance. The connection\nbetween structural and functional modules is not entirely clear. While they seem to be (or should be) correlated, it has been observed\nthat even very sparse inter-module connectivity does not always ensure functional separation of information processing. In this study,\nwe adopt the functional approach, assuming that structural modularity is only useful to the extent that it supports distinct functions\nof the units, and that often distinct functions must share information, making strict structural boundaries potentially detrimental. For\ninstance, in a complex visual scene, knowing \"what\" an object is can aid in determining \"where\" it is, and vice versa.\n\nOur work is most closely related to a series of papers by Watanabe and colleagues in which trained networks are decomposed into\nclusters of \"similar\" units with the aim of understanding and simplifying those networks. They quantify the similarity of units using\na combination of both incoming and outgoing weights. This is similar in spirit to our goal of identifying modules by clustering units,\nbut an interesting contrast to our approach, where we find stark differences between \"upstream\" and \"downstream\" similarity.\n\n3 Quantifying modularity by clustering similarity\n\nWe divide the task of identifying functional modules into two phases: evaluating the pairwise similarity of units, and then clustering\nbased on this similarity. For simplicity, we apply these steps separately to each hidden layer, although in principle, modules could be\nassessed in the same way after combining layers. Section 3.1 defines the set of pairwise functional similarity methods we use, and\nSection 3.2 describes the clustering phase.\n\nWhile we concentrate on similarity between pairs of individual units, our method is connected to, and inspired by, the question of\nwhat makes neural representations \"similar\" when comparing entire populations of neurons to each other. Instead of finding clusters\nof similar neurons as we do here, one could define modules in terms of dissimilarity between clusters of neurons. In preliminary\nwork, we explored such a definition of functional modules, using representational (dis)similarity between sub-populations of neurons.\nThe primary challenge with this approach is that existing representational similarity methods are highly sensitive to dimensionality\n(the number of neurons in each cluster), and it is not clear how best to account for this when calculating dissimilarity between clusters\nso that the method is not biased towards larger or smaller cluster sizes. To further justify our method, note that representational\nsimilarity analysis is closely related to tests for statistical (in)dependence between populations of neurons, and so the problem of\nfinding mutually \"dissimilar\" modules is analogous to the problem of finding independent subspaces. In Independent Subspace\nAnalysis (ISA), there is a similar issue of determining what constitutes a surprising amount of dependence between subspaces of\ndifferent dimensions, and various methods have been proposed with different inductive biases. However, Palmer Makeig showed\nthat a solution to the problem of detecting independent subspaces is to simply cluster the individual dimensions of the space. This\nprovides some justification for the methods we use here: some technicalities notwithstanding, the problem of finding subspaces of\nneural activity with \"dissimilar\" representations is, in many cases, reducible to the problem of clustering individual units based on\npairwise similarity, as we do here.\n\n3.1 Quantifying pairwise similarity of hidden units\n\nWhat constitutes \"functional similarity\" between two hidden units? In other words, we are looking for a similarity function S that\ntakes a neural network N, a dataset D, and a task T as inputs, and produces an n x n matrix of non-negative similarity scores for all\npairs among the n hidden units. We also require that the resulting matrix is symmetric, meaning Sij = Sji. Importantly, allowing S to\ndepend on the task T opens up the possibility of similarity measures where units are considered similar based on their downstream\ncontribution to a specific loss function.\n\nSimilarity by covariance. The first similarity measure we examine is the absolute value of the covariance of hidden unit activities\nacross inputs. Let xk be the kth input in the dataset, and hi(x) be the response of the ith hidden unit to input x, with i in 1, 2, ..., n.\nThen, we define similarity as\n\nScov\n\nij =\n\n1\nK\n\nK\n(cid:88)\n\nk=1\n\n|(hi(xk) \u2212 \u00afhi)(hj(xk) \u2212 \u00afhj)|\n\n(1)\n\n2\n\n\fwhere K is the number of items in D and \u00afhi is the mean response of unit i on the given dataset. Intuitively, the absolute value\ncovariance quantifies the statistical dependence of two units across inputs, making it an upstream measure of similarity.\nSimilarity by input sensitivity. While Scov measures similarity of responses across inputs, we next consider a measure of similar\nsensitivity to single inputs, which is then averaged over D. Let J h\nxk denote the n x d Jacobian matrix of partial derivatives of each\nhidden unit with respect to each of the d input dimensions. Then, we say two units i and j are similarly sensitive to input changes on\ninput xk if the dot product between the ith and jth row of J h\nxk has high absolute-value magnitude. In matrix notation over the entire\ndataset, we use\n\nSi\u2212sens\n\nij\n\n=\n\n1\nK\n\nK\n(cid:88)\n\nk=1\n\n|J h\nxk\n\n(J h\nxk\n\n)T |\n\n(2)\n\nwhere the superscript \"i-sens\" should be read as the \"input sensitivity.\"\n\nSimilarity by last-layer sensitivity. Let y denote the last-layer activity of the network. Using the same Jacobian notation as above, let\nJ y\nh denote the o x n matrix of partial derivatives of the last layer with respect to changes in the hidden activities h. Then, we define\nsimilarity by output sensitivity as\n\nSo\u2212sens\n\nij\n\n=\n\n1\nK\n\nK\n(cid:88)\n\nk=1\n\n|J y\n\nh (J y\n\nh )T |\n\n(3)\n\nlikewise with \"o-sens\" to be read as \"output-sensitivity.\" Note that both h and y depend on the particular input xk, but this has been\nleft implicit in the notation to reduce clutter.\n\nSimilarity by the loss Hessian. The \"function\" of a hidden unit might usefully be thought of in terms of its contribution to the task or\ntasks it was trained on. To quote Lipson, \"In order to measure modularity, one must have a quantitative definition of function... It is\nthen possible to take an arbitrary chunk of a system and measure the dependency of the system function on elements within that\nchunk. The more that the dependency itself depends on elements outside the chunk, the less the function of that chunk is localized,\nand hence the less modular it is.\"\n\nLipson then goes on to suggest that the \"dependence of system function on elements\" can be expressed as a derivative or gradient,\nand that the dependence of that dependence on other parts of the system can be expressed as the second derivative or Hessian.\nTowards this conception of modular functions on a particular task, we use the following definition of similarity:\n\nShess\n\nij =\n\n1\nK\n\nK\n(cid:88)\n\nk=1\n\n|\n\n\u22022L\n\u2202hi\u2202hj\n\n|\n\n(4)\n\nwhere L is the scalar loss function for the task, and should be understood to depend on the particular input xk. Importantly, each\nHessian on the right hand side is taken with respect to the activity of hidden units, not with respect to the network parameters as it is\ntypically defined.\nTo summarize, equations (1) through (4) provide four different methods to quantify pairwise similarity of hidden units. Scov and\nSi\u2212sens are upstream, while So\u2212sens and Shess are downstream. All four take values in [0, ). However, it is not clear if the raw\nmagnitudes matter, or only relative (normalized) magnitudes. For these reasons, we introduce an optional normalized version of\neach of the above four un-normalized similarity measures:\n\nS\u2032\n\nij =\n\nSij\nmax(Sii, Sjj, \u03f5)\n\n(5)\n\nwhere \u02d820ac is a small positive value included for numerical stability. Whereas Sij is in [0, ), the normalized values are restricted\nto S\u2032\nij in [0,1]. In total, this gives us eight methods to quantify pairwise similarity. These can be thought of as 2x2x2 product of\nmethods, as shown in the color scheme in Figure 2: the upstream vs downstream axis, the unnormalized vs normalized axis, and\nthe covariance vs gradient (i.e. sensitivity) axis. We group together both Scov and Shess under the term \"covariance\" because the\nHessian is closely related to the covariance of gradient vectors of the loss across inputs.\n\n3.2 Quantifying modularity by clustering\n\nDecomposing a set into clusters that are maximally similar within clusters and maximally dissimilar across clusters is a well-studied\nproblem in graph theory and network science. In particular, Girvan Newman proposed a method that cuts a graph into its maximally\nmodular subgraphs, and this tool has previously been used to study modular neural networks.\n\n3\n\n\fWe apply this tool from graph theory to our problem of detecting functional modules in neural networks by constructing an adjacency\nmatrix A from the similarity matrix S by simply removing the diagonal (self-similarity):\n\nAij =\n\n(cid:26)Sij\n0\n\nif i \u0338= j\notherwise\n\n(6)\n\nGiven A, we can simplify later notation by first constructing the normalized adjacency matrix, \u02dcA, whose elements all sum to one:\n\n\u02dcAij =\n\nAij\nij Aij\n\n(cid:80)\n\n(7)\n\nor, more compactly, \u02dcA = A/1T\nn A1n where 1n is a column vector of length n containing all ones. Let P be an n x c matrix that\nrepresents cluster assignments for each of n units to a maximum of c different clusters. Cluster assignments can be \"hard\" (Pij in 0,\n1) or \"soft\" (Pij in [0, 1]), but in either case the constraint P 1c = 1n must be met, i.e. that the sum of cluster assignments for each\nunit is 1. If an entire column of P is zero, that cluster is unused, so c only provides an upper-limit to the number of clusters, and in\npractice we set c = n. Girvan Newman propose the following score to quantify the level of \"modularity\" when partitioning the\nnormalized adjacency matrix \u02dcA into the cluster assignments P:\n\nQ( \u02dcA, P ) = T r(P T \u02dcAP ) \u2212 T r(P T \u02dcA1n1T\nn\n\n\u02dcAP )\n\n(8)\n\nThe first term sums the total connectivity (or, in our case, similarity) of units that share a cluster. By itself, this term is maximized\nwhen P assigns all units to a single cluster. The second term gives the expected connectivity within each cluster under a null\n\u02dcA is the product of marginal\nmodel where the elements of \u02dcA are interpreted as the joint probability of a connection, and so \u02dcA1n1T\nn\nprobabilities of each unit\u2019s connections. This second term encourages P to place units into the same cluster only if they are\nmore similar to each other than \"chance.\" Together, equation (8) is maximized by partitioning \u02dcA into clusters that are strongly\nintra-connected and weakly inter-connected.\n\nWe define the modularity of a set of neural network units as the maximum achievable Q over all P:\n\nP \u2217( \u02dcA) = argmaxP Q( \u02dcA, P )Q\u2217( \u02dcA) = Q( \u02dcA, P \u2217)\n\n(9)\n\nTo summarize, to divide a given pairwise similarity matrix S into modules, we first construct \u02dcA from S, then we find the cluster\nassignments P \u2217 that give the maximal value Q\u2217. Importantly, this optimization process provides two pieces of information: a\nmodularity score Q\u2217 which quantifies the amount of modularity in a set of neurons, for a given similarity measure. We also get\nthe actual cluster assignments P \u2217, which provide additional information and can be compared across different similarity measures.\nGiven a set of cluster assignments P \u2217, we quantify the number of clusters by first getting the fraction of units in each cluster,\nn P \u2217/n. We then use the formula for discrete entropy to measure the dispersion of cluster sizes: H(r) = \u2212 (cid:80)c\nr(P \u2217) = 1T\ni=1 rilogri.\nFinally we say that the number of clusters in P \u2217 is\n\nnumclusters(P \u2217) = eH(r(P \u2217))\n\n(10)\n\nWe emphasize that discovering the number of clusters in P \u2217 is included automatically in the optimization process; we set the\nmaximum number of clusters c equal to the number of hidden units n, but in our experiments we find that P \u2217 rarely uses more than 6\nclusters for hidden layers with 64 units (Supplemental Figure S4).\n\nIt is important to recognize that the sense of the word \"modularity\" in graph theory is in some important ways distinct from its\nmeaning in terms of engineering functionally modular systems. In graph-theoretic terms, a \"module\" is a cluster of nodes that are\nhighly intra-connected and weakly inter-connected to other parts of the network, defined formally by Q. This definition of graph\nmodularity uses a particular idea of a \"null model\" based on random connectivity between nodes in a graph. While this null-model\nof graph connectivity enjoys a good deal of historical precedence in the theory of randomly-connected graphs, where unweighted\ngraphs are commonly studied in terms of the probability of connection between random pairs of nodes, it is not obvious that the\nsame sort of null model applies to groups of \"functionally similar\" units in an ANN. This relates to the earlier discussion of ISA, and\nprovides a possibly unsatisfying answer to the question of what counts as a \"surprising\" amount of statistical independence between\nclusters; using Q makes the implicit choice that the product of average pairwise similarity, \u02dcA1n1T\n\u02dcA, gives the \"expected\" similarity\nn\nbetween units. An important problem for future work will be to closely reexamine the question of what makes neural populations\nfunctionally similar or dissimilar, above and beyond statistical similarity, and what constitutes a surprising amount of (dis)similarity\nthat may be indicative of modular design.\nFinding P \u2217 exactly is NP-complete, so in practice we use a variation on the approximate method proposed by Newman. Briefly, the\napproximation works in two steps: first, an initial set of cluster assignments is constructed using a fast spectral initialization method\nthat, similar to other spectral clustering algorithms, recursively divides units into clusters based on the sign of eigenvectors of the\n\n4\n\n\fmatrix B = \u02dcA \u2212 \u02dcA1n1T\n\u02dcA and its submatrices. Only subdivisions that increase Q are kept. In the second step, we use a Monte Carlo\nn\nmethod that repeatedly selects a random unit i then resamples its cluster assignment, holding the other n-1 assignments fixed. This\nresampling step involves a kind of exploration/exploitation trade-off: Q may decrease slightly on each move to potentially find a\nbetter global optimum. We found that it was beneficial to control the entropy of each step using a temperature parameter, to ensure\nthat a good explore/exploit balance was struck for all \u02dcA. Supplemental Figure S2 shows that both the initialization and the Monte\nCarlo steps play a crucial role in finding P \u2217, consistent with the observations of Newman. Full algorithms are given in Appendix\nA.1.",
  "methodology": "",
  "experiments": "4.1 Setup and initial hypotheses\n\nBecause our primary goal is to understand the behavior of the various notions of modularity above, i.e. based on the eight different\nmethods for quantifying pairwise similarity introduced in the previous section, we opted to study a large collection of simple networks\ntrained on MNIST. All pairwise similarity scores were computed using held-out test data. We trained 270 models, comprising 9 runs\nof each of 30 regularization settings, summarized in Table 1. We defined x (input layer) as the raw 784-dimensional pixel inputs and\ny (output layer) as the 10-dimensional class logits. We used the same basic feedforward architecture for all models, comprising\ntwo layers of hidden activity connected by three layers of fully-connected weights: Linear(784, 64), ReLU, dropout(p), Linear(64,\n64), ReLU, dropout(p), Linear(64, 10). We analyzed modularity in the two 64-dimensional hidden layers following the dropout\noperations. We discarded 21 models that achieved less than 80\n\nBefore running these experiments, we hypothesized that\n\n1. Dropout would decrease modularity by encouraging functions to be \"spread out\" over many units. 2. L2 regularization (weight\ndecay) would minimally impact modularity since the L2 norm is invariant to rotation while modularity depends on axis-alignment. 3.\nL1 regularization on weights would increase modularity by encouraging sparsity between subnetworks. 4. All similarity measures\nwould be qualitatively consistent with each other.\n\nAs shown below, all four of these hypotheses turned out to be wrong, to varying degrees.\n\n4.2 How modularity depends on regularization\n\nFigure 3 shows the dependence of trained networks\u2019 modularity score (Q\u2217) as a function of regularization strength for each of three\ntypes of regularization: an L2 penalty on the weights (weight decay), an L1 penalty on the weights, and dropout. The top row of\nFigure 3 shows four example \u02dcA matrices sorted by cluster, to help give an intuition behind the quantitative values of Q\u2217. In these\nexamples, the increasing value of Q\u2217 is driven by an increasing contrast between intra-cluster similarity and inter-cluster similarity.\nIn this example, it also appears that the number and size of clusters remains roughly constant; this observation is confirmed by\nplotting the number of clusters versus regularization strength in Supplemental Figure S4.\n\nFigure 3 shows a number of surprising patterns that contradict our initial predictions. First, and most saliently, we had predicted that\ndropout would reduce modularity, but found instead that it has the greatest effect on Q\u2217 among the three regularization methods we\ntried. This is especially apparent in the upstream methods (first two columns of the figure), and is also stronger for the first hidden\nlayer than the second (Supplemental Figure S3). In general, Q\u2217 can increase either if the network partitions into a greater number of\nclusters, or if the contrast between clusters is exaggerated. We found that this dramatic effect of dropout on Q\u2217 was accompanied\nby only minor changes to the number of clusters (Supplemental Figure S4), and so we can conclude that dropout increases Q\u2217 by\nincreasing the redundancy of hidden units. In other words, hidden units become more clusterable because they are driven towards\nbehaving like functional replicas of each other, separately for each cluster. This observation echoes, and may explain, why dropout\nalso increases the \"clusterability\" of network weights in a separate study.\nThe second surprising result in Figure 3 is that L2 regularization on the weights did, in fact, increase Q\u2217, whereas we had expected it\nto have no impact. Third, L1 regularization had a surprisingly weak effect, although its similarity to the L2 regularization results\nmay be explained by the fact that they actually resulted in fairly commensurate sparsity in the trained weights (Supplemental Figure\nS1 bottom row). Fourth, we had expected few differences between the eight different methods for computing similarity, but there\nappear to be distinctive trends by similarity type both in Figure S3 as well as in the number of clusters detected (Supplemental\nFigure S4). The next section explores the question of similarity in the results in more detail.\n\n4.3 Comparing modules discovered by different similarity methods\n\nThe previous section discussed idiosyncratic trends in the modularity scores Q\u2217 as a function of both regularization strength and\nhow pairwise similarity between units (S) is computed. However, such differences in the quantitative value of Q\u2217 are difficult to\ninterpret, and would largely be moot if the various methods agreed on the question of which units belong in which cluster. We now\nturn to the question of how similar the cluster assignments P \u2217 are across our eight definitions of functional modules. To minimize\nambiguity, we will use the term \"functional-similarity\" to refer to S, and \"cluster-similarity\" to refer to the comparison of different\ncluster assignments P \u2217.\n\n5\n\n\fQuantifying similarity between cluster assignments is a well-studied problem, and we tested a variety of methods in the clusim\nPython package. All cluster-similarity methods we investigated gave qualitatively similar results, so here we report only the \"Element\nSimilarity\" method of Gates et al., which is a value between 0 and 1 that is small when two cluster assignments are unrelated, and\nlarge when one cluster assignment is highly predictive of the other. Note that this cluster-similarity analysis is applied only to P \u2217\ncluster assignments computed in the same layer of the same model. Thus, any dissimilarity in clusters that we see is due entirely to\nthe different choices for functional-similarity, S.\n\nFigure 4a summarizes the results of this cluster-similarity analysis: there is a striking difference between clusters of units identified by\n\"upstream\" functional-similarity methods (Scov, \u02dcScov, Si\u2212sens, \u02dcSi\u2212sens) compared to \"downstream\" functional-similarity methods\n(Shess, \u02dcShess, So\u2212sens, \u02dcSo\u2212sens). This analysis also reveals secondary structure within each class of upstream and downstream\nmethods, where the choice to normalize not (S vs \u02dcS) appears to matter little, and where there is a moderate difference between\nmoment-based methods (Scov, Shess) and gradient-based methods (Si\u2212sens, So\u2212sens). It is worth noting that some of this secondary\nstructure is not robust across all types and levels of regularization; in particular, increasing L2 or L1 regularization strength appears\nto lead to (i) stronger dependence on normalization in the downstream methods, and (ii) a stronger overall agreement among the\nupstream methods (Supplemental Figure S5).\n\nWe next asked to what extent these cluster-similarity results are driven by training. As shown in Figure 4b, much of the structure\nin the downstream methods is unaffected by training (i.e. it is present in untrained models as well), while the cluster-similarity\namong different upstream methods only emerged as a result of training. Interestingly, this analysis further shows that the main\nupstream-vs-downstream distinction seen in Figure 4a is, in fact, attenuated slightly by training.\n\n5 Conclusions\n\nThe prevalence of \"modular\" designs in both engineered and evolved systems has led many to consider the benefits of modularity as\na design principle, and how learning agents like artificial neural networks might discover such designs. However, precisely defining\nwhat constitutes a \"module\" within a neural network remains an open problem. In this work, we operationalized modules in a neural\nnetwork as groups of hidden units that carry out similar functions. This naturally leads to the question of what makes any two units\nfunctionally similar. We introduced eight functional similarity measures designed to capture various intuitions about unit similarity\nand empirically evaluated cluster assignments based on each method in a large number of trained models.\nOne unexpected observation was that dropout increases modularity (as defined by Q\u2217), although this has little to do with the\ncommon-sense definition of a \"module.\" Instead, it is a byproduct of dropout causing subsets of units to behave like near-copies\nof each other, perhaps so that if one unit is dropped out, a copy of it provides similar information to the subsequent layer. To our\nknowledge, this redundancy-inducing effect of dropout has not been noted in the literature previously.\n\nOur main result is that there is a crucial difference between defining \"function\" in terms of how units are driven by upstream inputs,\nand how units drive downstream outputs. While we studied this distinction between upstream and downstream similarity in the\ncontext of modularity and clustering, it speaks to the deeper and more general problem of how best to interpret neural representations.\nFor example, some sub-disciplines of representation-learning (e.g. \"disentanglement\") have long emphasized that a \"good\" neural\nrepresentation is one where distinct features of the world drive distinct sub-populations or sub-spaces of neural activity. This is an\nupstream way of thinking about what is represented, since it depends only on the relationship between inputs and the unit activations\nand does not take into account what happens downstream. Meanwhile, many have argued that the defining characteristic of a neural\nrepresentation is its causal role in downstream behavior; this is, of course, a downstream way of thinking. At a high level, one way\nto interpret our results is is that upstream and downstream ways of thinking about neural representations are not necessarily aligned,\neven in trained networks. This observation is reminiscent of recent empirical work finding that \"disentangled\" representations in\nauto-encoders (an upstream concept) do not necessarily lead to improved performance or generalization to novel tasks (a downstream\nconcept).\n\nDespite its theoretical motivations, this is an empirical study. We trained over 250 feedforward, fully-connected neural networks on\nMNIST. While it is not obvious whether MNIST admits a meaningful \"modular\" solution, we expect that the main results we show\nhere are likely robust, in particular (i) the effect of weight decay, an L1 weight penalty, and dropout, and (ii) misalignment between\nupstream and downstream definitions of neural similarity.\n\nOur work raises the important questions: are neural representations defined by their inputs or their outputs? And, in what contexts\nis it beneficial for these to be aligned? We look forward to future work applying our methods to larger networks trained on more\nstructured data, as well as recurrent networks. We also believe it will be valuable to evaluate the effect of attempting to maximize\nmodularity, as we have defined it, during training, to see to what extent this is possible and whether it leads to performance benefits.\nNote that maximizing Q during training is challenging because (i) computing S may require large batches, and more importantly\n(ii) optimizing Q is highly prone to local minima, since neural activity and cluster assignments P will tend to reinforce each other,\nentrenching accidental clusters that appear at the beginning of training. We suspect that maintaining uncertainty over cluster\nassignments (e.g. using soft Pij in [0, 1] rather than hard P in 0, 1 cluster assignments) will be crucial if optimizing any of our\nproposed modularity metrics during training.\n\n6\n\n\fReferences\n\nMohammed Amer and Tom\u00e1s Maul. A review of modularization techniques in artificial neural networks. Artificial\nIntelligence Review, 52(1):527-561, 2019.\n\nJacob Andreas. Measuring compositionality in representation learning. arXiv, pp. 1-15, 2019.\n\nFarooq Azam. Biologically inspired modular neural networks. Phd, Virginia Polytechnic Institute and State University,\n2000.\n\nFrancis R. Bach and Michael I. Jordan. Kernel independent component analysis. Journal of Machine Learning Research,\n3(1):1-48, 2003.\n\nFrancis R. Bach and Michael I. Jordan. Beyond independent components: Trees and clusters. Journal of Machine Learning\nResearch, 4(7-8):1205-1233, 2004.\n\nShahab Bakhtiari, Patrick Mineault, Tim Lillicrap, Christopher C Pack, and Blake A Richards. The functional specialization\nof visual cortex emerges from training parallel pathways with self-supervised predictive learning. NeurIPS, 3, 2021.\n\nGabriel B\u00e9na and Dan F. M. Goodman. Extreme sparsity gives rise to functional specialization. arXiv, 2021.\n\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 35(8):1798-1828, 2013.\n\nU. Brandes, D. Delling, M. Gaertler, R. Gorke, M. Hoefer, Z. Nikoloski, and D. Wagner. On Modularity Clustering. IEEE\nTransactions on Knowledge and Data Engineering, 20(2):172-188, 2008.\n\nJeff Clune, Jean Baptiste Mouret, and Hod Lipson. The evolutionary origins of modularity. Proceedings of the Royal\nSociety B, 280, 2013.\n\nRion B Correia, Alexander J Gates, Xuan Wang, and Luis M Rocha. Cana: A python package for quantifying control and\ncanalization in boolean networks. Frontiers in physiology, 9:1046, 2018.\n\nCorinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Algorithms for learning kernels based on centered alignment.\nJournal of Machine Learning Research, 13:795-828, 2012.\n\nR\u00f3bert Csord\u00e1s, Sjoerd van Steenkiste, and J\u00fcrgen Schmidhuber. Are Neural Nets Modular? Inspecting Functional\nModularity Through Differentiable Weight Masks. ICLR, 2021.\n\nJ Denker, D Schwartz, B Wittner, S Solla, R Howard, L Jackel, and J Hopfield. Large Automatic Learning, Rule Extraction,\nand Generalization. Complex Systems, 1:877-922, 1987.\n\nAndrea Di Ferdinando, Raffaele Calabretta, and Domenico Parisi. Evolving Modular Architectures for Neural Networks.\nProceedings of the sixth Neural Computation and Psychology Workshop: Evolution, Learning, and Development, pp.\n253-262, 2001.\n\nCian Eastwood and Christopher K.I. Williams. A framework for the quantitative evaluation of disentangled representations.\nICLR, 2018.\n\nDaniel Filan, Stephen Casper, Shlomi Hod, Cody Wild, Andrew Critch, and Stuart Russell. Clusterability in Neural\nNetworks. arXiv, 2021.\n\nJustin Garson and David Papineau. Teleosemantics, Selection and Novel Contents. Biology Philosophy, 34(3), 2019.\n\nAlexander J. Gates, Ian B. Wood, William P. Hetrick, and Yong Yeol Ahn. Element-centric clustering comparison unifies\noverlaps and hierarchy. Scientific Reports, 9(1):1-13, 2019.\n\nM. Girvan and M. E.J. Newman. Community structure in social and biological networks. Proceedings of the National\nAcademy of Sciences of the United States of America, 99(12):7821-7826, 2002.\n\nMelvyn A. Goodale and A. David Milner. Separate visual pathways for perception and action. TINS, 15(1): 20-25, 1992.\n\nArthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Sch\u00f6lkopf. Measuring statistical dependence with Hilbert-\nSchmidt norms. In S. Jain, H. U. Simon, and E. Tomita (eds.), Lecture Notes in Artificial Intelligence, volume 3734, pp.\n63-77. Springer-Verlag, Berlin, 2005.\n\nHarold W Gutch and Fabian J Theis. Independent Subspace Analysis is Unique, Given Irreducibility. In Mike E Davies,\nChristopher J James, Samer A Abdallah, and Mark D Plumbley (eds.), Independent Component Analysis and Signal\nSeparation, volume 7. Springer, Berlin, 2007.\n\nIrina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and Alexander Lerchner.\nTowards a Definition of Disentangled Representations. arXiv, pp. 1-29, 2018.\n\nAapo Hyv\u00e4rinen, Patrik O. Hoyer, and Mika Inki. Topographic independent component analysis. Neural Computation,\n13(7):1527-1558, 2001.\n\nRobert A Jacobs, Michael I Jordan, and Andrew G Barto. Task Decomposition Through Competition in a Modular\nConnectionist Architecture:The What and Where Vision Tasks. Cognitive Science, pp. 219-250, 1991.\n\n7\n\n\fNadav Kashtan and Uri Alon. Spontaneous evolution of modularity and network motifs. Proceedings of the National\nAcademy of Sciences of the United States of America, 102(39):13773-13778, 2005.\nNadav Kashtan, Elad Noor, and Uri Alon. Varying environments can speed up evolution. Proceedings of the National\nAcademy of Sciences of the United States of America, 104(34):13711-13716, 2007.\nSimon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of Neural Network Representations\nRevisited. ICML, 36, 2019.\nYann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-Based Learning Applied to Document Recogni-\ntion. Proceedings of the IEEE, 86(11):2278-2324, 1998.\nH Lipson. Principles of modularity, regularity, and hierarchy for scalable systems. Journal of Biological Physics and\nChemistry, 7(4):125-128, 2007.\nFrancesco Locatello, Stefan Bauer, Mario Lucic, Sylvain Gelly, Bernhard Sch\u00f6lkopf, and Olivier Bachem. Challenging\nCommon Assumptions in the Unsupervised Learning of Disentangled Representations. arXiv, pp. 1-33, 2019.\nMilton Llera Montero, Casimir JJ Ludwig, Rui Ponte Costa, Guarav Malhotra, and Jeffrey Bowers. The role of disentangle-\nment in generalization. ICLR, 2021.\nM. E.J. Newman. Modularity and community structure in networks. Proceedings of the National Academy of Sciences of\nthe United States of America, 103(23):8577-8582, 2006.\nM. E.J. Newman and M. Girvan. Finding and evaluating community structure in networks. Physical Review E - Statistical,\nNonlinear, and Soft Matter Physics, 69(2 2):1-15, 2004.\nJason A. Palmer and Scott Makeig. Contrast functions for independent subspace analysis. In Fabian J. Theis, A. Cichocki,\nA. Yeredor, and M. Zibulevsky (eds.), Independent Component Analysis and Signal Separation, volume LNCS 7191, pp.\n115-122. Springer-Verlag, Berlin, 2012.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative\nstyle, high- performance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024-8035.\nCurran Associates, Inc., 2019.\nBarnab\u00e1s P\u00f3czos and Andr\u00e1s L\u02ddorincz. Independent Subspace Analysis Using Geodesic Spanning Trees. ICML, 22:673-680,\n2005.\nKarl Ridgeway and Michael C. Mozer. Learning deep disentangled embeddings with the F-statistic loss. Advances in\nNeural Information Processing Systems, pp. 185-194, 2018.\nJ. G. Rueckl, K. R. Cave, and S. M. Kosslyn. Why are \"what\" and \"where\" processed by separate cortical visual systems?\nA computational investigation. Journal of Cognitive Neuroscience, 1(2):171-186, 1989.\nBernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua\nBengio. Toward Causal Representation Learning. Proceedings of the IEEE, 109(5): 612-634, 2021.\nHerbert A Simon. The Architecture of Complexity. Proceedings of the American Philosophical Society, 106 (6), 1962.\nO. Tange. Gnu parallel - the command-line power tool. ;login: The USENIX Magazine, 36(1):42-47, Feb 2011.\nG\u00fcnter P. Wagner, Mihaela Pavlicev, and James M. Cheverud. The road to modularity. Nature Reviews Genetics,\n8(12):921-931, 2007.\nChihiro Watanabe. Interpreting Layered Neural Networks via Hierarchical Modular Representation. Communications in\nComputer and Information Science, 1143 CCIS:376-388, 2019.\nChihiro Watanabe, Kaoru Hiramatsu, and Kunio Kashino. Modular representation of layered neural networks. Neural\nNetworks, 97:62-73, 2018.\nChihiro Watanabe, Kaoru Hiramatsu, and Kunio Kashino. Understanding community structure in layered neural networks.\nNeurocomputing, 367:84-102, 2019.\nChihiro Watanabe, Kaoru Hiramatsu, and Kunio Kashino. Knowledge discovery from layered neural networks based on\nnon-negative task matrix decomposition. IEICE Transactions on Information and Systems, E103D(2):390-397, 2020.\nZongze Wu, Chunchen Su, Ming Yin, Zhigang Ren, and Shengli Xie. Subspace clustering via stacked independent subspace\nanalysis networks with sparse prior information. Pattern Recognition Letters, 146: 165-171, 2021.\n\nA Appendix\n\nA.1 Algorithms\n\nThis section provides pseudocode for the algorithm used to compute clusters P \u2217 from the normalized matrix of pairwise associations\nbetween units, \u02dcA. Before running these algorithms, we always remove all-zero rows and columns from \u02dcA; we consider these units to\nall be in a separate \"unused\" cluster.\n\n8\n\n\fL2 (weight decay) L1 weight penalty\n\ndropout prob.\n\nlogspace(-5,-1,9)\n1e-5\n1e-5\n\n0.0\nlogspace(-5,-2,7)\n0.0\n\n0.0\n0.0\nlinspace(0.05,0.7,14)\n\nTable 1: Each row describes one hyperparameter sweep, for a total of 30 distinct hyperparameter values. First row: varying weight\ndecay (L2 weight penalty) with no other regularization (9 values). Second row: varying L1 penalty on weights along with mild\nweight decay (7 values). Third row: varying dropout probability in increments of 0.05 along with mild weight decay (14 values).\n\nAlgorithm 1 Full clustering algorithm.\nRequire: Normalized pairwise associations \u02dcA\n1: P \u2190 GreedySpectralModules( \u02dcA)\n2: P \u2217 \u2190 MonteCarloModules( \u02dcA, P)\n3: return P \u2217\n\n. Initialize P using spectral method\n\n. Further refine P using Monte Carlo method\n\nAlgorithm 2 Pseudocode for greedy, approximate, spectral method for finding modules\n1: function GreedySpectralModules( \u02dcA)\n\n2:\n\n3:\n\n4:\n\n5:\n\n6:\n\n7:\n\n8:\n\n9:\n\n10:\n\n11:\n\n12:\n\n13:\n\n14:\n\n15:\n\n16:\n\n17:\n\n18:\n\n\u02dcA.\nB \u2190 \u02dcA \u2212 \u02dcA1n1T\nn\n... 0]T\nn\n\nP \u2190 [1 0 0\n\nqueue \u2190 [0]\nQ \u2190 T r(P T BP )\n\nwhile queue is not empty do\n\nc \u2190 queue.pop()\n\n. B is analogous to the graph Laplacian, but for modules\n\n. Initialize P to a single cluster, which will be (recursively) split later.\n\n. FILO queue keeping track of which cluster we\u2019ll try splitting next\n\n. Compute Q for the initial P\n\n. Pop the next (leftmost) cluster id\n\ni \u2190 indices of all units currently in cluster c according to P\n\nv \u2190 eig(B(i, i))\ni+ \u2190 subset of i where v was positive\ni\u2212 \u2190 subset of i where v was negative\n\n. Get the leading eigenvector of the submatrix of B containing just units in c\n\n. Split v by sign (if not possible, continue loop)\n\nc0 \u2190 index of the next available (all zero) column of P\nP 0 \u2190 P but with all i\u2212 units moved to cluster c0\nQ0 \u2190 T r(P 0T BP 0)\nif Q0 > Q then\n\n. Try splitting c into c, c0 based on sign of v\n. Compute updated Q for newly-split clusters P 0\n\nQ, P \u2190 Q0, P 0\n\nqueue.append(c, c0)\n\nelse\n\n. Push c and c0 onto the queue to consider further subdividing them\n\n. Update Q and P\n\n19:\nkeep the old P, Q values\n\n. Nothing to do - splitting c into c0 did not improve Q, so we don\u2019t add further subdivisions to the queue, and we\n\n20:\n\n21:\n\n22:\n\nend if\n\nend while\n\nreturn P\n\n23: end function\n\n. Once the queue is empty, P contains a good initial set of cluster assignments\n\nAlgorithm 3 Pseudocode for Monte Carlo method for improving clusters.\n1: function MonteCarloModules( \u02dcA, P, n)\n\n2:\n\n3:\n\nfor n steps do\n\ni \u2190 index of a single a randomly selected unit\n\n9\n\n\f4:\n\n5:\n\n6:\n\n7:\n\n8:\n\n9:\n\n10:\n\n11:\n\n12:\n\n13:\n\n14:\n\n15:\n\n16:\n\n17:\n\n18:\n\nc \u2190 index of the first empty cluster in P\n\u02dcA)P ), P\nQ\u2217, P \u2217 \u2190 T r(P T ( \u02dcA \u2212 \u02dcA1n1T\nn\nfor j = 1...c do\n\nP 0 \u2190 P with i reassigned to cluster j\n\n. Keep track of best Q, P pair found so far\n\n. Try moving unit i to each cluster j, including a new cluster at c\n\nQ0\n\nj \u2190 T r(P 0T ( \u02dcA \u2212 \u02dcA1n1T\n\nn\n\nif Q0\n\nj > Q\u2217 then\nQ\u2217, P \u2217 \u2190 Q0\n\nj , P 0\n\nend if\n\n\u02dcA)P 0)\n\n. Compute updated Q with re-assigned unit\n\n. Update Q\u2217, P \u2217 pair, even if we don\u2019t select this j later\n\nend for\n\u03c4 \u2190 whatever temperature makes p \u221d eQ0/\u03c4 have entropy H = 0.15\np \u2190 eQ0/\u03c4 / (cid:80)\nj\u2217 \u223c p\nP \u2190 P with unit i reassigned to cluster j\u2217, ensuring only the leftmost columns have nonzero values\n\nj eQ0\n\nj /\u03c4\n\n. We found H = 0.15 strikes a good balance between exploration and greedy ascent.\n\n. Sample new cluster assignment j from categorical distribution p\n\nend for\nreturn P \u2217\n\n19: end function\n\nA.2 Supplemental Figures\n\nFigure 1: Basic performance metrics as a function of regularization strength. Each column corresponds to a different regularization\nmethod, as in Table 1. Each row shows a metric calculated on the trained models. Thin colored lines are individual seeds, and thick\nblack line is the average \u00b1 standard error across runs. Horizontal gray line shows each metric computed on randomly initialized\nnetwork. Sparsity (bottom row) is calculated as the fraction of weights in the interval [-1e-3, +1e-3].\n\n[width=]images.png\n\n[width=0.45]image1.png [width=0.45]image2.png\n\nFigure 2: Both spectral initialization and Monte Carlo optimization steps contribute to finding a good value of Q\u2217. Left: The x-axis\nshows modularity scores (Q\u2217) achieved using only the greedy spectral method for finding P \u2217. The y-axis shows the actual scores we\nused in the paper by combining the spectral method for initialization plus Monte Carlo search. The fact that all points are on or\nabove the y=x line indicates that the Monte Carlo search step improved modularity scores. Right: The x-axis now shows modularity\nscores (Q\u2217) achieved using 1000 Monte Carlo steps, after initializing all units into a single cluster (we chose a random 5% of the\nsimilarity-matrices that were analyzed in the main paper to re-run for this analysis, which is why there are fewer points in this\nsubplot than in the left subplot). The fact that all points are on or above the y=x line indicates that using the spectral method to\ninitialize improved the search.\n\nFigure 3: Modularity score (Q\u2217) versus regularization, split by layer. Format is identical to Figure 3, which shows modularity scores\naveraged across layers. Here, we break this down further by plotting each layer separately. The network used in our experiments has\ntwo hidden layers. The first two rows (white background) shows modularity scores for the first hidden layer h1, and the last two\nrows (gray background) shows h2.\n\n[width=]image3.png\n\n10\n\n\f[width=]image4.png\n\nFigure 4: Number of clusters in P \u2217 versus regularization, split by layer. Layout is identical to Figure S3. Gray shading in the\nbackground shows 1\u03c3, 2\u03c3, and 3\u03c3 quantiles of number of clusters in untrained (randomly initialized) networks. Note that, for the\nmost part, training has little impact on the number of clusters detected, suggesting that consistently finding on the order of 2-6\nclusters is more a property of the MNIST dataset itself than of training. We computed the number of clusters using equation (10).\nThis measure is sensitive to both the number and relative size of the clusters.\n\n[width=]image5.png\n\nFigure 5: Further breakdown of cluster-similarity by regularization strength (increasing left to right) and type (L2/L1/dropout).\nResults in Figure 4 reflect an average of the results shown here. The six rows of this figure should be read in groups of two rows: in\neach group, the top row shows the similarity scores (averaged over layers and runs), and the bottom row shows the difference to\nuntrained models. A number of features are noteworthy here: (i) at low values of all three types of regularization, there is little\ncluster- similarity within the upstream methods, but it becomes very strong at as regularization strength grows; (ii) at the highest\nvalues of L2 and L1 regularization, the pattern inside the 4x4 block of downstream methods changes to depend more strongly on\nnormalization; (iii) a moderate amount of agreement between upstream and downstream methods is seen for large L1 regularization\nstrength, but curiously only for unnormalized downstream methods.\n\n11",
  "results": "",
  "conclusion": ""
}
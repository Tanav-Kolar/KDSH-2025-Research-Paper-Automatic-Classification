{
  "title": "Usefulness of LLMs as an Author Checklist Assistant\nfor Scientific Papers: Experiment",
  "abstract": "Large language models (LLMs) represent a promising, but controversial, tool in\naiding scientific peer review. This study evaluates the usefulness of LLMs in a con-\nference setting as a tool for vetting paper submissions against submission standards.\nWe conduct an experiment where 234 papers were voluntarily submitted to an\n201cLLM- based Checklist Assistant.201d This assistant validates whether papers\nadhere to the author checklist, which includes questions to ensure compliance with\nresearch and manuscript preparation standards. Evaluation of the assistant by paper\nauthors suggests that the LLM-based assistant was generally helpful in verifying\nchecklist completion. In post-usage surveys, over 70",
  "introduction": "Recent advancements in large language models (LLMs) have significantly enhanced their capabilities\nin areas such as question answering and text generation. One promising application of LLMs\nis in aiding the scientific peer-review process. However, the idea of using LLMs in peer review\nis contentious and fraught with potential issues. LLMs can hallucinate, exhibit biases, and may\ncompromise the fairness of the peer-review process. Despite these potential issues, LLMs may serve\nas useful analytical tools to scrutinize manuscripts and identify possible weaknesses or inaccuracies\nthat need addressing.\n\nIn this study, we take the first steps towards harnessing the power of LLMs in the application of\nconference peer review. We conduct an experiment at a premier conference in the field of machine\nlearning. While the wider ethical implications and appropriate use cases of LLMs remain unclear and\nmust be a larger community discussion, here, we evaluate a relatively clear-cut and low-risk use case:\nvetting paper submissions against submission standards, with results shown only to the authors.\n\nSpecifically, the peer-review process requires authors to submit a checklist appended to their\nmanuscripts. Such author checklists, utilized in as well as in other peer-review venues, contain\na set of questions designed to ensure that authors follow appropriate research and manuscript prepa-\nration practices. The Paper Checklist is a series of yes/no questions that help authors check if their\nwork meets reproducibility, transparency, and ethical research standards expected for papers. The\nchecklist is a critical component in maintaining standards of research presented at the conference.\nAdhering to the guidelines outlined by these checklists helps authors avoid mistakes that could lead\nto rejection during peer review.\n\nWe deploy and evaluate a Checklist Assistant powered by LLMs. This assistant scrutinizes au-\nthors2019 responses to the checklist, proposing enhancements for submissions to meet the confer-\nence2019s requirements. To prevent any potential bias in the review process, we confine its usage\nexclusively to the authors of papers, so the checklist assistant is not accessible to reviewers. We then\nsystematically evaluate the benefits and risks of LLMs by conducting a structured study to understand\nif LLMs can enhance research quality and improve efficiency by helping authors understand if their\nwork meets research standards. Specifically, we administered surveys both before and after use of\nthe Checklist Assistant asking authors about their expectations for and perceptions of the tool. We\n\n.\n\n\freceived 539 responses to the pre-usage survey, 234 submissions the the Checklist Assistant and 78\nresponses to the post-usage survey. Our main findings are as follows:\n\n(1) Authors generally reported that the LLM-assisted checklist review was a valuable enhancement to\nthe paper submission process.\n\n\u2022 The majority of surveyed authors reported a positive experience using the LLM assistant.\n\nAfter using the assistant, over 70\n\n\u2022 Authors 2019 expectations of the assistant 2019s effectiveness were even more positive\n\nbefore using it than their assessments after actually using it (Section 4.1.3).\n\n\u2022 Among the main issues reported by authors in qualitative feedback, the most frequently cited\nwere inaccuracy (20/52 respondents) and that the LLM was too strict in its requirements\n(14/52 respon- dents) (Section 4.1.4).\n\n(2) While changes in paper submissions cannot be causally attributed to use of the checklist verifi-\ncation assistant, we find qualitative evidence that the checklist review meaningfully helped some\nauthors to improve their submissions.\n\n\u2022 Analysis of the content of LLM feedback to authors indicates that the LLM provided\ngranular feedback to authors, generally giving 4-6 distinct and specific points of feedback\nper question across the 15 questions (Section 4.2.1).\n\n\u2022 Survey responses reflect that some authors made meaningful changes to their submissions\n201435 survey respondents described specific modifications they would make to their\nsubmissions in response to the Checklist Assistant (Section 4.2.2).\n\n\u2022 In 40 instances, authors submitted their paper twice to the checklist verifier (accounting for\n80 total paper submissions.) Between these two submissions, authors tended to increase the\nlength of their checklist justifications significantly, suggesting that they may have added\ncontent in response to LLM feedback (Section 4.2.3).\n\nFinally, we investigate how LLM-based tools can be easily manipulated 2013 specifically, we find\nthat with AI- assisted re-writing of the justifications, an adversarial author can make the Checklist\nAssistant significantly more lenient (Section 5.1).\n\nIn summary, the majority of authors found LLM assistance to be beneficial, highlighting the significant\npotential of LLMs to enhance scientific workflows 2014 whether by serving as direct assistants to\nauthors or helping journals and conferences verify guideline compliance. However, our findings also\nunderscore that LLMs cannot fully replace human expertise in these contexts. A notable portion of\nusers encountered inaccuracies, and the models were also vulnerable to adversarial manipulation.\n\ncode, LLM prompts,\n\nOur\nhttps://github.com/ihsaan-ullah/neurips-checklist-assistant\n\nand sample papers used for\n\ntesting are\n\navailable\n\nat:",
  "related_work": "In the following section, we provide background on the Author Checklist (Section 2.1) and on the\nuse of LLMs in the scientific peer review process (Section 2.2).\n\n2.1 The Author Checklist\n\nWe provide below the checklist questions used in submission template. We provide only the questions\nhere and give the full version including guidelines in Appendix A. These questions are designed\nby organizers, not specifically for this study, and questions are carried over from previous years.\nThe authors had to provide a response to each question, comprising 201cYes,201d 2018No201d or\n201cNA201d (Not Applicable), along with a justification for their answer.\n\nClaims: Do the main claims made in the abstract and introduction accurately reflect the paper2019s\ncontri- butions and scope?\n\nLimitations: Does the paper discuss the limitations of the work performed by the authors?\n\n2\n\n\fTheory Assumptions and Proofs: For each theoretical result, does the paper provide the full set of\nassumptions and a complete (and correct) proof?\n\nExperimental Result Reproducibility: Does the paper fully disclose all the information needed to\nreproduce the main experimental results of the paper to the extent that it affects the main claims\nand/or conclusions of the paper (regardless of whether the code and data are provided or not)?\n\nOpen access to data and code: Does the paper provide open access to the data and code, with sufficient\ninstructions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\n\nExperimental Setting/Details: Does the paper specify all the training and test details (e.g., data splits,\nhyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?\n\nExperiment Statistical Significance: Does the paper report error bars suitably and correctly defined or\nother appropriate information about the statistical significance of the experiments?\n\nExperiments Compute Resources: For each experiment, does the paper provide sufficient information\non the computer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\n\nCode Of Ethics: Does the research conducted in the paper conform, in every respect, with the Code\nof Ethics\n\nBroader Impacts: Does the paper discuss both potential positive societal impacts and negative societal\nimpacts of the work performed?\n\nSafeguards: Does the paper describe safeguards that have been put in place for responsible release of\ndata or models that have a high risk for misuse (e.g., pretrained language models, image generators,\nor scraped datasets)?\n\nLicenses for existing assets: Are the creators or original owners of assets (e.g., code, data, models),\nused in the paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\n\nNew Assets: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\n\nCrowdsourcing and Research with Human Subjects: For crowdsourcing experiments and research\nwith human subjects, does the paper include the full text of instructions given to participants and\nscreen- shots, if applicable, as well as details about compensation (if any)?\n\nInstitutional Review Board (IRB) Approvals or Equivalent for Research with Human Sub- jects:\nDoes the paper describe potential risks incurred by study participants, whether such risks were\ndisclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent\napproval/review based on the requirements of your country or institution) were obtained?\n\n2.2 Related work\n\nLanguage models have been used in the scientific peer review process for over a decade. The primary\napplication so far has been in assigning reviewers to papers. Here, a language model first computes\na 201csimilarity score 201d between every reviewer-paper pair, based on the text of the submitted\npaper and the text of the reviewer2019s previously published papers. A higher value of the similarity\nscore indicates that the language model considers this reviewer to have a higher expertise for this\npaper. Given these similarity scores, reviewers are then assigned to papers using an optimization\nroutine that maximizes the similarity scores of the assigned reviewer-paper pairs.\n\nThere have been recent works that design or use LLMs to write the entire review of papers. The\noutcome measures for evaluating the effectiveness of the LLM- generated reviews are based on\nratings sourced from authors or other researchers. It is not entirely clear how these ratings translate to\nmeeting the objectives of peer review in practice namely that of identifying errors, choosing better\npapers, and providing useful feedback to authors. Moreover, it is also known that evaluation of\npeer reviews themselves are fraught with biases, and the aggregate effect of such biases on these\nevaluations of reviews is not clear. Our work focuses on a more concrete task in reviewing papers\nthan generating an end-to-end review, namely validating that papers meet criteria specified in an\n\n3\n\n\fAuthor Checklist. Moreover, we evaluate the efficacy of LLMs in the setting of an actual peer review\nconference.\n\nRecent work also investigates whether LLMs can identify errors in papers and shows promising\ninitial results. The paper constructs a set of short papers with deliberately inserted errors and asks\nLLMs to identify errors. GPT-4 does identify the error more than half the time. Another experiment\ndescribed asks GPT-4 to identify deliberately inserted errors in three full papers. It successfully and\nconsis- tently does so on one paper, partially and occasionally on a second paper, and is consistently\nunsuccessful on the third. Note that in both experiments, the prompts specifically asked the LLM to\nfind errors rather than generically asking the LLM to review the paper. Moreover, both experiments\nhad small sample sizes in terms of the number of papers. In another set of experiments presented,\nevaluated the ability of large language models (LLMs) to compare the 201cstrength201d of results\nbetween papers, mirroring the goals of conferences and journals in selecting 2018better 2019 papers.\nThe experiment consisted of creating 10 pairs of abstracts, where one abstract in each pair was\nmade 2018 clearly 2019 and objectively stronger than the other. To simulate diverse, yet irrelevant\nconditions, the language of the abstracts was deliberately varied. In this test, GPT-4 performed no\nbetter than random chance in identifying the stronger abstract, underscoring that while LLMs may\nexcel at some complex tasks like scientific error identification, they often struggle with seemingly\nsimpler tasks.\n\nThe papers investigate the performance of LLMs in evaluating checklist compliance. These studies,\nhowever, were retrospective studies of published papers, whereas our work is deployed live associated\nto a peer-review venue and helps authors improve their checklist compliance before they make their\nsubmission.\n\nRecent work has highlighted the prevalence of the use of LLMs both in preparation of scientific paper\nmanuscripts and in the generation of scientific peer reviews. For example, estimates that as of January\n2024, 17.5",
  "methodology": "We design an LLM-based tool (Checklist Assistant) to assist authors in ensuring their submitted\nchecklists are thoroughly answered. Our platform interfaced with a third-party LLM (GPT-4 from\nOpenAI), using simple prompt engineering with these hyper-parameters: temperature = 1, topp = 1,\nand n = 1. For each checklist question, the LLM is provided with the author2019s checklist response\nand justification, alongside the complete paper and any appendices. The LLM2019s role is to assess\nthe accuracy and thoroughness of each response and justification, offering targeted suggestions for\nimprovement. Each checklist item is treated as an individual task, i.e., an API call with only one\nquestion, its answer and justification by the author, and the paper and appendices. The API call\nreturns a review and score for the submitted question.\n\nFigure 1 illustrates examples of feedback provided by the Checklist Assistant for two different papers.\nIn these examples, green indicates that the tool found 201cno significant concerns201d, while orange\nsignals 201cneeds improvement201d with the Paper Checklist standards. Authors are encouraged to\ncarefully review any orange feedback, validate the identified issues, and make the necessary revisions\nto align with the checklist requirements.\n\n3.1 Deployment\n\nWe deployed the Checklist Assistant on Codabench.org. We configured 15 Google Cloud CPU\nworkers, integrated with Codabench, to handle multiple paper submissions concurrently. The bulk of\nthe computations were carried out by the LLM third-party software (GPT-4 from OpenAI) via API\ncalls (one call per question, and additional calls in case of failure).\n\nParticipation was fully voluntary, and participants were recruited through a blog post that was released\n8 days before the abstract submission deadline. Interested participants were asked to register though\na Google form. Participants who submitted registration requests through the Google form were then\ngiven access to the Assistant on the Codabench platform. The submissions were entirely optional and\ncompletely separate from the paper submission system and the review process. The papers had to be\nformatted as specified in the call for papers (complete with appendices and checklist). Information\nprovided in external links was not taken into account by the assistant. We asked submitters to fill out\n\n4\n\n\fthe checklist to the best of their abilities. Submissions made via the Codabench landing page were\nprocessed as follows:\n\nChecklist Assistant: The paper was parsed using a PDF-to-text parser, then screened for any problems\nsuch as the format of the paper or checklist, etc. Each answered question in the checklist was\nprocessed by an LLM using an API.\n\nResult Compilation: LLM responses were combined for all questions and formatted in an HTML\ndocument with proper colors and structure for readability and user-friendliness.\n\nWe encountered several parsing issues with both paper texts and checklists. Initially, our parser\nstruggled with subsections and titles, prompting code improvements to handle sections accurately.\nChecklist parsing also faced issues due to spacing and incomplete checklists, which we addressed by\nrefining the code. Special characters, especially merged letters like 201cfi 201d and 201cfl 201d in\nthe submitted PDFs required further parsing updates.\n\n3.2 Prompt engineering\n\nIn this section we discuss design of a prompt given to the LLM, tasked to behave as Checklist\nAssistant. We provide the full prompt in Appendix B.\n\nWhile preparing the Checklist Assistant, we experimented with various prompt styles. Tuning was\ncarried out using a dozen papers. Some checklists were filled out with our best effort to be correct,\nand others included deliberately planted errors to verify robustness and calibrate the scores. We\nobserved that the LLM performed better with clear, step-by-step instructions.\n\nOur final prompt provided a sequence of instructions covering different aspects of the required\nreview, designed as follows: first, the context is set by indicating that the paper is under review for\nthe conference. Next, the main goal is clarified, specifying that the LLM2019s primary task is to\nassist the author in responding to the checklist question. The LLM is then directed to review the\nauthor2019s answer and justification, identifying any discrepancies with the paper based on the\nspecific guidelines of the question. It is instructed to provide itemized, actionable feedback according\nto the guidelines, offering suggestions for improvement, with clear examples for responses such as\n201cYes, 201d 201cNo, 201d or 201cNA. 201d At the end of the review, the LLM is asked to assign\na score: Score=1 for no issues, Score=0.5 for minor improvements, and Score=0 for critical issues.\nFinally, the LLM is provided with the checklist question, the author 2019s answer, justification, the\nrelevant guidelines, and the paper content.\n\nBefore prompt adjustments, LLM responses often mixed the review with the score. To fix this, we\nspecified that the score should be returned on a separate line at the end of the review. For long papers\nexceeding 35 pages (or 15,000 words), we processed only the first 15,000 words and notified authors\nwith a warning.\n\nWe hypothesized that users might find the LLM responses overly strict, vague, and lengthy (which\nwas indeed later confirmed), so we added prompt instructions like 201cuse 0 score sparingly 201d,\n201cprovide itemized, actionable feedback 201d, and 201cfocus on significant improvements. 201d\nAlthough the Checklist Assistant returned scores of 0, 0.5, and 1, we combined the 0 and 0.5 scores\nto indicate that improvement was needed, rather than differentiating between two levels of severity\n(with red for 0 and orange for 0.5). This decision was made due to concerns that the LLM 2019s\nevaluations might be too harsh. User feedback on LLM strictness and other issues is analyzed in\nSection 4.\n\nWe also tested whether the LLM was consistent in generating answers for reiterations of the same\ninput. As a sanity check, we test for each question, whether the variation of the output scores for\nmultiple runs on the same paper is comparable to the variation across papers. We find that the\nvariation in scores for multiple runs on the same paper is significantly lower than variation across\npapers (p < 0.05; based on a one sided permutation test after BH correction) for all but one question.\nThe only question that had a comparable variance within and across papers was the question on ethics\n(Q9; p > 0.4).\n\n5\n\n\f3.3 Anonymity, confidentiality, and consent\n\nThe authors could retain their anonymity by registering to Codabench with an email that did not\nreveal their identity, and by submitting anonymized papers. The papers and LLM outputs were\nkept confidential and were not be accessible to reviewers, meta reviewers, and program chairs. It is\nimportant to note that while authors retained ownership of their submissions, the papers were sent to\nthe API of an LLM service, and treated under their conditions of confidentiality.\n\nThis study was approved by the Carnegie Mellon University Institutional Review Board (IRB). The\nparticipants gave written documentation of informed consent to participate.",
  "experiments": "In our evaluations, we seek to address two main questions regarding the use of an LLM-automated\nAuthor Checklist Assistant:\n\n(1) Do authors perceive an LLM Author Checklist Assistant as a valuable enhancement to the paper\nsub- mission process?\n\n(2) Does the use of an Author Checklist Assistant meaningfully help authors to improve their paper\nsub- missions?\n\nIn order to understand author experience using the provided Author Checklist Assistant, we surveyed\nauthors before and after submitting to the Author Checklist Assistant. Additionally, we analyzed the\ncontent and submission patterns of author 2019s checklists and the LLM responses. A summary of our\nmain findings is given in Section 1. In this subsequent section we provide detailed analyses of survey\nresponses and usage of the Checklist Assistant. In Section 4.1, we give results on author perception\nand experience and in Section 4.2 we analyze changes made by authors to their submissions after\nusing the Author Checklist Assistant.\n\n4.1 Author Perception and Experience\n\nFirst, we analyze the authors 2019 usage patterns and perceptions of the Author Checklist Assistant,\nas captured through surveys. In Section 4.1.1, we provide an overview of how authors filled out the\nchecklist and the responses given by the LLM on their checklists. In Section 4.1.2, we detail the\nsurvey methodology used to understand author experience and in Section 4.1.3, we analyze results of\nthe survey. Finally, in Section 4.1.4, we overview the main challenges identified by authors when\nusing the Author Checklist Assistant.\n\n4.1.1 Overview of Checklist Usage and Responses\n\nA total of 234 papers, each accompanied by a checklist, were submitted to the assistant. For each\nchecklist question, authors could respond with Yes, No, NA, or TODO. As illustrated in Figure\n2a, most questions received a Yes response, indicating that the authors confirmed their paper met\nthe corresponding checklist criteria. However, for the questions on Theory, Impacts, Safeguards,\nDocumentation, Human Subjects, and Risks, a significant portion of authors selected NA. Additionally,\na notable number of authors responded No to the questions on Code and Data, and Error Bars.\n\nIn response to the authors 2019 checklists, the LLM provided written feedback, with green indicating\n2018No Concerns 2019 and orange indicating 2018Needs improvement 2019. Figure 2b illustrates\nthe distribution of LLM feedback for each checklist question. For most questions, the majority of\nfeedback suggested that the checklist or manuscript could be improved. However, for the questions\non Theory, Human Subjects, and Risks, many NA responses were deemed appropriate, leading the\nLLM to respond with 2019No Concerns. 2019 This likely reflects the LLM 2019s confidence in\nconfirming that certain papers did not include theory, human subjects research, or clear broader risks,\nmaking those checklist items irrelevant. In Figure 3, we show the distribution of LLM evaluations\nper submission. All submissions received several 2018Needs improvement 2019 ratings, with each\nbeing advised to improve on 8 to 13 out of the 15 checklist questions.\n\n6\n\n\f4.1.2 Survey Methodology\n\nTo assess authors 2019 perceptions of the usefulness of the Author Checklist Assistant, we conducted\na survey with all participants both at registration (pre-usage) and immediately after using the Author\nChecklist Assistant (post-usage). We provide the content of the surveys in Figure 4. Both surveys\ncontained the same four questions, with the pre-usage survey focusing on expectations and the post-\nusage survey on actual experience. Responses were recorded on a four-point Likert scale, ranging\nfrom strongly disagree to strongly agree. In the post-usage survey, we also asked authors to provide\nfreeform feedback on (1) any changes they planned to make to their paper, and (2) any issues they\nencountered while using the Checklist Assistant.\n\nWe received 539 responses to the pre-usage survey and 234 papers submitted. However, we received\nonly 78 responses to the post-usage survey, representing 63 unique participants (due to multiple\nsubmissions for the same paper). While completing the pre-registration survey was mandatory for all\nparticipants, the post-usage survey was optional. As a result, all participants in the post-usage survey\nhad also completed the pre-registration survey.\n\n4.1.3 Survey Responses\n\nFigure 5 presents the survey responses collected before and after using the checklist verification tool.\nWe include responses from authors who completed both surveys (n=63). In cases where authors\nsubmitted the survey multiple times for the same paper, we included only the earliest post-usage\nresponse. Including the duplicated responses made a negligible difference, with the proportion of\npositive responses changing by less than 0.02 across all questions.\n\nOverall, the majority of authors responded positively regarding their experience with the Checklist\nAssis- tant. 70\n\nIt is notable that authors were even more positive before using the tool. Comparing pre- and post-\nusage responses, there was a statistically significant drop in positive feedback on the 201cUseful 201d\nand 201cExcited to Use 201d questions 2014we run a permutation test with 50,0000 permutations to\ntest whether the difference between proportion of positive responses pre and post-usage is non-zero,\nwhich gives Benjamini-Hochberg adjusted p-values of 0.007 and 0.013 for 201cExcited to Use 201d\nand 201cUseful 201d respectively with effect sizes of 22120.23 and 22120.2.\n\nWe also assessed the correlation between post-usage survey responses and the number of 2018needs\nimprove- ment 2019 scores given by the LLM to authors. In Figure 6, we show mean number of\nneeds improvement scores for authors responding positively or negatively to each survey question.\nWe find no substantial ef- fect of number of 2018needs improvement 2019 scores on survey responses.\nThis may reflect that the number of 2018needs improvement 2019 scores was less important in author\n2019s perception than the written content of the LLM 2019s evaluation.\n\nFinally, we examined potential selection bias due to the drop-off in participation in the post-usage\nsurvey by analyzing the pre-usage survey responses across different groups. As noted earlier, only\na portion of the 539 participants who completed the pre-usage survey went on to submit papers\n(234 Submitters), and an even smaller group responded to the post-usage survey (78 Post-Usage\nRespondents). In Figure 7, we compare the pre-usage survey responses between Submitters and\nNon-Submitters, as well as between Post- Usage Respondents and Non-Respondents. No substantial\ndifferences in rates of positive responses were found (using a permutation test for the difference in\nmean response, gave p-values of > 0.3 for all questions before multiple testing correction), suggesting\nthere is no significant selection bias.\n\n4.1.4 Challenges in Usage\n\nIn addition to the structured survey responses, 52 out of the 78 post-usage survey submissions\nincluded freeform feedback detailing issues with the Checklist Assistant 2019s usage. We manually\ncategorized the reported issues from these responses and identified the following primary concerns,\nlisted in order of decreasing frequency (summarized in Figure 8):\n\nInaccurate: 20 authors reported that the LLM was inaccurate. Note that it is not possible to tell from\nthe responses how many inaccuracies participants found in individual questions since the survey did\nnot ask about individual checklist questions. Many participants noted specific issues, in particular\nthat the LLM overlooked content in the paper, requesting changes to either the checklist or the paper\n\n7\n\n\ffor elements that the authors believed were already addressed. Additionally, some authors reported\nmore nuanced accuracy issues. For instance, one author mentioned that the LLM misinterpreted a\n201cthought experiment 201d as a real experiment and incorrectly asked for more details about the\nexperimental setup. Another author reported that the LLM mistakenly assumed human subjects were\ninvolved due to a discussion of 201cinterpretability 201d in the paper.\n\nToo strict: 14 authors reported that the LLM was too strict.\n\nInfeasible to make changes due to page limits: 5 authors felt that they received useful feedback, but it\nwould not be possible to incorporate due to their papers already being at the page limit.\n\nToo generic: 4 authors reported that the feedback they received was not specific enough to their paper.\n\nInsufficient LLM capabilities: 4 authors complained that the LLM could not handle content over the\n(LLM assistant 2019s) page limit or that it was not multimodal and hence ignored figures.\n\nFeedback inconsistent across submissions: 3 authors reported that the LLM feedback changed across\nmultiple submissions to the server even though the paper and checklist content did not change.\n\nDesire for full paper review: 3 authors reported that they would like feedback on the entire paper, not\njust on checklist items.\n\nBad at theory (mathematical) papers: 2 authors wrote that the LLM seemed bad at theory (mathemati-\ncal) papers.\n\nToo verbose: 2 authors wrote that the LLM 2019s feedback was too wordy.\n\n4.2 Changes to Submissions in Response to Feedback\n\nIn the following analysis, we integrate an assessment of the LLM 2019s feedback with the authors\n2019 checklist answers, to better understand whether the Checklist Assistant helped authors make\nconcrete and meaningful changes to their papers. In Section 4.2.1, we analyze the types of feedback\ngiven by the LLM to authors. In Section 4.2.2, we overview the changes to their papers that authors\nself-reported making in survey responses. Lastly, in Section 4.2.3, we analyze changes made in\nmultiple submissions of the same paper to the Author Checklist Assistant.\n\n4.2.1 Characterization of LLM Feedback by Question\n\nFor authors to make meaningful changes to their papers, the Author Checklist Assistant must provide\nconcrete feedback. In this section, we analyze the type of feedback given by the Checklist Assistant\nto determine whether it is specific to the checklist answers or more generic.\n\nGiven the large volume of feedback, we employed an LLM to extract key points from the Checklist\nAssistant 2019s responses for each question on the paper checklist and to cluster these points into\noverarching categories. Specifically, for each of the 15 questions across the 234 checklist submissions,\nwe used GPT-4 to identify the main points of feedback provided to authors. We manually inspected\nthat the main points extracted by GPT-4 matched the long-form feedback on 10 randomly selected\nsubmitted paper checklists and found that GPT-4 was highly accurate in extracting these key feedback\npoints. We then passed the names and descriptions of these feedback points to GPT-4 to hierarchically\ncluster them into broader themes.\n\nThe most frequently identified feedback themes for 4 questions are shown in Figure 9. Here are our\nkey observations from this analysis.\n\nThe LLM identified many granular types of feedback within each checklist question. We illustrate with\nexamples of responses to four questions in Figure 9. For instance, the LLM gave granular feedback\nwithin the Experimental settings/details question on optimizer configuration details, implementation\ncode availability, and explicit mention of non-traditional experiments.\n\nThe LLM tended to provide 4-6 distinct points of feedback per question (for each of the 15 questions).\n\nThe LLM is capable of giving concrete and specific feedback for many questions. For example, on\nthe 201cClaims 201d question, the LLM commented on consistency and precision in documenting\nclaims on 50 papers, including feedback like matching the abstract and introduction and referencing\nappendices. On the 201cCompute resources 201d question the LLM commented specifically on\ndetailing compute / execution time of methods.\n\n8\n\n\fThe LLM tends to provide some generic boilerplate for each question. The most common category of\nfeedback for each question is a generic commentary on enhancing general aspects of the question.\n\nThere are certain topics that appear across many questions, in particular discussion of limitations and\nimproved documentation.\n\nThe LLM often expands the scope of checklist questions. For example, the LLM brings up repro-\nducibility as a concern in feedback to the code of ethics question and brings up anonymity quite\nfrequently in the code and data accessibility question.\n\nWe provide a full list of the summarized main themes of feedback in Appendix C. In summary, our\nanalysis of the feedback given by the LLM suggests that the LLM gave concrete and actionable\nfeedback to authors that they could potentially use to modify their paper submissions. Our analysis\nalso suggests that a more detailed checklist could be developed to provide more granular feedback,\nbased on the rubrics covered by the Author Checklist Assistant. Such a detailed checklist could be\nprocessed automatically by an LLM to systematically identify specific, commonly overlooked issues\nin scientific papers and flag concrete issues for authors to resolve.\n\n4.2.2 Authors2019 Descriptions of Submission Changes\n\nWe obtain additional evidence of changes made by authors in response to the Checklist Assistant\nthrough the post-usage survey. In the survey, we asked authors to detail in freeform feedback any\nchanges they had made or planned to make in responses to feedback from the LLM. Of the 78\nsurvey responses, 45 provided feedback to this question. Of these 45 responses, 35 actually described\nchanges they would make (the remainder used this freeform feedback to describe issues that they had\nin using the assistant). Based on manual coding of the comments, we identified the main themes in\nchanges they planned to make:\n\n14 authors said that they would improve justifications for their checklist answers by including more\ndetail and/or references to paper sections.\n\n6 authors said that they would add more details about experiments, datasets, or compute.\n\n2 authors said they would change an answer to the checklist that they filled out incorrectly.\n\n2 or fewer authors mentioned improving the intro/abstract, discussion of limitations, and discussion\nof standard errors.\n\nOverall, these responses indicate that some authors were motivated to modify their submissions due\nto feedback from the checklist verification.\n\n4.2.3 Analysis of Re-submissions\n\nFinally, we analyze changes made between submissions to the Checklist Assistant when authors\nsubmitted multiple times. There were 40 instances where an author submitted the same paper to\nthe checklist verification multiple times (out of 184 total distinct paper submissions to the checklist\nverification). In this analysis, we assess changes made to the paper checklist between the first and\nsecond submission to our checklist verifier in order to understand whether authors made substantive\nchanges to their checklists and/or paper manuscripts in response to feedback from the checklist\nverification.\n\n9",
  "results": "",
  "conclusion": ""
}
{
  "title": "AMR Parsing using Stack-LSTMs",
  "abstract": "We present a transition-based AMR parser that directly generates AMR parses from\nplain text. We use Stack-LSTMs to represent our parser state and make decisions\ngreedily. In our experiments, we show that our parser achieves very competitive\nscores on English using only AMR training data. Adding additional information,\nsuch as POS tags and dependency trees, improves the results further.",
  "introduction": "Transition-based algorithms for natural language parsing are formulated as a series of decisions that\nread words from a buffer and incrementally combine them to form syntactic structures in a stack.\nApart from dependency parsing, these models, also known as shift-reduce algorithms, have been\nsuccessfully applied to tasks like phrase-structure parsing, named entity recognition, CCG parsing,\njoint syntactic and semantic parsing and even abstract- meaning representation parsing.\n\nAMR parsing requires solving several natural language processing tasks; mainly named entity\nrecognition, word sense disambiguation and joint syntactic and semantic role labeling. Given the\ndifficulty of building an end-to-end system, most prior work is based on pipelines or heavily dependent\non precalculated features.\n\nInspired by we present a shift- reduce algorithm that produces AMR graphs directly from plain text.\npresented transition-based tree-to-graph transducers that traverse a dependency tree and transforms\nit to an AMR graph. input is a sentence and it is therefore more similar (with a different parsing\nalgorithm) to our approach, but their parser relies on external tools, such as dependency parsing,\nsemantic role labeling or named entity recognition.\n\nThe input of our parser is plain text sentences and, through rich word representations, it predicts\nall actions (in a single algorithm) needed to generate an AMR graph representation for an input\nsentence; it handles the detection and annotation of named entities, word sense disambiguation and\nit makes connections between the nodes detected towards building a predicate argument structure.\nEven though the system that runs with just words is very competitive, we further improve the results\nincorporating POS tags and dependency trees into our model.\n\nStack-LSTMs have proven to be useful in tasks related to syntactic and semantic parsing and named\nentity recognition. In this paper, we demonstrate that they can be effectively used for AMR parsing\nas well.\n\n2 Parsing Algorithm\n\nOur parsing algorithm makes use of a STACK (that stores AMR nodes and/or words) and a BUFFER\nthat contains the words that have yet to be processed. The parsing algorithm is inspired from the\nsemantic actions presented by , the transition-based NER algorithm by and the arc-standard algorithm.\nAs in the buffer starts with the root symbol at the end of the sequence. Figure 2 shows a running\nexample. The transition inventory is the following:\n\n\u2022 SHIFT: pops the front of the BUFFER and push it to the STACK.\n\n\f\u2022 CONFIRM: calls a subroutine that predicts the AMR node corresponding to the top of the\nSTACK. It then pops the word from the STACK and pushes the AMR node to the STACK.\nAn example is the prediction of a propbank sense: From occurred to occur-01.\n\n\u2022 REDUCE: pops the top of the STACK. It occurs when the word/node at the top of the stack\nis complete (no more actions can be applied to it). Note that it can also be applied to words\nthat do not appear in the final output graph, and thus they are directly discarded.\n\n\u2022 MERGE: pops the two nodes at the top of the STACK and then it merges them, it then\npushes the resulting node to the top of STACK. Note that this can be applied recursively.\nThis action serves to get multiword named entities (e.g. New York City).\n\n\u2022 ENTITY(label): labels the node at the top of the STACK with an entity label. This action\nserves to label named entities, such as New York City or Madrid and it is normally run after\nMERGE when it is a multi-word named entity, or after SHIFT if it is a single-word named\nentity.\n\n\u2022 DEPENDENT(label,node): creates a new node in the AMR graph that is dependent on the\nnode at the top of the STACK. An example is the introduction of a negative polarity to a\ngiven node: From illegal to (legal, polarity -).\n\n\u2022 LA(label) and RA(label): create a left/right arc with the top two nodes at the top of the\n\nSTACK. They keep both the head and the dependent in the stack to allow reentrancies (multiple\nincoming edges). The head is now a composition of the head and the dependent. They are enriched\nwith the AMR label.\n\n\u2022 SWAP: pops the two top items at the top of the STACK, pushes the second node to the front\nof the BUFFER, and pushes the first one back into the STACK. This action allows non-\nprojective arcs as in but it also helps to introduce reentrancies. At oracle time, SWAP is\nproduced when the word at the top of the stack is blocking actions that may happen between\nthe second element at the top of the stack and any of the words in the buffer.\n\nFigure 1 shows the parser actions and the effect on the parser state (contents of the stack, buffer) and\nhow the graph is changed after applying the actions.\n\nWe implemented an oracle that produces the sequence of actions that leads to the gold (or close to\ngold) AMR graph. In order to map words in the sentences to nodes in the AMR graph we need to\nalign them. We use the JAMR aligner provided by. It is important to mention that even though the\naligner is quite accurate, it is not perfect, producing a F1 score of around 0.90. This means that most\nsentences have at least one alignment error which implies that our oracle is not capable of perfectly\nreproducing all AMR graphs. This has a direct impact on the accuracy of the parser described in the\nnext section since it is trained on sequences of actions that are not perfect. The oracle achieves 0.895\nF1 Smatch score when it is run on the development set of the LDC2014T12.\n\nThe algorithm allows a set of different constraints that varies from the basic ones (not allowing\nimpossible actions such as SHIFT when the buffer is empty or not generating arcs when the words\nhave not yet been CONFIRMed and thus transformed to nodes) to more complicated ones based on\nthe propbank candidates and number of arguments. We choose to constrain the parser to the basic\nones and let it learn the more complicated ones.\n\n(r / recommend-01 :ARG1 (a / advocate-01 :ARG1 (i / it) :manner (v / vigorous)))\n\n3 Parsing Model\n\nIn this section, we revisit Stack-LSTMs, our parsing model and our word representations.\n\n3.1 Stack-LSTMs\n\nThe stack LSTM is an augmented LSTM that allows adding new inputs in the same way as LSTMs\nbut it also provides a POP operation that moves a pointer to the previous element. The output vector\nof the LSTM will consider the stack pointer instead of the rightmost position of the sequence.\n\n2\n\n\fStackt Buffert Action\n\nStackt + 1 Buffert + 1 Graph\n\nB\nu, S\nB\nu, S\nu, S\nB\nu, v, S B\nB\nu, S\nB\nu, S\nu, v, S B\nu, v, S B\nu, v, S B\n\nSHIFT\nCONFIRM\nREDUCE\nMERGE\nENTITY(l)\nDEPENDENT(r, d)\nRA(r)\nLA(r)\nSWAP\n\nu, S\nn, S\nS\n(u, v), S\n(u : l), S\nu, S\nu, v, S\nu, v, S\nu, S\n\nB\nB\nB\nB\nB\nB\nB\nB\nv, B\n\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nr \u2192 d\nr \u2192 v\nr \u2190 v\n\u2013\n\nTable 1: Parser transitions indicating the action applied to the stack and buffer and the resulting state.\n\nACTION\n\nSTACK\n\nBUFFER\n\nshould, it\n\nIt, should, be, vigorously, advocated, R\nshould, be, vigorously, advocated, R\nshould, be, vigorously, advocated, R\nbe, vigorously, advocated, R\nbe, vigorously, advocated, , R\nit, be, vigorously, advocated, R\nbe, vigorously, advocated, R\nbe, vigorously, advocated, R\nvigorously, advocated, R\nvigorously, advocated, R\nadvocated, R\nadvocated, R\nit, advocated, R\nrecommend-01, it, advocated, R\nrecommend-01, advocated, R\nadvocated, R\nrecommend-01, advocated, R\n\nvigorous, recommend-01\nvigorous\nvigorous\nvigorous, recommend-01\nit, vigorous\n\nrecommend-01\nit, recommend-01\nrecommend-01\nbe, it, recommend-01\nit, recommend-01\nvigorously, it, recommend-01\n\nINIT\nSHIFT\nit\nCONFIRM it\nSHIFT\nCONFIRM recommend-01, it\nSWAP\nSHIFT\nREDUCE\nSHIFT\nREDUCE\nSHIFT\nCONFIRM vigorous, it, recommend-01\nSWAP\nSWAP\nSHIFT\nSHIFT\nSHIFT\nCONFIRM advocate-01, it, recommend-01, vigorous R\nadvocate-01, it, recommend-01, vigorous R\nLA(ARG1)\nadvocate-01, recommend-01, vigorous\nSWAP\nit, advocate-01, recommend-01, vigorous R\nSHIFT\nR\nadvocate-01, recommend-01, vigorous\nREDUCE\nR\nadvocate-01, recommend-01, vigorous\nRA(ARG1)\nrecommend-01, R\nadvocate-01, vigorous\nSWAP\nrecommend01, advocate-01, vigorous\nSHIFT\nR\nR, recommend01, advocate-01, vigorous\nSHIFT\nR, recommend01, advocate-01, vigorous\nLA(root)\nrecommend01, advocate-01, vigorous\nREDUCE\nadvocate-01, vigorous\nREDUCE\nREDUCE\nvigorous\nREDUCE\n\nit R\n\nTable 2: Transition sequence for the sentence It should be vigorously advocated. R represents the\nroot symbol\n\n3\n\n\f3.2 Representing the State and Making Parsing Decisions\n\nThe state of the algorithm presented in Section 2 is represented by the contents of the STACK,\nBUFFER and a list with the history of actions (which are encoded as Stack-LSTMs). All of this\nforms the vector st that represents the state which s calculated as follows:\nst = max{0, W [st\nwhere W is a learned parameter matrix, d is a bias term and st\nthe Stack-LSTMs at time t.\n\nt, bt,at represent the output vector of\n\nt; bt; at] + d},\n\nPredicting the Actions: Our model then uses the vector st for each timestep t to compute the\nprobability of the next action as:\n\np(z|st) =\n\n(cid:80)\n\nexp(gz.st+qz)\n\nz\u2032 \u2208A exp(g\u2032\n\nz.st+q\u2032\n\nz) ,\n\nwhere gz is a column vector representing the (output) embedding of the action z, and qz is a bias term\nfor action z. The set A represents the actions listed in Section 2. Note that due to parsing constraints\nthe set of possible actions may vary. The total number of actions (in the LDC2014T12 dataset) is\n478; note that they include all possible labels (in the case of LA and RA ) and the different dependent\nnodes for the DEPENDENT action.\n\nPredicting the Nodes: When the model selects the action CONFIRM, the model needs to decide the\nAMR node that corresponds to the word at the top of the STACK, by using st, as follows:\n\np(e|st) =\n\n(cid:80)\n\nexp(ge.st+qe)\ne\u2032 \u2208N exp(ge\u2032 .st+qe\u2032 ) ,\n\nwhere N is the set of possible candidate nodes for the word at the top of the STACK. ge is a column\nvector representing the (output) embedding of the node e, and qe is a bias term for the node e. It is\nimportant to mention that this implies finding a propbank sense or a lemma. For that, we rely entirely\non the AMR training set instead of using additional resources.\n\nGiven that the system runs two softmax operations, one to predict the action to take and the second\none to predict the corresponding AMR node, and they both share LSTMs to make predictions, we\ninclude an additional layer with a tanh nonlinearity after st for each softmax.\n\n3.3 Word Representations\n\nWe use character-based representations of words using bidirectional LSTMs . They learn represen-\ntations for words that are orthographically similar. Note that they are updated with the updates to\nthe model. demonstrated that it is possible to achieve high results in syntactic parsing and named\nentity recognition by just using character-based word representations (not even POS tags, in fact, in\nsome cases the results with just character-based representations outperform those that used explicit\nPOS tags since they provide similar vectors for words with similar/same morphosyntactic tag); in this\npaper we show a similar result given that both syntactic parsing and named-entity recognition play a\ncentral role in AMR parsing.\n\nThese are concatenated with pretrained word embeddings. We use a variant of the skip n-gram model\nwith the LDC English Gigaword corpus (version 5). These embeddings encode the syntactic behavior\nof the words .\n\nMore formally, to represent each input token, we concatenate two vectors: a learned character-based\nrepresentation ( \u02c6wC); and a fixed vector representation from a neural language model ( \u02c6wLM ). A linear\nmap (V) is applied to the resulting vector and passed through a component-wise ReLU,\nx = max{0, V [ \u02c6wC; \u02c6wLM ] + b}.\nwhere V is a learned parameter matrix, b is a bias term and wC is the character-based learned\nrepresentation for each word, \u02c6wLM is the pretrained word representation.\n\n3.4 POS Tagging and Dependency Parsing\n\nWe may include preprocessed POS tags or dependency parses to incorporate more information into\nour model. For the POS tags we use the Stanford tagger while we use the Stack-LSTM parser trained\non the English CoNLL 2009 dataset to get the dependencies.\n\n4\n\n\fModel\n\nF1(Newswire)\n\nF1(ALL)\n\n(POS, DEP)\n(POS, DEP, NER)\n(POS, DEP, NER)\n(POS, DEP, NER, SRL)\n(POS, DEP, NER, SRL)\n(POS, CCG)\n(POS, DEP, NER)\n(POS, DEP, NER, SRL)\n(LM, NER)\n(Wordnet, LM, NER)\n(POS, DEP, NER)\n(POS, DEP, NER, SRL)\nOUR PARSER (NO PRETRAINED-NO CHARS)\nOUR PARSER (NO PRETRAINED-WITH CHARS)\nOUR PARSER (WITH PRETRAINED-NO CHARS)\nOUR PARSER\nOUR PARSER (POS)\nOUR PARSER (POS, DEP)\n\n0.59\n-\n0.62\n-\n-\n0.66\n0.70\n0.71\n-\n-\n0.63\n0.70\n0.64\n0.66\n0.66\n0.68\n0.68\n0.69\n\n0.58\n0.66\n-\n0.61\n0.64\n-\n-\n0.66\n0.61\n0.66\n0.59\n0.66\n0.59\n0.61\n0.62\n0.63\n0.63\n0.64\n\nTable 3: AMR results on the LDC2014T12 dataset; Newsire section (left) and full (right). Rows\nlabeled with OUR-PARSER show our results. POS indicates that the system uses preprocessed POS\ntags, DEP indicates that it uses preprocessed dependency trees, SRL indicates that it uses preprocessed\nsemantic roles, NER indicates that it uses preprocessed named entitites. LM indicates that it uses\na LM trained on AMR data and WordNet indicates that it uses WordNet to predict the concepts.\nSystems marked with * are pipeline systems that require a dependency parse as input. (WITH\nPRETRAINED-NO CHARS) shows the results of our parser without character-based representations.\n(NO PRETRAINED-WITH CHARS) shows results without pretrained word embeddings. (NO\nPRETRAINED-NO CHARS) shows results without character-based representations and without\npretrained word embeddings. The rest of our results include both pretrained embeddings and character-\nbased representations.\n\nPOS tags: The POS tags are preprocessed and a learned representation tag is concatenated with the\nword representations. This is the same setting as .\n\nDependency Trees: We use them in the same way as POS tags by concatenating a learned representa-\ntion dep of the dependency label to the parent with the word representation. Additionally, we enrich\nthe state representation st, presented in Section 3.2. If the two words at the top of the STACK have a\ndependency between them, st is enriched with a learned representation that indicates that and the\ndirection; otherwise st remains unchanged. st is calculated as follows:\nst = max{0, W [st\nwhere dept is the learned vector that represents that there is an arc between the two top words at the\ntop of the stack.\n\nt; bt; at; dept] + d},\n\n4 Experiments and Results\n\nWe use the LDC2014T12 dataset for our experiments. Table 1 shows results, including comparison\nwith prior work that are also evaluated on the same dataset.\n\nOur model achieves 0.68 F1 in the newswire section of the test set just by using character-based\nrepresentations of words and pretrained word embeddings. All prior work uses lemmatizers, POS\ntaggers, dependency parsers, named entity recognizers and semantic role labelers that use additional\ntraining data while we achieve competitive scores without that. reports 0.66 F1 in the full test by\nusing WordNet for concept identification, but their performance drops to 0.61 without WordNet. It is\nworth noting that we achieved 0.64 in the same test set without WordNet. without SRL (via Propbank)\nachieves only 0.63 in the newswire test set while we achieved 0.69 without SRL (and 0.68 without\ndependency trees).\n\n5\n\n\fIn order to see whether pretrained word embeddings and character-based embeddings are useful we\ncarried out an ablation study by showing the results of our parser with and without character-based\nrepresentations (replaced by standard lookup table learned embeddings) and with and without pre-\ntrained word embeddings. By looking at the results of the parser without character-based embeddings\nbut with pretrained word embeddings we observe that the character- based representation of words\nare useful since they help to achieve 2 points better in the Newswire dataset and 1 point more in the\nfull test set. The parser with character-based embeddings but without pretrained word embeddings,\nthe parser has more difficulty to learn and only achieves 0.61 in the full test set. Finally, the model\nthat does not use neither character-based embeddings nor pretrained word embeddings is the worst\nachieving only 0.59 in the full test set, note that this model has no explicity way of getting any\nsyntactic information through the word embeddings nor a smart way to handle out of vocabulary\nwords.\n\nAll the systems marked with * require that the input is a dependency tree, which means that they\nsolve a transduction task between a dependency tree and an AMR graph. Even though our parser\nstarts from plain text sentences when we incorporate more information into our model, we achieve\nfurther improvements. POS tags provide small improvements (0.6801 without POS tags vs 0.6822\nfor the model that runs with POS tags). Dependency trees help a bit more achieving 0.6920.\n\n5 Conclusions and Future Work\n\nWe present a new transition-based algorithm for AMR parsing and we implement it using Stack-\nLSTMS and a greedy decoder. We present competitive results, without any additional resources\nand external tools. Just by looking at the words, we achieve 0.68 F1 (and 0.69 by preprocessing\ndependency trees) in the standard dataset used for evaluation.\n\n6",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": ""
}
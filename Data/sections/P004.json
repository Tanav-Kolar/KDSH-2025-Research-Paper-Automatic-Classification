{
  "title": "Graph Neural Networks Without Training: Harnessing the Power of\nLabels as Input Features",
  "abstract": "This study introduces a novel concept of training-free graph neural networks (TFGNNs) for transductive node\nclassification, which can function immediately without any training and can optionally be enhanced through\nsubsequent training. Initially, we put forward the idea of using labels as features (LaF), a valid yet relatively\nunexplored method in graph neural networks. Our analysis demonstrates that incorporating labels as features\nsignificantly improves the representational capacity of GNNs. The design of TFGNNs is based on these findings.\nEmpirical evaluations show that TFGNNs surpass current GNNs in scenarios where training is not performed, and\nwhen training is optionally applied, they achieve convergence much faster than conventional GNNs.",
  "introduction": "Graph Neural Networks (GNNs) have gained prominence as effective models for handling graph-structured data. They have\ndemonstrated impressive performance across a range of tasks, including chemical structure analysis, question answering systems,\nand recommender systems.\n\nA common application for GNNs is transductive node classification. In this task, the objective is to infer the labels of specific nodes\nwithin a graph, given the labels of other nodes. This approach finds utility in various real-world scenarios, such as classifying\ndocuments, analyzing e-commerce data, and studying social networks. Several GNN architectures, including Graph Convolutional\nNetworks (GCNs) and Graph Attention Networks (GATs), have successfully addressed transductive node classification, yielding\nexcellent results.\n\nA significant hurdle in the practical application of GNNs is their computational demand. Real-world graphs, such as those\nrepresenting social networks or the structure of the web, can be enormous, containing billions of nodes. Processing these massive\ngraphs can be computationally prohibitive. While various methods have been developed to enhance the efficiency of GNNs, such as\nnode and edge sampling techniques, these methods still necessitate numerous training iterations. Other approaches, like PinSAGE,\nutilize parallel training and importance pooling to accelerate the training process, but they demand substantial computational\nresources. Consequently, the immediate deployment of GNNs with limited resources remains a challenge.\n\nIn this work, we introduce the concept of training-free graph neural networks (TFGNNs). To realize TFGNNs, we first propose the\ninnovative idea of using labels as features (LaF). In the context of transductive node classification, utilizing node labels as features is\na permissible approach. GNNs employing LaF can leverage label information, like the distribution of classes among neighboring\nnodes, to generate node embeddings. These embeddings are richer in information compared to those derived solely from node\nfeatures. We establish that incorporating labels as features demonstrably augments the expressive capability of GNNs.\n\nTFGNNs possess the unique ability to operate without any training, enabling immediate deployment upon initialization. This\neliminates the need for extensive hyperparameter tuning when used in training-free mode. Furthermore, TFGNNs can be refined\nthrough optional training. Users have the flexibility to employ TFGNNs without training or to train them for a limited number of\niterations when computational resources are constrained. This adaptability is particularly valuable in online learning scenarios,\nwhere data arrives sequentially, and the model needs to be updated promptly. TFGNNs can also undergo full training when resources\nare plentiful or when higher accuracy is paramount. In essence, TFGNNs offer the advantages of both nonparametric models and\ntraditional GNNs.\n\nOur experiments confirm that TFGNNs surpass existing GNNs when used without training and achieve convergence significantly\nfaster than traditional GNNs when training is applied.\n\nThe primary contributions of this research are outlined below:\n\n* We propose the utilization of labels as features (LaF) in transductive learning settings. * We provide formal proof that LaF enhances\nthe representational power of GNNs. * We introduce a novel architecture for training-free graph neural networks (TFGNNs). * We\nempirically demonstrate that TFGNNs outperform existing GNNs in the absence of training.\n\n\f2 Background\n\n2.1 Notations\n\nFor any positive integer n, [n] represents the set {1, 2, ..., n}. A graph is represented by a tuple comprising (i) a set of nodes V , (ii) a\nset of edges E, and (iii) node features X = [x1, x2, ..., xn]T \u2208 Rn\u00d7d. We assume nodes are numbered from 1 to n. Y denotes the\nset of possible labels. yv \u2208 R|Y | is the one-hot encoded label for node v. N (v) represents the set of neighboring nodes of node\nv. We use numpy-like indexing notation. For instance, X:,1 denotes the first column of X, X:,\u22121 denotes the last column, X:,\u22125:\ndenotes the last five columns, and X:,:\u22125 denotes all columns except the last five.\n\n2.2 Transductive Node Classification\n\n**Problem (Transductive Node Classification).** **Input:** A graph G = (V, E, X), a set of labeled nodes Vtrain \u2282 V , and\nthe corresponding labels Ytrain \u2208 Y Vtrain for these nodes. **Output:** Predicted labels Ytest \u2208 Y Vtest for the remaining nodes\nVtest = V \\ Vtrain.\n\nThe node classification problem has two distinct settings: transductive and inductive. In the transductive setting, a single graph\nis provided along with the labels for a subset of its nodes, and the task is to predict the labels for the unlabeled nodes within the\nsame graph. This contrasts with the inductive setting, where separate graphs are used for training and testing. For example, in the\ncontext of spam detection, if we label spam accounts on a social network like Facebook and then use a trained model to identify\nspam accounts on the same network, this is a transductive scenario. Conversely, if we use the model trained on Facebook data to\nidentify spam accounts on a different platform like Twitter, this is an inductive scenario.\n\nTransductive node classification is a widely studied problem in the GNN community. It has been employed in well-known GNN\nmodels like GCNs and GATs and is used in popular benchmark datasets such as Cora, PubMed, and CiteSeer. This setting also has\nnumerous practical applications, including document classification and fraud detection.\n\n2.3 Graph Neural Networks\n\nGNNs are a prevalent method for solving transductive node classification problems. We adopt the message-passing framework for\nGNNs. A message-passing GNN can be defined as follows:\nh(0)\nv = xv (\u2200v \u2208 V ),\nagg(h(l\u22121)\nv = f (l)\nh(l)\nv\n\u02c6yv = fpred(h(L)\nwhere f (l)\n\nagg is the aggregation function at layer l, and fpred is the prediction head, typically implemented using neural networks.\n\n|u \u2208 N (v)}) (\u2200l \u2208 [L], v \u2208 V ),\n\n) (\u2200v \u2208 V ),\n\n, {h(l\u22121)\nu\n\nv\n\n3 LaF is Admissible, but Not Explored Well\n\nWe remind the reader of the transductive node classification problem setup. We are given the node labels yv of the training nodes. A\nstandard approach is to input the node features xv of a training node v into the model, predict its label, calculate the loss based on\nthe true label yv, and update the model parameters. However, the use of yv is not restricted to this. We can also incorporate yv as a\nfeature for node v. This is the core concept behind LaF.\n\nGNNs with LaF initialize node embeddings as:\nh(0)\nv = [xv; \u02dcyv] \u2208 Rd+1+|Y |,\nwhere [\u00b7; \u00b7] denotes vector concatenation, and\n\n\u02dcyv = { [ 1; yv](v \u2208 Vtrain)\n01+|Y |(v \u2208 Vtest),\nis the label vector for node v, and 0d is a zero vector of dimension d. LaF allows GNNs to utilize label information, such as the class\ndistribution in neighboring nodes, to compute node embeddings. These embeddings are likely to be more informative than those\nwithout label information. LaF is considered admissible because it only uses information available in the transductive setting.\n\nWe emphasize that LaF has not been thoroughly investigated in the GNN literature, despite its simplicity, with a few exceptions.\nFor instance, GCNs and GATs use the transductive setting and could potentially use label information as features. However, they\ninitialize node embeddings as h(0)\nv = xv without using label information. One of the contributions of this paper is to highlight that\nLaF is permissible in the transductive setting.\n\n2\n\n\fCare must be taken when training GNNs with LaF. LaF might negatively impact generalization by creating a shortcut where the\nmodel simply copies the label feature h(0)\nv,d+1: to the prediction. To avoid this, we should remove the labels of the center nodes in the\nminibatch and treat them as test nodes. Specifically, if B \u2282 Vtrain is the set of nodes in the minibatch, we set\n\n\u02dcyv = { [ 1; yv](v \u2208 Vtrain \\ B)\n01+|Y |(v \u2208 Vtest \u222a B),\nand predict the label \u02c6yv for v \u2208 B, calculating the loss based on \u02c6yv and yv. This simulates the transductive setting where the label\ninformation of test nodes is unavailable, and GNNs learn to predict test node labels based on the label information and node features\nof surrounding nodes.\n\n4 LaF Strengthens the Expressive Power of GNNs\n\nWe demonstrate that incorporating labels as features (LaF) provably enhances the expressive capabilities of Graph Neural Networks\n(GNNs). Specifically, we show that GNNs utilizing LaF can effectively represent the label propagation algorithm, a crucial method\nfor transductive node classification, whereas GNNs without LaF cannot achieve this. This finding is significant in its own right and\nprovides a strong motivation for the design of TFGNNs.\n\nLabel propagation is a well-established method for transductive node classification. It operates by initiating random walks from a\ntest node and generating the label distribution of the labeled nodes that these random walks encounter first. The following theorem\nestablishes that GNNs with LaF can effectively approximate label propagation.\n\n**Theorem 4.1.** GNNs with LaF can approximate label propagation with arbitrary precision. Specifically, there exists a series of\nGNNs {f (l)\nagg}l and fpred such that for any positive \u03f5, for any connected graph G = (V, E, X), for any labeled nodes Vtrain \u2282 V and\nnode labels Ytrain \u2208 Y Vtrain, and test node v \u2208 V \\ Vtrain, there exists L \u2208 Z+ such that the l(\u2265 L)-th GNN (f (1)\nagg, fpred)\nwith LaF outputs an approximation of label propagation with an error of at most \u03f5, i.e.,\n||\u02c6yv \u2212 \u02c6yLP\nwhere \u02c6yLP\n\nis the output of label propagation for test node v.\n\nagg, ..., f (l)\n\n||1 < \u03f5,\n\nv\n\nv\n\n**Proof.** We prove the theorem by construction. Let\n\npl,v,idef = Pr[The random walk from node v hits Vtrain within l steps and the first hit label is i].\n\nFor labeled nodes, this is a constant:\npl,v,i = 1[i=yv] (\u2200l \u2208 Z\u22650, v \u2208 Vtrain, i \u2208 Y ).\nFor other nodes, it can be recursively computed as:\n\np0,v,i = 0 (\u2200v \u2208 V \\ Vtrain, i \u2208 Y ),\npl,v,i = (cid:80)\n\ndeg(v) \u00b7 pl\u22121,u,i.\n\nu\u2208N (v)\n\n1\n\nThese equations can be represented by GNNs with LaF. The base case\np0,v,i = { 1 [i=yv] (v \u2208 Vtrain)\n0(v \u2208 V \\ Vtrain),\ncan be computed from \u02dcyv in h(0)\ninformation. f (l)\ncomputable from \u02dcyv in h(l\u22121)\nequation, realizable by message passing in the second argument of f (l)\nagg.\nThe final output of the GNN is pl,v,i. The output of label propagation can be decomposed as:\n\nagg always concatenate its first argument (h(l\u22121)\nagg handles two cases based on \u02dcyv,1 \u2208 {0, 1}, indicating whether v is in Vtrain. If v \u2208 Vtrain, f (l)\n\n) to the output so the GNN retains input\nagg outputs 1[i=yv],\nagg aggregates pl\u22121,u,i from u \u2208 N (v) and averages them, as in the recursive\n\n. If v /\u2208 Vtrain, f (l)\n\nv . Let f (l)\n\nv\n\nv\n\n\u02c6yLP\nv,i = Pr[The first hit label is i]\n= pl,v,i+ Pr[The random walk from node v does not hit Vtrain within l steps and the first hit label is i].\n\nAs the second term converges to zero as l increases, GNNs can approximate label propagation with arbitrary precision by increasing\nl.\n\nWe then show that GNNs without LaF cannot represent label propagation.\n**Proposition 4.2.** GNNs without LaF cannot approximate label propagation. Specifically, for any series of GNNs {f (l)\nagg}l and\nfpred, there exists a positive \u03f5, a connected graph G = (V, E, X), labeled nodes Vtrain \u2282 V , node labels Ytrain \u2208 Y Vtrain, and a\ntest node v \u2208 V \\ Vtrain, such that for any l, the GNN (f (1)\n\nagg, fpred) without LaF has an error of at least \u03f5, i.e.,\n\nagg, ..., f (l)\n\n3\n\n\f||\u02c6yv \u2212 \u02c6yLP\nwhere \u02c6yLP\n\nv\n\nv\n\n||1 > \u03f5,\n\nis the output of label propagation for test node v.\n\n**Proof.** We construct a counterexample. Let G be a cycle of four nodes numbered 1, 2, 3, 4 clockwise. All nodes have the same\nfeature x. Let Vtrain = {1, 2} and Ytrain = [1, 0]T . Label propagation classifies node 4 as class 1 and node 3 as class 0. However,\nGNNs without LaF always predict the same label for nodes 3 and 4 since they are isomorphic. Thus, for any GNN without LaF,\nthere is an irreducible error for either node 3 or 4.\n\nTheorem 4.1 and Proposition 4.2 demonstrate that LaF provably enhances the expressive power of GNNs. These results indicate that\nGNNs with LaF are more powerful than traditional message-passing GNNs like GCNs, GATs, and GINs without LaF. Notably, while\nGINs are considered the most expressive message-passing GNNs, they cannot represent label propagation without LaF, whereas\nmessage-passing GNNs with LaF can. This does not lead to a contradiction since the original GINs do not take the label information\nas input. In other words, the input domains of the functions differ. These findings highlight the importance of considering both the\ninput and the architecture of GNNs to maximize their expressive power.\n\n5 Training-free Graph Neural Networks\n\nWe propose training-free graph neural networks (TFGNNs) based on the analysis in the previous section. TFGNNs can be used\nwithout training and can also be improved with optional training.\n\nFirst, we define training-free models.\n\n**Definition 5.1 (Training-free Model).** We say a parametric model is training-free if it can be used without optimizing the\nparameters.\n\nIt should be noted that nonparametric models are training-free by definition. The real worth of TFGNNs is that it is training-free\nwhile it can be improved with optional training. Users can enjoy the best of both worlds of parametric and nonparametric models by\nchoosing the trade-off based on the computational resources for training and the accuracy required.\n\nThe core idea of TFGNNs is to embed label propagation in GNNs by Theorem 4.1. TFGNNs are defined as follows:\nh(0)\nv = [xv; \u02dcyv],\nv = { ReLU (S(l)h(l\u22121)\nh(l)\nReLU (T (l)h(l\u22121)\n\n(cid:80)\nu\u2208N (v) W (l)h(l\u22121)\n\nu\u2208N (v) W (l)h(l\u22121)\n\n)(v \u2208 Vtrain, l \u2208 [L])\n\n)(v \u2208 Vtest, l \u2208 [L]),\n\n+ 1\n(cid:80)\n\nv\n+ 1\n\n|N (v)|\n\nu\n\nu\n\nv\n\n|N (v)|\n\nv\n\n),\n\n\u02c6yv = sof tmax(U h(L)\nThe architecture of TFGNNs is standard, i.e., TFGNNs transform the center nodes and carry out mean aggregation from the\nneighboring nodes. The key to TFGNNs lies in initialization. The parameters are initialized as follows:\n\u2212(1+|Y |):,:\u2212(1+|Y |) = 0, S(l)\nS(l)\nW (l)\n\u2212(1+|Y |):,\u2212(1+|Y |): = I1+|Y |, U:,:\u2212|Y | = 0, U:,\u2212|Y |: = I|Y |,\n\n\u2212(1+|Y |):,\u2212(1+|Y |): = I1+|Y |, V (l)\n\n\u2212(1+|Y |): = 0, W (l)\n\n\u2212(1+|Y |): = 0, T (l)\n\n\u2212(1+|Y |):,:\u2212(1+|Y |) = 0,\n\ni.e., the parameters of the last (1 + |Y |) rows or |Y | rows are initialized by 0 or 1 in a special pattern (Figure 1). Other parameters\nare initialized randomly, e.g., by Xavier initialization. The following proposition shows that the initialized TFGNNs approximate\nlabel propagation.\n\n**Proposition 5.2.** The initialized TFGNNs approximate label propagation. Specifically,\nh(L)\nv,\u2212(|Y |\u2212i+1) = pL,v,i\nholds, where pL,v,i is defined in Eq. (8), and\nargmaxi \u02c6yv,i = argmaxipL,v,i\nholds, and pL,v,i \u2192 \u02c6yLP\n**Proof.** By the definitions of TFGNNs,\nh(0)\nv,\u2212|Y |: = { y\n0|Y |(v \u2208 Vtest),\nh(l)\nv,\u2212|Y |: = { h\n(cid:80)\n1\n|N (v)|\n\n(l\u22121)\nv,\u2212|Y |: (v \u2208 Vtrain, l \u2208 [L])\n\nu,\u2212|Y |:(v \u2208 Vtest, l \u2208 [L]).\n\nu\u2208N (v) h(l\u22121)\n\nv,i as L \u2192 \u221e.\n\nv (v \u2208 Vtrain)\n\nThis recursion is the same as Eqs. (9) \u2013 (13). Therefore,\n\n4\n\n\fh(L)\nv,\u2212(|Y |\u2212i+1) = pL,v,i\nholds. As U picks the last |Y | dimensions, and softmax is monotone,\n\nargmaxi \u02c6yv,i = argmaxipL,v,i\nholds. pL,v,i \u2192 \u02c6yLP\nTherefore, the initialized TFGNNs can be used for transductive node classification as are without training. The approximation\nalgorithm of label propagation is seamlessly embedded in the model parameters, and TFGNNs can also be trained as usual GNNs.\n\nv,i as L \u2192 \u221e is shown in the proof of Theorem 4.1.",
  "related_work": "7.1 Labels as Features and Training-free GNNs\n\nThe most relevant work is by Wang et al., who proposed to use node labels in GNNs. This technique was also used by Addanki et al.\nand analyzed by Wang et al. The underlying idea is common with LaF, i.e., use of label information as input to transductive GNNs.\nA similar result as Theorem 4.1 was also shown in Wang et al. However, the focus is different, and there are different points between\nthis work and theirs. We propose the training-free + optional training framework for the first time. The notable characteristics of\nGNNs are (i) TFGNNs receive both original features and LaF, (ii) TFGNNs can be deployed without training, and (iii) TFGNNs can\nbe improved with optional training. Besides, we provide detailed analysis and experiments including the speed of convergence and\nnoise robustness. Our results provide complementary insights to the existing works.\n\nAnother related topic is graph echo state networks, which lead to lightweight models for graph data. The key idea is to use randomly\ninitialized fixed weights for aggregation. The main difference is that graph echo state networks still require to train the output layer,\nwhile TFGNNs can be used without training. These methods are orthogonal, and it is an interesting direction to combine them to\nfurther improve the performance.\n\n7.2 Speeding up GNNs\n\nVarious methods have been proposed to speed up GNNs to handle large graph data. GraphSAGE is one of the earliest methods to\nspeed up GNNs. GraphSAGE employs neighbor sampling to reduce the computational cost of training and inference. It samples a\nfixed number of neighbors for each node and aggregates the features of the sampled neighbors. An alternative sampling method is\nlayer-wise sampling introduced in FastGCN. Huang et al. further improved FastGCN by using an adaptive node sampling technique\nto reduce the variance of estimators. LADIES combined neighbor sampling and layer-wise sampling to take the best of both worlds.\nAnother approach is to use smaller training graphs. ClusterGCN uses a cluster of nodes as a mini-batch. GraphSAINT samples\nsubgraphs by random walks for each mini-batch.\n\nIt should also be noted that general techniques to speed up neural networks, such as mixed-precision training, quantization, and\npruning can be applied to GNNs.\n\nThese methods mitigate the training cost of GNNs, but they still require many training iterations. In this paper, we propose\ntraining-free GNNs, which can be deployed instantly as soon as the model is initialized. Besides, our method can be improved with\noptional training. In the optional training mode, the speed up techniques mentioned above can be combined with our method to\nreduce the training time further.\n\n7.3 Expressive Power of GNNs\n\nExpressive power (or representation power) means what kind of functional classes a model family can realize. The expressive power\nof GNNs is an important field of research in its own right. If GNNs cannot represent the true function, we cannot expect GNNs to\nwork well however we train them. Therefore, it is important to elucidate the expressive power of GNNs. Originally, Morris et al. and\nXu et al. showed that message-passing GNNs are at most as powerful as the 1-WL test, and they proposed k-GNNs and GINs, which\nare as powerful as the k-(set)WL and 1-WL tests, respectively. GINs are the most powerful message-passing GNNs. Sato and Loukas\nshowed that message-passing GNNs are as powerful as a computational model of distributed local algorithms, and they proposed\nGNNs that are as powerful as port-numbering and randomized local algorithms. Loukas showed that GNNs are Turing-complete\nunder certain conditions (i.e., with unique node ids and infinitely increasing depths). Some other works showed that GNNs can\nsolve or cannot solve some specific problems, e.g., GNNs can recover the underlying geometry, GNNs cannot recognize bridges and\narticulation points. There are various efforts to improve the expressive power of GNNs by non-message-passing architectures. We\nrefer the readers to survey papers for more details on the expressive power of GNNs.\n\n6\n\n\fWe contributed to the field of the expressive power of GNNs by showing that GNNs with LaF are more powerful than GNNs without\nLaF. Specifically, we showed that GNNs with LaF can represent an important model, label propagation, while GNNs without LaF\ncannot. It should be emphasized that GINs, the most powerful message-passing GNNs, and Turing-complete GNNs cannot represent\nlabel propagation without LaF because they do not have access to the label information label propagation uses, and also noted that\nGINs traditionally do not use LaF. This result indicates that it is important to consider what to input to the GNNs as well as the\narchitecture of the GNNs for the expressive power of GNNs. This result provides a new insight into the field of the expressive power\nof GNNs.\n\n8 Limitations\n\nOur work has several limitations. First, LaF and TFGNNs cannot be applied to inductive settings while most GNNs can. We do not\nregard this as a negative point. Popular GNNs such as GCNs and GATs are applicable to both transductive and inductive settings and\nare often used for transductive settings. However, this also means that they do not take advantage of transductive-specific structures\n(those that are not present in inductive settings). We believe that it is important to exploit inductive-specific techniques for inductive\nsettings and transductive-specific techniques (such as LaF) for transductive settings in order to pursue maximum performance.\n\nSecond, TFGNNs cannot be applied to heterophilious graphs, or its performance degrades as TFGNNs are based on label propagation.\nThe same argument mentioned above applies. Relying on homophilious graphs is not a negative point in pursuing maximum\nperformance. It should be noted that LaF may also be exploited in heterophilious settings as well. Developing training-free GNNs\nfor heterophilious graphs based on LaF is an interesting future work.\n\nThird, we did not aim to achieve the state-of-the-art performance. Exploring the combination of LaF with fancy techniques to\nachieve state-of-the-art performance is left as future work.\n\nFinally, we did not explore applications of LaF other than TFGNNs. LaF can help other GNNs in non-training-free settings as well.\nExploring the application of LaF to other GNNs is left as future work.",
  "methodology": "",
  "experiments": "6.1 Experimental Setup\n\nWe use the Planetoid datasets (Cora, CiteSeer, PubMed), Coauthor datasets, and Amazon datasets in the experiments. We use 20\nnodes per class for training, 500 nodes for validation, and the rest for testing in the Planetoid datasets following standard practice,\nand use 20 nodes per class for training, 30 nodes per class for validation, and the rest for testing in the Coauthor and Amazon\ndatasets. We use GCNs and GATs for the baselines. We use three-layered models with a hidden dimension of 32 unless otherwise\nspecified. We train all models with AdamW with a learning rate of 0.0001 and weight decay of 0.01.\n\n6.2 TFGNNs Outperform Existing GNNs in Training-free Setting\n\nWe compare the performance of TFGNNs with GCNs and GATs in the training-free setting by assessing the accuracy of the models\nwhen the parameters are initialized. The results are shown in Table 1. TFGNNs outperform GCNs and GATs in all the datasets.\nSpecifically, both GCNs and GATs are almost random in the training-free setting, while TFGNNs achieve non-trivial accuracy. These\nresults validate that TFGNNs meet the definition of training-free models. We can also observe that GCNs, GATs, and TFGNNs do\nnot benefit from LaF in the training-free settings if randomly initialized. These results indicate that both LaF and the initialization of\nTFGNNs are important for training-free performance.\n\nTable 1: Node classification accuracy in the training-free setting. The best results are shown in bold. CS: Coauthor CS, Physics:\nCoauthor Physics, Computers: Amazon Computers, Photo: Amazon Photo. TFGNNs outperform GCNs and GATs in all the datasets.\nThese results indicate that TFGNNs are training-free. Note that we use three-layered TFGNNs to make the comparison fair although\ndeeper TFGNNs perform better in the training-free setting as we confirm in Section 6.3.\n\nCora\n\nCiteSeer\n\nPubMed CS\n\nPhysics Computers\n\nGCNs\nGCNs + LaF\nGATs\nGATs + LaF\nTFGNNs + random initialization\nTFGNNs (proposed)\n\n0.163\n0.119\n0.177\n0.319\n0.149\n0.600\n\n0.167\n0.159\n0.229\n0.077\n0.177\n0.362\n\n0.180\n0.407\n0.180\n0.180\n0.180\n0.413\n\n0.079\n0.080\n0.040\n0.076\n0.023\n0.601\n\n0.101\n0.146\n0.163\n0.079\n0.166\n0.717\n\n0.023\n0.061\n0.058\n0.025\n0.158\n0.730\n\n6.3 Deep TFGNNs Perform Better in Training-free Setting\n\nWe confirm that deeper TFGNNs perform better in the training-free setting. We have used three-layered TFGNNs so far to make\nthe comparison fair with existing GNNs. Proposition 5.2 shows that the initialized TFGNNs converge to label propagation as the\ndepth goes to infinity, and we expect that deeper TFGNNs perform better in the training-free setting. Figure 2 shows the accuracy of\nTFGNNs with different depths for the Cora dataset. We can observe that deeper TFGNNs perform better in the training-free setting\nuntil the depth reaches around 10, where the performance saturates. It is noteworthy that GNNs have been known to suffer from the\noversmoothing problem, and the performance of GNNs degrades as the depth increases. It is interesting that TFGNNs do not suffer\nfrom the oversmoothing problem in the training-free setting. It should be noted that it does not necessarily mean that deeper models\nperform better in the optional training mode because the optional training may break the structure introduced by the initialization of\nTFGNNs and may lead to oversmoothing and/or overfitting. We leave it as a future work to overcome these problems by adopting\ncountermeasures such as initial residual and identity mapping, MADReg, and DropEdge.\n\n6.4 TFGNNs Converge Fast\n\nIn the following, we investigate the optional training mode of TFGNNs. We train the models with three random seeds and report the\naverage accuracy and standard deviation. We use baseline GCNs without LaF (i.e., the original GCNs) as the baseline.\n\nFirst, we confirm that TFGNNs in the optional training mode converge faster than GCNs. We show the training curves of TFGNNs\nand GCNs for the Cora dataset in Figure 3. TFGNNs converge much faster than GCNs. We hypothesize that TFGNNs converge\n\n5\n\n\ffaster because the initialized TFGNNs are in a good starting point, while GCNs start from a completely random point and require\nmany iterations to reach a good point. We can also observe that fully trained TFGNNs perform on par with GCNs. These results\nindicate that TFGNNs enjoy the best of both worlds: TFGNNs perform well without training and can be trained faster with optional\ntraining.\n\n6.5 TFGNNs are Robust to Feature Noise\n\nAs TFGNNs use both node features and label information while traditional GNNs rely only on node features, we expect that\nTFGNNs are more robust to feature noise than traditional GNNs. We confirm this in this section. We add i.i.d. Gaussian noise with\nstandard deviation \u03c3 to the node features and evaluate the accuracy of the models. We train TFGNNs and GCNs with the Cora\ndataset. The results are shown in Figure 4. TFGNNs are more robust to feature noise especially in high noise regimes where the\nperformance of GCNs degrades significantly. These results indicate that TFGNNs are more robust to i.i.d. Gaussian noise to the\nnode features than traditional GNNs.",
  "results": "",
  "conclusion": "In this paper, we made the following contributions.\n\n* We advocated the use of LaF in transductive learning (Section 3). * We confirmed that LaF is admissible in transductive learning,\nbut LaF has not been explored in the field of GNNs such as GCNs and GATs. * We formally showed that LaF strengthens the\nexpressive power of GNNs (Section 4). * We showed that GNNs with LaF can represent label propagation (Theorem 4.1) while\nGNNs without LaF cannot (Proposition 4.2). * We proposed training-free graph neural networks, TFGNNs (Section 5). * We\nshowed that TFGNNs defined by Eqs. (19) \u2013 (29) meet the requirementsarticle graphicx\n\n7"
}
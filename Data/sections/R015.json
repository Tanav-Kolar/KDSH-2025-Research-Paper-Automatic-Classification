{
  "title": "Examining the Convergence of Denoising Diffusion Probabilistic\nModels: A Quantitative Analysis",
  "abstract": "provides a precise upper limit for the Wasserstein distance between a learned distribution by a diffusion model\nand the target distribution. In contrast to earlier research, this analysis does not rely on presumptions regarding\nthe learned score function. Furthermore, the findings are applicable to any data-generating distributions within\nrestricted instance spaces, even those lacking a density relative to the Lebesgue measure, and the upper limit is not\nexponentially dependent on the ambient space dimension. The primary finding expands upon recent research by\nMbacke et al. (2023), and the proofs presented are fundamental.",
  "introduction": "families of deep generative models. These models have demonstrated remarkable empirical results in generating images and audio,\nas well as in various other applications.\n\nTwo primary methods exist for diffusion models: denoising diffusion probabilistic models (DDPMs) and score-based generative\nmodels (SGMs). DDPMs incrementally convert samples from the desired distribution into noise via a forward process, while\nsimultaneously training a backward process to reverse this transformation, enabling the creation of new samples. Conversely, SGMs\nemploy score-matching methods to approximate the score function of the data-generating distribution, subsequently generating new\nsamples through Langevin dynamics. Recognizing that real-world distributions might lack a defined score function, adding varying\nnoise levels to training samples to encompass the entire instance space and training a neural network to concurrently learn the score\nfunction for all noise levels has been proposed.\n\nAlthough DDPMs and SGMs may initially seem distinct, it has been demonstrated that DDPMs implicitly approximate the score\nfunction, with the sampling process resembling Langevin dynamics. Moreover, a unified perspective of both methods using stochastic\ndifferential equations (SDEs) has been derived. The SGM can be viewed as a discretization of Brownian motion, and the DDPM as a\ndiscretization of an Ornstein-Uhlenbeck process. Consequently, both DDPMs and SGMs are commonly referred to as SGMs in the\nliterature. This explains why prior research investigating the theoretical aspects of diffusion models has adopted the score-based\nframework, necessitating assumptions about the effectiveness of the learned score function.\n\nIn this research, a different strategy is employed, applying methods created for VAEs to DDPMs, which can be viewed as hierarchical\nVAEs with fixed encoders. This method enables the derivation of quantitative, Wasserstein-based upper bounds without making\nassumptions about the data distribution or the learned score function, and with simple proofs that do not need the SDE toolkit.\nFurthermore, the bounds presented here do not involve any complex discretization steps, as the forward and backward processes are\nconsidered discrete-time from the beginning, rather than being viewed as discretizations of continuous-time processes.\n\n1.1 Related Works\n\nThere has been an increasing amount of research aimed at providing theoretical findings on the convergence of SGMs. However,\nthese studies frequently depend on restrictive assumptions regarding the data-generating distribution, produce non-quantitative upper\nbounds, or exhibit exponential dependencies on certain parameters. This work successfully circumvents all three of these limitations.\nSome bounds are based on very restrictive assumptions about the data-generating distribution, such as log-Sobolev inequalities,\nwhich are unrealistic for real-world data distributions. Furthermore, some studies establish upper bounds on the Kullback-Leibler\n(KL) divergence or the total variation (TV) distance between the data-generating distribution and the distribution learned by the\ndiffusion model; however, unless strong assumptions are made about the support of the data-generating distribution, KL and TV\nreach their maximum values. Such assumptions arguably do not hold for real-world data-generating distributions, which are widely\nbelieved to satisfy the manifold hypothesis. Other work establishes conditions under which the support of the input distribution\nis equal to the support of the learned distribution, and generalizes the bound to all f-divergences. Assuming L2 accurate score\n\n\festimation, some establish Wasserstein distance upper bounds under weaker assumptions on the data-generating distribution, but\ntheir Wasserstein-based bounds are not quantitative. Quantitative Wasserstein distance upper bounds under the manifold hypothesis\nhave been derived, but these bounds exhibit exponential dependencies on some of the problem parameters.\n\n1.2 Our contributions\n\nIn this study, strong assumptions about the data-generating distribution are avoided, and a quantitative upper bound on the Wasserstein\ndistance is established without exponential dependencies on problem parameters, including the ambient space dimension. Moreover,\na common aspect of the aforementioned studies is that their bounds are contingent on the error of the score estimator. According to\nsome, providing precise guarantees for the estimation of the score function is challenging, as it necessitates an understanding of the\nnon-convex training dynamics of neural network optimization, which is currently beyond reach. Therefore, upper bounds are derived\nwithout making assumptions about the learned score function. Instead, the bound presented here is dependent on a reconstruction\nloss calculated over a finite independent and identically distributed (i.i.d.) sample. Intuitively, a loss function is defined, which\nquantifies the average Euclidean distance between a sample from the data-generating distribution and the reconstruction obtained by\nsampling noise and passing it through the backward process (parameterized by \u02d803b8). This method is inspired by previous work on\nVAEs.\n\nThis approach offers numerous benefits: it does not impose restrictive assumptions on the data-generating distribution, avoids\nexponential dependencies on the dimension, and provides a quantitative upper bound based on the Wasserstein distance. Furthermore,\nthis method benefits from utilizing very straightforward and basic proofs.\n\n2 Preliminaries\n\nThroughout this paper, lowercase letters are used to represent both probability measures and their densities with respect to the\nLebesgue measure, and variables are added in parentheses to enhance readability (e.g., q(xt|xt\u22121) to denote a time-dependent\nconditional distribution). An instance space X, which is a subset of RD with the Euclidean distance as the underlying metric, and\na target data-generating distribution \u00b5 \u2208 M +\n1 (X) are considered. Note that it is not assumed that \u00b5 has a density with respect to\nthe Lebesgue measure. Additionally, || \u00b7 || represents the Euclidean (L2) norm, and Ep(x) is used as shorthand for Ex\u223cp(x). Given\nprobability measures p, q \u2208 M +\n\n1 (X) and a real number k > 1, the Wasserstein distance of order k is defined as (Villani, 2009):\n\nWk(p, q) = inf\n\n\u03b3\u2208\u0393(p,q)\n\n(cid:18)(cid:90)\n\nX\u00d7X\n\n||x \u2212 y||kd\u03b3(x, y)\n\n(cid:19)1/k\n\n,\n\nwhere \u0393(p, q) denotes the set of couplings of p and q, meaning the set of joint distributions on X \u00d7 X with respective marginals p\nand q. The product measure p \u2297 q is referred to as the trivial coupling, and the Wasserstein distance of order 1 is simply referred to\nas the Wasserstein distance.\n\n2.1 Denoising Diffusion Models\n\nInstead of employing the SDE framework, diffusion models are presented using the DDPM formulation with discrete-time processes.\nA diffusion model consists of two discrete-time stochastic processes: a forward process and a backward process. Both processes are\nindexed by time 0 \u2264 t \u2264 T , where the number of time steps T is a predetermined choice.\n\n**The forward process.** The forward process transforms a data point x0 \u223c \u00b5 into a noise distribution q(xT |x0) through a sequence\nof conditional distributions q(xt|xt\u22121) for 1 \u2264 t \u2264 T . It is assumed that the forward process is defined such that for sufficiently\nlarge T , the distribution q(xT |x0) is close to a simple noise distribution p(xT ), which is referred to as the prior distribution. For\ninstance, p(xT ) = N (xT ; 0, I), the standard multivariate normal distribution, has been chosen in previous work.\n\n**The backward process.** The backward process is a Markov process with parametric transition kernels. The objective of the\nbackward process is to perform the reverse operation of the forward process: transforming noise samples into (approximate) samples\nfrom the distribution \u00b5. Following previous work, it is assumed that the backward process is defined by Gaussian distributions\np\u03b8(xt\u22121|xt) for 2 \u2264 t \u2264 T as\n\nand\n\np\u03b8(x0|x1) = g\u03b8\n\n1(x1),\n\np\u03b8(xt\u22121|xt) = N (xt\u22121; g\u03b8\n\nt (xt), \u03c32\n\nt I),\n\nwhere the variance parameters \u03c32\nneural network (with parameters \u03b8) for 2 \u2264 t \u2264 T , and g\u03b8\nnetwork has been used for the functions g\u03b8\n\nt \u2208 R\u22650 are defined by a fixed schedule, the mean functions g\u03b8\n\nt : RD \u2192 RD are learned using a\n1 : RD \u2192 X is a separate function dependent on \u03c31. In practice, the same\n\nt for 2 \u2264 t \u2264 T , and a separate discrete decoder for g\u03b8\n1.\n\n2\n\n\fGenerating new samples from a trained diffusion model is accomplished by sampling xt\u22121 \u223c p\u03b8(xt\u22121|xt) for 1 \u2264 t \u2264 T , starting\nfrom a noise vector xT \u223c p(xT ) sampled from the prior p(xT ).\n\nThe following assumption is made regarding the backward process.\n**Assumption 1.** It is assumed that for each 1 \u2264 t \u2264 T , there exists a constant K \u03b8\n\nt > 0 such that for every x1, x2 \u2208 X,\n\n||g\u03b8\n\nt (x1) \u2212 g\u03b8\n\nt (x2)|| \u2264 K \u03b8\n\nt ||x1 \u2212 x2||.\n\nIn other words, g\u03b8\n\nt is K \u03b8\n\nt -Lipschitz continuous. This assumption is discussed in Remark 3.2.\n\n2.2 Additional Definitions\n\nThe distribution \u03c0\u03b8(\u00b7|x0) is defined as\n\n\u03c0\u03b8(\u00b7|x0) = q(xT |x0)p\u03b8(xT \u22121|xT )p\u03b8(xT \u22122|xT \u22121) . . . p\u03b8(x1|x2)p\u03b8(\u00b7|x1).\n\nIntuitively, for each x0 \u2208 X, \u03c0\u03b8(\u00b7|x0) represents the distribution on X obtained by reconstructing samples from q(xT |x0) through\nthe backward process. Another way to interpret this distribution is that for any function f : X \u2192 R, the following equation holds:\n\nE\u03c0\u03b8(\u02c6x0|x0)[f (\u02c6x0)] = Eq(xT |x0)Ep\u03b8(xT \u22121|xT ) . . . Ep\u03b8(x1|x2)Ep\u03b8(\u02c6x0|x1)[f (\u02c6x0)].\n\nGiven a finite set S = {x1\n\n0, . . . , xn\n\n0 } i.i.d. \u223c \u00b5, the regenerated distribution is defined as the following mixture:\n\n\u00b5\u03b8\n\nn =\n\n1\nn\n\nn\n(cid:88)\n\ni=1\n\n\u03c0\u03b8(\u00b7|xi\n\n0).\n\nThis definition is analogous to the empirical regenerated distribution defined for VAEs. The distribution on X learned by the\ndiffusion model is denoted as \u03c0\u03b8(\u00b7) and defined as\n\nIn other words, for any function f : X \u2192 R, the expectation of f with respect to \u03c0\u03b8(\u00b7) is\n\n\u03c0\u03b8(\u00b7) = p(xT )p\u03b8(xT \u22121|xT )p\u03b8(xT \u22122|xT \u22121) . . . p\u03b8(x1|x2)p\u03b8(\u00b7|x1).\n\nE\u03c0\u03b8(\u02c6x0)[f (\u02c6x0)] = Ep(xT )Ep\u03b8(xT \u22121|xT ) . . . Ep\u03b8(x1|x2)Ep\u03b8(\u02c6x0|x1)[f (\u02c6x0)].\n\nHence, both \u03c0\u03b8(\u00b7) and \u03c0\u03b8(\u00b7|x0) are defined using the backward process, with the difference that \u03c0\u03b8(\u00b7) starts with the prior\np(xT ) = N (xT ; 0, I), while \u03c0\u03b8(\u00b7|x0) starts with the noise distribution q(xT |x0).\n\nFinally, the loss function l\u03b8 : X \u00d7 X \u2192 R is defined as\n\nl\u03b8(xT , x0) = Ep\u03b8(xT \u22121|xT )Ep\u03b8(xT \u22122|xT \u22121) . . . Ep\u03b8(x1|x2)Ep\u03b8(\u02c6x0|x1)[||x0 \u2212 \u02c6x0||].\n\nHence, given a noise vector xT and a sample x0, the loss l\u03b8(xT , x0) represents the average Euclidean distance between x0 and any\nsample obtained by passing xT through the backward process.\n\n2.3 Our Approach\n\nThe goal is to upper-bound the distance W1(\u00b5, \u03c0\u03b8(\u00b7)). Since the triangle inequality implies\n\nW1(\u00b5, \u03c0\u03b8(\u00b7)) \u2264 W1(\u00b5, \u00b5\u03b8\n\nn) + W1(\u00b5\u03b8\n\nn, \u03c0\u03b8(\u00b7)),\n\nn) is obtained using a straightforward adaptation of a proof. First, W1(\u00b5, \u00b5\u03b8\n\nthe distance W1(\u00b5, \u03c0\u03b8(\u00b7)) can be upper-bounded by upper-bounding the two expressions on the right-hand side separately. The\nupper bound on W1(\u00b5, \u00b5\u03b8\nn) is upper-bounded using the\nexpectation of the loss function l\u03b8, then the resulting expression is upper-bounded using a PAC-Bayesian-style expression dependent\non the empirical risk and the prior-matching term.\nThe upper bound on the second term W1(\u00b5\u03b8\nis determined by the corresponding initial distributions: q(xT |xi\nclose, and if the steps of the backward process are smooth (see Assumption 1), then \u03c0\u03b8(\u00b7|xi\n\nn. Intuitively, the difference between \u03c0\u03b8(\u00b7|xi\n0) and \u03c0\u03b8(\u00b7)\n0) and p(xT ) for \u03c0\u03b8(\u00b7). Hence, if the two initial distributions are\n\nn, \u03c0\u03b8(\u00b7)) uses the definition of \u00b5\u03b8\n\n0) and \u03c0\u03b8(\u00b7) are close to each other.\n\n3\n\n\f3 Main Result\n\n3.1 Theorem Statement\n\nWe are now ready to present the main result: a quantitative upper bound on the Wasserstein distance between the data-generating\ndistribution \u00b5 and the learned distribution \u03c0\u03b8(\u00b7).\n**Theorem 3.1.** Assume the instance space X has finite diameter \u2206 = supx,x\u2032\u2208X ||x \u2212 x\u2032|| < \u221e, and let \u03bb > 0 and \u03b4 \u2208 (0, 1) be\nreal numbers. Using the definitions and assumptions of the previous section, the following inequality holds with probability at least\n1 \u2212 \u03b4 over the random draw of S = {x1\n\n0, . . . , xn\n\n0 } i.i.d. \u223c \u00b5:\n\nW1(\u00b5, \u03c0\u03b8(\u00b7)) \u2264\n\n1\nn\n\n+\n\n+\n\nn\n(cid:88)\n\nEq(xT |xi\n\n0)[l\u03b8(xT , xi\n\n0)] +\n\n1\n\u03bbn\n\nn\n(cid:88)\n\ni=1\n\nKL(q(xT |xi\n\n0)||p(xT )) +\n\n1\n\u03bbn\n\nlog\n\nn\n\u03b4\n\n+\n\n\u03bb\u22062\n8n\n\ni=1\n(cid:32) T\n(cid:89)\n\n(cid:33)\n\nK \u03b8\nt\n\nEq(xT |xi\n\n0)Ep(yT )[||xT \u2212 yT ||]\n\nt=1\n\nT\n(cid:88)\n\n(cid:32)t\u22121\n(cid:89)\n\nt=2\n\ni=1\n\n(cid:33)\n\nK \u03b8\ni\n\n\u03c3tE\u03f5,\u03f5\u2032[||\u03f5 \u2212 \u03f5\u2032||],\n\nwhere \u03f5, \u03f5\u2032 \u223c N (0, I) are standard Gaussian vectors.\n\n**Remark 3.1.** Before presenting the proof, let us discuss Theorem 3.1.\n\n0) is only computed with respect to the noise distribution q(xT |xi\n\n* Because the right-hand side of the equation depends on a quantity computed using a finite i.i.d. sample S, the bound holds with\nhigh probability with respect to the randomness of S. This is the price we pay for having a quantitative upper bound with no\nexponential dependencies on problem parameters and no assumptions on the data-generating distribution \u00b5. * The first term of the\nright-hand side is the average reconstruction loss computed over the sample S = {x1\n0 }. Note that for each 1 \u2264 i \u2264 n, the\nexpectation of l\u03b8(xT |xi\n0 itself. Hence, this term\nmeasures how well a noise vector xT \u223c q(xT |xi\n0 using the backward process, and averages over\nthe set S = {x1\nt < 1 for all 1 \u2264 t \u2264 T , then the larger T is, the smaller the upper\nbound gets. This is because the product of K \u03b8\nt < 1\nfor all t is a quite reasonable one. * The hyperparameter \u03bb controls the trade-off between the prior-matching (KL) term and the\ndiameter term \u22062. If K \u03b8\nt < 1 for all 1 \u2264 t \u2264 T and T \u2192 \u221e, then the convergence of the bound largely depends on the choice of \u03bb.\nIn that case, \u03bb \u221d n1/2 leads to faster convergence, while \u03bb \u221d n leads to slower convergence to a smaller quantity. This is because\nthe bound stems from PAC-Bayesian theory, where this trade-off is common. * The last term of the equation does not depend on the\nsample size n. Hence, the upper bound given by Theorem 3.1 does not converge to 0 as n \u2192 \u221e. However, if the Lipschitz factors\n(K \u03b8\n\nt \u2019s then converges to 0. In Remark 3.2 below, we show that the assumption that K \u03b8\n\nt )1\u2264t\u2264T are all less than 1, then this term can be very small, especially in low-dimensional spaces.\n\n0 }. * If the Lipschitz constants satisfy K \u03b8\n\n0) recovers the original sample xi\n\n0) defined by xi\n\n0, . . . , xn\n\n0, . . . , xn\n\n3.2 Proof of the main theorem\n\nThe following result is an adaptation of a previous result.\n\n**Lemma 3.2.** Let \u03bb > 0 and \u03b4 \u2208 (0, 1) be real numbers. With probability at least 1 \u2212 \u03b4 over the randomness of the sample\nS = {x1\n\n0 } i.i.d. \u223c \u00b5, the following holds:\n\n0, . . . , xn\n\nW1(\u00b5, \u00b5\u03b8\n\nn) \u2264\n\n1\nn\n\nn\n(cid:88)\n\ni=1\n\nEq(xT |xi\n\n0)[l\u03b8(xT , xi\n\n0)] +\n\n1\n\u03bbn\n\nn\n(cid:88)\n\ni=1\n\nKL(q(xT |xi\n\n0)||p(xT )) +\n\n1\n\u03bbn\n\nlog\n\nn\n\u03b4\n\n+\n\n\u03bb\u22062\n8n\n\n.\n\nThe proof of this result is a straightforward adaptation of a previous proof.\nNow, let us focus our attention on the second term of the right-hand side of the equation, namely W1(\u00b5\u03b8\nn, \u03c0\u03b8(\u00b7)). This part is trickier\nthan for VAEs, for which the generative model\u2019s distribution is simply a pushforward measure. Here, we have a non-deterministic\nsampling process with T steps.\n\nAssumption 1 leads to the following lemma on the backward process.\n\n**Lemma 3.3.** For any given x1, y1 \u2208 X, we have\n\nMoreover, if 2 \u2264 t \u2264 T , then for any given xt, yt \u2208 X, we have\n\nEp\u03b8(x0|x1)Ep\u03b8(y0|y1)[||x0 \u2212 y0||] \u2264 K \u03b8\n\n1 ||x1 \u2212 y1||.\n\n4\n\n\fEp\u03b8(xt\u22121|xt)Ep\u03b8(yt\u22121|yt)[||xt\u22121 \u2212 yt\u22121||] \u2264 K \u03b8\n\nt ||xt \u2212 yt|| + \u03c3tE\u03f5,\u03f5\u2032[||\u03f5 \u2212 \u03f5\u2032||],\n\nwhere \u03f5, \u03f5\u2032 \u223c N (0, I), meaning E\u03f5,\u03f5\u2032 is a shorthand for E\u03f5,\u03f5\u2032\u223cN (0,I).\n\n**Proof.** For the first part, let x1, y1 \u2208 X. Since according to the equation p\u03b8(x0|x1) = \u03b4g\u03b8\nthen\n\n1 (x1)(x0) and p\u03b8(y0|y1) = \u03b4g\u03b8\n\n1 (y1)(y0),\n\nEp\u03b8(x0|x1)Ep\u03b8(y0|y1)[||x0 \u2212 y0||] = ||g\u03b8\n\n1(x1) \u2212 g\u03b8\n\n1(y1)|| \u2264 K \u03b8\n\n1 ||x1 \u2212 y1||.\n\nFor the second part, let 2 \u2264 t \u2264 T and xt, yt \u2208 X. Since p\u03b8(xt\u22121|xt) = N (xt\u22121; g\u03b8\nthat sampling xt\u22121 \u223c p\u03b8(xt\u22121|xt) is equivalent to setting\n\nt (xt), \u03c32\n\nt I), the reparameterization trick implies\n\nUsing the above equation, the triangle inequality, and Assumption 1, we obtain\n\nxt\u22121 = g\u03b8\n\nt (xt) + \u03c3t\u03f5t, with \u03f5t \u223c N (0, I).\n\nEp\u03b8(xt\u22121|xt)Ep\u03b8(yt\u22121|yt)[||xt\u22121 \u2212 yt\u22121||]\n= E\u03f5t,\u03f5\u2032\n\u2264 E\u03f5t,\u03f5\u2032\n\u2264 K \u03b8\n\nt (xt) + \u03c3t\u03f5t \u2212 g\u03b8\nt (xt) \u2212 g\u03b8\n\nt\u223cN (0,I)[||g\u03b8\nt\u223cN (0,I)[||g\u03b8\n\nt ||xt \u2212 yt|| + \u03c3tE\u03f5,\u03f5\u2032[||\u03f5 \u2212 \u03f5\u2032||],\n\nt (yt) \u2212 \u03c3t\u03f5\u2032\nt (yt)||] + \u03c3tE\u03f5t,\u03f5\u2032\n\nt||]\nt\u223cN (0,I)[||\u03f5t \u2212 \u03f5\u2032\n\nt||]\n\nwhere \u03f5, \u03f5\u2032 \u223c N (0, I).\n\nNext, we can use the inequalities of Lemma 3.3 to prove the following result.\n\n**Lemma 3.4.** Let T \u2265 1. The following inequality holds:\n\nEp\u03b8(xT \u22121|xT )Ep\u03b8(yT \u22121|yT )Ep\u03b8(xT \u22122|xT \u22121)Ep\u03b8(yT \u22122|yT \u22121) . . . Ep\u03b8(x0|x1)Ep\u03b8(y0|y1)[||x0 \u2212 y0||]\n\n\u2264\n\n(cid:32) T\n(cid:89)\n\nt=1\n\n(cid:33)\n\nK \u03b8\nt\n\n||xT \u2212 yT || +\n\nT\n(cid:88)\n\n(cid:32)t\u22121\n(cid:89)\n\nt=2\n\ni=1\n\n(cid:33)\n\nK \u03b8\ni\n\n\u03c3tE\u03f5,\u03f5\u2032[||\u03f5 \u2212 \u03f5\u2032||],\n\nwhere \u03f5, \u03f5\u2032 \u223c N (0, I).\n\n**Proof Idea.** Lemma 3.4 is proven by induction using Lemma 3.3 in the induction step.\nUsing the two previous lemmas, we obtain the following upper bound on W1(\u00b5\u03b8\n**Lemma 3.5.** The following inequality holds:\n\nn, \u03c0\u03b8(\u00b7)).\n\nW1(\u00b5\u03b8\n\nn, \u03c0\u03b8(\u00b7)) \u2264\n\n1\nn\n\nn\n(cid:88)\n\n(cid:32) T\n(cid:89)\n\n(cid:33)\n\nK \u03b8\nt\n\ni=1\n\nt=1\n\nEq(xT |xi\n\n0)Ep(yT )[||xT \u2212 yT ||] +\n\nT\n(cid:88)\n\n(cid:32)t\u22121\n(cid:89)\n\nt=2\n\ni=1\n\n(cid:33)\n\nK \u03b8\ni\n\n\u03c3tE\u03f5,\u03f5\u2032[||\u03f5 \u2212 \u03f5\u2032||],\n\nwhere \u03f5, \u03f5\u2032 \u223c N (0, I).\n**Proof.** Using the definition of W1, the trivial coupling, the definitions of \u00b5\u03b8\nCombining Lemmas 3.2 and 3.5 with the triangle inequality yields Theorem 3.1.\n\nn and \u03c0\u03b8(\u00b7), and Lemma 3.4, we get the desired result.\n\n3.3 Special case using the forward process of Ho et al. (2020)\n\nTheorem 3.1 establishes a general upper bound that holds for any forward process, as long as the backward process satisfies\nAssumption 1. In this section, we specialize the statement of the theorem to the particular case of the forward process defined in\nprevious work.\nLet X \u2286 RD. The forward process is a Gauss-Markov process with transition densities defined as\n\nwhere \u03b11, . . . , \u03b1T is a fixed noise schedule such that 0 < \u03b1t < 1 for all t. This definition implies that at each time step 1 \u2264 t \u2264 T ,\n\nq(xt|xt\u22121) = N (xt;\n\n\u221a\n\n\u03b1txt\u22121, (1 \u2212 \u03b1t)I),\n\n5\n\n\fq(xt|x0) = N (xt;\n\n\u221a\n\n\u00af\u03b1tx0, (1 \u2212 \u00af\u03b1t)I), with \u00af\u03b1t =\n\nt\n(cid:89)\n\ni=1\n\n\u03b1i.\n\nThe optimization objective to train the backward process ensures that for each time step t, the distribution p\u03b8(xt\u22121|xt) remains close\nto the ground-truth distribution q(xt\u22121|xt, x0) given by\n\nq(xt\u22121|xt, x0) = N (xt\u22121; \u02dc\u00b5q\n\nt (xt, x0), \u02dc\u03c32\n\nt I),\n\nwhere\n\n\u02dc\u00b5q\nt (xt, x0) =\n\n\u221a\n\n\u03b1t(1 \u2212 \u00af\u03b1t\u22121)\n1 \u2212 \u00af\u03b1t\n\nxt +\n\n\u221a\n\n\u00af\u03b1t\u22121(1 \u2212 \u03b1t)\n1 \u2212 \u00af\u03b1t\n\nx0.\n\nNow, we discuss Assumption 1 under these definitions.\n**Remark 3.2.** We can get a glimpse at the range of K \u03b8\np\u03b8(xt\u22121|xt) is optimized to be as close as possible to q(xt\u22121|xt, x0).\nFor a given x0 \u223c \u00b5, let us take a look at the Lipschitz norm of x (cid:55)\u2192 \u02dc\u00b5q\n\nt for a trained DDPM by looking at the distribution q(xt\u22121|xt, x0), since\n\nt (x, x0). Using the above equation, we have\n\nt (xt, x0) \u2212 \u02dc\u00b5q\n\u02dc\u00b5q\n\nt (yt, x0) =\n\n\u221a\n\n\u03b1t(1 \u2212 \u00af\u03b1t\u22121)\n1 \u2212 \u00af\u03b1t\n\n(xt \u2212 yt).\n\nHence, x (cid:55)\u2192 \u02dc\u00b5q\n\nt (x, x0) is K \u2032\n\nt-Lipschitz continuous with\n\n\u221a\n\nK \u2032\n\nt =\n\n\u03b1t(1 \u2212 \u00af\u03b1t\u22121)\n1 \u2212 \u00af\u03b1t\n\n.\n\nNow, if \u03b1t < 1 for all 1 \u2264 t \u2264 T , then we have 1 \u2212 \u00af\u03b1t > 1 \u2212 \u00af\u03b1t\u22121, which implies K \u2032\nRemark 3.2 shows that the Lipschitz norm of the mean function \u02dc\u00b5q\nequation, we can see that for any initial x0, the Lipschitz norm K \u2032\nt =\nSince g\u03b8\nnorm K \u2032\n\nt (\u00b7, x0) is optimized to match \u02dc\u00b5q\nt, we believe it is reasonable to assume g\u03b8\n\nt < 1 for all 1 \u2264 t \u2264 T .\nt (\u00b7, x0) does not depend on x0. Indeed, looking at the previous\nonly depends on the noise schedule, not x0 itself.\nt (\u00b7, x0) have the same Lipschitz\n\nt is Lipschitz continuous as well. This is the intuition behind Assumption 1.\n\nt (\u00b7, x0) for each x0 in the training set, and all the functions \u02dc\u00b5q\n\n\u03b1t(1\u2212 \u00af\u03b1t\u22121)\n1\u2212 \u00af\u03b1t\n\n\u221a\n\n**The prior-matching term.** With the definitions of this section, the prior matching term KL(q(xT |x0)||p(xT )) has the following\nclosed form:\n\nKL(q(xT |x0)||p(xT )) =\n\n1\n2\n\n(cid:2)\u2212D log(1 \u2212 \u00af\u03b1T ) \u2212 D \u00af\u03b1T + \u00af\u03b1T ||x0||2(cid:3) .\n\n**Upper-bounds on the average distance between Gaussian vectors.** If \u03f5, \u03f5\u2032 are D-dimensional vectors sampled from N (0, I), then\n\nMoreover, since q(xT |x0) = N (xT ;\n\n\u221a\n\n\u00af\u03b1T x0, (1 \u2212 \u00af\u03b1T )I) and the prior p(yT ) = N (yT ; 0, I),\n\nE\u03f5,\u03f5\u2032[||\u03f5 \u2212 \u03f5\u2032||] \u2264\n\n\u221a\n\n2D.\n\nEq(xT |x0)Ep(yT )[||xT \u2212 yT ||] \u2264 (cid:112)\u00af\u03b1T ||x0||2 + (2 \u2212 \u00af\u03b1T )D.\n\n**Special case of the main theorem.** With the definitions of this section, the inequality of Theorem 3.1 implies that with probability\nat least 1 \u2212 \u03b4 over the randomness of {x1\n\n0, . . . , x\n\n6",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": ""
}
{
  "title": "Symbiotic Adversarial Robustness for Graph Neural\nNetworks: Combining Poisoning and Evasion",
  "abstract": "Deep learning models are known to be vulnerable to small input perturbations,\nwhich are known as adversarial examples. Adversarial examples are commonly\ncrafted to deceive a model either at training (poisoning) or testing (evasion). We\nstudy the combination of poisoning and evasion attacks. We show that using both\nthreat models can significantly improve the damaging effect of adversarial attacks.\nSpecifically, we study the robustness of Graph Neural Networks (GNNs) under\nstructural perturbations and develop a memory-efficient adaptive end-to-end attack\nfor this novel threat model using first-order optimization.",
  "introduction": "Graph neural networks (GNNs) are increasingly used across many different fields, including product\nrecommendations and drug discovery. GNNs are, however, vulnerable to adversarial attacks in many\ndifferent tasks such as node classification, graph classification, link prediction and node embeddings.\nGiven that such attacks are able to scale to very large graphs, studying the adversarial robustness of\nGNNs has become increasingly important. GNNs can be attacked at test time (evasion) or during\ntraining (poisoning). However, a combined threat model that includes both evasion and poisoning\nhas not been considered in prior literature. Such a model, is, nonetheless, plausible given the public\navailability of graphs or those extracted from sources such as social media sites.\n\nOur work is based on the concept of a symbiotic attack, which combines both evasion and poisoning\nattacks. A symbiotic attack aims to minimize classification accuracy on a test set. The attacker is\nconstrained by a global budget and manipulates the entire graph, rather than individual nodes. We\nprovide a comparison of our approach against plain poisoning and evasion attacks. To this end, we\nadapt the previous PR-BCD attack to the symbiotic threat model, which results in attacks that are\nmemory-efficient and scalable to large graphs. Our main findings are that symbiotic attacks are more\neffective than poisoning attacks alone, and that evasion attacks are affected by the size of the test set,\nwhile symbiotic attacks are less sensitive to test set size. The potential improvement given by the\nsymbiotic threat model indicates that it requires further study.\n\n2 Preliminaries\n\nNotation. We denote a graph by G, with n nodes, an adjacency matrix A \u2208 {0, 1}n\u00d7n, and a feature\nmatrix X \u2208 Rn\u00d7d. A GNN applied to the graph is represented by f\u03b8(G) with parameters \u03b8. We\ndenote the set of possible adversarial graphs that can be created from G as \u03a6(G). Also, Latk and\nLtrain denote the adversarial and training objectives.\n\n2.1 Adversarial Robustness of GNNs\n\nAn adversarial attack on a GNN can modify the graph\u2019s structure, by inserting or removing edges\nand nodes, or modify the node features. This work focuses on node classification and edge-level\nstructural perturbations.\n\n.\n\n\fAttacks can be categorized as either evasion or poisoning. In an evasion attack, a fixed GNN (with\nparameters \u03b8 trained on a clean graph) is targeted, and the attacker aims to solve the optimization\nproblem\n\nmax\n\u02c6G\u2208\u03a6(G)\n\nLatk(f\u03b8( \u02c6G)),\n\nwhereas a poisoning attack is performed before training, aiming to degrade the performance of the\nGNN after training. This can be described as\n\nmax\n\u02c6G\u2208\u03a6(G)\n\nLatk(f\u03b8\u2217 ( \u02c6G)), where \u03b8\u2217 = argmin\u03b8Ltrain(f\u03b8( \u02c6G)).\n\nA poisoning attack is generally more challenging. Previous work has investigated using evasion\nperturbations as poisoning perturbations. Also, the optimization may include unrolling the training\nprocedure to calculate meta-gradients (gradients of Latk with respect to A).\n\nSince we consider only changes to the binary adjacency matrix, we define \u03a6(G) to include graphs\nreachable from G after at most \u2206 edge perturbations.\n\n2.1.1 PR-BCD\n\nOur work extends on the Projected Randomized Block Coordinate Descent (PR-BCD) attack. Simi-\nlarly to the Projected Gradient Descent (PGD) attack, the adjacency matrix is relaxed to P \u2208 [0, 1]n\u00d7n,\nenabling continuous gradient updates. Each entry indicates the probability of flipping an edge, with the\nfinal perturbations sampled from Bernoulli(P ). However, as the adjacency matrix grows quadratically\nwith the number of nodes, scaling of the PGD becomes difficult with larger graphs.\n\nPR-BCD uses Randomized Block Coordinate Descent (R-BCD), updating a block of P at each\niteration. The projection step ensures the budget is enforced in expectation, i.e. E[Bernoulli(P )] =\n(cid:80) P < \u2206 and P \u2208 [0, 1]n\u00d7n. After each iteration, rather than sampling the block again, the\npromising entries of the block are kept, and only the remaining entries are resampled.\n\nPGD can also be applied for a poisoning attack (Meta-PGD). In our attacks, we employ the same\nprinciple with PR-BCD for better scalability. While we only consider a single global budget \u2206, it is\npossible to include more complex constraints when needed for a given application.\n\n3 Symbiotic Attacks\n\nThe Symbiotic Objective. A symbiotic attack has a similar form to the bi-level optimization problem\nbut has an added dependence on the evasion graph G\u2217 in addition to the parameters \u03b8\u2217:\n\nmax\n\u02c6G\u2208\u03a6(G)\n\nLpois(f\u03b8\u2217 (G\u2217)) where \u03b8\u2217 = argmin\u03b8Ltrain(f\u03b8( \u02c6G)), and G\u2217 = argmax \u02c6G\u2208\u03a6( \u02c6G)Lev(f\u03b8\u2217 ( \u02c6G))\n\nHere, Lpois and Lev are separated for clarity even though they could be the same loss.\n\nThreat Model. We model an attacker who aims to reduce a model\u2019s performance on node classifica-\ntion tasks. Our attacker has full access to the graph, has knowledge of the model\u2019s architecture, can\ncreate surrogate models, and can only access the trained model as a black-box. Finally, our attacker\nhas a limited global budget of edge insertions/removals.\n\nThe Sequential Attack. A simple way to launch a symbiotic attack is to divide the budget and launch\na poisoning attack with the first half, followed by an evasion attack with the second half. In this\nattack, the poisoning step is not aware of a future evasion, but can improve performance by reducing\nthe classification margin of certain nodes.\n\nThe Joint Attack. The poisoning attack can be designed to \"fit\" the future evasion graph by including\nthe evasion attack in the poisoning loss. The poisoning loss is computed using the poisoned model\nover the evasion graph. This results in a poisoning attack which not only reduces the model\u2019s accuracy,\nbut also makes it more vulnerable to evasion.\n\nBoth the sequential and joint attacks can be instantiated using different evasion/poisoning attacks. We\nbuild upon PR-BCD because it scales well to larger graphs. Note that the sequential attack is actually\na special case of the joint attack, with zero iterations per inner evasion attack.\n\n2\n\n\f4 Evaluation\n\n4.1 Setup\n\nWe compare the symbiotic threat model with evasion and poisoning attacks, using PR-BCD to\nimplement the evasion and poisoning attacks. These are evaluated on Cora, CiteSeer, and PubMed\ndatasets. We study the robustness of GCN, GAT, APPNP, and GPRGNN models. We also consider\nR-GCN and Jaccard purification as potential defense mechanisms. For each dataset, we allocate 20\nnodes of each class for the labeled training set and 10\n\nTable 1: Numbers of nodes, edges, and classes in the datasets we include in our evaluations.\n\nDataset\n\nNodes\n\nEdges\n\nClasses\n\nCora\nCiteSeer\nPubMed\n\n2,708\n3,327\n19,717\n\n10,556\n9,104\n88,648\n\n7\n6\n3\n\n4.2 Results\n\nTable 2 displays the perturbed accuracy values on the test set (10 percent of nodes) for our benchmark\ndatasets and models, averaged over 10 runs, with the standard error of the mean also shown. The\nattacker is given a 5 percent budget of the number of edges, and this budget is split equally between\npoisoning and evasion for the symbiotic attacks. We report the best performing of the two symbiotic\nattacks, and also note that the symbiotic attacks are consistently stronger than the poisoning attacks,\nand stronger than plain evasion. The symbiotic threat model is especially evident on the larger\nPubMed graph, where the accuracy drops to almost zero, for example, using a GCN.\n\n4.3 Effect of the Number of Test Nodes\n\nTo highlight the differences between poisoning and evasion objectives, Figure 2 shows the perturbed\naccuracies for evasion, poisoning, and symbiotic attacks with varying fractions of test nodes with a\nGCN and a 5\n\nAs the number of test nodes increases, evasion becomes much more challenging across all datasets.\nAlthough poisoning and symbiotic attacks also become more difficult with more test nodes, especially\non PubMed, they are more robust than the evasion attack. Therefore, the reduction in performance\ncannot be explained by the attacks having to target a larger number of nodes with the same budget.\nThe poisoning attack is less affected since it can manipulate the flow of information during training.\nThe symbiotic attacks also benefit from this since they can reduce the base accuracy, making nodes\neasier to misclassify during the evasion phase. The symbiotic attacks are also stronger than poisoning\nalone.\n\n4.4 Hyperparameters\n\nBlock size. Figure 3 shows the results of the four attacks with varying block sizes, using a fixed 5\npercent budget and 125 iterations against a GCN. For small block sizes, the attacks are less effective\nsince the PR-BCD optimization can only cover a small part of the adjacency matrix. However, larger\nblocks have decreasing marginal benefit when a large part of the adjacency matrix can be covered.\n\nBudget. Figure 4 shows how all four attacks follow a similar trend when increasing budget size. On\nPubMed, changing 5 percent of edges is enough to achieve near-zero accuracy under the symbiotic\nmodel. This highlights the devastating effect of joint attacks, especially in larger graphs with a small\nnumber of labeled train nodes.\n\n5 Conclusion and Future Work\n\nIn this work, we have introduced the symbiotic threat model for GNNs, which combines evasion and\npoisoning attacks. We proposed two methods to generate adversarial perturbations for this model and\n\n3\n\n\fTable 2: Average (\u00b1 standard error) perturbed accuracies for the evasion, poisoning, and symbiotic\nattacks with a 5 percent budget. The -J suffix indicates the graph has been pre-processed with Jaccard\npurification. (ind.) stands for inductive learning. The strongest (lowest accuracy) results for each\nsetup are written in bold.\n\nDataset\n\nClean\n\nEvasion\n\nPoisoning\n\nSymbiotic\n\nModel\n\nGCN\n\nGAT\n\nAPPNP\n\nCiteSeer\nCiteSeer (ind.)\nCiteSeer-J\nCora\nCora (ind.)\nCora-J\nPubMed\nPubMed-J\nCiteSeer\nCiteSeer (ind.)\nCiteSeer-J\nCora\nCora (ind.)\nCora-J\nPubMed\nPubMed-J\nCiteSeer\nCiteSeer (ind.)\nCiteSeer-J\nCora\nCora (ind.)\nCora-J\nPubMed\nPubMed-J\n\nGPRGNN CiteSeer\n\nCiteSeer (ind.)\nCiteSeer-J\nCora\nCora (ind.)\nCora-J\nPubMed\nPubMed-J\nCiteSeer\nCora\nPubMed\n\nRGCN\n\n0.68 \u00b1 0.01\n0.67 \u00b1 0.01\n0.68 \u00b1 0.01\n0.78 \u00b1 0.01\n0.75 \u00b1 0.02\n0.74 \u00b1 0.01\n0.78 \u00b1 0.01\n0.77 \u00b1 0.01\n0.62 \u00b1 0.02\n0.68 \u00b1 0.01\n0.64 \u00b1 0.01\n0.69 \u00b1 0.02\n0.77 \u00b1 0.01\n0.67 \u00b1 0.01\n0.73 \u00b1 0.01\n0.74 \u00b1 0.01\n0.69 \u00b1 0.01\n0.71 \u00b1 0.01\n0.68 \u00b1 0.01\n0.82 \u00b1 0.02\n0.82 \u00b1 0.02\n0.82 \u00b1 0.01\n0.79 \u00b1 0.0\n0.77 \u00b1 0.01\n0.66 \u00b1 0.01\n0.67 \u00b1 0.01\n0.65 \u00b1 0.01\n0.82 \u00b1 0.01\n0.8 \u00b1 0.02\n0.79 \u00b1 0.01\n0.78 \u00b1 0.01\n0.78 \u00b1 0.01\n0.63 \u00b1 0.01\n0.74 \u00b1 0.02\n0.77 \u00b1 0.01\n\n0.41 \u00b1 0.01\n0.41 \u00b1 0.01\n0.41 \u00b1 0.01\n0.41 \u00b1 0.01\n0.42 \u00b1 0.01\n0.39 \u00b1 0.01\n0.41 \u00b1 0.01\n0.41 \u00b1 0.01\n0.27 \u00b1 0.02\n0.37 \u00b1 0.01\n0.32 \u00b1 0.03\n0.22 \u00b1 0.02\n0.21 \u00b1 0.01\n0.23 \u00b1 0.02\n0.38 \u00b1 0.04\n0.34 \u00b1 0.04\n0.45 \u00b1 0.01\n0.47 \u00b1 0.01\n0.43 \u00b1 0.01\n0.48 \u00b1 0.03\n0.53 \u00b1 0.02\n0.5 \u00b1 0.01\n0.46 \u00b1 0.01\n0.45 \u00b1 0.01\n0.34 \u00b1 0.01\n0.37 \u00b1 0.01\n0.35 \u00b1 0.01\n0.46 \u00b1 0.01\n0.44 \u00b1 0.01\n0.44 \u00b1 0.01\n0.42 \u00b1 0.01\n0.42 \u00b1 0.01\n0.39 \u00b1 0.01\n0.44 \u00b1 0.01\n0.43 \u00b1 0.01\n\n0.4 \u00b1 0.01\n0.62 \u00b1 0.01\n0.41 \u00b1 0.02\n0.46 \u00b1 0.02\n0.68 \u00b1 0.03\n0.43 \u00b1 0.02\n0.12 \u00b1 0.02\n0.11 \u00b1 0.01\n0.41 \u00b1 0.02\n0.64 \u00b1 0.02\n0.41 \u00b1 0.03\n0.48 \u00b1 0.03\n0.61 \u00b1 0.04\n0.45 \u00b1 0.02\n0.41 \u00b1 0.01\n0.38 \u00b1 0.04\n0.56 \u00b1 0.01\n0.66 \u00b1 0.02\n0.52 \u00b1 0.02\n0.64 \u00b1 0.02\n0.78 \u00b1 0.01\n0.67 \u00b1 0.01\n0.21 \u00b1 0.02\n0.19 \u00b1 0.03\n0.44 \u00b1 0.02\n0.56 \u00b1 0.01\n0.44 \u00b1 0.01\n0.53 \u00b1 0.01\n0.74 \u00b1 0.01\n0.54 \u00b1 0.01\n0.28 \u00b1 0.03\n0.38 \u00b1 0.04\n0.59 \u00b1 0.02\n0.74 \u00b1 0.01\n0.42 \u00b1 0.04\n\n0.38 \u00b1 0.01\n0.33 \u00b1 0.01\n0.38 \u00b1 0.01\n0.35 \u00b1 0.01\n0.3 \u00b1 0.01\n0.36 \u00b1 0.01\n0.03 \u00b1 0.01\n0.02 \u00b1 0.0\n0.3 \u00b1 0.03\n0.56 \u00b1 0.02\n0.3 \u00b1 0.03\n0.29 \u00b1 0.02\n0.35 \u00b1 0.03\n0.28 \u00b1 0.02\n0.2 \u00b1 0.03\n0.19 \u00b1 0.02\n0.47 \u00b1 0.01\n0.4 \u00b1 0.01\n0.45 \u00b1 0.02\n0.51 \u00b1 0.04\n0.37 \u00b1 0.01\n0.54 \u00b1 0.01\n0.09 \u00b1 0.01\n0.1 \u00b1 0.02\n0.33 \u00b1 0.01\n0.34 \u00b1 0.01\n0.35 \u00b1 0.01\n0.4 \u00b1 0.01\n0.35 \u00b1 0.01\n0.4 \u00b1 0.01\n0.08 \u00b1 0.02\n0.15 \u00b1 0.04\n0.47 \u00b1 0.01\n0.52 \u00b1 0.02\n0.15 \u00b1 0.03\n\nshowed that symbiotic attacks can be more effective than the evasion or poisoning approaches on\ntheir own. We will outline several avenues for future work.\n\nThe joint attack can be implemented using other evasion attacks, or attacks designed for the symbiotic\nthreat model. In addition, our work considered global budgets, but it is easy to consider per-node\nlocal budgets and targeted attacks as well. Moreover, we did not consider the use of different loss\nfunctions for the poisoning and evasion parts, which may also further improve attack performance.\nWe plan to include further evaluations on these settings as our next step. Finally, novel poisoning\nattacks can be developed which utilize knowledge of a future evasion attack.\n\nA Proof of Theorem 2.1\n\nProof. Let x \u2208 Ai. Then, \u03c3i(x) = 0, and for all b \u2208 O where bi = 0, wb(x) = 0. Thus,\n\nF (x) =\n\n(cid:88)\n\nwb(x)Gb(x)\n\nb\u2208O,bi=1\n\n4\n\n\fIf bi = 1, then Gb(x) \u2208 Bi, and therefore F (x) is also in Bi due to the convexity of Bi.\n\nB Sub-Gaussian Covering Numbers for ReLU Networks\n\nFigure 2 depicts an example of applying our safe predictor to a notional regression problem. This\nexample uses inputs and outputs in 1-D with one input-output constraint. The unconstrained network\nconsists of a single hidden layer with a dimension of 10, ReLU activations, and a fully connected layer.\nThe safe predictor shares this structure with constrained predictors, G0 and G1, but each predictor\nhas its own fully connected layer. The training uses a sampled subset of points from the input space.\nFigure 3 shows an example of applying the safe predictor to a notional regression problem with a 2-D\ninput and 1-D output and two overlapping constraints. The unconstrained network has two hidden\nlayers of dimension 20 with ReLU activations, followed by a fully connected layer. The constrained\npredictors, G00, G10, G01, and G11, share the hidden layers and have an additional hidden layer of\nsize 20 with ReLU followed by a fully connected layer. Again, training uses a sampled subset of\npoints from the input space and the learned predictors are shown for the continuous input space.\n\nC Details of VerticalCAS Experiment\n\nC.1 Safeability Constraints\n\nThe \"safeability\" property from prior work can be encoded into a set of input-output constraints. The\n\"safeable region\" for a given advisory is the set of input space locations where that advisory can be\nselected such that future advisories exist that will prevent an NMAC. If no future advisories exist, the\nadvisory is \"unsafeable\" and the corresponding input region is the \"unsafeable region\". Examples of\nthese regions, and their proximity functions are shown in Figure 5 for the CL1500 advisory.\n\nThe constraints we enforce in our safe predictor are: x \u2208 Aunsafeable,i \u21d2 Fi(x) < maxj Fj(x), \u2200i.\nTo make the output regions convex, we approximate by enforcing Fi(x) = minj Fj(x) \u2212 \u03f5, for all\nx \u2208 Aunsafeable,i.\n\nC.2 Proximity Functions\n\nWe start by generating the unsafeable region bounds. Then, a distance function is computed between\npoints in the input space (vO \u2212 vI , h, \u03c4 ), and the unsafeable region for each advisory. These are not\ntrue distances but are 0 if and only if the data point is within the unsafeable set. These are then used\nto produce proximity functions. Figure 5 shows examples of the unsafeable region, distance function,\nand proximity function for the CL1500 advisory.\n\nC.3 Structure of Predictors\n\nThe compressed policy tables for ACAS Xu and VerticalCAS use neural networks with six hidden\nlayers with a dimension of 45, and ReLU activation functions. We used the same architecture for the\nunconstrained network. For constrained predictors, we use a similar architecture, but share the first\nfour layers for all predictors. This provides a common learned representation of the input space, while\nallowing each predictor to adapt to its constraints. Each constrained predictor has two additional\nhidden layers and their outputs are projected onto our convex approximation of the safe output region,\nusing Gb(x) = minj Gj(x) \u2212 \u03f5. In our experiments, we used \u03f5 = 0.0001.\n\nWith this construction, we needed 30 separate predictors to enforce the VerticalCAS safeability\nconstraints. The number of nodes for the unconstrained and safe implementations were 270 and\n2880, respectively. Our safe predictor is smaller than the original look-up tables by several orders of\nmagnitude.\n\nC.4 Parameter Optimization\n\nWe use PyTorch for defining our networks and performing parameter optimization. We optimize both\nthe unconstrained network and our safe predictor using the asymmetric loss function, guiding the\nnetwork to select optimal advisories while accurately predicting scores from the look-up tables. Each\n\n5\n\n\fdataset is split using an 80/20 train/test split with a random seed of 0. The optimizer is ADAM, with\na learning rate of 0.0003, a batch size of 216, and training for 500 epochs.\n\n6",
  "related_work": "",
  "methodology": "",
  "experiments": "",
  "results": "",
  "conclusion": ""
}
A Chinese Span-Extraction Dataset for Machine
Reading Comprehension

Abstract

This paper introduces a novel dataset for Chinese machine reading comprehension,
focusing on span extraction. The data set is constructed using roughly 20,000 real-
world questions that are annotated by experts on passages extracted from Wikipedia.
A challenge set is also created with questions that demand a deep understanding
and inference across multiple sentences. We also show several baseline models and
anonymous submission scores to emphasize the challenges present in this dataset.
The release of this dataset facilitated the Second Evaluation Workshop on Chinese
Machine Reading Comprehension, also called CMRC 2018. We anticipate that this
dataset will further facilitate research in Chinese machine reading comprehension.

1

Introduction

The capacity to interpret and comprehend natural language is a crucial component of achieving
advanced artificial intelligence. Machine Reading Comprehension (MRC) is designed to understand
the context of given texts and respond to related questions. Numerous types of MRC datasets have
been developed, such as cloze-style reading comprehension, span-extraction reading comprehension,
open-domain reading comprehension, and multiple-choice reading comprehension. Along with the
increasing availability of reading comprehension datasets, several neural network methods have been
proposed, leading to substantial advancements in this area.

There have also been various efforts to create Chinese machine reading comprehension datasets.
In cloze-style reading comprehension, a Chinese cloze-style reading comprehension dataset was
proposed, namely People’s Daily Children’s Fairy Tale. To increase the difficulty of the dataset, they
also release a human-annotated evaluation set in addition to the automatically generated development
and test sets. Later, another dataset was introduced using children’s reading materials. To promote
diversity and explore transfer learning, they also offer a human-annotated evaluation dataset using
more natural queries compared to the cloze type. This dataset was the main component in the first
evaluation workshop on Chinese machine reading comprehension (CMRC 2017). Furthermore, a
large-scale open-domain Chinese machine reading comprehension dataset (DuReader) was created,
containing 200k queries from search engine user query logs. There is also a reading comprehension
dataset in Traditional Chinese.

While current machine learning techniques have outperformed human-level performance on datasets
like SQuAD, it is still unclear whether similar results can be achieved on datasets using different
languages. To accelerate the progress of machine reading comprehension research, we present a
span-extraction dataset tailored for Chinese.

2 The Proposed Dataset

2.1 Task Definition

The reading comprehension task can be described as a triple (P, Q, A), where P is the passage, Q
represents the question, and A is the answer. Specifically, in span-extraction reading comprehension,

questions are created by humans which is a more natural way of creating data than the cloze-style
MRC datasets. The answer A should consist of a specific span from the given passage P. The task can
be simplified by predicting the start and end indices of the answer within the passage.

2.2 Data Pre-Processing

We downloaded the Chinese portion of Wikipedia from a specified date and used an open-source
toolkit to process the raw files into plain text. Additionally, the Traditional Chinese characters were
converted to Simplified Chinese to ensure consistency using another open-source tool.

2.3 Human Annotation

The questions in this dataset were created entirely by human experts, setting it apart from prior works
that relied on automated data generation methods. Initially, documents are divided into passages,
each containing no more than 500 Chinese words. Annotators are required to assess each passage for
its suitability, discarding those that are too difficult for public understanding. Passages were discarded
based on the following rules:

• If more than 30% of the passage consists of non-Chinese characters.
• If the passage includes too many specialized or professional terms.
• If the passage has a large number of special characters or symbols.
• If the paragraph is written in classical Chinese.

After determining that the passage is suitable, annotators generate questions and their corresponding
primary answers based on the provided passage. During this question annotation, the following rules
are used.

• Each passage should have no more than five questions.
• Answers must be a span from the passage.
• Question diversity is encouraged such as questions of type who, when, where, why, and

how.

• Avoid copying descriptions from the passage directly. Use paraphrasing or syntax transfor-

mations to make answering more difficult.

• Long answers (over 30 characters) will be discarded.

For the evaluation sets, which include the development, test, and challenge sets, three answers are
available for a more thorough assessment. Besides the primary answer generated by the question
proposer, two additional annotators write a second and third answer for each question. These
additional annotators do not see the primary answer to avoid biased answers.

2.4 Challenge Set

A challenge set was made to evaluate how effectively models can perform reasoning over diverse
clues in the context, while still maintaining the span-extraction format. This annotation was also
completed by three annotators. The questions in this set need to meet the following criteria:

• The answer can not be deduced from a single sentence in the passage if the answer is a
single word or a short phrase. The annotation should encourage asking complex questions
that need an overall view of the passage to answer correctly.

• If the answer is a named entity or belongs to a particular genre, it cannot be the only instance
in the passage. There should be more than one instance to make the correct choice more
difficult for the model.

2.5 Statistics

The overall statistics of the pre-processed data are shown in Table 1. The distribution of question
types in the development set is shown in Figure 2.

2

Table 1: Statistics of the CMRC 2018 dataset.

Train

Dev

Test

Challenge

Question #
Answer # per query
Max passage tokens
Max question tokens
Max answer tokens
Avg passage tokens
Avg question tokens
Avg answer tokens

10,321
1
962
89
100
452
15
17

3,351
3
961
56
85
469
15
9

4,895
3
980
50
92
472
15
9

504
3
916
47
77
464
18
19

3 Evaluation Metrics

This paper uses two evaluation metrics. Common punctuations and white spaces are ignored for
normalization during evaluation.

3.1 Exact Match

The Exact Match (EM) score measures the exact overlap between the prediction and the ground truth
answer. If the match is exact, then the score is 1; otherwise, the score is 0.

3.2 F1-Score

The F1-score evaluates the fuzzy overlap at the character level between the prediction and the ground
truth answers. Instead of treating the answers as a bag of words, we calculate the longest common
sequence (LCS) between the prediction and the ground truth and then compute the F1-score. The
maximum F1 score among all the ground truth answers is taken for each question.

3.3 Estimated Human Performance

The estimated human performance is computed to measure the difficulty of the proposed dataset.
Each question in the development, test, and challenge set has three answers. We use a cross-validation
method to compute the performance. We treat the first answer as a human prediction and consider the
other two answers as ground truth. Using this process, three human prediction scores are generated.
Finally, we calculate the average of these three scores as the estimated human performance.

4 Experimental Results

4.1 Baseline System

We use BERT as the foundation of our baseline system. We modified the original script to accommo-
date our dataset. The initial learning rate was set to 3e-5, with a batch size of 32, and the training
was conducted for two epochs. The document and query maximum lengths were set to 512 and 64
respectively.

4.2 Results

The results are in Table 2. Besides the baseline results, we include the results of the participants
in the CMRC 2018 evaluation. The training and development sets were released to the public, and
submissions were accepted to evaluate the models on the hidden test and challenge sets. As we can
see that most of the participants achieved an F1 score above 80 in the test set. On the other hand, the
EM metric shows considerably lower scores in comparison to the SQuAD dataset, highlighting that
determining the precise span boundary is crucial for performance enhancement in Chinese reading
comprehension.

As shown in the last column of Table 2, the top-ranked systems achieve decent results on the
development and test sets but struggle to give satisfactory results on the challenge set. The estimated

3

Table 2: Baseline results and CMRC 2018 participants’ results.

Development
F1
EM

Test

EM

F1

Challenge
F1

EM

Estimated Human Performance
Z-Reader (single model)
MCA-Reader (ensemble)
RCEN (ensemble)
MCA-Reader (single model)
OmegaOne (ensemble)
RCEN (single model)
GM-Reader (ensemble)
OmegaOne (single model)
GM-Reader (single model)
R-NET (single model)
SXU-Reader (ensemble)
SXU-Reader (single model)
T-Reader (single model)
BERT-base (Chinese)
BERT-base (Multi-lingual)

91.083
79.776
66.698
76.328
63.902
66.977
73.253
58.931
64.430
56.322
45.418
40.292
37.310
39.422
63.6
64.1

97.348
92.696
85.538
91.370
82.618
84.955
89.750
80.069
82.699
77.412
69.825
66.451
66.121
62.414
83.9
84.4

92.400
74.178
71.175
68.662
68.335
66.272
64.576
64.045
64.188
60.470
50.112
46.210
44.270
44.883
67.8
68.6

97.914
88.145
88.090
85.753
85.707
82.788
83.136
83.046
81.539
80.035
73.353
70.482
70.673
66.859
86.0
86.8

90.382
13.889
15.476
15.278
13.690
12.103
10.516
15.675
10.119
13.690
9.921
N/A
6.548
7.341
18.4
18.6

95.248
37.422
37.104
34.479
33.964
30.859
30.994
37.315
29.716
33.990
29.324
N/A
28.116
22.317
42.1
43.8

human performance remains similar across the development, test, and challenge sets, indicating
that the difficulty is consistent across all three data sets. Even though Z-Reader achieved the best
performance on the test set, its EM metric performance was not consistent on the challenge set. This
highlights that current models are limited in their ability to process difficult questions that require
complex reasoning over numerous clues throughout the passage.

BERT-based methods demonstrated competitive performance compared to the submissions of par-
ticipants. Traditional models have higher scores in the test set. However, the BERT-based models
perform better on the challenge set, indicating the importance of rich representations to address
complex questions.

5 Conclusion

This paper introduces a span-extraction dataset for Chinese machine reading comprehension, con-
sisting of roughly 20,000 questions annotated by human experts, along with a challenge set which
contains questions that need reasoning over different clues in the passage. The results from the
evaluation suggest that models can achieve excellent scores on the development and test sets, close
to the human performance in F1-score. However, the scores on the challenge set decline drastically,
while human performance remains consistent. This shows there are still potential challenges in
creating models that can perform well on difficult reasoning questions. We expect that this dataset
will contribute to linguistic diversity in machine reading comprehension and facilitate additional
research on questions that require comprehensive reasoning across multiple clues.

4

